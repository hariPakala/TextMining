The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings.  Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text.  In this article, we discuss text mining as a young and in terdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.
Forecasting of scrapped products to recycling poses severe problems to recycling and remanufacturing companies due to uncertainties in timing and quantities of returns. In this paper an extended forecasting method to provide prognoses for return values (amount and time) of scrapped products to recycling is presented. The suggested model is based on relevant influencing factors and product life cycle data and has been applied to a case study (photocopiers) for evaluation and usability. The approach employs a simulation study, the design of a fuzzy inference system for the forecasting of the return in a determined planning period and the design of a neuro-fuzzy system for the forecasting of return values with respect to time.
Fuzzy systems are currently being used in a wide field of industrial and scientific applications. Since the design and especially the optimization process of fuzzy systems can be very time consuming, it is convenient to have algorithms which construct and optimize them automatically. One popular approach is to combine fuzzy systems with learning techniques derived from neural networks. Such approaches are usually called neuro-fuzzy systems. In this paper we present our view of neuro-fuzzy systems and an implementation in the area of control theory: the NEFCON-Model. This model is able to learn and optimize the rule base of a Mamdani like fuzzy controller online by a reinforcement learning algorithm that uses a fuzzy error measure. Therefore, we also describe some methods to determine a fuzzy error measure for a dynamic system. In addition we present some implementations of the model and an application example. The presented implementations are available free of charge for non-commercial purposes.
In this article we present a prototypical implementation of a software tool for document retrieval which groups/arranges (pre-processed) documents based on a similarity measure. The prototype was developed based on self-organising maps to realise interactive associative search and visual exploration of document databases. This helps a user to navigate through similar documents. The navigation, especially the search for the first appropriate document, is supported by conventional keyword search methods. The usability of the presented approach is shown by a sample search.
Naive Bayes classifiers are a well-known and powerful type of classifier that can easily be induced from a dataset of sample cases. However, the strong conditional independence and distribution assumptions underlying them can sometimes lead to poor classification performance. Another prominent type of classifier are neuro-fuzzy classification systems which derive (fuzzy) classifiers from data using neural network inspired learning methods. Since there are certain structural similarities between a neuro-fuzzy classifier and a naive Bayes classifier, the idea suggests itself to mapping the latter to the former in order to improve its capabilities.
Over 80 approaches for academic literature recommendation exist today. The approaches were introduced and evaluated in more than 170 research articles, as well as patents, presentations and blogs. We reviewed these approaches and found most evaluations to contain major shortcomings. Of the approaches proposed, 21% were not evaluated. Among the evaluated approaches, 19% were not evaluated against a baseline. Of the user studies performed, 60% had 15 or fewer participants or did not report on the number of participants. Information on runtime and coverage was rarely provided. Due to these and several other shortcomings described in this paper, we conclude that it is currently not possible to determine which recommendation approaches for academic literature are the most promising. However, there is little value in the existence of more than 80 approaches if the best performing approaches are unknown.
While eye tracking is becoming more and more relevant as a promising input channel, diverse applications using gaze control in a more natural way are still rather limited. Though several researchers have indicated the particular high potential of gaze-based interaction for pointing tasks, often gaze-only approaches are investigated. However, time-consuming dwell-time activations limit this potential. To overcome this, we present a gaze-supported fisheye lens in combination with (1) a keyboard and (2) and a tilt-sensitive mobile multi-touch device. In a user-centered design approach, we elicited how users would use the aforementioned input combinations. Based on the received feedback we designed a prototype system for the interaction with a remote display using gaze and a touch-and-tilt device. This eliminates gaze dwell-time activations and the well-known Midas Touch problem (unintentionally issuing an action via gaze). A formative user study testing our prototype provided further insights into how well the elaborated gaze-supported interaction techniques were experienced by users.
Although clustering under constraints is a current research topic, a hierarchical setting, in which a hierarchy of clusters is the goal, is usually not considered. This paper tries to fill this gap by analyzing a scenario, where constraints are derived from a hierarchy that is partially known in advance. This scenario can be found, e.g., when structuring a collection of documents according to a user specific hierarchy. Major issues of current approaches to constraint based clustering are discussed, especially towards the hierarchical setting. We introduce the concept of hierarchical constraints and continue by presenting and evaluating two approaches using them. The approaches cover the two major fields of constraint based clustering, i.e. instance and metric based constraint integration. Our objects of interest are text documents. Therefore, the presented algorithms are especially fitted to work for these where necessary. Despite showing the properties and ideas of the algorithms in general, we evaluated the case of constraints that are unevenly scattered over the instance space, which is very common for real-world problems but not satisfyingly covered in other work so far.
Current work on Query-by-Singing/Humming (QBSH) focusses mainly on databases that contain MIDI files. Here, we present an approach that works on real audio recordings  that  bring  up  additional  challenges.   To  tackle  the problem of extracting the melody of the lead vocals from recordings, we introduce a method inspired by the popular “karaoke effect” exploiting information about the spatial arrangement of voices and instruments in the stereo mix. The extracted signal time series are aggregated into symbolic strings preserving the local approximated values of a feature and revealing higher-level context patterns. This allows distance measures for string pattern matching to be applied in the matching process.  A series of experiments are conducted to assess the discrimination and robustness of this representation.  They show that the proposed approach provides a viable baseline for further development and point out several possibilities for improvement.
Because of the increasing complexity of surgical interventions research in surgical simulation became more and more important over the last years. However, the simulation of tissue deformation is still a challenging problem, mainly due to the short response times that are required for real-time interaction. The demands to hard and software are even larger if not only the modeled human anatomy is used but the anatomy of actual patients. This is required if the surgical simulator should be used as training medium for expert surgeons rather than students. In this article, suitable visualization and simulation methods for surgical simulation utilizing actual patient’s datasets are described. Therefore, the advantages and disadvantages of direct and indirect volume rendering for the visualization are discussed and a neuro-fuzzy system is described, which can be used for the simulation of interactive tissue deformations. The neuro-fuzzy system makes it possible to define the deformation behavior based on a linguistic description of the tissue characteristics or to learn the dynamics by using measured data of real tissue. Furthermore, a simulator for minimally-invasive neurosurgical interventions is presented that utilizes the described visualization and simulation methods. The structure of the simulator is described in detail and the results of a system evaluation by an experienced neurosurgeon—a quantitative comparison between different methods of virtual endoscopy as well as a comparison between real brain images and virtual endoscopies—are given. The evaluation proved that the simulator provides a higher realism of the visualization and simulation then other currently available simulators.
Foundations of Neuro-Fuzzy Systems reflects the current trend in intelligent systems research towards the integration of neural networks and fuzzy technology. The authors demonstrate how a combination of both techniques enhances the performance of control, decision-making and data analysis systems. Smarter and more applicable structures result from marrying the learning capability of the neural network with the transparency and interpretability of the rule-based fuzzy system. Foundations of Neuro-Fuzzy Systems highlights the advantages of integration making it a valuable resource for graduate students and researchers in control engineering, computer science and applied mathematics. The authors' informed analysis of practical neuro-fuzzy applications will be an asset to industrial practitioners using fuzzy technology and neural networks for control systems, data analysis and optimization tasks.
Neuro-fuzzy systems have recently gained a lot of interest in research and application. Neuro-fuzzy models as we understand them are fuzzy systems that use local learning strategies to learn fuzzy sets and fuzzy rules. Neuro-fuzzy techniques have been developed to support the development of e.g. fuzzy controllers and fuzzy classifiers. In this paper we discuss a learning method for fuzzy classification rules. The learning algorithm is a simple heuristics that is able to derive fuzzy rules from a set of training data very quickly, and tunes them by modifying parameters of membership functions. Our approach is based on NEFCLASS, a neuro-fuzzy model for pattern classification. We also discuss some results obtained by our software implementation of NEFCLASS, which is freely available on the Internet.
For many application problems classifiers can be used to support a decision making process. In some domains-in areas like medicine especially-it is preferable not to use black box approaches. The user should be able to understand the classifier and to evaluate its results. Fuzzy rule based classifiers are especially suitable, because they consist of simple linguistically interpretable rules and do not have some of the drawbacks of symbolic or crisp rule based classifiers. Classifiers must often be created from data by a learning process, because there is not enough expert knowledge to determine their parameters completely. A simple and convenient way to learn fuzzy classifiers from data is provided by neuro-fuzzy approaches. In this paper we discuss extensions to the learning algorithms of neuro-fuzzy classification (NEFCLASS), a neuro-fuzzy approach for data analysis that we have presented before. We present interactive strategies for pruning rules and variables from a trained classifier to enhance its readability, and demonstrate our approach on a small example.
We describe an implementation of the well-known apriori algorithm for the induction of association rules [Agrawal et al. (1993), Agrawal et al. (1996)] that is based on the concept of a prefix tree. While the idea to use this type of data structure is not new, there are several ways to organize the nodes of such a tree, to encode the items, and to organize the transactions, which may be used in order to minimize the time needed to find the frequent itemsets as well as to reduce the amount of memory needed to store the counters. Consequently, our emphasis is less on concepts, but on implementation issues, which, however, can make a considerable difference in applications.
Fuzzy control at the executive level can be interpreted as an approximation technique for a control function based on typical, imprecisely specified input-output tuples that are represented by fuzzy sets. The imprecision is characterized by similarity relations that are induced by transformations of the canonical distance function between real numbers. Taking this interpretation of fuzzy controllers into account, in order to derive a fuzzy controller from observed data typical input-output tuples have to be identified. In addition, a concept of similarity based on a transformations of the canonical distance is needed in order to characterize the typical input-output tuples by suitable fuzzy sets. A variety of fuzzy clustering algorithms that are exactly working in this spirit exists: They identify prototypes and assign fuzzy sets to the prototypes on the basis of a suitable transformed distance. In this paper we discuss how such fuzzy clustering techniques can be applied to construct a fuzzy controller from data and introduce special clustering algorithms that are tailored for this problem.
The aim of this paper is to introduce a fuzzy control model with well-founded semantics in order to explain the concepts applied in fuzzy control. Assuming that the domains of the input and output variables for the process are endowed with equality relations, that reflect the indistinguishability of values lying closely together, the use of triangular and trapezoidal membership functions can be justified and max-⨅ inference where ⨅ is a t-norm turns out to be a consequence of our model. Distinguishing between a functional and a relational view of the control rules it is possible to explain when defuzzification strategies like MOM or COA are appropriate or lead to undesired results.
A kind of neural network architecture designed for control tasks is presented. It is called the fuzzy neural network. The structure of the network can be interpreted in terms of a fuzzy controller. It has a three-layered architecture and uses fuzzy sets as its weights. The fuzzy error backpropagation algorithm, a special learning algorithm inspired by the standard BP-procedure for multivariable neural networks, is able to learn the fuzzy sets. The extended version that is presented is also able to learn fuzzy-if-then rules by reducing the number of nodes in the hidden layer of the network. The network does not learn from examples, but by evaluating a special fuzzy error measure.
The problem of combining pieces of information issued from several sources can be encountered in various fields of application. This paper aims at presenting the different aspects of information fusion in different domains, such as databases, regulations, preferences, sensor fusion, etc., at a quite general level. We first present different types of information encountered in fusion problems, and different aims of the fusion process. Then we focus on representation issues which are relevant when discussing fusion problems. An important issue is then addressed, the handling of conflicting information. We briefly review different domains where fusion is involved, and describe how the fusion problems are stated in each domain. Since the term fusion can have different, more or less broad, meanings, we specify later some terminology with respect to related problems, that might be included in a broad meaning of fusion. Finally we briefly discuss the difficult aspects of validation and evaluation.
We explore an approach to possibilistic fuzzy clustering that avoids a severe drawback of the conventional approach, namely that the objective function is truly minimized only if all cluster centers are identical. Our approach is based on the idea that this undesired property can be avoided if we introduce a mutual repulsion of the clusters, so that they are forced away from each other. We develop this approach for the possibilistic fuzzy c-means algorithm and the Gustafson–Kessel algorithm. In our experiments we found that in this way we can combine the partitioning property of the probabilistic fuzzy c-means algorithm with the advantages of a possibilistic approach w.r.t. the interpretation of the membership degrees.
Neuro-fuzzy systems have recently gained a lot of interest in research and applications. These are approaches that learn fuzzy systems from data. Many of them use rule weights for this task. In this paper we discuss the influence of rule weights on the interpretability of fuzzy systems. We show how rule weights can be equivalently replaced by modifications in the membership functions of a fuzzy system. We elucidate the effects rule weights have on a fuzzy rule base. Using our neuro-fuzzy model NEFCLASS we demonstrate the problems of using rule weights in a simple example, and we show that learning in fuzzy systems can be done without them.
With the increase in the size of data sets, data mining has recently become an important research topic and is receiving substantial interest from both academia and industry. At the same time, interest in temporal databases has been increasing and a growing number of both prototype and implemented systems are using an enhanced temporal understanding to explain aspects of behavior associated with the implicit time-varying nature of the universe. This paper investigates the confluence of these two areas, surveys the work to date, and explores the issues involved and the outstanding problems in temporal data mining.
Web-usage mining has become the subject of intensive research, as its potential for personalized services, adaptive Web sites and customer profiling is recognized. However, the reliability of Web-usage mining results depends heavily on the proper preparation of the input datasets. In particular, errors in the reconstruction of sessions and incomplete tracing of users’ activities in a site can easily result in invalid patterns and wrong conclusions. In this study, we evaluate the performance of heuristics employed to reconstruct sessions from the server log data. Such heuristics are called to partition activities first by user and then by visit of the user in the site, where user identification mechanisms, such as cookies, may or may not be available. We propose a set of performance measures that are sensitive to two types of reconstruction errors and appropriate for different applications in knowledge discovery (KDD) applications. We have tested our framework on the Web server data of a frame-based Web site. The first experiment concerned a specific KDD application and has shown the sensitivity of the heuristics to particularities of the site's structure and traffic. The second experiment is not bound to a specific application but rather compares the performance of the heuristics for different measures and thus for different application types. Our results show that there is no single best heuristic, but our measures help the analyst in the selection of the heuristic best suited for the application at hand.
The analysis of web usage has mostly focused on sites composed of conventional static pages. However, huge amounts of information available in the web come from databases or other data collections and are presented to the users in the form of dynamically generated pages. The query interfaces of such sites allow the specification of many search criteria. Their generated results support navigation to pages of results combining cross-linked data from many sources. For the analysis of visitor navigation behaviour in such web sites, we propose the web usage miner (WUM), which discovers navigation patterns subject to advanced statistical and structural constraints. Since our objective is the discovery of interesting navigation patterns, we do not focus on accesses to individual pages. Instead, we construct conceptual hierarchies that reflect the query capabilities used in the production of those pages. Our experiments with a real web site that integrates data from multiple databases, the German SchulWeb, demonstrate the appropriateness of WUM in discovering navigation patterns and show how those discoveries can help in assessing and improving the quality of the site.
The navigational behaviour of users in the web is essential for the providers of information, services and goods. Search engines can help a user find a provider of interest, but it is the proper organization of the provider’s site that stimulates the user’s propensity to consume. To verify whether the site is effectively organized, knowledge on the navigation patterns occuring during visits to the site must be obtained. Our Web Utilization Miner WUM can assist in obtaining this knowledge. WUM is a mining system for the discovery of navigation patterns. A navigation pattern is a directed graph that summarizes the traversal movements of a group of visitors and satisfies certain human-centric criteria that make it “interesting”. Instead of focussing the mining process on the statistically dominant but not always interesting patterns, WUM supports the specification of interestingness criteria on their structure, content and statistics. WUM provides a declarative mining language, MINT, with which the human expert can specify interestingness criteria on the fly. To discover the navigation patterns satisfying the expert’s criteria, WUM exploits an innovative aggregated storage representation for the information in the web server log.
With the growth in the size of datasets, data mining has recently become an important research topic and is receiving substantial interest from both academia and industry. At the same time, a greater recognition of the value of temporal and spatial data has been evident and the first papers looking at the confluence of these two areas are starting to emerge. This short paper provides a few comments on this research and provides a bibliography of relevant research papers investigating temporal, spatial and spatio-temporal data mining.
There is much recent work on detecA software product line is a family of related programs that are distinguished in terms of features. A feature implements a stakeholders' requirement. Different program variants specified by distinct feature selections are produced from a common code base. The optional feature problem describes a common mismatch between variability intended in the domain and dependencies in the implementation. When this situation occurs, some variants that are valid in the domain cannot be produced due to implementation issues. There are many different solutions to the optional feature problem, but they all suffer from drawbacks such as reduced variability, increased development effort, reduced efficiency, or reduced source code quality. We examine the impact of the optional feature problem in two case studies from the domain of embedded database systems, and we survey different state-of-the-art solutions and their trade-offs. Our intension is to raise awareness of the problem, to guide developers in selecting an appropriate solution for their product line, and to identify opportunities for future research.ting and tracking change in clusters, often based on the study of the spatiotemporal properties of a cluster. For the many applications where cluster change is relevant, among them customer relationship management, fraud detection and marketing, it is also necessary to provide insights about the nature of cluster change: Is a cluster corresponding to a group of customers simply disappearing or are its members migrating to other clusters? Is a new emerging cluster reflecting a new target group of customers or does it rather consist of existing customers whose preferences shift? To answer such questions, we propose the framework MONIC for modeling and tracking of cluster transitions. Our cluster transition model encompasses changes that involve more than one cluster, thus allowing for insights on cluster change in the A software product line is a family of related programs that are distinguished in terms of features. A feature implements a stakeholders' requirement. Different program variants specified by distinct feature selections are produced from a common code base. The optional feature problem describes a common mismatch between variability intended in the domain and dependencies in the implementation. When this situation occurs, some variants that are valid in the domain cannot be produced due to implementation issues. There are many different solutions to the optional feature problem, but they all suffer from drawbacks such as reduced variability, increased development effort, reduced efficiency, or reduced source code quality. We examine the impact of the optional feature problem in two case studies from the domain of embedded database systems, and we survey different state-of-the-art solutions and their trade-offs. Our intension is to raise awareness of the problem, to guide developers in selecting an appropriate solution for their product line, and to identify opportunities for future research.whole clustering. Our transition tracking mechanism is not based on the topological properties of clusters, which are only available for some types of clustering, but on the contents of the underlying data stream. We present our first results on monitoring cluster transitions over the ACM digital library.
For many companies, effective web presence is indispensable for their success to the global market. In recent yeSmoke rendering is a standard technique for flow visualization. Most approaches are based on a volumetric, particle based, or image based representation of the smoke. This paper introduces an alternative representation of smoke structures: as semi-transparent streak surfaces. In order to make streak surface integration fast enough for interactive applications, we avoid expensive adaptive retriangulations by coupling the opacity of the triangles to their shapes. This way, the surface shows a smoke-like look even in rather turbulent areas. Furthermore, we show modifications of the approach to mimic smoke nozzles, wool tufts, and time surfaces. The technique is applied to a number of test data sets.ars, several methods have been developed for measuring and improving the effectiveness of commercial sites. However, they mostly concentrate on web page design and on access analysis. A software product line is a family of related programs that are distinguished in terms of features. A feature implements a stakeholders' requirement. Different program variants specified by distinct feature selections are produced from a common code base. The optional feature problem describes a common mismatch between variability intended in the domain and dependencies in the implementation. When this situation occurs, some variants that are valid in the domain cannot be produced due to implementation issues. There are many different solutions to the optional feature problem, but they all suffer from drawbacks such as reduced variability, increased development effort, reduced efficiency, or reduced source code quality. We examine the impact of the optional feature problem in two case studies from the domain of embedded database systems, and we survey different state-of-the-art solutions and their trade-offs. Our intension is to raise awareness of the problem, to guide developers in selecting an appropriate solution for their product line, and to identify opportunities for future research.In this study, we propose a methodology of assessing the quality of a web site in turning its users into customers. Our methodology is based on the discovery and comparison of navigation patterns of customers and non-customers. This comparison leads to rules on how the site’s topology should be improved. We further propose a technique for dynamically adapting the site according to those rules.
Data mining and knowledge discovery have become important issues for research over the past decade. This has been caused not only by the growth in the size of datasets but also in the availability of otherwise unavailable datasets over the Internet and the increased value that organisations now place on the knowledge that can be gained from data analysis. It is therefore not surprising that the increased interest in temporal and spatial data has led also to an increased interest in mining such data. This bibliography subsumes an earlier bibliography and shows that the value of investigating temporal, spatial and spatio-temporal data has been growing in both interest and applicability.
The analysis of user behavior on the Web presupposes a reliable reconstruction of the users’ navigational activities. Cookies and server-generated session identifiers have been designed to allow an accurate session reconstruction. However, in the absence of reliable methods, analysts must employ heuristics (a) to identify unique visitors to a site, and (b) to distinguish among the activities of such users during independent sessions. The characteristics of the site, such as the site structure, as well as the methods used for data collection (e.g., the existence of cookies and reliable synchronization across multiple servers) may necessitate the use of different types of heuristics. In this study, we extend our work on the reliability of sessionizing mechanisms, by investigating the impact of site structure on the quality of constructed sessions. Specifically, we juxtapose sessionizing on a frame-based and a frame-free version of a site. We investigate the behavior of cookies, server-generated session identification, and heuristics that exploit session duration, page stay time and page linkage. Different measures of session reconstruction quality, as well as experiments on the impact on the prediction of frequent entry and exit pages, show that different reconstruction heuristics can be recommended depending on the characteristics of the site. We also present first results on the impact of session reconstruction heuristics on predictive applications such as Web personalization.
A social network consists of people who interact in some way such as members of online communities sharing information via the WWW. To learn more about how to facilitate community building e.g. in organizations, it is important to analyze the interaction behavior of their members over time. So far, many tools have been provided that allow for the analysis of static networks and some for the temporal analysis of networks - however only on the vertex and edge level. In this paper we propose two approaches to analyze the evolution of two different types of online communities on the level of subgroups: The first method consists of statistical analyses and visualizations that allow for an interactive analysis of subgroup evolutions in communities that exhibit a rather membership structure. The second method is designed for the detection of communities in an environment with highly fluctuating members. For both methods, we discuss results of experiments with real data from an online student community.
One of the reasons that topological methods have a limited popularity for the visualization of complex 3D flow fields is the fact that such topological structures contain a number of separating stream surfaces. Since these stream surfaces tend to hide each other as well as other topological features, for complex 3D topologies the visualizations become cluttered and hardly interpretable. This paper proposes to use particular stream lines called saddle connectors instead of separating stream surfaces and to depict single surfaces only on user demand. We discuss properties and computational issues of saddle connectors and apply these methods to complex flow data. We show that the use of saddle connectors makes topological skeletons available as a valuable visualization tool even for topologically complex 3D flow data.
We present an approach to define shape deformations by constructing and interactively modifying C1 continuous time-dependent divergence-free vector fields. The deformation is obtained by a path line integration of the mesh vertices. This way, the deformation is volume-preserving, free of (local and global) self-intersections, feature preserving, smoothness preserving, and local. Different modeling metaphors support the approach which is able to modify the vector field on-the-fly according to the user input. The approach works at interactive frame rates for moderate mesh sizes, and the numerical integration preserves the volume with a high accuracy.
Visual exploration of multivariate data typically requires projection onto lower-dimensional representations. The number of possible representations grows rapidly with the number of dimensions, and manual exploration quickly becomes ineffective or even unfeasible. This paper proposes automatic analysis methods to extract potentially relevant visual structures from a set of candidate visualizations. Based on features, the visualizations are ranked in accordance with a specified user task. The user is provided with a manageable number of potentially useful candidate visualizations, which can be used as a starting point for interactive data analysis. This can effectively ease the task of finding truly useful visualizations and potentially speed up the data exploration task. In this paper, we present ranking measures for class-based as well as non class-based Scatterplots and Parallel Coordinates visualizations. The proposed analysis methods are evaluated on different datasets.
We present an approach to visualizing correlations in 3D multifield scalar data. The core of our approach is the computation of correlation fields, which are scalar fields containing the local correlations of subsets of the multiple fields. While the visualization of the correlation fields can be done using standard 3D volume visualization techniques, their huge number makes selection and handling a challenge. We introduce the multifield-graph to give an overview of which multiple fields correlate and to show the strength of their correlation. This information guides the selection of informative correlation fields for visualization. We use our approach to visually analyze a number of real and synthetic multifield datasets
Smoke rendering is a standard technique for flow visualization. Most approaches are based on a volumetric, particle based, or image based representation of the smoke. This paper introduces an alternative representation of smoke structures: as semi-transparent streak surfaces. In order to make streak surface integration fast enough for interactive applications, we avoid expensive adaptive retriangulations by coupling the opacity of the triangles to their shapes. This way, the surface shows a smoke-like look even in rather turbulent areas. Furthermore, we show modifications of the approach to mimic smoke nozzles, wool tufts, and time surfaces. The technique is applied to a number of test data sets.
This paper describes approaches to topologically segmenting 2D time-dependent vector fields. For this class of vector fields, two important classes of lines exist: stream lines and path lines. Because of this, two segmentations are possible: either concerning the behavior of stream lines or of path lines. While topological features based on stream lines are well established, we introduce path line oriented topology as a new visualization approach in this paper. As a contribution to stream line oriented topology, we introduce new methods to detect global bifurcations like saddle connections and cyclic fold bifurcations as well as a method of tracking all isolated closed stream lines. To get the path line oriented topology, we segment the vector field into areas of attracting, repelling, and saddle-like behavior of the path lines. We compare both kinds of topologies and apply them to a number of test data sets.
In nature and in flow experiments particles form patterns of swirling motion in certain locations. Existing approaches identify these structures by considering the behavior of stream lines. However, in unsteady flows particle motion is described by path lines which generally gives different swirling patterns than stream lines. We introduce a novel mathematical characterization of swirling motion cores in unsteady flows by generalizing the approach of Sujudi/Haimes to path lines. The cores of swirling particle motion are lines sweeping over time, i.e., surfaces in the space-time domain. They occur at locations where three derived 4D vectors become coplanar. To extract them, we show how to re-formulate the problem using the parallel vectors operator. We apply our method to a number of unsteady flow fields.
We introduce a new technique for estimating the curvature tensor of a triangular mesh. The input of the algorithm is only a single triangle equipped with its (exact or estimated) vertex normals. This way we get a smooth junction of the curvature tensor inside each triangle of the mesh. We show that the error of the new method is comparable with the error of a cubic fitting approach if the incorporated normals are estimated. If the exact normals of the underlying surface are available at the vertices, the error drops significantly. We demonstrate the applicability of the new estimation at a rather complex data set.
This paper presents an approach to extracting and classifying higher order critical points of 3D vector fields. To do so, we place a closed convex surface s around the area of interest. Then we show that the complete 3D classification of a critical point into areas of different flow behavior is equivalent to extracting the topological skeleton of an appropriate 2D vector field on s, if each critical point is equipped with an additional bit of information. Out of this skeleton, we create an icon which replaces the complete topological structure inside s for the visualization. We apply our method to find a simplified visual representation of clusters of critical points, leading to expressive visualizations of topologically complex 3D vector fields.
We introduce an approach to visualize stationary 2D vector fields with global uncertainty obtained by considering the transport of local uncertainty in the flow. For this, we extend the concept of vector field topology to uncertain vector fields by considering the vector field as a density distribution function. By generalizing the concepts of stream lines and critical points we obtain a number of density fields representing an uncertain topological segmentation. Their visualization as height surfaces gives insight into both the flow behavior and its uncertainty. We present a Monte Carlo approach where we integrate probabilistic particle paths, which lead to the segmentation of topological features. Moreover, we extend our algorithms to detect saddle points and present efficient implementations. Finally, we apply our technique to a number of real and synthetic test data sets.
In this paper, we motivate and present a data model for conceptual design of structural and behavioural aspects of databases. We follow an object centered design paradigm in the spirit of semantic data models. The specification of structural aspects is divided into modelling of object structures and modelling of data types used for describing object properties. The specification of object structures is based on an Extended Entity-Relationship (EER) model. The specification of behavioural aspects is divided into the modelling of admissible database state evolutions by means of temporal integrity constraints and the formulation of database (trans)actions. The central link for integrating these design components is a descriptive logic-based query language for the EER model. The logic part of this language is the basis for static constraints and descriptive action specifications by means of pre- and postconditions. A temporal extension of this logic is the specification language for temporal integrity constraints. We emphasize that the various aspects of a database application are specified using several appropriate, but yet compatible formalisms, which are integrated by a unifying common semantic.
This paper presents FeatureC++, a novel language extension to C++ that supports Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP). Besides well-known concepts of FOP languages, FeatureC++ contributes several novel FOP language features, in particular multiple inheritance and templates for generic programming. Furthermore, FeatureC++ solves several problems regarding incremental software development by adopting AOP concepts. Starting our considerations on solving these problems, we give a summary of drawbacks and weaknesses of current FOP languages in expressing incremental refinements. Specifically, we outline five key problems and present three approaches to solve them: Multi Mixins, Aspectual Mixin Layers, and Aspectual Mixins that adopt AOP concepts in different ways. We use FeatureC++ as a representative FOP language to explain these three approaches. Finally, we present a case study to clarify the benefits of FeatureC++ and its AOP extensions.
Two programming paradigms are gaining attention in the overlapping fields of software product lines (SPLs) and incremental software development (ISD). Feature-oriented programming (FOP) aims at large-scale compositional programming and feature modularity in SPLs using ISD. Aspect-oriented programming (AOP) focuses on the modularization of crosscutting concerns in complex software. Although feature modules, the main abstraction mechanisms of FOP, perform well in implementing large-scale software building blocks, they are incapable of modularizing certain kinds of crosscutting concerns. This weakness is exactly the strength of aspects, the main abstraction mechanisms of AOP. We contribute a systematic evaluation and comparison of FOP and AOP. It reveals that aspects and feature modules are complementary techniques. Consequently, we propose the symbiosis of FOP and AOP and aspectual feature modules (AFMs), a programming technique that integrates feature modules and aspects. We provide a set of tools that support implementing AFMs on top of Java and C++. We apply AFMs to a nontrivial case study demonstrating their practical applicability and to justify our design choices.
FeatureIDE is an open-source framework for feature-oriented software development (FOSD) based on Eclipse. FOSD is a paradigm for the construction, customization, and synthesis of software systems. Code artifacts are mapped to features, and a customized software system can be generated given a selection of features. The set of software systems that can be generated is called a software product line (SPL). FeatureIDE supports several FOSD implementation techniques such as feature-oriented programming, aspect-oriented programming, delta-oriented programming, and preprocessors. All phases of FOSD are supported in FeatureIDE, namely domain analysis, requirements analysis, domain implementation, and software generation.
Feature-Oriented Programming (FOP) decomposes complex software into features. Features are main abstractions in design and implementation. They reflect user requirements and incrementally refine one another. Although, features crosscut object-oriented architectures they fail to express all kinds of crosscutting concerns. This weakness is exactly the strength of aspects, the main abstraction mechanism of Aspect-Oriented Programming (AOP). In this article we contribute a systematic evaluation and comparison of both paradigms, AOP and FOP, with focus on incremental software development. It reveals that aspects and features are not competing concepts. In fact AOP has several strengths to improve FOP in order to implement crosscutting featuresSymmetrically, the development model of FOP can aid AOP in implementing incremental designs. Consequently, we propose the architectural integration of aspects and features in order to profit from both paradigms. We introduce aspectual mixin layers (AMLs), an implementation approach that realizes this symbiosis. A subsequent evaluation and a case study reveal that AMLs improve the crosscutting modularity of features as well as aspects become well integrated into incremental development style.
TROLL is a language particularly suited for the early stages of information system development, when the universe of discourse must be described. In TROLL the descriptions of the static and dynamic aspects of entities are integrated into object descriptions. Sublanguages for data terms, for first-order and temporal assertions, and for processes, are used to describe respectively the static properties, the behavior, and the evolution over time of objects. TROLL organizes system design through object-orientation and the support of abstractions such as classification, specialization, roles, and aggregation. Language features for state interactions and dependencies among components support the composition of the system from smaller modules, as does the facility of defining interfaces on top of object descriptions.
Tools support is crucial for the acceptance of a new programming language. However, providing such tool support is a huge investment that can usually not be provided for a research language. With FeatureIDE, we have built an IDE for AHEAD that integrates all phases of feature-oriented software development. To reuse this investment for other tools and languages, we refactored FeatureIDE into an open source framework that encapsulates the common ideas of feature-oriented software development and that can be reused and extended beyond AHEAD. Among others, we implemented extensions for FeatureC++ and FeatureHouse, but in general, FeatureIDE is open for everybody to showcase new research results and make them usable to a wide audience of students, researchers, and practitioners.
Dynamic integrity constraints are used to specify admissible sequences of database states. We present algorithmic fundamentals of monitoring constraints expressed in temporal logic. The essential means are finite transition graphs which can be constructed from temporal formulae by utilizing an appropriate normalform. To ensure admissibility of a state sequence, the integrity monitor has to follow a corresponding path through the graph and to check certain nontemporal conditions in each state; these conditions are provided as edge labels. Thus, monitoring dynamic integrity is reduced to a controlled checking of static integrity. All errors in present database behaviour are detected as well as many inevitable future errors.
Software product line engineering is an efficient means of generating a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test them all and ensure properties like type safety for the entire product line. We present a product-line-aware type system that can type check an entire software product line without generating each variant in isolation. Specifically, we extend the Featherweight Java calculus with feature annotations for product-line development and prove formally that all program variants generated from a well typed product line are well typed. Furthermore, we present a solution to the problem of typing mutually exclusive features. We discuss how results from our formalization helped implement our own product-line tool CIDE for full Java and report of our experience with detecting type errors in four existing software product line implementations.
A software product line is a family of related programs that are distinguished in terms of features. A feature implements a stakeholders' requirement. Different program variants specified by distinct feature selections are produced from a common code base. The optional feature problem describes a common mismatch between variability intended in the domain and dependencies in the implementation. When this situation occurs, some variants that are valid in the domain cannot be produced due to implementation issues. There are many different solutions to the optional feature problem, but they all suffer from drawbacks such as reduced variability, increased development effort, reduced efficiency, or reduced source code quality. We examine the impact of the optional feature problem in two case studies from the domain of embedded database systems, and we survey different state-of-the-art solutions and their trade-offs. Our intension is to raise awareness of the problem, to guide developers in selecting an appropriate solution for their product line, and to identify opportunities for future research.
Medical students were asked in a questionnaire to evaluate the importance of all their courses and lectures with respect to their relevance to training to become a doctor. This survey was carried out at the end of the undergraduate curriculum. As the response rate was over 90% (n = 323) the data are undoubtedly representative. There was wide variation in how relevant the students graded the preclinical and clinical courses to be. Some interesting aspects were the differences found between female and male students as well as the correlation with the subject the students planned to specialize in. About 50% of the students had spent at least one month of their clinical clerkships abroad, 20% spent at least four months in the final intensive year in another county and 6% a whole academic year abroad. Retrospective surveys provide important additional information to surveys held at the end of the individual courses.
Dendritic cells (DC) as key mediators of tolerance and immunity perform crucial immunosurveillance functions at epithelial surfaces. In order to induce an immune response, the DC have to gain access to antigens present at the luminal surface of mucosal epithelia. The mechanisms of this process are still largely unclear. We have therefore analysed the distribution of DC in the porcine intestinal and respiratory mucosa and their spatial relationship to epithelial cells by immunohistology. Immunofluorescence analysis of cryosections taken from jejunal Peyer’s patches and double-stained for DC and M cells (specialised for antigen uptake) have revealed that 35.2±3.9% of M cells are located directly adjacent to DC in the subepithelial domes, representing possible antigen transfer sites. In normal jejunal villi, a rare population of lamina propria DC extending cytoplasmic processes between enterocytes has been identified as a possible correlate for direct luminal antigen uptake. Like small intestinal DC, DC in the porcine trachea mostly co-express CD16 with MHC-II. Tracheal DC have been found at high densities both above and below the basement membrane (BM) of the tracheal epithelium, with 32.4 DC/mm BM and 23.0 DC/mm BM, respectively. The intraepithealial DC population forms a dense network, with many of the cytoplasmic processes being directed towards the tracheal lumen. Our morphological analyses indicate that DC at mucosal epithelial sites are ideally positioned for the uptake of luminal antigens.
Mammalian tissues show significant differences in the activity of sulfite oxidase (EC 1.8.3.1) which detoxifies sulfite by oxidation to sulfate. Lung tissue and phagocytic cells such as alveolar macrophages, peritoneal macrophages, Kupffer cells and granulocytes show very low activities of sulfite oxidase. Liver tissue and hepatocytes, however, exhibit high activities of sulfite oxidase. Lung tissue and macrophages show an almost 100% decrease of the intracellular ATP levels when incubated with 1 mM sulfite at pH 6 for 30 min. In addition, the O2 consumption of lung tissue is inhibited by 1 mM sulfite at pH 6 by more than 80%. This sulfite-induced decrease of the ATP level and of the O2 consumption of lung tissue is enhanced between pH 6.0 and pH 7.4 with decreasing pH value of the incubation medium. In contrast, the ATP levels in liver tissue and hepatocytes are not affected by 1 mM sulfite at pH 6. The O2 consumption of liver tissue and hepatocytes is significantly increased by sulfite due to the high activities of sulfite oxidase. Therefore, the activity of the ‘sulfite-detoxifying enzyme’ sulfite oxidase and the sensitivity of the energy metabolism to sulfite show a reciprocal relationship in the tissues and cells studied.
The trichothecene mycotoxin deoxynivalenol (DON) causes systemic immuno-suppression in pigs and possibly also in humans after chronic dietary exposure. Since the outcome of every immune response is largely controlled by dendritic cells (DC), we hypothesised that a direct influence of DON on DC function might play a role in mediating DON immunotoxicity. To test this hypothesis, a 2×2 factorial design study was performed. Pigs were fed a control diet or a diet containing DON (DON-diet); monocyte-derived DC (MoDC) from these pigs were then treated with DON in vitro or left untreated. Phenotype and function of the MoDC were analysed. In vitro DON-treatment of MoDC from pigs fed the control diet resulted in a down-regulation of CD80/86 and CD40. This was associated with an activation of the mitogen-associated protein kinases ERK1/2 and JNK. The endocytic activity of MoDC was decreased after in vitro DON-exposure while their T cell stimulatory capacity was not altered. MoDC derived from pigs that had been fed the DON-diet failed to up-regulate MHC-II in response to LPS/TNFα. Dietary exposure of pigs to DON inhibited endocytosis of FITC-dextran by MoDC, but did not influence T cell stimulatory capacity. ERK1/2 and JNK were constitutively activated in MoDC from pigs fed the DON-diet. If MoDC derived from pigs fed the DON-diet were exposed to DON in vitro, this resulted in an up-regulation of MHC-II and CD80/86, but not CD40. In comparison to untreated MoDC from pigs fed DON-diet, endocytic capacity was further down-regulated, whereas mitogen-activated protein kinase activation was increased. In summary, DON disrupts porcine DC function in vitro and in vivo, which might contribute to the immunosuppressive effects of this mycotoxin.
Cholera toxin (Ctx) is a powerful mucosal adjuvant with potential applications for oral vaccination of swine. Dendritic cells (DC) play a key role in the decision between immunity and tolerance, and are likely target cells for mediating Ctx functions in vivo. Therefore, we examined the capacity of Ctx to enhance stimulatory activity of porcine monocyte-derived DC (MoDC). Ctx promoted the development of a semi-mature DC phenotype, with decreased levels of MHC class II and CD40, but increased CD80/86 expression. These changes were associated with activation of extracellular signal-regulated kinase (ERK), but not NF${\kappa}$B or c-Jun N-terminal kinase (JNK). Functionally, Ctx-priming greatly diminished T cell stimulatory capacity both in antigen-specific and superantigen-induced proliferation assays. The lower proliferation rate was not due to increased apoptosis of either DC or T cells. Ctx suppressed TNF${\alpha} $ secretion by MoDC, but induced IL-10 production. The observed effects on T cell proliferation could only be partially mimicked by IL-10 alone. However, addition of recombinant TNF${\alpha} $ to co-cultures of Ctx-primed MoDC and lymphocytes restored lymphocyte proliferation in a concentration-dependent manner. Ctx-primed DC were not actively tolerogenic, since they could not suppress proliferative T cell reactions induced by untreated DC. 
The nasal mucosa surface is continuously confronted with a broad variety of environmental antigens, ranging from harmless agents to potentially harmful pathogens. This area is under rigorous control of professional antigen presenting cells (APCs), such as dendritic cells (DCs) and macrophages. Mucosal APCs play a crucial role in inducing primary immune responses and the establishment of an immunological memory. In the present study, a detailed characterization of CD172a+ cells, containing the APCs residing in the equine nasal mucosa was performed for the first time. CD172a+ cells were isolated from collagenase-treated equine nasal mucosa fragments by MACS. Expression of surface markers was determined by flow cytometry and functional analysis was done by measuring the uptake of FITC conjugated ovalbumin (FITC-OVA). Cell surface phenotype of the isolated cells was as follows: 90% CD172a+, 30% CD1c+, 46% CD83+, 42% CD206+ and 28% MHC II+. This clearly differs from the phenotype of blood-derived monocytes: 96% CD172a+, 4% CD1c+, 11% CD83+, 9% CD206+, 72% MHC II+ and blood monocyte derived DCs: 99% CD172a+, 13% CD1c+, 30% CD83+, 51% CD206+ and 93% MHC II+. The CD172a+ nasal mucosal cells were functionally able to endocytose FITC-OVA but to a lesser degree than monocyte-derived DCs. Together, these results demonstrate that the isolated CD172a+ nasal mucosal cells resemble immature DCs in the nasal area.
Cholera toxin (Ctx) is an important mucosal adjuvant with potential experimental applications in pigs. However, little is known about the direct effects of Ctx on porcine immune cells. Therefore, we analysed the influence of Ctx on mitogen-induced lymphocyte proliferation. Ctx inhibited peripheral blood mononuclear cell (PBMC) proliferation with an IC50 of 34±17 ng/mL. This inhibition was not due to increased cell death. Lymphoblast formation in cultures stimulated with concanavalin A and Ctx was decreased at 24 h, but had reached the levels of control cultures again at 72 and 120 h, indicating that suppression was transient. Analysis of T cell subsets revealed that Ctx treatment specifically reduced the percentage of CD4−CD8+ and γδ T cells, whereas the proportion of CD4+CD8− increased. Furthermore, Ctx caused secretion of IL-10 by PBMC cultures, but depressed TNFα secretion.
Almitrine, a peripheral chemoreceptor stimulating drug, was given 100 mg orally to six patients with advanced chronic obstructive pulmonary disease (COPD), and its effects on hemodynamics, blood gases, lung mechanics, and the distribution of ventilation/perfusion ratios (VA/Q), determined by the inert gas elimination technique, were investigated. Arterial Po2 increased from 52 +/- 4 to 59 +/- 3 mm Hg, mean +/- SEM, p less than 0.01, arterial Pco2 decreased from 46 +/- 3 to 43 +/- 3 mm Hg, p less than 0.05, and venous admixture from 30 +/- 6 to 19 +/- 3 percent, p less than 0.02. No change occurred in ventilation, variables of lung mechanics, systemic and pulmonary hemodynamics, except an increase in pulmonary vascular resistance (from 364 +/- 103 to 438 +/- 99 dyne.s.cm-5, p less than 0.05). A reduction in VA/Q inequality could be demonstrated with a redistribution of blood flow into the lungs by a diversion of 15 percent of total blood flow from units with low VA/Q (between 0.08 and 0.4) to units with normal VA/Q (between 0.5 and 1.8). These changes might be explained by an enhancement of hypoxic pulmonary vasoconstriction. Pharmacologic peripheral chemoreceptor stimulation, at an infra-ventilatory analeptic dosage, might be of therapeutic interest to patients with respiratory insufficiency due to VA/Q inequality.
Clinical, radiological, and pathologic data for nine children with adult respiratory distress syndrome (ARDS) were reviewed. The children ranged in age from 7 months to 15 years (mean age, 7.4 yrs). Underlying diseases and precipitating events included sepsis, pneumonia, near drowning, aspiration pneumonia, central nervous system trauma, and malignancy. All patients had the rapid onset of diffuse bilateral lung opacification, required assisted ventilation for periods of 5-86 days (mean, 25.2 days), and received high levels of inspired oxygen for 2-41 days (mean, 12.7 days). Eight patients manifested air leak complications; these problems persisted until the patients died or were weaned from the respirator. Five of the nine patients died. Autopsy in three patients demonstrated alveolar duct fibrosis characteristic of the late proliferative phase of ARDS and consistent with oxygen toxicity. Two survivors demonstrated mild restrictive changes on follow-up pulmonary function tests and showed persistent linear densities on chest radiographs.
Transmission of bluetongue virus (BTV) by a vector species of Culicoides was studied using immunohistochemistry, virus titration and in vitro transmission tests. Adult female C. variipennis were used from two colonies that are either “transmission competent” or “transmission refractory” after oral infection with BTV. Intrathoracic (IT) injection of BTV into the haemocoel always resulted in a fully disseminated infection and transmission of virus in saliva. However, after ingestion of an infectious blood meal, only 30% (approximately) of midges from either colony became persistently infected. Although none of the orally infected insects from the “refractory” colony were able to transmit virus, 12% of those from the “competent” colony (containing ≥ 103.0TCID50 of virus/midge) did transmit BTV in their saliva. The most important barriers to BTV transmission in Culicoides vector species appeared to be a mesenteron infection barrier (MIB), which controls initial establishment of persistent infection, a mesenteron escape barrier (MEB) which can restrict virus to gut cells and a dissemination barrier (DB) which can prevent virus which enters the haemocoel from infecting secondary target organs.Culicoides variipennis do not appear to present either a salivary gland infection barrier (SGIB), or a salivary gland escape barrier (SGEB) to BTV.
One of the research direction of Horst Lippmann during his whole scientific career was devoted to the possibilities to explain complex material behavior by generalized continua models. A representative of such models is the Cosserat continuum. The basic idea of this model is the independence of translations and rotations (and by analogy, the independence of forces and moments). With the help of this model some additional effects in solid and fluid mechanics can be explained in a more satisfying manner. They are established in experiments, but not presented by the classical equations. In this paper the Cosserat-type theories of plates and shells are debated as a special application of the Cosserat theory.
At present the shell theory find out new branches of applications. Biological membranes, thin polymeric films and thin structures made from shape memory materials may be pointed out as examples . In addition, the manufacturing technology of shells leads to significant changes of the material properties. As a result the conventional variants of the shell theory, based on the derivation of the basic equations from the 3D-theory of elasticity, cannot be used. The effective elastic moduli of the shell must be found directly for the shell structure. That means that we have to use the direct method for the formulation of the shell theory. The main idea of the direct approach is the introduction of a 2D-continuum with some physical properties. The basic laws of mechanics and thermodynamics are applied directly to this 2D-continuum. The main advantage of the direct approach is the possibility to obtain quite strict equations.
Below we discuss the derivation of the governing nonlinear shell equations taking into account the surface stresses. The surface effects are significant for the modeling of some structures as nanofilms, nanoporous materials and other nano-size structures. In particular, the surface stresses are responsible for the size effect, i.e. dependence of the material properties on the specimen size. The theory of elasticity with surface stresses is applied to the modeling of shells with nano-scaled thickness. It will be shown that the resultant stress and couple stress tensors can be represented as a sum of two terms. The first term in the sum depends on the stress distribution in the bulk material while the second one relates to the surface stresses. Hence, the resultant stress and couple stress tensors are linear functions with respect to the surface stresses. As an example the effective stiffness properties of a linear elastic Cosserat shells taking into account the surface stresses are presented.
Two-dimensional shear-deformable laminated plate theories can be classified as equivalent single-layer theories and layerwise theories. Layerwise theories lead to better approximations than equivalent single-layer theories, but the large number of independent unknowns in these theories requires more computational power in comparison with calculations, based on equivalent single-layer theories. The quality of any equivalent single-layer theory based calculation is influenced by the correct determination of the effective stiffnesses. Many theories result in identical stiffnesses for bending, tension/compression, in-plane shear and torsion. The differences between the approaches are connected with the transverse shear stiffnesses. The method of determination of the transverse shear stiffnesses proposed here leads to expressions which depend on the solution of a Sturm–Liouville-problem. For special cases, the stiffnesses are calculated and compared with results from other authors. It can be shown that the present approach can be useful not only in the case of laminated plates, but also for sandwich plates.
In this paper we employ the direct approach to the theory of rods and beams, which is based on the deformable curve model with a triad of rotating directors attached to each point. We show that this model (also called directed curve) is an efficient approach for analyzing the deformation of elastic beams with a complex material structure. Thus, we consider non-homogeneous, composite and functionally graded beams made of isotropic or orthotropic materials and we determine the effective stiffness properties in terms of the three-dimensional elasticity constants. We present general analytical expressions of the effective stiffness coefficients, valid for beams of arbitrary cross-section shape. Finally, we apply this method for FGM beams made of metal foams and compare our analytical results with the numerical results obtained by a finite element analysis.
The theory of simple shells is a surface-related Cosserat model for thin elastic shells. In this direct approach, each material point is connected with a triad of rigidly rotating directors. This paper presents a study of the governing equations for orthotropic elastic simple shells in the framework of the linearized theory. We establish the uniqueness of classical solutions, without any restrictive assumption on the strain energy function. The continuous dependence of solutions on the body loads and initial data is proved. Also, the existence of weak solutions to the equations of simple shells is proved by means of an inequality of Korn's type established for such directed surfaces.
In this paper we present a mathematical study of the equations of motion for orthotropic thermoelastic simple shells. We use a direct approach to the mechanics of thin shells, in which the shell-like body is modeled as a deformable surface endowed with a triad of orthonormal vectors connected with each material point. The thermal effects are described by introducing two temperature fields which represent the temperature on the two major faces of the three-dimensional shell. In the framework of the linear theory, we establish first the uniqueness of solution to the associated boundary–initial–value problem. Then we prove the properties of reciprocity, we give a variational characterization, and we investigate the continuous dependence of solutions on the external data. Finally, we present an existence result for the weak solutions to the equations of motion of thermoelastic simple shells.
Acceleration waves in nonlinear thermoelastic micropolar media are considered. We establish the kinematic and dynamic compatibility relations for a singular surface of order 2 in the media. An analogy to the Fresnel–Hadamard–Duhem theorem and an expression for the acoustic tensor are derived. The condition for acceleration wave’s propagation is formulated as an algebraic spectral problem. It is shown that the condition coincides with the strong ellipticity of equilibrium equations. As an example, a quadratic form for the specific free energy is considered and the solutions of the corresponding spectral problem are presented.
We construct equations of equilibrium and constitutive relations of linear theory of plates and shells with transverse shear strain taken into account, which are based on reducing the spatial elasticity relations with surface stresses taken into account to two-dimensional equations given on the shell median surface. We analyze the influence of surface elasticity moduli on the effective stiffness of plates and shells.
The mathematical investigation of the initial-boundary and boundary value problems in the linear elasticity considering surface stresses is presented. Weak setup of the problems based on mechanical variational principles is studied. Theorems of uniqueness and existence of the weak solution in energy spaces of static and dynamic problems are formulated and proved. Some properties of the spectrum of the problems under consideration are established. The studies are performed applying the functional analysis techniques. Finally, the Rayleigh principle for eigenfrequencies is constructed.
The purpose of this paper is twofold. In the first part, we give a review on the current state of nonlinear model predictive control (NMPC). After a brief presentation of the basic principle of predictive control we outline some of the theoretical, computational, and implemen-tational aspects of this control strategy. Most of the theoretical developments in the area of NMPC are based on the assumption that the full state is available for measurement, an assumption that does not hold in the typical practical case. Thus, in the second part of this paper we focus on the output feedback problem in NMPC. After a brief overview on existing output feedback NMPC approaches we derive conditions that guarantee stability of the closed-loop if an NMPC state feedback controller is used together with a full state observer for the recovery of the system state.
While linear model predictive control is popular since the 70s of the past century, the 90s have witnessed a steadily increasing attention from control theoretists as well as control practitioners in the area of nonlinear model predictive control (NMPC). The practical interest is driven by the fact that today’s processes need to be operated under tighter performance specifications. At the same time more and more constraints, stemming for example from environmental and safety considerations, need to be satisfied. Often these demands can only be met when process nonlinearities and constraints are explicitly considered in the controller. Nonlinear predictive control, the extension of well established linear predictive control to the nonlinear world, appears to be a well suited approach for this kind of problems. In this note the basic principle of NMPC is reviewed, the key advantages/disadvantages of NMPC are outlined and some of the theoretical, computational, and implementational aspects of NMPC are discussed. Furthermore, some of the currently open questions in the area of NMPC are outlined. 
Optimization problems in chemical engineering often involve complex systems of nonlinear DAE as the model equations. The direct multiple shooting method has been known for a while as a fast off-line method for optimization problems in ODE and later in DAE. Some factors crucial for its fast performance are briefly reviewed. The direct multiple shooting approach has been successfully adapted to the specific requirements of real-time optimization. Special strategies have been developed to effectively minimize the on-line computational effort, in which the progress of the optimization iterations is nested with the progress of the process. They use precalculated information as far as possible (e.g. Hessians, gradients and QP presolves for iterated reference trajectories) to minimize response time in case of perturbations. In typical real-time problems they have proven much faster than fast off-line strategies. Compared with an optimal feedback control computable upper bounds for the loss of optimality can be established that are small in practice. Numerical results for the Nonlinear Model Predictive Control (NMPC) of a high-purity distillation column subject to parameter disturbances are presented.
This paper provides a solution to the problem of robust output feedback model predictive control of constrained, linear, discrete-time systems in the presence of bounded state and output disturbances. The proposed output feedback controller consists of a simple, stable Luenberger state estimator and a recently developed, robustly stabilizing, tube-based, model predictive controller. The state estimation error is bounded by an invariant set. The tube-based controller ensures that all possible realizations of the state trajectory lie in a simple uncertainty tube the ‘center’ of which is the solution of a nominal (disturbance-free) system and the ‘cross-section’ of which is also invariant. Satisfaction of the state and input constraints for the original system is guaranteed by employing tighter constraint sets for the nominal system. The complexity of the resultant controller is similar to that required for nominal model predictive control.
While linear model predictive control is popular since the 70s of the past century, only since the 90s there is a steadily increasing interest from control theoreticians as well as control practitioners in nonlinear model predictive control (NMPC). The practical interest is mainly driven by the fact that today’s processes need to be operated under tight performance specifications. At the same time more and more constraints, stemming for example from environmental and safety considerations, need to be satisfied. Often, these demands can only be met when process nonlinearities and constraints are explicitly taken into account in the controller design. Nonlinear predictive control, the extension of the well established linear predictive control to the nonlinear world, is one possible candidate to meet these demands. This paper reviews the basic principle of NMPC, and outlines some of the theoretical, computational, and implementational aspects of this control strategy. 
Batteries are the key technology for enabling further mobile electrification and energy storage. Accurate prediction of the state of the battery is needed not only for safety reasons, but also for better utilization of the battery. In this work we present a state estimation strategy for a detailed electrochemical model of a lithium-ion battery. The benefit of using a detailed model is the additional information obtained about the battery, such as accurate estimates of the internal temperature, the state of charge within the individual electrodes, overpotential, concentration and current distribution across the electrodes, which can be utilized for safety and optimal operation. Based on physical insight, we propose an output error injection observer based on a reduced set of partial differential-algebraic equations. This reduced model has a less complex structure, while it still captures the main dynamics. The observer is extensively studied in simulations and validated in experiments for actual electric-vehicle drive cycles. Experimental results show the observer to be robust with respect to unmodeled dynamics as well as to noisy and biased voltage and current measurements. The available state estimates can be used for monitoring purposes or incorporated into a model based controller to improve the performance of the battery while guaranteeing safe operation.
We present and investigate a Newton type method for online optimization in nonlinear model predictive control, the so called ``real-time iteration scheme''. In this scheme only one Newton type iteration is performed per sampling instant, and the control of the system and the solution of the optimal control problem are performed in parallel. In the resulting combined dynamics of system and optimizer, the actual feedback control in each step is based on the current solution estimate, and the solution estimates are at each sampling instant refined and transferred to the next optimization problem by a specially designed transition. This approach yields an efficient online optimization algorithm that has already been successfully tested in several applications. Due to the close dovetailing of system and optimizer dynamics, however, stability of the closed-loop system is not implied by standard nonlinear model predictive control results. In this paper, we give a proof of nominal stability of the scheme which builds on concepts from both, NMPC stability theory and convergence analysis of Newton type methods. The principal result is that -- under some reasonable assumptions -- the combined system-optimizer dynamics can be guaranteed to converge towards the origin from significantly disturbed system-optimizer states.
The problem of output feedback model predictive control of discrete time systems in the presence of additive but bounded state and output disturbances is considered. The overall controller consists of two components, a stable state estimator and a tube based, robustly stabilizing model predictive controller. Earlier results are extended by allowing the estimator to be time varying. The proposed robust output feedback controller requires the online solution of a standard quadratic program. The closed loop system renders a specified invariant set robustly exponentially stable.
While linear model predictive control is popular since the 70s of the past century, only since the 90s there is a steadily increasing interest from control theoreticians as well as control practitioners in nonlinear model predictive control (NMPC). The practical interest is mainly driven by the fact that today's processes need to be operated under tight performance specifications. At the same time more and more constraints, stemming for example from environmental and safety considerations, need to be satisfied. Often, these demands can only be met when process nonlinearities and constraints are explicitly taken into account in the controller design. Nonlinear predictive control, the extension of the well established linear predictive control to the nonlinear world, is one possible candidate to meet these demands. This paper reviews the basic principle of NMPC, and outlines some of the theoretical, computational, and implementational aspects of this control strategy.
Thanks to their cheap startup costs, flexibility, and standard infrastructure re-usability, Networked Control Systems have gained the attention of both the control community and the industry. Unfortunately, the presence of communication networks might introduce nondeterminism due to (random) delays and/or (unpredictable) information losses. In this paper, an event-based model predictive control approach for nonlinear continuous time systems under state and input constraints is presented. This method is able to counteract bounded delays, information losses, as well as deal with event triggering due to sensors and actuators. Under standard weak assumptions, closed loop stability, in the sense of asymptotic convergence, is achieved. Simulation results for a planar vertical takeoff and landing aircraft are provided.
For liver surgical planning, the structure and morphology of the hepatic vessels and their relationship to tumors are of major interest. To achieve a fast and robust assistance with optimal quantitative and visual information, we present methods for a geometrical and structural analysis of vessel systems. Starting from the raw image data a sequence of image processing steps has to be carried out until a three-dimensional representation of the relevant anatomic and pathologic structures is generated. Based on computed tomography (CT) scans, the following steps are performed. 1) The volume data is preprocessed and the vessels are segmented. 2) The skeleton of the vessels is determined and transformed into a graph enabling a geometrical and structural shape analysis. Using this information the different intrahepatic vessel systems are identified automatically. 3) Based on the structural analysis of the branches of the portal vein, their vascular territories are approximated with different methods. These methods are compared and validated anatomically by means of corrosion casts of human livers. 4) Vessels are visualized with graphics primitives fitted to the skeleton to provide smooth visualizations without aliasing artifacts. The image analysis techniques have been evaluated in the clinical environment and have been used in more than 170 cases so far to plan interventions and transplantations.
Rendering systems generally treat the production of images as an objective process governed by the laws of physics. However, perception and understanding on the part of viewers are subjective processes influenced by a variety offactors. For example, in the presentation of architectural drawings, the apparent precision with which the drawings are made will affect whether the viewer considers the design as part of a preliminary design or as part of a final polished project, and to some extent the level of confidence the viewer has in the encoded information.
In this paper we develop techniques for rendering images in a way that differs from the usual photorealistic or wire-frame output of renderers. In particular, our techniques allow a user to adjust the rendering of a scene to produce images using primitives with variable degrees of precision, from approximations that resemble vague “five-minute-sketches” to more mature but still hand-drawn images. We provide a theoretical framework for analysing the information flow from the computer to the user via such images. Finally, we describe the design and implementation of a prototypical renderer and show examples of its output.
We describe a pipeline of image processing steps for deriving symbolic models of vascular structures from radiological data which reflect the branching pattern and diameter of vessels. For the visualization of these symbolic models, concatenated truncated cones are smoothly blended at branching points. We put emphasis on the quality of the visualizations which is achieved by anti-aliasing operations in different stages of the visualization. The methods presented are referred to as HQVV (high quality vessel visualization). Scalable techniques are provided to explore vascular structures of different orders of magnitude. The hierarchy as well as the diameter of the branches of vascular systems are used to restrict visualizations to relevant subtrees and to emphasize parts of vascular systems. Our research is inspired by clear visualizations in textbooks and is targeted toward medical education and therapy planning. We describe the application of vessel visualization techniques for liver surgery planning. For this application it is crucial to recognize the morphology and branching pattern of vascular systems as well as the basic spatial relations between vessels and other anatomic structures.
HepaVision2, a user friendly software application for preoperative planning based on CT images in liver surgery is presented. It is intended for both, evaluation of potential donors in living-related liver transplantation and planning of oncologic resections. The planning takes into account the patient’s individual anatomy allowing for fully automatic calculation of individual resection proposals including volumetric analysis. The results are visualized in 3D, thus allowing the surgeon to choose the optimal strategy for each patient. The software was tested in over 50 cases by our clinical partners and our institution. Average time needed per case is below one hour, therefore allowing the use of the software application in clinical routine.
We present a method for visualizing vasculature based on clinical computed tomography or magnetic resonance data. The vessel skeleton as well as the diameter information per voxel serve as input. Our method adheres to these data, while producing smooth transitions at branchings and closed, rounded ends by means of convolution surfaces. We examine the filter design with respect to irritating bulges, unwanted blending and the correct visualization of the vessel diameter. The method has been applied to a large variety of anatomic trees. We discuss the validation of the method by means of a comparison to other visualization methods. Surface distance measures are carried out to perform a quantitative validation. Furthermore, we present the evaluation of the method which has been accomplished on the basis of a survey by 11 radiologists and surgeons.
We present real-time vascular visualization methods, which extend on illustrative rendering techniques to particularly accentuate spatial depth and to improve the perceptive separation of important vascular properties such as branching level and supply area. The resulting visualization can and has already been used for direct projection on a patient's organ in the operation theater where the varying absorption and reflection characteristics of the surface limit the use of color. The important contributions of our work are a GPU-based hatching algorithm for complex tubular structures that emphasizes shape and depth as well as GPU-accelerated shadow-like depth indicators, which enable reliable comparisons of depth distances in a static monoscopic 3D visualization. In addition, we verify the expressiveness of our illustration methods in a large, quantitative study with 160 subjects
Due to the progress in computer graphics hardware high resolution 3d models may be explored at interactive frame rates, and facilities to explore them are a part of modern radiological workstations and therapy planning systems. Despite their advantages, 3d visualizations are only employed by a minority of potential users and even these employ 3d visualizations for a few selected tasks only. We hypothesize that this results from a lack of intuitive interaction techniques for 3d rotation. In this paper, we compare existing techniques with respect to design principles derived by clinical applications and present results of an empirical study. These results are relevant beyond clinical applications and strongly suggest that the presented design principles are crucial for comfortable and predictable interaction techniques for 3d rotation.
Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.
The rapidly increasing performance of graphics processors, improving programming support and excellent performance-price ratio make graphics processing units (GPUs) a good option for a variety of computationally intensive tasks. Within this survey, we give an overview of GPU accelerated image registration. We address both, GPU experienced readers with an interest in accelerated image registration, as well as registration experts who are interested in using GPUs. We survey programming models and interfaces and analyze different approaches to programming on the GPU. We furthermore discuss the inherent advantages and challenges of current hardware architectures, which leads to a description of the details of the important building blocks for successful implementations.
The development of new multimedia services and environments requires new concepts both to support the new working process and to protect the multimedia data during the production and distribution. We address image/video authentication and copyright protection as major security demands in digital marketplaces. First we present a content-based signature technique for image and video authenticity and integrity. Based on this technique, we introduce a tool for interactive video authentication and propose content-fragile watermarking, a concept which combines watermarking and content-based digital signatures to ensure copyright protection and detection of integrity violation.
We briefly present the architecture of a public automated evaluation service we are developing for still images, sound and video. We also detail new tests that will be included in this platform. The set of tests is related to audio data and addresses the usual equalisation and normalisation but also time stretching, pitch shifting and specially designed audio attack algorithms. These attacks are discussed and results on watermark attacks and perceived quality after applying the attacks are provided.
We discuss the issues related to image watermarking benchmarking and scenarios based on digital rights management requirements. We show that improvements are needed in image quality evaluation, especially related to image geometrical deformation assessments, in risk evaluation related to specific delivery scenarios and in multidimensional criteria evaluation. Efficient benchmarking is still an open issue and we suggest the use of open-source Web-based evaluation systems for collective progress in this domain.
In this paper a first approach for digital media forensics is presented to determine the used microphones and the environments of recorded digital audio samples by using known audio steganalysis features. Our first evaluation is based on a limited exemplary test set of 10 different audio reference signals recorded as mono audio data by four microphones in 10 different rooms with 44.1 kHz sampling rate and 16 bit quantisation. Note that, of course, a generalisation of the results cannot be achieved. Motivated by the syntactical and semantical analysis of information and in particular by known audio steganalysis approaches, a first set of specific features are selected for classification to evaluate, whether this first feature set can support correct classifications. The idea was mainly driven by the existing steganalysis features and the question of applicability within a first and limited test set. In the tests presented in this paper, an inter-device analysis with different device characteristics is performed while intra-device evaluations (identical microphone models of the same manufacturer) are not considered. For classification the data mining tool WEKA with K-means as a clustering and Naive Bayes as a classification technique are applied with the goal to evaluate their classification in regard to the classification accuracy on known audio steganalysis features. Our results show, that for our test set, the used classification techniques and selected steganalysis features, microphones can be better classified than environments. These first tests show promising results but of course are based on a limited test and training set as well a specific test set generation. Therefore additional and enhanced features with different test set generation strategies are necessary to generalise the findings.
One of the main problems, which darkens the future of digital watermarking technologies, is the lack of detailed evaluation of existing marking schemes. This lack of benchmarking of current algorithms is blatant and confuses rights holders as well as software and hardware manufacturers and prevents them from using the solution appropriate to their needs. Indeed basing long-lived protection schemes on badly tested watermarking technology does not make sense. In this paper we will present the architecture of a public automated evaluation service we have developed for still images, sound and video. We will detail and justify our choice of evaluation profiles, that is the series of tests applied to different types of wa-termarking schemes. These evaluation profiles allow us to measure the reliability of a marking scheme to different levels from low to very high. Beside the known StirMark transformations, we will also detail new tests that will be included in this platform. One of them is intended to measure the real size of the key space. Indeed, if one is not careful, two different watermarking keys may produce interfering watermarks and as a consequence the actual space of keys is much smaller than it appears. Another set of tests is related to audio data and addresses the usual equalisation and normalisation but also time stretching, pitch shifting. Finally we propose a set of tests for fingerprinting applications. This includes: averaging of copies with different fingerprint, random ex-change of part between different copies and comparison between copies with selection of most/less frequently used position differences.
Digital watermarking is the enabling technology to prove ownership on copyrighted material, detect originators of illegally made copies, monitor the usage of the copyrighted multimedia data and analyze the spread spectrum of the data over networks and servers. Embedding of unique customer identification as a watermark into data is called fingerprinting to identify illegal copies of documents. Basically, watermarks embedded into multimedia data for enforcing copyrights must uniquely identify the data and must be difficult to remove, even after various media transformation processes. Digital fingerprinting raises the additional problem that we produce different copies for each customer. Attackers can compare several fingerprinted copies to find and destroy the embedded identification string by altering the data in those places where a difference was detected. In our paper we present a technology for combining a collusion-secure fingerprinting scheme based on finite geometries and a watermarking mechanism with special marking points for digital images. The only marking positions the pirates can not detect are those positions which contain the same letter in all the compared documents, called intersection of different fingerprints. The proposed technology for a maximal number d of pirates, puts enough information in the intersection of up to d fingerprints to uniquely identify all the pirates.
The IT security of automotive systems is an evolving area of research. To analyse the current situation we performed several practical tests on recent automotive technology, focusing on automotive systems based on CAN bus technology. With respect to the results of these tests, in this paper we discuss selected countermeasures to address the basic weaknesses exploited in our tests and also give a short outlook to requirements, potential and restrictions of future, holistic approaches.
A new method for on-line signature authentication is presented, which is based on a event-string modelling of features derived from pen-position and pressure signals of digitizer tablets. A distance measure well known from textual pattern recognition, the Levenshtein distance, is used for comparison of signatures and classification is carried out applying a nearest neighbor classifier. Results from a test set of 1376 signatures from 41 persons are presented, which have been conducted for four different feature sets. The results are rather encouraging, with correct identification rates of 96% at zero false classifications.
Digital watermarking is a technique for embedding information into data, such as images, 3D models, or audio files, such that some properties (i.e., security, imperceptibility, robustness) are maintained. While most of the existing watermarking techniques focus on encoding copyright information where security is one of the most important properties, we have developed algorithms for embedding Illustration Watermarking, i.e., content-related annotations for the image. Robustness against common media transformations, high capacity, and blind detection are our aspired properties while security and the usage of secure keys are less important. The medium that we are using for embedding data are 2D vector graphics. We introduce algorithms that change line attributes, introduce new vertices in certain patterns, and replace existing stroke segments by new lines in a stylistic way. Based on the modification they introduced, we categorize our techniques into whether they change the appearance of the image or not and whether the changes are perceivable by the naked eye or not. We demonstrate our techniques with silhouette lines obtained from 3D models. Such line drawings are a very common style utilized in many illustrations, in particular in the medical and technical domain.
Digital watermarking has become an accepted technology for enabling multimedia protection schemes. While most efforts concentrate on user authentication, recently interest in data authentication to ensure data integrity has been increasing. Existing concepts address mainly image data. Depending on the necessary security level and the sensitivity to detect changes in the media, we differentiate between fragile, semifragile, and content-fragile watermarking approaches for media authentication. Furthermore, invertible watermarking schemes exist while each bit change can be recognized by the watermark which can be extracted and the original data can be reproduced for high-security applications. Later approaches can be extended with cryptographic approaches like digital signatures. As we see from the literature, only few audio approaches exist and the audio domain requires additional strategies for time flow protection and resynchronization. To allow different security levels, we have to identify relevant audio features that can be used to determine content manipulations. Furthermore, in the field of invertible schemes, there are a bunch of publications for image and video data but no approaches for digital audio to ensure data authentication for high-security applications. In this paper, we introduce and evaluate two watermarking algorithms for digital audio data, addressing content integrity protection. In our first approach, we discuss possible features for a content-fragile watermarking scheme to allow several postproduction modifications. The second approach is designed for high-security applications to detect each bit change and reconstruct the original audio by introducing an invertible audio watermarking concept. Based on the invertible audio scheme, we combine digital signature schemes and digital watermarking to provide a public verifiable data authentication and a reproduction of the original, protected with a secret key.








































