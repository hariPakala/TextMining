Probabilistic inference algorithms for finding the most probable explanation, the maximum aposteriori hypothesis, and the maximum expected utility and for updating belief are reformulated as an elimination--type algorithm called bucket elimination. This emphasizes the principle common to many of the algorithms appearing in that literature and clarifies their relationship to nonserial dynamic programming algorithms. We also present a general way of combining conditioning and elimination within this framework. Bounds on complexity are given for all the algorithms as a function of the problem&apos;s structure. 1 INTRODUCTION An external observer attempting to sort out the core ideas behind current algorithms for processing influence diagrams or Bayesian networks normally find the topic confusing
We present Voxel Column Culling, an occlusion culling  algorithm for real-time rendering of large terrain models. This technique  can greatly reduce the number of polygons that must be rendered every  frame, allowing a larger piece of terrain to be rendered at an interactive  frame-rate. This is accomplished by using a form of 3D cell-based  occlusion culling. A visibility table for the terrain model is precomputed  and stored on disk. This table is then used to quickly bypass portions of  the model that are not visible due to self-occlusion of the terrain model. This
We present a novel framework for studying recommendation algorithms in terms of the  `jumps&apos; that they make to connect people to artifacts. This approach emphasizes reachability  via an algorithm within the implicit graph structure underlying a recommender  dataset and allows us to consider questions relating algorithmic parameters to properties  of the datasets. For instance, given a particular algorithm `jump,&apos; what is the average  path length from a person to an artifact? Or, what choices of minimum ratings and  jumps maintain a connected graph? We illustrate the approach with a common jump  called the `hammock&apos; using movie recommender datasets.
Risk assessment of gene flow from GM crops into the environment requires both the development of physical transport models and biological models for the assessment of outcrossing probabilities. Our starting point is a Lagrangian approach for pollen dispersal, which describes the concentration statistics in terms of the stochastic properties of the paths of ensembles of particles. Transport of a particle from a location (x&apos;,y&apos;,z&apos;) to a location (x,y,z) is mediated by a probability density or transfer function  Q(x,y,z|x&apos;,y&apos;,z&apos;). The transfer function depends on the statistics of the wind field during pollination. The total amount of pollen, which reaches a single plant, is then derived by the integral over all donators. In the context of gene flow, particle transport is but one aspect. The target variable is not primarily pollen density but the amount of outcrossing. The transfer function Q thus has to take into account both transport and biological processes and is devised to combine a transport submodel capable of integrating the statistics of wind velocities, a pollen viability submodel, a phenological submodel, a submodel for pollen redistribution by insects and a pollen competition submodel. Model parameters are estimated from data of outcrossing studies of maize and oil seed rape. The model is then applied to study the effect of field geometries on outcrossing rates.
We develop a way of analyzing the behavior of systems modeled using  Discrete Time Markov Chains (DTMC). Specifically, we define iLTL, an LTL  with linear inequalities on the pmf vectors as atomic propositions. iLTL allows us  to express not only properties such as the expected number of jobs or the expected  energy consumption of a protocol during a time interval, but also inequalities over  such values. We present an algorithm for model checking properties of DTMCs  expressed in iLTL. Our model checker differs from existing probabilistic ones in  that the latter do not check properties of the transitions on the probability mass  function (pmf) itself. Thus, iLTLChecker can check, given an interval estimate  of current pmf, whether future pmfs will always satisfy a specification. We believe  such properties often arise in distributed systems and networks and may,  in particular, be useful in specifying requirements for routing or load balancing  protocols. Our algorithm has been implemented in a tool called iLTLChecker and  we illustrate the use of the tool by means of some examples.
In this paper, we study several properties of binary-feedback congestion control in rate-based applications. We first derive necessary conditions for generic binary-feedback congestion control to converge to fairness monotonically (which guarantees asymptotic stability of the fairness point) and show that AIMD is the only TCP-friendly binomial control with monotonic convergence to fairness. We then study steady-state behavior of binomial controls with n competing flows on a single bottleneck. Our main result here shows that combined probing for new bandwidth by all flows results in significant overshoot of the available bandwidth and rapid (often super-linear as a function of n) increase in packet loss. We also show that AIMD has the best scalability and lowest packet loss increase among all TCP-friendly binomial schemes. We conclude the paper by deriving the conditions necessary to achieve constant packet loss regardless of the number of competing flows n and examine one new scheme with such constant packet loss called Ideally Scalable Congestion Control (ISCC) in both simulation and streaming experiments.
This paper addresses three questions. Is it useful  to attempt to learn a Bayesian network structure  with hundreds of thousands of nodes? How  should such structure search proceed practically?  The third question arises out of our approach  to the second: how can Frequent Sets (Agrawal  et al., 1993), which are extremely popular in the  area of descriptive data mining, be turned into a  probabilistic model?  Large sparse datasets with hundreds of thousands  of records and attributes appear in social  networks, warehousing, supermarket transactions  and web logs. The complexity of structural  search made learning of factored probabilistic  models on such datasets unfeasible. We propose  to use Frequent Sets to significantly speed  up the structural search. Unlike previous approaches,  we not only cache n-way sufficient  statistics, but also exploit their local structure.
Several studies have pointed out that the values produced by the execution of a program&apos;s instructions are often quite repetitive. There are basically two approaches that have been proposed for exploiting this value locality -- 1) reusing the results of a prior execution of an instruction, and 2) predicting the value that will be produced by the current execution of an instruction based on the previous values it has produced. Existing value reuse and prediction schemes operate at the level of a single instruction or short sequence of instructions so that the caches used to temporarily store previous values are typically indexed using the instruction addresses. However, we have found that different instructions often produce the same values. We introduce the Speculative Value Cache (SVC) to globally exploit values previously produced by any instructions by indexing this cache with a hash function of the values of the instruction&apos;s input operands rather than indexing with the instruction address. With this approach, the current instruction can directly use the result value found in the speculative value cache even if it has been previously produced by a different instruction. We partition the SVC into several different sections based on the instruction category, such as arithmetic, shift, or load, for instance, since different types of instructions will produce different output results when given the same input operands. Our simulation results based on the SimpleScalar simulator show that embedding the SVC in a realistic 4-issue superscalar processor could improve the performance of the SPEC95 integer benchmarks by as much as 25%. Increasing the issue-with allows this mechanism to achieve even higher performance with an 8-issue processor having speedups of 5-42% and a 16-...
System resource management for high-assurance applications  such as the command and control of a battle group  is a complex problem. These applications often require  guaranteed computing services that must satisfy both hard  and soft deadlines. In addition, their resource demands can  vary significantly over time with bursts of high activity  amidst periods of inactivity. A traditional solution has been  to dedicate resources to critical application tasks and to  share resources among noncritical tasks. With the increasing  complexity of high-assurance applications and the  need to reduce system costs, dedicating resources is not a  satisfactory solution. The Amaranth Project at Carnegie  Mellon is researching and developing a framework for allocating  shared resources to support multiple quality of  service (QoS) dimensions and to provide probabilistic assurances  of service. This paper is an overview of the Amaranth  framework, the current results from applying the  framework, and t...
In this paper, we present and evaluate an implementation of a prototype scalable web server. The  prototype consists of a load-balanced cluster of hosts that collectively accept and service TCP  connections. The host IP addresses are advertised using the Round Robin DNS (RR-DNS)  technique, allowing any host to receive requests from any client. Once a client attempts to  establish a TCP connection with one of the hosts, a decision is made as to whether or not the  connection should be redirected to a different host---namely, the host with the lowest number of  established connections. We use the low-overhead Distributed Packet Rewriting (DPR) technique  [Bestavros, Crovella, Liu, and Martin 1998] to redirect TCP connections. In our prototype, each  host keeps information about the remaining hosts in the system. Load information is maintained  using periodic multicast amongst the cluster hosts. Performance measurements suggest that our  prototype outperforms both pure RR-DNS and the sta...
An interactive system is presented for three dimensional reconstruction of human models from multiple images taken fromahumanbody image capturing setup. The reconstruction is basedontwocomputer vision techniques -- volumetric intersection by cross sections approach and stereo. The basic theory behind these techniques is outlined. The main components of the system -- image capturing, image processing, cameracalibration, 3D reconstruction and 3D surface representation -- are explained and various user interaction tools are mentioned. In the system, different parts of a human body will bereconstructedseparately. A stack of parallel cross sections, each of which is represented by a NURBS curve, will becomputedforeach part of the body by volumetric intersection. The concave parts of the body like chest of a female may not be captured by the cross sections. In those cases, the cross sections are modified by capturing the concave parts by stereo using structured light grid points. Then the cross sections are fine tuned by user interaction, wherever necessary, for smoothness and accuracy.
A mechanism is described that enables a robotic agent to create temporal  categories for conceptualizing the world. The creation of a new category is  triggered when the agent is unable to temporally distinguish an event from  the other events in the context using already adopted categories. This is different  from most other approaches where ontological categories are defined  by humans and the ontologies are fixed in advance.
The World Wide Web creates opportunities for search systems using adaptive distributed agents. This paper presents a threaded implementation of InfoSpiders, a client-based system that uses an evolving population of intelligent agents to browse the Web at query time. We consider different fitness functions based on network resource consumption and show that taxing agents in proportion to latency results in better efficiency without penalties in the quality of the retrieved documents. The tool is available to the public as a Java applet.
In Autonomic Computing, an application needs to be aware of its environment. While the term &quot;environment&quot; is not normally understood as being a physical environment, in Pervasive Computing many applications do actually need to monitor the physical environment in which they are deployed. Monitoring the environment often includes gathering information about the people working or living in this environment. Applications that self-adapt to changes in the monitored environment are known as context-aware. As context-aware applications need environment sensor data, testing these applications usually require deployment at a physical test location, often in a research laboratory. Our project aims to design a simulation model of contexts as a means to test the context logic of a context-aware application, by allowing sensor data to be produced from a description of contexts, i.e. the location and activities of people, thereby allowing initial testing of a contextaware application without requiring physical deployment.
Detecting changes in a data stream is an important  area of research with many applications.
We introduce a new type of Self-Organizing Map (SOM) to navigate  in the Semantic Space of large text collections. We propose a &quot;hyperbolic  SOM&quot; (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood  around a point in hyperbolic space provides more freedom to map the  complex information space arising from language into spatial relations.
This paper is devoted to scheduling a large collection of independent tasks onto a large distributed heterogeneous platform, which is composed of a set of servers. Each server is a processor cluster equipped with a file repository. The tasks to be scheduled depend upon (input) files which initially reside on the server repositories. A given file may well be shared by several tasks. For each task, the problem is to decide which server will execute it, and to transfer the required files (those which the task depends upon) to that server repository. The objective is to find a task allocation, and to schedule the induced communications, so as to minimize the total execution time. The contribution of this paper is twofold. On the theoretical side, we establish complexity results that assess the difficulty of the problem. On the practical side, we design several new heuristics, including an extension of the min-min heuristic to the decentralized framework, and several lower cost heuristics, which we compare through extensive simulations.
This paper introduces a novel data-driven methodology named Evolutionary Polynomial Regression (EPR), which permits the multi-purpose modelling of physical phenomena, through the simultaneous solution of a number of models. Multipurpose modelling or &quot;multi-modelling&quot;, enables the user to make a more robust choice of those models aimed at (a) the knowledge based on data modelling, (b) on-line and offline forecasting, and (c) data augmentation (i.e. infilling of missing data in time series). This methodology is particularly useful in modelling environmental phenomena, for which it is usually impossible to obtain physical data at a laboratory scale. In particular, the non-linearity of phenomena and non Gaussian nature of background noise make on-line forecasting complex, and where data are available, they often contain discontinuities (i.e. missing data). The use of EPR in modelling and analysis is illustrated by application to a case study containing all these limitations. The application of EPR to thermal behaviour of a stream gives not only a good physical insight of the phenomenon, but also allows infilling of missing data, resulting in good models that forecast the water temperature.
In an increasingly competitive marketplace, one of the most interesting and challenging problems is how to identify and profile customers who are most likely to be interested in new products or services. At the same time, minimizing the number of variables used in the prediction task is important with large databases. In this paper we consider a novel application of evolutionary multiobjective algorithms for customer targeting. Evolutionary algorithms are considered effective in solving multiobjective problems because of their inherent parallelism. We use ELSA, an evolutionary local selection algorithm that maintains a diverse population of solutions approximating the Pareto front in a multi-dimensional objective space. We use artificial neural networks (ANNs) for customer prediction and ELSA to search for promising subsets of features. Our results on a real data set show that our approach is easier to interpret and more accurate than the traditional method used in marketing.  1 
Semantically heterogeneous and distributed data sources are  quite common in several application domains such as bioinformatics and  security informatics. In such a setting, each data source has an associated  ontology. Different users or applications need to be able to query  such data sources for statistics of interest (e.g., statistics needed to learn  a predictive model from data). Because no single ontology meets the  needs of all applications or users in every context, or for that matter,  even a single user in different contexts, there is a need for principled  approaches to acquiring statistics from semantically heterogeneous data.
This paper presents an approach for the integrated consideration of both technical and valuation uncertainties during decision making supported by LCA-type environmental performance information. Key elements of this approach include &quot;distinguishability analysis&quot; to determine whether the uncertainty in the performance information is likely to make it impossible to distinguish between the activities under consideration, and the use of a multivariate statistical analysis approach, called principal components analysis (PCA), which facilitates the rapid analysis of large numbers of parallel sets of results, and enables the identification of choices that lead to similar and/or opposite evaluations of activities. The integrated approach for the management of uncertainty is demonstrated for a technology selection decision for the recommissioning of a coal-based power station. Distinguishability analysis showed that it was not possible to obtain a conclusive answer with regard to the preferred technology due to the extensive uncertainty in the LCA-based environmental performance information. PCA of the ranking of the design scenarios demonstrated that valuation uncertainties associated with choices made during intra- and inter-criterion preference modelling had a more significant effect on the ranking of the design scenarios than the inclusion/exclusion of environmental indicators reflecting local concerns or the choice of the position of the LCIA impact indicators in the cause-effect network. The results suggest that stakeholder involvement in intra- and inter-criterion preference modelling is important, and that the &quot;encoding&quot; of value judgements and preferences into LCA environmental performance information is to be avoided. As a whole, the paper supports a call for diversity...
INTRODUCTION  Many problems in control system analysis and design can be posed in a setting where a system with a ffxed model structure and nominal parameter values is affected by parameter variations. An example is parametric robustness analysis, where the parameters might representphysical quantities that are known only to within a certain accuracy,orvary depending on operating conditions etc. Frequently asked questions here deal with performance issues: ffHow bad can a certain performance measure of the system be over all possible values of the parameters?&quot; Another example is parametric controller design, where the parameters represent degrees of freedom available to the control system designer. A typical question here would be: ffWhat is the best choice of parameters, one that optimizes a certain design objective?&quot;  Many of the questions abovemay be directly restated as optimization problems: If q denotes the vector of parameters,    init the set of values that q might assume and fffqff 
We consider the problem of content search and retrieval in peer-to-peer (P2P) communities. P2P computing is a potentially powerful model for information sharing between ad hoc groups of users because of its low cost of entry and natural model for resource scaling with community size. As P2P communities grow in size, however, locating information distributed across the large number of peers becomes problematic. We present a distributed text-based content search and retrieval algorithm to address this problem. Our algorithm is based on a state-of-the-art text-based document ranking algorithm: the vector-space model, instantiated of TFxIDF would require each peer in a community to collect an inverted index of the entire community. we show how TFxIDF can be approximated given compact summaries of peers&apos; local inverted indexes. We make three contributions: (a) we show how the TFxIDF rule can be adapted to use the index summaries, (b) we provide a heuristic for adaptively determining the set of peers that should be contacted for a query, and (c) we show that our algorithm tracks TFxIDF&apos;s performance very closely, regardless of how documents are distributed throughout the community. Furthermore, our algorithm preserves the main flavor of TFxIDF by retrieving close to the same set of documents for any given query.
The integration of Java and CORBA has opened the way for a wide variety of Internet applications. However, such applications will frequently come across communication and node failures which can affect both their performance and consistency. Therefore, there is a need for techniques which will allow applications to tolerate such failures and continue to provide expected services. A common technique is through the use of atomic transactions, which ensure that only consistent state changes take place despite concurrent access and failures. However, they may be insufficient to guarantee forward progress. This can be accomplished by replicating resources, so that the failure of a subset of replicas can be masked from users. This paper describes a toolkit that integrates transactions and replication by making use of standard CORBA services.
XML allows to represent both content and structure of documents. Taking advantage of the  document structure promises to greatly improve the retrieval precision. In this paper, we  present a retrieval technique that adopts the similarity measure of the vector space model,  incorporates the document structure, and supports structured queries. Our query model is  based on tree matching as a simple and elegant means to formulate queries without knowing  the exact structure of the data. Using this query model we propose a logical document concept  by deciding on the document boundaries at query time. We combine structured queries  and term-based ranking by extending the term concept to structural terms which include  substructures of queries and documents. The notions of term frequency and inverse document  frequency are adapted to logical documents and structural terms. We introduce an ecient  technique to calculate all necessary term frequencies and inverse document frequencies at  query time. By adjusting parameters of the retrieval process we are able to model two contrary  approaches: the classical vector space model, and the original tree matching approach.
Reinforcement learning is an effective technique for learning action policies in discrete stochastic environments, but its efficiency can decay exponentially with the size of the state space. In many situations significant portions of a large state space may be irrelevant to a specific goal and can be aggregated into a few, relevant, states. The U Tree algorithm generates a tree based state discretization that efficiently finds the relevant state chunks of large propositional domains. In this paper, we extend the U Tree algorithm to challenging domains with a continuous state space for which there is no initial discretization. This Continuous U Tree algorithm transfers traditional regression tree techniques to reinforcement learning. We have performed experiments in a variety of domains that show that Continuous U Tree effectively handles large continuous state spaces. In this paper, we report on results in two different domains, one gives a clear visualization of the algorithm and another empirically demonstrates an effective state discretization in a simple multi-agent environment.
Tool installation and automation of administrative tasks in heterogeneous computer networks becomes of increasing importance with the availability of complex heterogeneous computer networks. This article introduces a new approach for dynamic network tool management, i.e., TRMS. A variant of TRMS using SNMP - a well established standard for network administration -- is outlined and illustrated by the application of the integration and management of design tools for Printed Circuit Boards (PCBs).
Many approaches and systems for recommending information,  goods, or other kinds of objects have been developed  in recent years. In these systems, machine learning methods  are often used that need training input to acquire a user  interest profile. Such methods typically need positive and  negative evidence of the user&apos;s interests. To obtain both  kinds of evidence, many systems make users rate relevant  objects explicitly. Others merely observe the user&apos;s behavior,  which yields positive evidence only; in order to be able  to apply the standard learning methods, these systems  mostly use heuristics to also find negative evidence in  observed behavior.  In this paper, we present an approach for learning interest  profiles from positive evidence only, as it is contained in  observed user behavior. Thus, both the problem of  interrupting the user for ratings and the problem of  somewhat artificially determining negative evidence are  avoided.  A methodology for learning explicit user p...
The pelvic floor gives support to the organs in the abdominal cavity. Using the dataset made public in [11] we have reconstructed the geometry of one of the most important parts of the pelvic floor, the levator ani, using NURB surfaces. Once the surface is triangulated, this mesh is used in a finite element analysis with shell elements.
We present a theory of timed interfaces, which is capable of  specifying both the timing of the inputs a component expects from the  environment, and the timing of the outputs it can produce. Two timed  interfaces are compatible if there is a way to use them together such  that their timing expectations are met. Our theory provides algorithms  for checking the compatibility between two interfaces and for deriving  the composite interface; the theory can thus be viewed as a type system  for real-time interaction. Technically, a timed interface is encoded as a  timed game between two players, representing the inputs and outputs of  the component. The algorithms for compatibility checking and interface  composition are thus derived from algorithms for solving timed games.
The problem of a groundwater model calibration is posed as a multiextremum (global) optimization problem, rather than the more widely considered single-extremum (local) optimization problem. Several algorithms of randomized search incorporated in the global optimization tool GLOBE are considered (including the canonical genetic algorithm and more recently developed adaptive cluster covering), and applied to the calibration of the groundwater model TRIWACO. The results show the usefulness of global optimization algorithms in the automatic calibration of even complex models having considerable running times.
This paper investigates GA approaches for solving the reliable communication network design problem. For solving this problem a graph with minimum cost must be found that satisfies a given network reliability constraint. To consider the additional reliability constraint different approaches are possible. We show that existing approaches using penalty functions can result in invalid solutions and are therefore not appropriate for solving this problem. To overcome these problems we present a repair heuristic, which is based on the number of spanning trees in a graph. This heuristic always generates a valid solution, which when compared to a greedy cheapest repair heuristic shows that the new approach finds better solutions with less computational effort.
This paper introduces the GeoCrystal and the SuperTable + Scatterplot as new information visualization techniques (based on metadata and metaphors) to support accessing geospatial data archives and site planning in general
There is strong evidence that face processing in the brain is localized. The double dissociation between prosopagnosia, a face recognition deficit occurring after brain damage, and visual object agnosia, difficulty recognizing other kinds of complex objects, indicates that face and non-face object recognition may be served by partially independent neural mechanisms. In this paper, we use computational models to show how the face processing specialization apparently underlying prosopagnosia and visual object agnosia could be attributed to (1) a relatively simple competitive selection mechanism that, during development, devotes neural resources to the tasks they are best at performing, (2) the developing infant&apos;s need to perform subordinate classification (identification) of faces early on, and (3) the infant&apos;s low visual acuity at birth. Inspired by de Schonen, Mancini and Liegeois&apos; arguments (1998) [de Schonen, S., Mancini, J., Liegeois, F. (1998). About functional cortical specializat...
Modern embedded processors use data caches with higher and higher degrees of associativity in order to increase performance. A set-associative data cache consumes a significant fraction of the total power budget in such embedded processors. This paper describes a technique for reducing the D-cache power consumption and shows its impact on power and performance of an embedded processor. The technique utilizes cache line address locality to determine (rather than predict) the cache way prior to the cache access. It thus allows only the desired way to be accessed for both tags and data. The proposed mechanism is shown to reduce the average L1 data cache power consumption when running the MiBench embedded benchmark suite for 8, 16 and 32--way set--associate caches by, respectively, an average of 66%, 72% and 76%. The absolute power savings from this technique increase significantly with associativity. The design has no impact on performance and, given that it does not have mis-prediction penalties, it does not introduce any new non-deterministic behavior in program execution.
  A fundamental activity underlying cognition that plays an important role in creativity and discovery is the integration of sensory stimuli, such as visual or acoustic stimuli, into a system of concepts. A sensory stimulus, for example a picture, is often considered as a model such that reasoning with these sensory stimuli is thought to be a kind of model-based reasoning [Johnson-Laird, 1983]. We claim, however, that sensory stimuli such as visual patterns can only be considered as models when we take the principle of perception into account. In fact, we believe that any sensory stimuli can theoretically be interpreted in various ways resulting in dierent models and that the perceived model of a sensory stimulus is the one which satis  es the principles of human perception. The interesting point is that analogical reasoning, a certain kind of model-based reasoning, is itself thought to play an important role in human perception and thus in determining the perceived models of sensory stimuli that are reason with. In particular, we will show that the perceived model of a sensory stimuli is partly determined on the basis of analogical reasoning. This implies that the resemblance of sensory stimuli which is the basis of an argument by analogy is not a simple subtraction of the models of those stimuli in separation, but created hand in hand with the process of analogical reasoning.
Objectives: The objective of this study is to characterize the definitions of anatomical concepts in  a general terminological system (WordNet) and a domain-specific one (a medical dictionary). Methods:
This paper describes the design and evaluation of PROTOMOL, a high  performance object-oriented software framework for molecular dynamics (MD).
The aim of this paper is to question the well-established view in constraint based grammars on Romance causatives, i.e. that they are functionally monoclausal, as well as its most important corollary, that their lexical representation is underspecified with regard to argument structure (Alsina 1996, Abeill, Godard &amp; Sag 1998) that gets fully specified by merging with another predicate. This is the generally accepted view, even though it contradicts initial claims on lexical integrity. The discussion will rest upon a few problematic sentences of which the monoclausal theories give an unsatisfactory account. I argue that the monoclausality of causatives is only a surface (i.e. a c-structure) monoclausality. The `merging effect &apos; observed at the level of surface realisation, and the apparent extra arguments of the causative predicate will be accounted for by some other means. The idea is to resort to the previous conception of a biclausal f-structure, but with a more `complex&apos; use of structure sharing. I will mainly be concerned with the implementation of this concept in Lexical Functional Grammar (LFG) in this paper, but a short comparison with a similar Head-Driven Phrase Structure Grammar (HPSG) approach will support the idea that the proposal has some empirical relevance.
Energy is one of the most important resources in wireless sensor  networks. We use an idealized mathematical model to study the impact of  routing on energy consumption. We find explicit bounds on the minimal and  maximal energy routings will consume, and use them to bound the lifetime of  the network. The bounds are sharp and can be achieved in many situations of  interest. Our results apply to sensor networks with a continuous data delivery  model, in which all sensors transmit with the same power. Within this class,  the results are very general as they apply to arbitrary topologies, routings and  radio energy models. We illustrate the theory with some examples. Among  these, there is one contradicting the popular belief that it is always the nodes  deployed nearest to base nodes that are the most heavily loaded and, hence,  the ones that die first.
Let  be a process equivalence. A formula &apos; is preserved by -quotients i for every process s of a transition system T we have that if s satis  es &apos;, then also [s] satis  es &apos;, where [s] is the equivalence class of s in the quotient of T under . We classify all formulae of Hennessy-Milner logic which are preserved by -quotients of image-  nite processes. Our result is generic in the sense that it works for a large class of process equivalences which admit a modal characterization in Hennessy-Milner logic satisfying certain closure properties. A practical applicability of the result is demonstrated on equivalences of the linear/branching time spectrum of [14].
The combination of mobile computing and collaborative Augmented Reality into a single system makes the power of computer enhanced interaction and communication in the real world accessible anytime and everywhere. This paper describes our work to build a mobile collaborative Augmented Reality system that supports true stereoscopic 3D graphics, a pen and pad interface and direct interaction with virtual objects. The system is assembled from offthe -shelf hardware components and serves as a basic test bed for user interface experiments related to computer supported collaborative work in Augmented Reality. A mobile platform implementing the described features and collaboration between mobile and stationary users are demonstrated. 
This primarily conceptual paper introduces &apos;Co-opetitive Learning and Knowledge Exchange Networks&apos; (CoLKENs) as a specific context for decision making. CoLKENs play a pivotal role within our global, increasingly knowledge based society and shape knowledge sharing processes among their members and organizations. The inherent balancing act between cooperation and competition requires designing and implementing specific management processes to enable economic value maximization for individuals and firms. CoLKENs, their components and generic structure are described. A first taxonomy is suggested. Two specific CoLKENs in open source type environments, SourceForge and CodeX (Xerox) are explored. Finally, future research lines are identified.
When sensitive information is exchanged with the user of a computer  system, the security of the system&apos;s user interface must be considered.
Coordinated action for a team of robots is a challenging  problem, especially in dynamic, unpredictable environments.
Suffix arrays are powerful data structures for text indexing. In this paper
Almost all graphics architectures today support Gouraud shading, linear color interpolation between vertices; system designers aim toward a Phong shading model, linear interpolants of surface normals with a lighting model that supports both diffuse and specular components, as a superior means of rendering accurate images. However, the Phong model still retains serious artifacts. In this paper, we point out the shortcomings of linear interpolation of normals, and present a surface interrogation method for parametrically defined surfaces.
This paper details the attack on RC6 which was announced  in a report published in the proceedings of the second AES candidate  conference (March 1999). Based on an observation on the RC6 statistics,  we show how to distinguish RC6 from a random permutation and to  recover the secret extended key for a fair number of rounds.
Shadow generation has been subject to serious investigation in computer graphics, and many clever algorithms  have been suggested. However, previous algorithms cannot render high quality soft shadows onto arbitrary, animated  objects in real time. Pursuing this goal, we present a new soft shadow algorithm that extends the standard  shadow volume algorithm by replacing each shadow quadrilateral with a new primitive, called the penumbra  wedge. For each silhouette edge as seen from the light source, a penumbra wedge is created that approximately  models the penumbra volume that this edge gives rise to. Together the penumbra wedges can render images that  often are remarkably close to more precisely rendered soft shadows. Furthermore, our new primitive is designed  so that it can be rasterized efficiently. Many real-time algorithms can only use planes as shadow receivers, while  ours can handle arbitrary shadow receivers. The proposed algorithm can be of great value to, e.g., 3D computer  games, especially since it is highly likely that this algorithm can be implemented on programmable graphics hardware  coming out within the next year, and because games often prefer perceptually convincing shadows.
This paper examines the access control requirements of distributed  health care information networks. Since the electronic sharing  of an individual&apos;s personal health information requires their informed  consent, health care information networks need an access control framework  that can capture and enforce individual access policies tailored to  the specific circumstances of each consumer. Role Based Access Control  (RBAC) is examined as a candidate access control framework. While  it is well suited to the task in many regards, we identify a number of  shortcomings, particularly in the range of access policy expression types  that it can support. For effciency and comprehensibility, access policies  that grant access to a broad range of entities whilst explicitly denying  it to subgroups of those entities need to be supported in health information  networks. We argue that RBAC does not support policies of this  type with suffcient flexibility and propose a novel adaptation of RBAC  principles to address this shortcoming. We also describe a prototype distributed  medical information system that embodies the improved RBAC  model.
This paper presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e. generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis framework. We have used this system to automatically parallelize two complete scientific computations: the Barnes-Hut Nbody solver and the Water code. This paper presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can ser...
Intelligent retrieval of hypermedia documents requires sophisticated document representations and querying facilities that allow for content-based and fact-based querying as well as considering the structure of documents. This paper describes POOL, a Probabilistic Object-Oriented four-valued Logic, which allows a uniform view on hypermedia documents for the purpose of their retrieval: documents, images, authors, dates, etc. are treated as objects and POOL models the content of objects, the facts about objects, and the structure of objects to provide for a relevance-based ranking of hypermedia documents.
In the current Internet, most of the trac is transmitted by TCP (Transmission Control Protocol). In our previous work,  we have proposed a modeling approach for the entire network, including TCP congestion control mechanisms operating at source hosts and the network seen by TCP connections, as a single feedback system. However, our analytic model is limited to a simple network, where TCP connections have the identical propagation delay. In this paper, we therefore extend our analytic approach to a more generic network, where multiple TCP connections are allowed to have dierent propagation delays. We derive the packet loss probability in the network, the throughput and the average round-trip time of each TCP connection in steady state. By presenting several numerical examples, we quantitatively investigate how the fairness among TCP connections is degraded when multiple TCP connections with dierent propagation delays share the single bottleneck link.
This paper describes a behavior-based architecture which integrates existing potential field approaches concerning motion planning as well as the evaluation and selection of actions into a single architecture. This combination allows, together...
We present a method for separating two speakers from a single microphone channel. The method exploits the fine structure of male and female speech and relies on a strong high frequency resolution model for the source signals.
We study an approach to text categorization that combines distributional clustering of  words and a Support Vector Machine (SVM) classifier. This word-cluster representation is  computed using the recently introduced Information Bottleneck method, which generates  a compact and efficient representation of documents. When combined with the classification  power of the SVM, this method yields high performance in text categorization. This  novel combination of SVM with word-cluster representation is compared with SVM-based  categorization using the simpler bag-of-words (BOW) representation. The comparison is  performed over three known datasets. On one of these datasets (the 20 Newsgroups) the  method based on word clusters significantly outperforms the word-based representation  in terms of categorization accuracy or representation effciency. On the two other sets  (Reuters-21578 and WebKB) the word-based representation slightly outperforms the wordcluster  representation. We investigate the potential reasons for this behavior and relate it  to structural differences between the datasets.
This paper presents Netbait, a planetary-scale service for distributed detection of Internet worms. Netbait allows users to pose queries that identify which machines on a given network have been compromised based on the collective view of a geographically distributed set of machines. It is based on a distributed query processing architecture that evaluates queries expressed using a subset of SQL against a single logical database table. This single logical table is realized using a distributed set of relational databases, each populated by local intrusion detection systems running on Netbait server nodes. For speed, queries in Netbait are processed in parallel by distributing them over dynamically constructed query processing trees built over Tapestry, a distributed object and location routing (DOLR) layer. For effciency, query results are compressed using application-specific aggregation and compact encodings.
The problem of goal-directed, collision-free motion in a complex, unpredictable environment can be solved by tightly integrating high-level deliberative planning with low-level reactive control. This thesis presents two such architectures for a nonholonomic mobile robot. To achieve real-time performance, reactive control capabilities have to be fully realized so that the deliberative planner can be simplified. These architectures are enriched with reactive target reaching and obstacle avoidance modules. Their target reaching modules use indirect-mapping Extended Kohonen Map to provide finer and smoother motion control than direct-mapping methods. While one architecture fuses these modules indirectly via command fusion, the other one couples them directly using cooperative Extended Kohonen Maps, enabling the robot to negotiate unforeseen concave obstacles. The planner for both architectures use a slippery cells technique to decompose the free workspace into fewer cells, thus reducing search time. Any two points in the cell can still be traversed by reactive motion.
This paper presents a new parallel algorithm for the convex hull problem. This algorithm is a parallel adaptation of the Graham Scan Algorithm. The computational model selected for this algorithm is the associative computing model (ASC) which supports massive parallelism through the use of data parallelism and constant time associative search and maximum functions. Also, ASC can be supported on existing SIMD computers. This algorithm requires O(n) space, O(n log n) average cost, and O(n&amp;sup2;) worst case cost. The algorithm has been implemented and tested on random data.
Cybersecurity is an issue of increasing concern since the events of September 11    . Many questions have been raised concerning the security of the Internet and the rest of US&apos;s information infrastructure. This paper begins to examine the issue by analyzing the  Internet&apos;s autonomous system (AS) map. Using the AS map, generic malicious infections are simulated and different defense strategies are considered in a cost effectiveness analysis framework. The results show that protecting the most connected nodes provides significant gains in security and that after the small minority of the most connected nodes are protected there are diminishing returns for further protection.
This report has been  submittedforted 17881   outside of ITC and will probably be copyrighted if accepted for publication. It has been  issued as a Technical Reportfort 17 dissemination of its contents. In view of the transfert of copy right tot outside publisher, its  distribution outside of ITCprior7 publication should be limited to peer communications andspecific1084 4 After outside publication,  material will be available onlyin 1 form authorized by the copyright owner
There are so many kinds of software structures in a very large software product, that it is almost impossible to build a specific visualization tool for each specific need. In such a context, flexibility is very important. In this paper we claim, that producing a specific view on a large software product, should be as simple as using a spreadsheet to produce a new view on an arbitrary set of data. Instead of building visualization tools from scratch, existing components should be reused whenever possible. In particular it should be possible to connect interactively source components (those providing information on software) and visualization components (those displaying graphical views). To support this approach, we have built G  SEE  , a Generic Software Exploration Environment making it possible to visualize virtually any kind of software structures at a very low cost.
This paper presents a statistical framework for assessing wireless systems performance using hierarchical data  mining techniques. We consider WCDMA (wideband code division multiple access) systems with two-branch  STTD (space time transmit diversity) and 1/2 rate convolutional coding (forward error correction codes). Monte  Carlo simulation estimates the bit error probability (BEP) of the system across a wide range of signal-to-noise  ratios (SNRs). A performance database of simulation runs is collected over a targeted space of system configurations.
  The inverted lists strategy is frequently used as an index data structure for very large textual databases. Its implementation and comparative performance has been studied in sequential and parallel applications. In the latter, with relatively few studies, there has been a sort of &quot;which-is-better&quot; discussion about two alternative parallel realizations of the basic data structure and algorithms. We suggest that a mix between the two is actually a better alternative. Depending on the workload generated by the users, the composite inverted lists algorithm we propose in this paper can operate either as a local or global inverted list, or both at the same time.  
Great amount of data under varying intrinsic features is thought of as high dimensional nonlinear  manifold in the observation space. How to analyze the mapping relationship between the high dimensional  manifold and the corresponding intrinsic low dimensional one quantitatively is important to machine  learning and cognitive science. In this paper, we propose SVD (singular value decomposition) based magnification  factors and spread direction for quantitative analyzing the relationship. The result of conducting  experiments on several databases show the advantages of this proposed SVD-based approach in manifold  learning.
Speculative Concurrency Control (SCC) [Best92a] is a new concurrency  control approach especially suited for real-time database applications.
We recall a characterization of hereditary indecomposability originally  obtained by Krasinkiewicz and Minc, and show how it may be used  to give unified constructions of various hereditarily indecomposable continua.
We study the relationship between the number of rounds needed to repeatedly perform a  private computation (i.e. where there are many sets of inputs sequentially given to the players  on which the players must compute a function privately) and the overall randomness needed for  this task. For the xor function, we show that for k sets of inputs, if instead of using totally  fresh (i.e., independent) random bits for each of these k sets of inputs, we re-use the same  ` random bits then we can significantly speedup the round-complexity of each computation  compared to what is achieved by the naive strategy of partitioning the ` random bits between  the k computations.
This thesis proposes the use of a new routing paradigm to enable communication in highly mobile, ad hoc networks, which operate wirelessly in the absence of dedicated master stations or fixed infrastructure. Due to the mobility of the nodes, the network topology changes frequently and unpredictably. We explore the new routing paradigm in the context of inter-vehicle communication. In such highly mobile ad hoc networks, the nodes commonly do not know the identity of their communication partners in advance. Rapid topology changes and scarce bandwidth prevent the nodes from exchanging updates regularly throughout the network. Therefore, we advocate a new routing paradigm that implicitly addresses message destinations based on the current situation of the network. [...]
In this paper, we first focus our attention on the question of how much space remains for performance improvement over current association rule mining algorithms. Our strategy is to compare their performance against an &quot;Oracle algorithm &quot; that knows in advance the identities of all frequent itemsets in the database and only needs to gather their actual supports to complete the mining process. Our experimental results show that current mining algorithms do not perform uniformly well with respect to the Oracle for all database characteristics and support thresholds. In many cases there is a substantial gap between the Oracle&apos;s performance and that of the current mining algorithms. Second, we present a new mining algorithm, called ARMOR, that is constructed by making minimal changes to the Oracle algorithm. ARMOR consistently performs within a factor of two of the Oracle on both real and synthetic datasets over practical ranges of support specifications.
Assuming that watermarking is feasible (say, against a limited  set of attacks of significant interest), current methods use a secret  key to generate and embed a watermark. However, if the same key is  used to watermark different items, then each instance may leak partial  information and it is possible that one may extract the whole secret from  a collection of watermarked items. Thus it will be ideal to derive content  dependent keys, using a perceptual hashing algorithm (with its own secret  key) that is resistant to small changes and otherwise having randomness  and unpredictability properties analogous to cryptographic MACs. The techniques
Only recently, advanced direct volume visualization techniques have  been widely used due to the availability of low cost hardware accelerators;  such techniques have a great potential of use for many applications of the  virtual reality in medicine. We proposed and implemented a low cost system  for interactive and stereoscopic 3D visualization of the full color visible human  dataset. Potential use of the proposed system includes anatomical atlases and  surgical simulators. A prototype of the proposed system is rendering full color  volumes with 256x152x180 in real time (15-20Hz) with stereoscopy.
In this study, we present a new scalable scheme for adapting the transmission  rate of multimedia applications to the congestion level of the network.  The scheme is based on the end-to-end Real Time transport protocol  (RTP). Results achieved through simulations and measurements suggest the  efficiency of the scheme in utilizing the network resources and decreasing  the loss rates. However, with no support from the network, adaptive applications  tend to be unfair towards long distance connections and lead to the  starvation of competing TCP connections.  This work was funded in part by the BMBF (German Ministry of Education and Research) and the DFN (German Research Network).  y  In case of acceptance the paper will be presented by Dorgham Sisalem  1  Sisalem: Fairness of Adaptive Multimedia Applications 1 1 Introduction  The vast majority of multimedia applications used currently in the Internet, such as VIC [14], VAT [13], NEVOT [2] and NEVIT [22] are based on the UDP and RTP trans...
This paper describes the addition of an extra piece of software, a rollback manager, to implement state saving and rollback management for optimistic federates in the High Level Architecture (HLA). This mechanism uses computational reflection techniques to create a rollback manager meta-object that extends the low-level time management services provided by HLA. The main propose of the rollback manager is to relieve the federate from the burden of handling problems related to the federate state saving management and recovery. Some experimental results are shown, to prove the feasibility of the proposed mechanism.
The Immune System (IS) constitutes the defence mechanism of higher level organisms to micro organismic threats: it is a real distributed system providing mechanisms of adaptation to unknown threats through the cooperation of heterogenous entities, and learning capabilities. This paper describes how the Situated Cellular Agents (SCA) model was applied to model the IS. After a brief description of the composing parts and internal mechanisms of the IS, the SCA model will be introduced and exploited to represent them    .
Analysis and modeling of speaker variability, such as gender, accent, age, speech rate, and phones realizations, are important issues in speech recognition. It is known that existing feature representations describing speaker variations can be of very high dimension. In this paper, we introduce two powerful multivariate statistical analysis methods, namely, principal component analysis (PCA) and independent component analysis (ICA), as tools for analysis of such variability and extraction of low dimensional feature representation. Our findings are the following: (1) the first two principal components correspond to the gender and accent, respectively. The result that the second component corresponding to the accent has never been reported before, to the best of our knowledge. (2) It is shown that ICA based features yield better classification performance than PCA ones. Using 2dimensional ICA representation, we achieved about 6.1% and 13.3% error rate in gender and accent classification, respectively, for 980 speakers.
A framework for segmenting multiple objects in an image based on deformable contours is proposed. In this framework, multiple snakes are applied with a new kind of energy called the ffgroup energy.&quot; The group energy is introduced to handle sharing of properties across multiple objects in the image. Our framework allows contours of ffstrong objects&quot; to guide contours of ffweak objects&quot; by utilizing deformable templates. We also automatically generate the necessary weighting parameters for energy minimization. A new approach for multiple snake optimization which is based on dynamic programming is also proposed. We applied our framework to the problem of image analysis of gene expression in microarrays. Comprehensive experiments were performed and comparisons were made between the individual energy based method and the proposed group energy based method. Our results are highly encouraging and have many potential applications in a variety of tracking scenarios.
Microaggregation is a statistical disclosure control technique.
The communicative power of face makes modeling and tracking of the face an active research topic. The new MPEG4  standard includes support not only for natural video and audio, but also for synthetic graphics and sound. Representation and animation control of human faces are defined by deforming a generic facial model. We hereby use this standard as a testbed for our nonrigid motion analysis on the face data.
In this paper we study the properties of the class of head-cycle-free extended disjunctive logic programs (HEDLPs), which includes, as a special case, all nondisjunctive extended logic programs. We show that any propositional HEDLP can be mapped in polynomial time into a propositional theory such that each model of the latter corresponds to an answer set, as defined by stable model semantics, of the former. Using this mapping, we show that many queries over HEDLPs can be determined by solving propositional satisfiability problems. Our mapping has several important implications: It establishes the NP-completeness of this class of disjunctive logic programs; it allows existing algorithms and tractable subsets for the satisfiability   problem to be used in logic programming; it facilitates evaluation of the expressive power of disjunctive logic programs; and it leads to the discovery of useful similarities between stable model semantics and Clark&apos;s predicate completion.  1 Introduction ...
Parallel simulation is expected to speed up simulation run time in a significant way. This paper describes a framework that is used to evaluate the performance of parallel simulation algorithms. The framework&apos;s core is DVSIM, a parallel event-driven VHDL simulator. The framework provides several mechanisms to calculate sensible bases for speed-up calculation. Monitoring tools are employed to observe and to improve the algorithmic performance. A first implementation of DVSIM used a conservative synchronization method, but a Time Warp protocol has recently been completed. Influencing factors for speed-up such as partitioning and mapping methods are discussed. Experience shows that even with conservative synchronization schemes moderate speed-ups can be obtained for larger circuits. The speed-up values are compared to theoretically possible acceleration factors, and the reasons why these ideal maximum speed-up values can in general not be reached are explained. Keywords: Distributed Simul...
In recent years, some papers have tried to bridge the gap between the two  main approaches in credit risk modelling: structural and reduced form models.
This paper provides an overview on the approach of the IST OMEGA  project for the development of correct software for embedded systems based on  the use of UML as modelling language. The main contributions of the project are  the definition of a useful subset of UML and some extensions, a formal dynamic  semantics integrating all notations and a tool set for the validation of models  based on this semantics.
This paper describes a novel system for 3D head tracking under partial occlusion from 2D monocular image sequences. In this system, The Extended Superquadric (ESQ) is used to generate a geometric 3D face model in order to reduce the shape ambiguity. Optical flow is then employed with this model to estimate the 3D rigid motion. To deal with occlusion, a new motion segmentation algorithm using motion residual error analysis is developed. The occluded areas are successfully detected and discarded as noise by the system. Also, accumulation error is heavily reduced by a new post-regularization process based on edge flow. This makes the system more stable over long occlusion image sequences. To show the accuracy, the system is applied on a synthetic occlusion sequence and comparisons with the ground truth are reported. To show the robustness, experiments on long occlusion image sequences, including synthetic and real ones, are reported.
We discuss distributed denial of service attacks in the Internet. We were motivated by the widely known February 2000 distributed attacks on Yahoo!, Amazon.com, CNN.com, and other major Web sites. A denial of service is characterized by an explicit attempt by an attacker to prevent legitimate users from using resources. An attacker may attempt to: &quot;flood&quot; a network and thus reduce a legitimate user&apos;s bandwidth, prevent access to a service, or disrupt service to a specific system or a user. We describe methods and techniques used in denial of service attacks, and we list possible defenses. In our study, we simulate a distributed denial of service attack using ns-2 network simulator. We examine how various queuing algorithms implemented in a network router perform during an attack, and whether legitimate users can obtain desired bandwidth. We find that under persistent denial of service attacks, class based queuing algorithms can guarantee bandwidth for certain  classes of input flows.
This contribution describes the design and performance testing of an Advanced Encryption Standard (AES) compliant encryption chip that delivers 2.29 GB/s of encryption throughput at 56 mw of power consumption. We discuss how the high level reference specification in C is translated into a parallel architecture. Design decisions are motivated from a system level viewpoint. The prototyping setup is discussed.
Complex application examples or scenarios such as site planning  need a lot of information, especially spatially referenced data    in order to find a  best place for a new building or an industrial area and to solve individual planning  tasks such as generating ecological, environmental or traffic reports. Recent  initiatives to geospatial data archives offer access to a wealth of distributed  data covered by the widespread information spectrum of different geodata disciplines  (e.g. environmental data, geologic data, cadastral data, remote sensing  data or socio-demographic data), but offer only basic levels of interactivity and  user assistance. The EU-funded project INVISIP (Information Visualization in  Site Planning, IST-2000-29640) addresses this lack of usability and aims to develop  new concepts and methods to support all involved parties within the different  phases of the multi-step site planning process. This paper describes  INVISIP and introduces GeoCrystal and SuperTable as new information visualization  techniques to support users in this process.
Keyphrases provide a semantic metadata that summarize and characterize documents. Since keyphrases summarize documents very concisely, they can be used as a low-cost measure of similarity between documents, making it possible to group documents by measuring overlap between the keyphrases they are assigned to. In this paper we investigate the usefulness of keyphrases in text categorization (TC). We apply a keyphrase extraction algorithm to the Reuters Corpus Volume I (RCVI) and then we use the keyphrases extracted as features of the TC algorithm.
In recent years Sequential Monte Carlo (SMC) algorithms have been applied to capture the motion of humans. In this paper we apply a SMC algorithm to capture the motion of an articulated chain, e.g., a human arm, and show how the SMC algorithm can be improved in this context by applying auxiliary information. In parallel to a model-based approach we detect skin color blobs in the image as our auxiliary information and find the probabilities of each blob representing the hand. The probabilities of these blobs are used to control the drawing of particles in the SMC algorithm and to correct the predicted particles. The approach is tested against the standard SMC algorithm and we find that our approach improve the standard SMC algorithm.
The trend in high-performance microprocessor design is toward increasing computational power on the chip. At the same time, memory size is increasing but memory speed is not. The result is an imbalance between computation speed and memory speed. This imbalance is leading machine designers to use more complicated memory hierarchies. In turn, programmers are explicitly restructuring codes to perform well on particular memory systems, leading to machine-specific programs. Can we enhance our compiler technology to obviate the need for explicit programming of the memory hierarchy? This paper surveys some recent experiments with compiler techniques designed to address this problem. The results, though only preliminary, show great promise.
The maximum intersection problem for a matroid and a greedoid, given by polynomial-time oracles, is shown NP -hard by expressing the satisfiability of boolean formulas in 3-conjunctive normal form as such an intersection. The corresponding approximation problems are shown NP -hard for certain approximation performance bounds. Moreover, some natural parameterized variants of the problem are shown W [P ]-hard. The results are in contrast with the maximum matroid-matroid intersection which is solvable in polynomial time by an old result of Edmonds. We also prove that it is NP -hard to approximate the weighted greedoid maximization within 2^n^O(1)  where n is the size of the domain of the greedoid.
this paper we demonstrate that this approach can successfully locate ff-helices at moderate resolutions. It remains to be seen if it can do so more eciently than the frequency domain approach, in general or in some signi  cant subclass of problems
This article reports the results of an experiment addressing extrapolation in function learning, in  particular the issue of whether participants can extrapolate in a nonmonotonic manner. Existing models  of function learning, including the extrapolation association model of function learning (EXAM; E. L
Two tasks that entail a certain degree of difficulty when dealing with computers are retrieving and installing new software. So, the need arises to provide users with tools that help them retrieve the necessary software and install it. In the context of mobile computers those tasks acquire a special relevance due to their limited resources (memory, disk space, autonomy, etc.).  In this paper we present a software retrieval service that provides mobile computer users with mechanisms to select and retrieve the selected software in an easy and efficient way. Although this service can be used for fixed computers it puts special emphasis on optimizing the use of battery and wireless communications. The implementation of that software retrieval service is based on agent technology.  1. 
Embedded processors like Intel&apos;s XScale use dynamic branch prediction to improve performance. Due to the presence of context switches, the accuracy of these predictors is reduced because they end up storing prediction histories for several processes. This paper shows that the loss in accuracy can be significant and depends on predictor type and size. Several new schemes are proposed to save and restore the predictor state on context switches in order to improve prediction accuracy. The schemes differ in the amount of information they save and vary in their accuracy improvement. It is shown that even for a small 128 entry skew predictor, 2 - 6% improvement in prediction rate can be achieved (for an average context interval of 100K instructions) for different embedded applications while saving and restoring a minimal amount of state information (less than 32bits) on a context switch.
This paper discusses models of generic mobile services. The goal is  to gain understanding of the challenges in designing, developing and deploying  advanced mobile data services. First, a composition model describing the components  of a generic mobile service and the components relationships is given.
We designed and built a high-capacity neural network based on volume holographic interconnections in a photorefractive crystal. We used this system to implement a Kohonen topological map. We describe and justify our optical setup and present some experimental results of self-organization in the learning database.
In this paper, we prove the existence of the product formula for the spherical functions  on symmetric spaces of noncompact type. To this end, we study the analyticity properties  of the Cartan decomposition and we find a limited Taylor expansion of the abelian factor  in this decomposition.
Many systems gather content from multiple input sources and provide it to multiple output channels. Usually content has to be (partly) generated, and content has to be converted to different formats. In this paper, we will discuss the domain of digital content broadcasting with the Multimedia Home Platform (MHP) as a case study domain. However, there are other domains that require content generation and conversion as well, such as web engineering and content management. As a solution, we will present a generic XML-based architecture for dynamic content generation and conversion. It provides content converters for multiple input and output formats. Content format templates, fragments, and content format builders are alternatives for dynamic content generation. Page templates are used to impose common styles and portal layouts for interdependent content. A Service Abstraction Layer supports service-based integration of different new media platforms.
We show that hereditarily indecomposable spaces can be characterized  by a special instance of the Intermediate Value Theorem in their  ring of continuous functions.
In this chapter we describe the Trajectory Tree, or TTree, algorithm. TTree uses a small set of supplied policies to help solve a Semi-Markov Decision Problem (SMDP). The algorithm uses a learned tree based discretization of the state space as an abstract state description and both user supplied and auto-generated policies as temporally abstract actions. It uses a generative model of the world to sample the transition function for the abstract SMDP defined by those state and temporal abstractions, and then finds a policy for that abstract SMDP. This policy for the abstract SMDP can then be mapped back...
This paper examines a conflation method based on the N-grams approach and evaluates its performance relative to the results achieved by other techniques such as Porter algorithm and successor variety stemming. In addition to that, an alternative way of enhancing the N-grams method, derived from the concept of inverse frequency weighing, is introduced and evaluated. The experimental results generated using standard collections ADI, CISI and Medlars show an improvement over the traditional conflation methods, as well as demonstrate the viability of the introduced inverse frequency multiplier technique.
Geometric straight-line programs [5, 8] can be used to model geometric  constructions and their implicit ambiguities. In this paper we discuss the  complexity of deciding whether two instances of the same geometric straight-line  program are connected by a continuous path, the Complex Reachability Problem.
A novel local scale controlled piecewise linear diffusion for selective smoothing and edge detection is presented. The diffusion stops at the place and time determined by the minimum reliable local scale and a spatial variant, anisotropic local noise estimate. It shows nisotropic, nonlinear diffusion equation using diffusion coefficients/tensors that continuously depend on the gradient is not necessary to achieve sharp, undistorted, stable edge detection across many scales. The new diffusion is anisotropic and asymmetric only at places it needs to be, i.e., at significant edges. It not only does not diffuse across significant edges, but also enhances edges. It advances geometry-driven diffusion because it is a piecewise linear model rather than a full nonlinear model, thus it is simple to implement and analyze, and avoids the difficulties and problems associated with nonlinear diffusion. It advances local scale control by introducing spatial variant, anisotropic local noise estimation, and loca...
Design of modern communication systems is increasingly inefficient using current design methodologies. Significant increases in efficiency, reduction of time to market and improvement in quality can be achieved by adopting a consistent design methodology. Such a design process, based on a single system description resident in a database and integrating all tools used by all design teams is proposed. An implementation of the single system description database and a tool chain based on SystemC is presented. Also shown are the results of processing a real-world wireless communications algorithm through this tool chain.
The problem that we address in this paper is how a mobile robot can plan in order  to arrive at its goal with minimum uncertainty. Traditional motion planning algorithms  often assume that a mobile robot can track its position reliably, however, in real  world situations, reliable localization may not always be feasible. Partially Observable  Markov Decision Processes (POMDPs) provide one way to maximize the certainty of  reaching the goal state, but at the cost of computational intractability for large state  spaces.  The method we propose explicitly models the uncertainty of the robot&apos;s position as  a state variable, and generates trajectories through the augmented pose-uncertainty  space. By minimizing the positional uncertainty at the goal, the robot reduces the  likelihood it becomes lost. We demonstrate experimentally that coastal navigation  reduces the uncertainty at the goal, especially with degraded localization.   
This paper presents a novel method for extracting information from  collections of Web pages across different sites. Our method uses a standard  wrapper induction algorithm and exploits named entity information. We  introduce the idea of post-processing the extraction results for resolving  ambiguous facts and improve the overall extraction performance. Postprocessing  involves the exploitation of two additional sources of information:  fact transition probabilities, based on a trained bigram model, and confidence  probabilities, estimated for each fact by the wrapper induction system. A  multiplicative model that is based on the product of those two probabilities is  also considered for post-processing. Experiments were conducted on pages  describing laptop products, collected from many different sites and in four  different languages. The results highlight the effectiveness of our approach.
Retargetable C compilers are key tools for efficient architecture exploration for embedded processors. In this paper we describe a novel approach to retargetable compilation based on LISA, an industrial processor modeling language for efficient application-specific instruction set processor (ASIP) design. In order to circumvent the well-known trade-off between flexibility and code quality in retargetable compilation, we propose a user-guided, semi-automatic methodology that in turn builds on a powerful existing C compiler design platform. Our approach allows to include generated C compilers into the ASIP architecture exploration loop at an early stage, thereby allowing for a more efficient design process and avoiding application /architecture mismatches. We present the corresponding methodology and tool suite and provide experimental data for two real-life embedded processors that prove the feasibility of the approach.
In this paper we propose a metatheory, MT which represents the computation which  implements its object theory, OT, and, in particular, the computation which implements  deduction in OT. To emphasize this fact we say that MT is a metatheory of a mechanized  object theory. MT has some &quot;unusual&quot; properties, e.g. it explicitly represents  failure in the application of inference rules, and the fact that large amounts of the code  implementing OT are partial, i.e. they work only for a limited class of inputs. These  properties allow us to use MT to express and prove tactics, i.e. expressions which specify  how to compose possibly failing applications of inference rules, to interpret them procedurally  to assert theorems in OT, to compile them into the system implementation  code, and, finally, to generate MT automatically from the system code. The definition  of MT is part of a larger project which aims at the implementation of self-reflective  systems, i.e. systems which are able to intros...
A key difficulty in the design of multi-agent robotic systems is the size and complexity of the space of possible designs. In order to make principled design decisions, an understanding of the many possible system configurations is essential. To this end, we present a taxonomy that classifies multiagent systems according to communication, computational and other capabilities. We survey existing efforts involving multi-agent systems according to their positions in the taxonomy. We also present additional results concerning multi-agent systems, with the dual purposes of illustrating the usefulness of the taxonomy in simplifying discourse about robot collective properties, and also demonstrating that a collective can be demonstrably more powerful than a single unit of the collective.
This paper explores an approach for extracting scene text from a sequence of images with relative motion between the camera and the scene. It is assumed that the scene text lies on planar surfaces, whereas the other features are likely to be at random depths or undergoing independent motion. The motion model parameters of these planar surfaces are estimated using gradient based methods, and multiple motion segmentation. The equations of the planar surfaces, as well as the camera motion parameters are extracted by combining the motion models of multiple planar surfaces. This approach is expected to improve the reliability and robustness of the estimates, which are used to perform perspective correction on the individual surfaces. Perspective correction can lead to improvement in OCR performance. This work could be useful for detecting road signs and billboards from a moving vehicle.
This paper comments some aspects of the project &quot;Intelligent Sweet Home&quot; that is primarily focused on development of control strategies and human-machine interaction in the smart house for assisting aged people and persons with disabilities. In our study, we propose a conception for control of the robot and home appliances by predefined hand gestures remotely sensed by ceilingmounted CCD cameras. Pointing at the equipments by his/her hand, user can select the device that should be controlled. Then, by gesturing and changing the hand posture, user sets the operation that should be executed. The approach complements the inconvenience of the conventional remote controllers giving additional freedom to persons with movement deficits and people without disabilities.
In this paper, two local approximation techniques for prediction are explored. First, a pattern recognition technique called Pattern Modelling and Recognition System (PMRS) is explored for making multiple forecasts. We then describe a single nearest neighbour based prediction system for multiple forecasting. Both models are based on using local neighbourhoods in data for making prediction. Multiple prediction profiles are generated and analysed for four time-series data. These multiple forecasts define a predicted behavioural profile of given univariate systems. The predicted profiles are compared against the actual behaviour of the studied systems on a number of proposed error measures. The results show that local approximation used in the two models for making multiple forecasts is an important method of profiling the true behaviour of univariate systems.  Keywords  Pattern Recognition and Modelling System Multiple Forecasting Behaviour Profiling  Nearest Neighbours Time-Series Predi...
This paper considers the problem of mining closed frequent itemsets over a sliding window using limited memory space. We design a synopsis data structure to monitor transactions in the sliding window so that we can output the current closed frequent itemsets at any time. Due to time and memory constraints, the synopsis data structure cannot monitor all possible itemsets. However, monitoring only frequent itemsets will make it impossible to detect new itemsets when they become frequent. In this paper, we introduce a compact data structure, the closed enumeration tree (CET), to maintain a dynamically selected set of itemsets over a sliding-window. The selected itemsets consist of a boundary between closed frequent itemsets and the rest of the itemsets. Concept drifts in a data stream are reflected by boundary movements in the CET. In other words, a status change of any itemset (e.g., from non-frequent to frequent) must occur through the boundary. Because the boundary is relatively stable, the cost of mining closed frequent itemsets over a sliding window is dramatically reduced to that of mining transactions that can possibly cause boundary movements in the CET. Our experiments show that our algorithm performs much better than previous approaches.
A (directed) network of people connected by ratings or trust scores, and a model for propagating those trust scores, is a fundamental building block in many of today&apos;s most successful e-commerce and recommendation systems. We develop a framework of trust propagation schemes, each of which may be appropriate in certain circumstances, and evaluate the schemes on a large trust network consisting of 800K trust scores expressed among 130K people. We show that a small number of expressed trusts/distrust per individual allows us to predict trust between any two people in the system with high accuracy. Our work appears to be the first to incorporate distrust in a computational trust propagation setting.
A competitive learning network, called Multi-Module Minimization  (MMM) Neural Network, is proposed for unsupervised classification. Our objective 
This report investigates how reliable and useful the four schemes described above are for a sampling of front pages of Danish web sites, and whether the reliability and usefulness of the indicators is randomly distributed or depends on the server. 4 2. Methodology  In our experiment, we sampled a cross-section of the front pages of Danish web sites every other night over a period of one month. For each page, we recorded the date, the Etag, the size, and an MD5 sum of the body of the page. The list of Danish domains was generously supplied by DK Hostmaster, and contains 465,374 sites in the .dk domain. We harvested only the front page of each domain by requesting http://www.&lt;domain&gt;.dk/. We used Wget version 1.8.2 with the parameters --S --r --l 0 --t 1 --T 30. The contents of each page was stored in a file and an MD5 checksum of the contents was found using md5sum. The headers, size of body and MD5 sum of body were stored for further processing. After each harvest, the resulting files were processed with a Perl script to extract the name of the domain, the size, the MD5 sum, the Etag header, and the Last-Modified-Date header. In case of redirects, only the data for the last page in the redirect chain was used, but stored under the name of the domain. Missing headers were marked in the file as well. Once a sufficient amount of harvesting had been done, the processed data was compared to find changes in Etags, datestamps and MD5 sums. We checked each domain for changes between successful downloads, and for each such download we recorded whether the contents (not the headers) had changed since the last successful download, whether datestamps and Etags were present, and whether or not they had changed. Some servers sent out Etags or datestamps on some but not all visits. Fo...
In this paper we propose a new method for learning a combination of estimators. Classically, committee networks are constructed after training the networks independently from each other. Here we present a learning strategy where the training is done in a coupled way. We illustrate that combining parameterized kernel methods with output coupling and use of a synchronization set of data points leads to an improved generalization. Examples are given on artificial and real life data sets.
We use properties of observational equivalence for a probabilistic  process calculus to prove an authentication property of a cryptographic  protocol. The process calculus is a form of  -calculus, with  probabilistic scheduling instead of nondeterminism, over a term language  that captures probabilistic polynomial time. The operational semantics  of this calculus gives priority to communication over private channels, so  that the presence of private communication does not affect the observable  probability of visible actions. Our definition of observational equivalence  involves asymptotic comparison of uniform process families, only requiring  equivalence to within vanishing error probabilities. This definition  differs from previous notions of probabilistic process equivalence that require  equal probabilities for corresponding actions; asymptotics fit our  intended application and make equivalence transitive, thereby justifying  the use of the term &quot;equivalence.&quot; Our security proof uses a series of lemmas  about probabilistic observational equivalence that may well prove  useful for establishing correctness of other cryptographic protocols.
This paper gives a general presentation of the project: the baseline technologies, an overview of the system, the case studies, and related work
Abstract: A means to define multiple activity instantiation is an important feature of a business process modelling language. In this paper we suggest to extend BPEL4WS with structured activities for multiple instantiation. In particular, we propose to extend BPEL4WS with a collect and a broadcast activity to model multiple instance behavior as well as array data structures to handle messages of multiple parties that act according to the same role. 1
Collecting and analyzing real-time data from multiple sources requires processes to continuously monitor and respond to a wide variety of events. Such processes are well suited to execution by intelligent agents. Architectures for such agents need to be general enough to support experimentation with various analysis techniques but must also implement enough functionality to provide a solid back end for data collection, storage, and reuse. In this paper, we present the architecture of Calvin, an research grade agent system for supporting and analyzing users&apos; document access. Calvin provides specific applications for collecting, storing, and retrieving data to be used for information retrieval, but its extensible object oriented implementation of resource types makes the architecture sufficiently flexible to be useful in multiple task domains. In addition, the architecture provides the ability to capture and &quot;replay&quot; data streams during processing, enabling the automatic creation of data testbeds for use in experiments comparing alternative analysis algorithms.
This paper addresses these problems using a functional language transformation, illustrated in the context of a particular application. We show that the overhead introduced by the transformation is relatively small, but the benefit derived is substantial, since the functional programming discipline enforces an implementation-independent definition of core parallel requirements, which can then be mapped onto a broad set of parallel architectures, ranging from shared and distributed memory conventional multiprocessors to direct hardware implementations constructed using silicon compilers and FPGA technology
Given Q 2 R[X1 ; : : : ; Xk ] with deg(Q)  d; we give an algorithm that outputs a semi-algebraic description for each of the semi-algebraically connected components of  Z(Q) ae R  k  : The complexity of the algorithm as well as the size of the output are bounded by d  O(k  3  )  :  More generally, given any semi-algebraic set S defined by a quantifier-free formula involving a family of polynomials, P = fP1 ; : : : ; Psg ae R[X1 ; : : : ; Xk ] whose degrees are at most d; we give an algorithm that outputs a semi-algebraic description for each of the semialgebraically connected components of S: The complexity of the algorithm as well as the size of the output is bounded by s  k+1  d  O(k  3  )  : This improves the previously best known bound of (sd)  k O(1)  for this problem due to Canny, Grigor&apos;ev, Vorobjov and Heintz, Roy and Solern`o [9, 14].  1 Introduction  Let R be a real closed field. A semi-algebraic set in R  k  is the set of points which satisfy a boolean combination of polynom...
. The following paper describes the design principles of decision  making in the Karlruhe Brainstormers team that participated in  the RoboCup Simulator League in Stockholm 1999. It is based on two  basic ingredigents: the priority - probability - quality (PPQ) concept is a  hybrid rule-based/ learning approach for tactical decisons, whereas the  denition of goal-orientented moves allows to apply neural network based  reinforcement learning techniques on the lower level.  1 Introduction  The main interest behind the Karlsruhe Brainstormer&apos;s eort in the robocup soccer domain is to develop and to apply machine learning techniques in complex domains. Especially, we are interested in Reinforcement Learning methods, where the training signal is only given in terms of success or failure. So our nal goal is a learning system, where we only plug in &apos;Win the match&apos; - and our agents learn to generate the appropriate behaviour. Unfortunately, even from very optimistic complexity estimations it...
This research outlines the need for a real-time, a ship borne radar facility SHIRA for the tracking of nearby oil spills during oil containment and cleaning operations at sea with high quality imagery. SHIRA is imaging digital X-band radar, developed by TNO for the measurement of oceanographic features. It was especially adapted for the imaging of oil slicks by implementing a number of improvements, based on the outcome of a previous oil detection experiment. The main aspects that make SHIRA suitable for the current purpose are its high sensitivity and dynamic range, and its ability to integrate a large number of images of the same scene, after correction for platform motions. Quantitative comparison has been made between various filters, which are able to reduce variance in homogeneous areas, preserve edges and lines, suppress point scatter, and preserve spatial variability, while avoiding artefacts. Gamma Filter yields the best smoothing and despeckling. Moreover, it seems to keep the edges without blurring the minimum. Eight textures are applied, based on the cooccurrence matrix. These textures include mean, variance, homogeneity, contrast, dissimilarity, entropy, second moment, and correlation. Co-occurrence measures use a gray-tone spatial dependence matrix to calculate texture values. This is a matrix of relative frequencies with which pixel values occur in two neighbouring processing windows separated by a specified distance and direction. It shows the number of occurrences of the relationship between a pixel and its specified neighbour. The use of the correlation texture implies a scaling of the axes so that the features receive a unit variance. It prevents certain features from dominating the image because of their large value. Correlation texture performs best...
We study the on-line call admission problem in optical networks. We present a general  technique that allows us to reduce the problem of call admission and wavelength selection  to the call admission problem. We then give randomized algorithms with logarithmic competitive  ratios for specific topologies in switchless and reconfigurable optical networks. We  conclude by considering full duplex communications.
MONA implements an effcient decision procedure for the  weak second-order logic WS1S, and has already been applied in many  non-trivial problems. Among these, we follow on from the work of Smith  and Klarlund on the verification of a sliding-window protocol. This paper  extends the scope of MONA to the verification of time-dependent  protocols. We present Discrete Timed Automata (DTA) as a suitable  formalism to specify and verify such protocols. In this paper our case  study will be the specification and verification of a multimedia stream.
Given two geographic databases, a fusion algorithm  should produce all pairs of corresponding  objects (i.e., objects that represent the  same real-world entity). Four fusion algorithms,  which only use locations of objects,  are described and their performance is measured  in terms of recall and precision. These  algorithms are designed to work even when locations  are imprecise and each database represents  only some of the real-world entities.
The core idea of the Semantic Web is to make information accessible to human and software  agents on a semantic basis. Hence, web sites may feed directly from the Semantic Web exploiting  the underlying structures for human and machine access. We have developed a generic approach  for developing semantic portals, viz. SEAL (SEmantic portAL), that exploits semantics for providing  and accessing information at a portal as well as constructing and maintaining the portal.  In this paper, we discuss the role that semantic structures make for establishing communication  between different agents in general. We elaborate on a number of intelligent means that make  semantic web sites accessible from the outside, viz. semantics-based browsing, semantic querying  and querying with semantic similarity, semantic personalization, and machine access to semantic  information at a semantic portal. As a case study we refer to the AIFB web site --- a place that is  increasingly driven by Semantic Web tec...
We present a new architecture to realize a fully programmable rank order  filter (ROF), based on Capacitive Threshold Logic (CTL) gates. Variants  of ROFs, especially median filters, are widely used in digital signal and  image/video processing and image enhancement. The CTL realization of  the majority gates used in the ROF architecture allows the filter rank and  the window size to be user-programmable, using a much smaller silicon area.
This paper proposes a declarative paradigm in which parallelism is implicit and machine-independent, and the programs so developed are intrinsically parallel. This paradigm is obtained by generalizing the notion of rewriting to make it more widely applicable and capable of expressing not only functional computations but also a wide variety of parallel computations that are highly nonfunctional in nature. The generalization in question is provided by rewriting logic, a logic of change in which the states of a system are understood as algebraically axiomatized data structures, and the basic local changes that can concurrently occur in a system are axiomatized as rewrite rules that correspond to local patterns that, when present in the state of a system, can change into other patterns. Simple Maude, a carefully designed sublanguage of rewriting logic supporting three types of rewriting -- term, graph, and object-oriented --, is then proposed as a machine-independent parallel programming ...
We describe a new, practical, constructive method for solving the well-known conflict-free scheduling problem for the locally sequential, globally parallel (LSGP) case of systolic array synthesis. Previous solutions have an important practical disadvantage. Here we provide a closed form solution that enables the enumeration of all conflict-free schedules. The second part of the paper discusses reduction of the cost of hardware whose function is to control the flow of data, enable or disable functional units, and generate memory addresses. We present a new technique for controlling the complexity of these housekeeping functions in a systolic array.
We present a gesture-based user interface to Free-Form Deformation (FFD). Traditional interfaces for FFD require the manipulation of individual points in a lattice of control vertices, a process which is both time-consuming and error-prone. In our system, the user can bend, twist, and stretch/squash the model as if it were a solid piece of clay without being unduly burdened by the mathematical details of FFD. We provide the user with a small but powerful set ofgesture-based &quot;ink stroke&quot; commands that are invoked simply by drawing them on the screen. The system automatically infers the user&apos;s intention from the stroke and deforms the model without any vertex-specific input from the user. Both the stroke recognition and FFD algorithms are executed in real-time on a standard PC.
Scheduling sports competitions is often a difficult task because it is usually very difficult to construct schedules that are considered to be fair by all competitors. In addition, the schedules normally need to satisfy a considerably large number of additional requirements and constraints. Most of the sports scheduling problems that have been tackled in the literature refer to competitions in which a venue is associated with each competitor (home ground). In these cases, selecting the venue in which each tie will be played is not an issue because this is defined by the status, home or away, of the competitors. Scheduling problems in most sports leagues (football, baseball, rugby, cricket, etc.) fall into this category because each team has its own venue. However, many other sports competitions take place on a set of venues that are neutral to all competitors. This is the case in some international competitions (such as the football world cup, and the Wimbledon tennis tournament) and in some recreational leagues. In these cases, choosing the venue to play each tie is part of the scheduling process and this often makes the problem more difficult to solve. Here, we propose the application of heuristic methods for constructing schedules for this type of sports competition and also the use of metaheuristics for improving the quality of a given schedule. Initial experiments demonstrate the promise of these approaches.
After the emergence of numerous Internet streaming applications, rate-distortion (R-D) modeling of scalable video encoders has become an important issue. In this paper, we examine the performance of existing R-D models in scalable coders by using the example of MPEG-4 FGS and PFGS, and propose a novel R-D model based on approximation theory. Experimental results demonstrate that the proposed model is very accurate and significantly outperforms those in previous work.
Model Predictive Control or MPC can provide robust control for processes with variable gain and dynamics, multivariable interaction, measured loads and unmeasured disturbances. In this paper a novel approach for the implementation of Nonlinear MPC is proposed using Genetic Algorithms (GAs). The proposed method formulates the MPC as an optimization problem and genetic algorithms is used in the optimization process. Application to two types of Nonlinear models namely Hammerstein and Wiener Models is studied and the simulation results are shown for the case of two chemical processes to demonstrate the performance of the proposed scheme.
We give fast filtering algorithms to search for a 2-dimensional pattern in a 2-dimensional text allowing any rotation of the pattern. We consider the cases of exact and approximate matching under several matching models, improving the previous results. For a text of size n &amp;times; n characters and a pattern of size m &amp;times; m characters, the exact matching takes average time O(n ), which is optimal. If we allow k mismatches of characters, then our best algorithm achieves k log m=m ) average time, for reasonable k values. For large k, we obtain an O(n log m=m) average time algorithm. We generalize the algorithms for the matching model where the sum of absolute differences between characters is at most k. Finally, we show how to make the algorithms optimal in the worst case, achieving the lower bound ).
For a multiplexer fed with a large number of sources we derive conditions under which a single source can be substituted for a given subset of the sources while preserving the buffer overflow probability and the dominant time scales of buffer overflows. This equivalence is stronger than simple effective bandwidth equality and takes into account the context in which multiplexing takes place. This allows a substitution to be made for arbitrarily large proportions of the traffic without changing the operating point of the multiplexer as experienced by the rest of the traffic. It corresponds to defining a single source which is equivalent in a sense &quot;local&quot; to a given context, rather than equivalent in a sense which is &quot;universal&quot; to all contexts. The proposed methodology does not rely on traffic models and obtains the necessary information from the actual traffic traces. We study the case of fractional Brownian motion as a single source substitute and provide theoretical and experimental ...
This paper reports on a pragmatic query language for Racer. The abstract syntax  and semantics of this query language is defined. Next, the practical relevance of  this query language is shown, applying the query answering algorithms to the  problem of consistency maintenance between object-oriented design models.
The semidefinite programming (SDP) formulation of the Lovász theta number (  G) does not give only one of the best polynomial bounds on the chromatic number of a graph (G) but also leads to a heuristic for the graph coloring problem. Karger, Motwani and Sudan have proved the best known worst case bound for the number of the colors needed by such a heuristic. Their ideas can be employed to produce a recursive heuristic based on the Lovász theta number or its strengthenings. Those recursive heuristics can benefit from the standard combinatorial tricks for the graph coloring.
This paper deals with the validation of topological maps of an environmentby  an active agent ffsuch as a mobile robotff, and the localization of an agent in a given  map. The agent is assumed to have neither compass nor other instruments for measuring  orientation or distance, and, therefore, no associated metrics. The topological  maps considered are similar to conventional graphs. The robot is assumed to have  enough sensory capability to traverse graph edges autonomously, recognize when it  has reachedavertex, and enumerate edges incident upon the currentvertex, locating  them relative to the edge via whichitentered the currentvertex. In addition, the  robot has access to a set of visually detectable, portable, distinct markers. We present  algorithms, along with worst case complexity bounds and experimental results for  representative classes of graphs for two fundamental problems.  The ffrst problem is the validation problem: if the robot is given an input map  and its current posit...
In this paper, we compare the effects of multiple motion in spatial and spectral representation of an image sequence. We describe multiple motion in both domains and establish a comparison regarding their inir8xfl properties when discretized. Though the spectral model provides us withan explicit description of both occlusion an tranion8qk( itturn out that itsresolution is very limited. We show that the spatialdomain represenkU by the spatio-temporal derivatives has superiorresolution propertiesan is thus more appropriate for the treatmen ofocclusion Wepresen an algorithm which basedon an ined8E estimate of the ne8(x of motion uses the shift-anqkTUx)8n techn-an to localize occlusion bounionx an to track their movement in occlusion sequenn8 The same technique is used to distin8qxfl occlusion from tranEOT8qxO an to decompose tranose8kkfl scen inn multi-layers.
Motivated by (natural as well as formal) language analysis, we introduce a special kind of the linear bounded automaton -- so called restarting automaton (R-automaton). Its computation proceeds in certain cycles; in each cycle, a (bounded) subsequence of the input word is removed and the computation restarts on the arising shorter word. We consider both nondeterministic and deterministic versions and introduce a natural property of monotonicity. The main result illustrating usefulness of the introduced notions is an elegant characterization of deterministic context-free languages -- by deterministic monotonic R-automata. We also show...
In the early stages of the design process of aerospace vehicles, the search for optimal configurations encompasses a broad range of possibilities, and the use of local optimization tools may risk missing the best designs. Therefore, global optimization methods are attractive for the early design stage. Unfortunately, global design optimization usually requires the evaluation of a very large number of designs, a formidable computational challenge. The present work uses a highly effective Lipschitzian global optimization algorithm in the multidisciplinary optimization of a High Speed Civil Transport (HSCT). The use of massively parallel computers to handle this computational challenge is also demonstrated. A variety of load balancing methods are evaluated to assess the utilization of the computer nodes.
... configuration, however the one which dominates Java applications is that of security restrictions. Because an application may be provided different capabilities by different users, it becomes difficult to write &quot;build-once, run-anywhere&quot; applications. Insisting that all security sensitive applications execute within controlled or restricted environments may limit the types of application which can be built. Therefore, in this paper we shall describe how we have constructed a configuration infrastructure in Java which allows applications to dynamically adapt themselves to the types of security restrictions that exist when they are executed. Because the system does not change the language it is portable across Java implementations. We shall also describe how we have used this system to build a toolkit for the construction of electronic commerce applications, which allow atomic transactions to span Web browsers and servers.
Theoretical analysis of the Web graph is often used to improve the efficiency of search engines. The PageRank algorithm, proposed by [5], is used by the Google search engine [4] to improve the results of the queries.
In this paper, we prove that formally real fields satisfying some explicit conditions  on the v-invariant verify a Lam&apos;s conjecture concerning signatures of quadratic  forms. In particular, it gives a short proof of a Marshall&apos;s conjecture for Pythagorean  fields.
The fill unit is the structure which collects blocks of instructions and combines them into multi-block segments for storage in a trace cache. In this paper, we expand the role of the fill unit to include four dynamic optimizations: (1) Register move instructions are explicitly marked, enabling them to be executed within the decode logic. (2) Immediate values of dependent instructions are combined, if possible, which removes a step in the dependency chain. (3) Dependent pairs of shift and add instructions are combined into scaled add instructions. (4) Instructions are arranged within the trace segment to minimize the impact of the latency through the operand bypass network. Together, these dynamic trace optimizations improve performance on the SPECint95 benchmarks by more than 17% and over all the benchmarks studied by slightly more than 18%.
When applying unsupervised learning techniques like ICA or temporal decorrelation for BSS, a key question is whether the discovered projections are reliable. In other words: can we give error bars or can we assess the quality of our separation ? We use resampling methods to tackle these questions and show experimentally that our proposed variance estimations are strongly correlated to the separation error. We demonstrate that this reliability estimation can be used to choose an appropriate ICA-model, to enhance significantly the separation performance, and, most important, to mark the components that can really have a physical meaning. An application to data from an MEG    -experiment underlines the usefulness of our approach.
We present a new method for detecting the interface, or edge, structure present in diffusion MRI. Interface detection is an important first step for applications including segmentation and registration. Additionally, due to the higher dimensionality of tensor data, humans are visually unable to detect edges as easily as in scalar data, so edge detection has potential applications in diffusion tensor visualization. Our method employs the computer vision techniques of local structure filtering and normalized convolution. We detect the edges in the tensor field by calculating a generalized local structure tensor, based on the sum of the outer products of the gradients of the tensor components. The local structure tensor provides a rotationally invariant description of edge orientation, and its shape after local averaging describes the type of edge. We demonstrate the ability to detect not only edges caused by differences in tensor magnitude, but also edges between regions of different tensor shape. We demonstrate the method&apos;s performance on synthetic data, on major fiber tract boundaries, and in one gray matter region.
An asymptotic expansion is developed for the joint density of the sum and maximum of an  i.i.d. sequence when the parent distribution is in the domain of extreme value attraction of the Gumbel distribution. Previous results by Chow and Teugels, extended by Anderson and Turkman, show that in this situation, the normalized sum and normalized maximum of the sample converge to independent normal and Gumbel distributions, but they have not characterized the rate of convergence. The present development proceeds via three technical propositions. The first extends previous results by Cohen and by Smith to derive the rate of convergence of the density of the sample maximum to a limiting Gumbel density. The second technical proposition is a conditional Edgeworth expansion for the sum given the maximum. The third concerns the expansion of conditional means and variances. By combining these propositions, a leading-term expansion is developed for the dependence between the sum and the maximum, and uniform convergence is proved over an expanding sequence of subsets of the plane. Simulations allow us to assess the practical applicability of the results. They show that for moderate sample sizes, the sum and the maximum are far from being independent, but that the leading-term asymptotic expansion is a substantial improvement over independence of the two random variables.  
HSF-SPIN is a Promela model checker based on heuristic search strategies. It utilizes heuristic estimates in order to direct the search for finding software bugs in concurrent systems. As a consequence, HSF-SPIN is able to find shorter trails than blind depth-first search.
Its copyright notice follows below: Copyright 1997 Springer-Verlag This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting, reproduction on microfilms or in any other way, and storage in data banks. Duplication of this publication in its current version, and permission for use must always be obtained from Springer-Verlag. Violations are liable for prosecution under the German Copyright Law. The position papers are copyrighted by their authors. All rights are reserved.
Most automatic facial expression analysis (AFEA) systems attempt to recognize facial expressions from data collected in a highly controlled environment with very high resolution frontal faces ( face regions greater than 200 x 200 pixels ). However, in real environments, the face image is often in lower resolution and with head motion. It is unclear that the performance of AFEA systems for low resolution face images. The general approach to AFEA consists of 3 steps: face acquisition, facial feature extraction, and facial expression recognition. This paper explores the effects of different image resolutions for each step of facial expression analysis. The different approaches are compared for face detection, face data extraction and expression recognition. A total of five different resolutions of the head region are studied (288x384, 144x192, 72x96, 36x48, and 18Xx24) based on a widely used public database [16]. The lower resolution images are down-sampled from the originals.
Previous proposals for soft-error tolerance have called for redundantly executing a program as two concurrent threads on a superscalar microarchitecture. In a balanced superscalar design, the extra workload from redundant execution induces a severe performance penalty due to increased contention for resources throughout the datapath. This paper identifies and analyzes four key factors that affect the performance of redundant execution, namely 1) issue bandwidth and functional unit contention, 2) issue queue and reorder buffer capacity contention, 3) decode and retirement bandwidth contention, and 4) coupling between redundant threads&apos; dynamic resource requirements. Based on this analysis, we propose the SHREC microarchitecture for asymmetric and staggered redundant execution. This microarchitecture addresses the four factors in an integrated design without requiring prohibitive additional hardware resources. In comparison to conventional single-threaded execution on a state-ofthe -art superscalar microarchitecture with comparable cost, SHREC reduces the average performance penalty to within 4% on integer and 15% on floating-point SPEC2K benchmarks by sharing resources more efficiently between the redundant threads.
Using a method initially developed by George Young (1819-1889), Arthur Cayley (18211895) , and later by George Watson (1886-1965), an explicit quartic is constructed to enable the solution in radicals of a quintic when it is a solvable equation. Not one, but two sextic resolvents are derived which are important to forming the coefficients of this quartic. Certain difficulties and their solutions as well as a novel consequence to the method are also addressed in this paper. Mathematics Subject Classification. Primary: 12E12; Secondary: 12F10. It was the year 450 AH (c. 11   century AD). The poet-mathematician bent over the sand and scribbled some words. In the distance, a caravan of merchants and their camels were trudging on the dirt road going to the great city of Samarkand, but the poet paid them no heed. He was looking at the words he had written which modern mathematicians would understand as the cubic equation 0 32 20   = + + x x . It was just yesterday that the poet solved a similar equation using conic sections. He frowned, as a thought occurred to him, and he made several scribbles. The equation became the fifth degree equation 0 32 20   = + + x x . The poet stared at it for a moment. It was probably impossible to solve, he thought, as there were only the three dimensions of length, breath, and width. As he continued to ponder over the matter, an idea for a poem came to him. He erased the equation, and started to write on the sand, &quot;The Moving finger writes, and having writ...&quot; Little did he know that it would take almost a thousand years before such equations could be solved. I. 
This paper proposes a method of  nding a discriminative linear transformation that enhances the data&apos;s degree of conformance to the compactness hypothesis and its inverse. The problem formulation relies on inter-observation distances only, which is shown to improve non-parametric and non-linear classi  er perfromance on benchmark and real-world data sets. The proposed approach is suitable for both binary and multiple-category classi  cation problems, and can be applied as a dimensionality reduction technique. In the latter case, the number of necessary discriminative dimensions can be determined exactly. The sought transformation is found as a solution to an optimization problem using iterative majorization.
Neurofuzzy approaches for predicting financial time series are investigated and shown to perform well in the context of various trading strategies involving stocks and options. The horizon of prediction is typically a few days and trading strategies are examined using historical data. Two methodologies are presented wherein neural predictors are used to anticipate the general behavior of financial indexes (moving up, down,or  staying constant) in the context of stocks and options trading. The methodologies are tested with actual financial data and show considerable promise as a decision making and planning tool.
The Minimum Cost Multicommodity Flow problem plays a central role in today&apos;s operations research theory with applications ranging from transportation and logistics to telecommunications network routing. In this paper, we introduce a novel Lagrangianrelaxation technique, which, given an initial feasible solution, can solve the minimum cost multicommodity flow problem as a sequence of single-commodity flow problems. Our methodology is best suited for OSPF traffic engineering, because it can rapidly improve a given path set towards approximate optimality while simultaneously provides the link weights, which implement the paths as shortest paths.
We present a rigorous but transparent semantics definition of the SpecC language that covers the execution of SpecC behaviors and their interaction with the kernel process. The semantics include wait, waitfor, par,andtry statements as they are introduced in SpecC. We present our definition in form of distributed Abstract State Machine (ASM) rules strictly following the lines of the SpecC Language Reference Manual [4]. We mainly see our formal semantics in three application areas. First, it is a concise, unambiguous description for documentation and standardization. Second, it applies as a high--level, pseudo code--oriented specification for the implementation of a SpecC simulator. Finally, it is a first step for SpecC synthesis in order to identify similar concepts with other languages like VHDL and SystemC for the definition of common patterns and language subsets.
In this paper we give a robust logical and computational characterisation of peer-to-peer database systems. We first define a precise model-theoretic semantics of a peer-to-peer system, which allows for local inconsistency handling. We then characterise the general computational properties for the problem of answering queries to such a peer-topeer system. Finally, we devise tight complexity bounds and distributed procedures for the problem of answering queries in few relevant special cases.
This paper presents a secure Generalized Vickrey Auction  (GVA) scheme that does not require third-party servers, i.e., the scheme  is executed only by an auctioneer and bidders. Combinatorial auctions,  in which multiple goods are sold simultaneously, have recently attracted  considerable attention. The GVA can handle combinatorial auctions and  has good theoretical characteristics such as incentive compatibility and  Pareto effciency.
This paper describes the Visible Human (VH) part of the Trans-Pacific Demonstrations of the G7 Information Society-Global Interoperability for Broadband Network (GIBN) Projects. Aiming at a world-wide Visible Human Anatomical Co-laboratory, an application (VHP Viewer) was developed, which was used for data transmission testing (Trans-Pacific Demonstration of Visible Human) through broadband satellite links between the US and Japan. The demonstration includes (1) remote VH database access and (2) network multi-parallel computing access. It is shown that wide-area database access and high-speed multi-parallel computing could be effectively demonstrated via broadband satellite networks by circumventing a large time-delay by using the Mentat SkyX Gateway system and Personal File System (PFS). Elements of the demonstration verified here could be also applied to distance education and telemedicine as well as a postgenome project.
This paper deals with techniques to detect abrupt scene transitions when random brightness variations (flicker) are present.
It is increasingly common for users to interact with the web using a number of different aliases. This trend is a doubleedged sword. On one hand, it is a fundamental building block in approaches to online privacy. On the other hand, there are economic and social consequences to allowing each user an arbitrary number of free aliases. Thus, there is great interest in understanding the fundamental issues in obscuring the identities behind aliases.
Augmented Reality has now progressed to the point where real-time applications are being considered and needed. At the same time it is important that synthetic elements are rendered and aligned in the scene in an accurate and visually acceptable way. In order to address these issues a real-time, robust and efficient 3D model-based tracking algorithm is proposed for a &apos;video see through&apos; monocular vision system. The tracking of objects in the scene amounts to calculating the pose between the camera and the objects. Virtual objects can then be projected into the scene using the pose. Here, non-linear pose computation is formulated by means of a virtual visual servoing approach. In this context, the derivation of point-to-curves interaction matrices are given for different features including lines, circles, cylinders and spheres. A local moving edges tracker is used in order to provide real-time tracking of points normal to the object contours. A method is proposed for combining local position uncertainty and global pose uncertainty in an efficient and accurate way by propagating uncertainty. Robustness is obtained by integrating a M-estimator into the visual control law via an iteratively re-weighted least squares implementation. The method presented in this paper has been validated on several complex image sequences including outdoor environments. Results show the method to be robust to occlusion, changes in illumination and misstracking. 1. 
The local search algorithm Wsat is one of the most successful algorithms  for solving the satisfiability (SAT) problem. It is notably  effective at solving hard Random 3-SAT instances near the so-called  `satisfiability threshold&apos;, but still shows a peak in search cost near  the threshold and large variations in cost over different instances. We  make a number of significant contributions to the analysis of Wsat on  high-cost random instances, using the recent concept of the backbone  of a SAT instance. The backbone is the set of literals which are entailed  by an instance. We find that the number of solutions predicts  the cost well for small-backbone instances but is much less relevant  for the large-backbone instances which appear near the threshold and  dominate in the overconstrained region. We show a very strong correlation  between search cost and the Hamming distance to the nearest  solution early in Wsat&apos;s search. This pattern leads us to introduce a  measure of the backbone f...
This paper introduces four different ways to overcome the problem of balancing incoming traffic over multiple links. The four ways are setting of the MED parameter, prepending of the AS_PATH parameter, usage of communities and defining of more specific routes. They are explored in own sections and compared in conclusion section. This paper is intended to readers with basic knowledge of Border Gateway Protocol (BGP) and routing.
Early phases in the design of concurrent computer--based systems are known as critical and error-prone. Mistakes and wrong decisions during these phases can result in major breakdowns later on. Breakdowns usually require cost--intensive redesign or can lead to a failure of an entire project. A major problem in large designs is the comprehension of the individual componentinteraction. Visual languages can support designers with a better understanding of a system&apos;s behavior since they take advantage of people&apos;s visual system.
Text document clustering plays an important role in providing intuitive navigation and browsing mechanisms by organizing large sets of documents into a small number of meaningful clusters. The bag of words representation used for these clustering methods is often unsatisfactory as it ignores relationships between important terms that do not cooccur literally. In order to deal with the problem, we integrate core ontologies as background knowledge into the process of clustering text documents. Our experimental evaluations compare clustering techniques based on precategorizations of texts from Reuters newsfeeds and on a smaller domain of an eLearning course about Java. In the experiments, improvements of results by background knowledge compared to a baseline without background knowledge can be shown in many interesting combinations.
Recent concentration of indoor livestock farming in the Highlands of the Reunion Island generates environmental risks. Livestock effluent management is often difficult due to the lack of suitable spreading areas close to the farms. However, crop farms in the Lowlands are demanding organic matter to maintain soil fertility. This is the case of the locality of Grand-Ilet where pig and poultry effluents are intensively produced that should be exported towards the coastal zone where large areas of sugar-cane are available. This paper presents how we are supporting Grand-Ilet stakeholders to devise territorial management strategies of their wastes. To this end, a stepwise approach is being tested. First, the supply of animal wastes and the management by farmers are characterized through farm surveys and agronomic expertise. Then, the crop demand is determined by using a GIS considering a large zone surrounding Grand-Ilet onto which several constraints are applied to eliminate unsuitable spreading areas. Comparing supply and demand enables one to draw general conclusions about the strategic choices to be considered. In the case of Grand-Ilet, distances between livestock farms and crop locations, and amounts of liquid manure to be processed, led one to consider implementing a treatment plant collectively managed by farmers. To devise sustainable management strategies for such a trade-off, we are developing a participatory approach based on several simulation models (spreadsheet, hybrid dynamical system, agent-based). These models, developed in our team, aim at tackling several decision-making issues: (i) Which treatment process should be chosen? (ii) How to supply it with raw wastes? (iii) How to best organise organic material fluxes amongst farms at the district level? Toge...
This paper describes a method to reduce the effects of the system immanent control delay for the RoboCup small size leag e. It explains how we solved the task by predicting the movement of our robots using a neural network. Recently sensed robot positions and orientations as well as the most recent motion commands sent to the robot are used as input for the prediction. The neural network is trained with data recorded from real robots. We have successfully field-tested the system at several RoboCup competitions with our FU-Fighters team. The predictions improve speed and accuracy of play. 1 
In this note, we study effective Cartier divisors with totally real or totally  complex supports on a projective curve over R. We give some numerical conditions  for an invertible sheaf to be isomorphic or not to such a divisor. We show that these  conditions are strongly related to the singularities of the curve and to topological  properties of the real part of the Jacobian variety.
In this paper we study the rational balance between local and global policies in virtual communities of agents. To study this problem we use a logical framework for modelling obligations and permissions in multiagent systems. In particular, the logical framework allows agents to trade off the decision of respecting a norm against the consequences of not respecting it: the possibility that they are considered violators and thus sanctioned. To formalize decision making we use a qualitative game theory. n-player games are based on recursive modelling: the bearer of a norm models the behavior of local normative authorities as agents who are in turn subject to other norms and thus model global normative agents.
This paper describes the new integration platform GEOHARP (Geographic harmonizing Platform) for data interconnectivity. This platform is meant to integrate Geographical Databases, which are located at different operational facilities, and serve different users segments independently. Therefore GEOHARP facilitates an higher level of information sharing and integration, allowing the user to access a broader base of data not necessarily coherent in terms of geographical projection datum, and semantic. The significance of coherent layers of geographical data is particularly important in the context of objects reconnaissance based on high and very high resolution satellite data. The implementation of GEOHARP requires a comprehensive analysis of the existing archiving systems, spanning from commercially available to customized ones.
Robotic soccer is a challenging research domain which involves multiple agents that need to collaborate in an  adversarial environment to achieve specificobjectives. In this paper, we describe CMUnited, the team of small robotic  agents that we developed to enter the RoboCup-97 competition. We designed and built the robotic agents, devised  the appropriate vision algorithm, and developed and implemented algorithms for strategic collaboration between the  robots in an uncertain and dynamic environment. The robots can organize themselves in formations, hold specific  roles, and pursue their goals. In game situations, they have demonstrated their collaborative behaviors on multiple  occasions. We present an overview of the vision processing algorithm which successfully tracks multiple moving  objects and predicts trajectories. The paper then focusses on the agent behaviors ranging from low-level individual  behaviors to coordinated, strategic team behaviors. CMUnited won the RoboCup-97 small-robot competition at  IJCAI-97 in Nagoya, Japan.
this paper we present some ideas for new architecture of such communication hardware, which will combine the benefits of SCI distributed shared memory with high effectiveness and low latencies of protected user level DMA of the Virtual Interface Architecture (VIA). The final goal of our research is to develop PCI-SCI bridge with completely new design which will be very suitable for building inexpensive clusters optimized for message passing applications.  I. Motivation and Introduction  Scaleable Coherent Interface (SCI)[1] is modern communication technology with very high bandwidth, extremely low latency, scalable architecture and support of distributed multiprocessing that allows building of large systems out of many inexpensive units.  SCI acts like a modern equivalent of processor/memory/IO bus and Local Area network, which uses a single address space to specify data, as well as its source and destination when being transported and in this way implements distributed
This paper describes research investigating ways in which a mobile decision support system might be implemented. Our view is that the mobile decision maker will be better supported if he/she is aware of the Quality of the Data (QoD) used in deriving the decision, and how QoD improves or deteriorates while he/she is on the move. We propose a QoD model that takes into account static and dynamic properties of the mobile decision making environment, use multicriteria decision analysis to represent the user&apos;s decision model and to derive a single QoD parameter, and investigate use of powerful graphics to relay information to the user.
... In this paper we will examine  the types of unexpected goals that may be given to the underlying planning system and  thereby how humans change the way planning must be performed. Users may want to  achieve goals in terms of actions as well as states, they may specify goals that vary along a  dimension of abstraction and specificity, and they may mix both top-level goals and subgoals  when describing what they want a plan to do. We show how the Prodigy planning  system has met these challenges when integrated with a force deployment tool called ForMAT  and describe what opportunities this poses for a generative planning framework
Naive Bayes is often used as a baseline in text classification because it is fast and easy to implement. Its severe assumptions make such efficiency possible but also adversely affect the quality of its results. In this paper we propose simple, heuristic solutions to some of the problems with Naive Bayes classifiers, addressing both systemic issues as well as problems that arise because text is not actually generated according to a multinomial model. We find that our simple...
New emerging IP services based on differentiated services and the IP security architecture offer the level of communication support that corporate Internet applications need nowadays. However, these services add an additional degree of complexity to IP networks which will require sophisticated management support. The management of enhanced IP services for their customers is thus an emerging important task for Internet service providers. This article describes a potential management architecture service providers will need for that task, considering problems such as multiprovider services and service automation. We will focus on a quality-enhanced virtual private network service which is particularly useful for corporate internetworking.
Modelling and programming process requires a completely different approach in discrete event simulation of complex and large systems of many moving objects. In this paper we describe the public interface of a C++ class library that addresses this problem. Our class library has the virtue of being a flexible and application-independent object-oriented environment which presents to the user a world view that simplifies the simulation of these systems. 1 Introduction  Discrete event simulation can be a powerful analytical tool in the study of complex systems based on many moving objects. Its application ranges through a variety of systems, including airplane or vehicular traffic, message passing on wide area networks or microscopic fluids. The generic problem consists in simulating the behavior of each moving object in a space populated by many other objects. The main aspect of the simulation are the interactions between objects. For example, an airplane which enters a region covered by a...
A frequent type of query in spatial networks  (e.g., road networks) is to find the K nearest  neighbors (KNN) of a given query object.
In this paper a new technique, called Fault Injection Simulation (FIS), is presented that is suited for deriving results for steady-state measures in discrete event dynamic systems which are strongly influenced by rarely occurring events. FIS is based on decomposition of the observations in those that are affected and those that are not affected by these events. If methods are available FIS can be used as a (partly) analytical technique, else as a pure fast simulation technique. It is shown that under intuitively appealing assumptions FIS gives an unbiased estimator and a variance reduction. Furthermore it is shown how to adjust FIS during the simulation to obtain maximum variance reduction. FIS is discussed in the context of rarely occurring failures in networks for realtime applications. Comparisons with other fast simulation techniques are made and results of FIS for an M/M/1-queue with rarely occurring service breakdowns are included.
This paper presents an optimization-based approach to solve the wireless fair scheduling problem under a multirate  TDMA (Time Division Multiple Access)-based MAC (Medium Access Control) framework. By formulating the  fair scheduling problem as an assignment problem, we propose an ORCA-MRT (Optimal Radio Channel Allocation  for Multi-Rate Transmission) algorithm for fair bandwidth allocation in wireless data networks which support multirate  transmission at the radio link level. The key feature of ORCA-MRT is that while allocating transmission rate to  each flow fairly it keeps the inter-access delay bounded under a certain limit. We investigate the performance of the  proposed ORCA-MRT scheduler in comparison to another recently proposed multi-rate fair scheduling algorithm.
The evaluation of texture features is important for several image processing applications. Texture analysis forms  the basis of object recognition and classification in several domains. There is a range of texture extraction  methods and their performance evaluation is an important part of understanding the utility of feature extraction  tools in image analysis. In this paper we evaluate five different feature extraction methods. These are autocorrelation,  edge frequency, primitive-length, Law&apos;s method, and co-occurrence matrices. All these methods are  used for texture analysis of Meastex database. This is a publicly available database and therefore a meaningful  comparison between the various methods is useful to our understanding of texture algorithms. Our results show  that the Law&apos;s method and co-ccurrence matrix method yield the best results. The overall best results are  obtained when we use features from all five methods. Results are produced using leave-one-out method.
Current discussions and trends in digital reference have emphasized the use of real-time digital reference services. Recent articles have questioned both the utility and use of asynchronous services such as email. This article uses data from the AskERIC digital reference service to demonstrate that asynchronous services are not only useful and used but may have greater utility than real-time systems. 
This paper outlines several methods for determining 3x3 matrix transformations from camera spectral spaces to a standard color space based on the ITU-R BT.709 red, green, and blue (RGB) primaries
We propose a novel and robust hashing paradigm that uses  iterative geometric techniques and relies on observations that main geometric  features within an image would approximately stay invariant  under small perturbations. A key goal of this algorithm is to produce  sufficiently randomized outputs which are unpredictable, thereby yielding  properties akin to cryptographic MACs. This is a key component for  robust multimedia identification and watermarking (for synchronization  as well as content dependent key generation). Our algorithm withstands  standard benchmark (e.g Stirmark) attacks provided they do not cause  severe perceptually significant distortions. As verified by our detailed experiments,  the approach is relatively media independent and works for  audio as well.
Network processors (NPs) implement a balance between hardware  and software that addresses the demand of performance and programmability  in active networks (AN). We argue that this makes them  an important player in the implementation and deployment of ANs. Besides  a general introduction into the relationship of NPs and ANs, we  describe the power of this combination in a framework for secure and  safe capsule-based active code. We also describe the advantages of offloading  AN control point functionality into the NP and how to execute  active code in the data path effciently. Furthermore, the paper reports  on experiences about implementing active networking concepts on the  IBM PowerNP network processor.
Currently in Requirements Engineering the attention is being focused more and more on the understanding of a problem by studying the existing organizational setting in which the system will operate. In this paper we present the application of the Tropos early requirements analysis to a real case study, the  Ice Co. We introduce a new type of analysis for actor diagrams based on two different parameters, complexity and criticality, and we show the results we obtained during the case study.
Classical conditioning is a basic learning mechanism in animals and can be found in almost all organisms. If we want to construct robots with abilities matching those of their biological counterparts, this is one of the learning mechanisms that needs to be implemented first. This article describes a computational model of classical conditioning where the goal of learning is assumed to be the prediction of a temporally discounted reward or punishment based on the current stimulus situation.
In this paper we give a new bound on the sum of the Betti numbers of semi-algebraic sets. This extends a well-known bound due to Oleinik and Petrovsky [19], Thom [23] and Milnor [18]. In separate papers they proved that the sum of the Betti numbers of a semi-algebraic set S ae R  k  ;  defined by P 1  0; : : : ; P s  0; deg(P i )  d; 1  i  s; is bounded by (O(sd))  k  : Given a semialgebraic set S ae R  k  defined as the intersection of a real variety, Q = 0; deg(Q)  d; whose real dimension is k  0  ; with a set defined by a quantifier-free Boolean formula with atoms of the form,  P i = 0; P i ? 0; P i ! 0; deg(P i )  d; 1  i  s; we prove that the sum of the Betti numbers of S is bounded by s  k  0  (O(d))  k  : In the special case, when S is defined by Q = 0; P 1 ? 0; : : : ; P s ? 0; we have a slightly tighter bound of  \Gamma s k 0  \Delta  (O(d))  k  : This result generalises the Oleinik-Petrovsky-Thom-Milnor bound in two directions. Firstly, our bound applies to arbitrary semi-alg...
This paper describes solutions for parallel video processing based on LAN-connected PC-like workstations. We outline application scenarios for the processing of video with broadcast TV resolution and promising areas for future research. Additionally, we describe a prototype system implemented in our workgroup. This Network-of-Workstations is based on Gigabit Ethernet, free Java software and standard network protocols. Video data is streamed to and from processing hosts using IP multicast and the Real-time Transfer Protocol. Control mechanisms are based on network file sharing. In comparison to related approaches, our system makes use of highperformance network hardware and open source software. In consequence, our system is easier to implement, cheaper and more flexible than, for example, highly integrated commercial solutions.
The objective of video segmentation is to segment a video sequence into parts called shots corresponding to  a continuous set of frames taken from one camera. Transitions between shots can be abrupt (cuts) or gradual. Abrupt
There are many algorithms to cluster sample data points based on nearness or a similarity measure. Often the implication is that points in different clusters come from different underlying classes, whereas those in the same cluster come from the same class. Stochastically, the underlying classes represent different random processes. The inference is that clusters represent a partition of the sample points according to which process they belong. This paper discusses a model-based clustering toolbox that evaluates cluster accuracy. Each random process is modeled as its mean plus independent noise, sample points are generated, the points are clustered, and the clustering error is the number of points clustered incorrectly according to the generating random processes. Various clustering algorithms are evaluated based on process variance and the key issue of the rate at which algorithmic performance improves with increasing numbers of experimental replications. The model means can be selected by hand to test the separability of expected types of biological expression patterns. Alternatively, the model can be seeded by real data to test the expected precision of that output or the extent of improvement in precision that replication could provide. In the latter case, a clustering algorithm is used to form clusters, and the model is seeded with the means and variances of these clusters. Other algorithms are then tested relative to the seeding algorithm. Results are averaged over various seeds. Output includes error tables and graphs, confusion matrices, principal-component plots, and validation measures. Five algorithms are studied in detail: K-means, fuzzy C-means, self-organizing maps, hierarchical Euclidean-distance-based and correlation-based clustering. The toolbox is appl...
Agent-based systems technologies are of emerging interest in the specification and implementation of complex systems. This article introduces the CASA agent development system which seamlessly combines the BDI (Belief Desire Intention) approach with the FIPA agent communication language standard and an integrated specification of fuzzy controllers. The behavior of agents is defined by strategies which basically correspond to extended guarded horn clauses with priorities. The presented concepts are introduced by an example from Computer Integrated Manufacturing (CIM). The example gives the specification of a fuzzy controller for a manufacturing station in the context of a holonic manufacturing system (HMS).
This article describes a state-centric, agent-based  design methodology to mediate between a system developer&apos;s mental model of physical phenomena and the distributed execution of DSAN applications. Building on the ideas of data-centric networking,   sensor databases,    and proximity-based group formation,  3  we introduce the notion of collaboration  groups, which abstracts common patterns in application-specific communication and resource allocation. An application developer specifies computations as the creation, aggregation, and transformation of states, which naturally map to the vocabulary used by signal processing and control engineers. More specifically, programmers write applications as algorithms for state update and retrieval, with input supplied by dynamically created collaboration groups. As a result, programs written in the state-centric framework are more invariant to system configuration changes, making the resulting software more modular and portable across multiple platforms. Using a distributed tracking application with sensor networks, we&apos;ll demonstrate how state-centric programming can raise the abstraction level for application developers
In [2], Bouajjani and others presented an automata-based approach to a  number of elementary problems on context-free grammars. This approach  is of pedagogical interest since it provides a uniform solution to decision  procedures usually solved by independent algorithms in textbooks, e.g. [7].  This paper improves upon [2] in a number of ways. We present a new  algorithm which not only has a better space complexity but is also (in  our opinion) easier to read and understand. Moreover, a closer inspection  reveals that the new algorithm is competitive to well-known solutions for  most (but not all) standard problems.  
To support for interdisciplinary research in the Titech 21COE LKR Program, we are developing an advanced information storage system to manage the large-scale knowledge resources. The flexibility and extensibility are key issues to provide the required services. To realize the flexibility, we take an approach to combine two types of systems by web-service APIs: an information storage system providing common functions for handling the variety of data formats and external systems dedicated for special applications based on the stored knowledge resources. We can add or modify the external systems to cope with the resource variety and requirement diversity. As a hardware configuration of the information storage system, we adopt a storage area network (SAN) with a number of servers and RAIDs to make the system extensible and reliable. This paper reports the functions and configurations of the advanced information storage system and some examples of the external application systems.
We formulate the regression problem as one of maximizing the minimum  probability, symbolized  by  , that future predicted outputs of the  regression model will be within some &quot; bound of the true regression  function. Our formulation is unique in that we obtain a direct estimate  of this lower probability  bound  . The proposed framework, minimax  probability machine regression (MPMR), is based on the recently described  minimax probability machine classification algorithm [Lanckriet  et al.] and uses Mercer Kernels to obtain nonlinear regression models.
We formalize the Dolev-Yao model of security protocols, using a notation  based on multi-set rewriting with existentials. The goals are to provide a simple  formal notation for describing security protocols, to formalize the assumptions  of the Dolev-Yao model using this notation, and to analyze the complexity  of the secrecy problem under various restrictions. We prove that, even for the  case where we restrict the size of messages and the depth of message encryption,  the secrecy problem is undecidable for the case of an unrestricted number of  protocol roles and an unbounded number of new nonces. We also identify  several decidable classes, including a dexp-complete class when the number of  nonces is restricted, and an np-complete class when both the number of nonces  and the number of roles is restricted. We point out a remaining open complexity  problem, and discuss the implications these results have on the general topic of  protocol analysis.
We study the fundamental limitations of relational  algebra (RA) and SQL in supporting  sequence and stream queries, and present effective  query language and data model enrichments  to deal with them. We begin by observing  the well-known limitations of SQL in  application domains which are important for  data streams, such as sequence queries and  data mining. Then we present a formal proof  that, for continuous queries on data streams,  SQL suffers from additional expressive power  problems. We begin by focusing on the notion  of nonblocking (NB) queries that are the only  continuous queries that can be supported on  data streams. We characterize the notion of  nonblocking queries by showing that they are  equivalent to monotonic queries. Therefore  the notion of NB-completeness for RA can be  formalized as its ability to express all monotonic  queries expressible in RA using only the  monotonic operators of RA. We show that RA  is not NB-complete, and SQL is not more  powerful than RA for monotonic queries.
The development of high-performance autonomous multi robot control systems requires intensive experimentation in controllable, repeatable, and realistic robot settings. The need for experimentation is even higher in applications where the robots should automatically learn substantial parts of their controllers. We propose to solve such learning tasks as a three step process. First, we learn a simulator of the robots&apos; dynamics. Second, we perform the learning tasks using the learned simulator. Third, we port the learned controller to the real robot and cross validate the performance gains obtained by the learned controllers. In this paper, we describe M-ROSE, our learning simulator, and provide empirical evidence that it is a powerful tool for learning of sophisticated control modules for real robots.
The dynamic range of many real-world environments exceeds the capabilities of current display technology by several orders of magnitude. In this paper we discuss the design of two different display systems that are capable of displaying images with a dynamic range much more similar to that encountered in the real world. The first display system is based on a combination of an LCD panel and a DLP projector, and can be built from off-the-shelf components. While this design is feasible in a lab setting, the second display system, which relies on a custom-built LED panel instead of the projector, is more suitable for usual office workspaces and commercial applications. We describe the design of both systems as well as the software issues that arise. We also discuss the advantages and disadvantages of the two designs and potential applications for both systems.
. A recent theoretical result by Achlioptas et al. shows that  many models of random problems become trivially insoluble as problem  size increases. This insolubility is due to the presence of `flawed variables  &apos;, variables whose values are all `flawed&apos; (or unsupported). In this  paper, we analyse how seriously existing work has been affected and show  how future work can be cured of flaws by the use of a flawless variant of  the existing models. We prove that models B and C do not suffer from  such flaws when the constraint tightness is less than the reciprocal of  domain size. We survey the literature to identify experimental studies  that use models and parameters that may have been affected by flaws.  We then estimate theoretically and measure experimentally the size at  which flawed variable can be expected to occur. We also study a new  model proposed by Achlioptas et al. to tackle such flaws. We show that  flawed values still occur in this model at problem sizes similar to thos...
For a     ff with b\a infinite, the set D =         b} is called a doughnut. Doughnuts are equivalent to conditions of Silver forcing, and so, a set S     is called Silver measurable,  also known as completely doughnut, if for every doughnut D there is a doughnut D ff   D which is contained or disjoint from S. In this paper, we investigate the Silver measurability of ff    sets of reals and compare it to other regularity properties like the Baire and the Ramsey property and Miller and Sacks measurability. 0. 
This paper describes RIME (Replicated IMage dEtector), an alternative approach to watermarking for detecting unauthorized image copying on the Internet. RIME profiles internet images and stores the feature vectors of the images and their URLs in its repository. When a copy detection request is received, RIME matches the requested image&apos;s feature vector with the vectors stored in the repository and returns a list of suspect URLs. RIME characterizes each image using Daubechies&apos; wavelets. The wavelet coefficients are stored as the feature vector. RIME uses a multidimensional extensible hashing scheme to index these high-dimensional feature vectors. Our preliminary result shows that it can detect image copies effectively: It can find the top suspects and copes well with image format conversion, resampling, and requantization.  Keywords: copy detection, wavelets, multidimensional indexes  1. INTRODUCTION  Advancement in the internet and world-wide web technology has led to the growth of inf...
We present a mathematical construct which provides a cryptographic protocol to verifiably shuffle a sequence of k modular integers, and discuss its application to secure, universally verifiable, multi-authority election schemes. The output of the shuffle operation is another sequence of k modular integers, each of which is the same secret power of a corresponding input element, but the order of elements in the output is kept secret. Though it is a trivial...
Historically, data visualization has been limited primarily to 2 dimensions (e.g., histograms, scatter plots). Available software packages (e.g., Data Desk      4.04, SPSS    10.0) are capable of producing 3-D scatter plots with (varying degrees of) user interactivity. We constructed our own data visualization application with The Visualization Toolkit (Schroeder, Martin, &amp; Lorensen, 1998) and Tcl/Tk to display multivariate data through the application of glyphs (Ware, 2000). A glyph is a visual object onto which many data parameters may be mapped, each with a different visual attribute (e.g., size, color). We used our Multi-Dimensional Data Viewer to explore data from several psycholinguistic experiments. The graphical interface provides flexibility when users dynamically explore the multi-dimensional image rendered from raw experimental data. We highlight advantages of multidimensional data visualization and consider some potential limitations.
This paper introduces a model to represent the evolution of VR objects over time. Based on this model a new classification of virtual reality objects is proposed and the semantics associated with each class of object are described. This paper also presents a rationale for modifying object semantics under manipulations by an observer of the object behavior. The manipulation of the spatio-temporal configurations of VR objects and their behaviors allows observers to create their own visualization of the environment, gaining insight and discovering relationships among dynamic phenomena.
We present an evaluation of four knowledge base systems with respect to use in large Semantic Web applications. We discuss the performance of each system. In particular, we show that existing systems need to place a greater emphasis on scalability.
Modelling and simulation applications for operations in urban areas often set high resolution and fidelity requirements for the 3D environment models. For applications supporting ongoing operations, e.g decision support, mission planning, etc., additional requirements such as time frame and accessibility to the area of interest must also be taken into account in the modelling task. One possible approach to handle all these requirements is to use methods that allow for automatic 3D modelling using recent data from reconnaissance and surveillance sensors. In future urban operations we can expect to see different types of UAVs and UGVs equipped with sensors like 3D laser radars, IR- and visual cameras. The data from these sensors can be used for environment modelling.
Inspired by the inventive uses of the fictional product of transportable holes demonstrated in several animated cartoons, we have endeavored to create a environment that allows us to create windows within our virtual worlds. These windows open the often closed environments of head-mounted displays to the richness of the physical world that surrounds us. We have created a video-based stereo see-through head-mounted display to enable the merging of 3D windows onto reality. We describe the many obstacles to merging stereo images generated by a computer graphics system and views of the physical environment. While this paper describes only a single implementation, the issues remain general to anyone intending to construct a similar environment and the motivation is not implementation specific. We point out obstacles, solutions, workarounds, and other issues for others wishing to attain the same goal. We also discuss the advantages of adding familiar of physical input devices to virtual environments.
We describe a method for shape-based image database search that uses deformable prototypes to represent categories. Rather than directly comparing a candidate shape with all shape entries in the database, shapes are compared in terms of the types of nonrigid deformations (differences) that relate them to a small subset of representative prototypes. To solve the shape correspondence and alignment problem, we employ the technique of  modal matching, an information-preserving shape decomposition for matching, describing, and comparing shapes despite sensor variations and nonrigid deformations. In modal matching, shape is decomposed into an ordered basis of orthogonal principal components. We demonstrate the utility of this approach for shape comparison in 2-D image databases.  Keywords: Deformable models, deformable templates, combinations of models, shape matching, modal matching. 1 Introduction  Shape categories can be represented as deformations from a subset of standard or prototypica...
This paper describes work in progress for the development of a gestural controller interface for contemporary vocal performance and electronic processing. The paper includes a preliminary investigation of the gestures and movements of vocalists who use microphones and microphone stands. This repertoire of gestures formsthe foundation of a well-practiced `language&apos; and social code for communication between performersand audiences andservesasabasisfor alternate controller design principles. A prototype design, based on a modified microphone stand, is presented along with a discussion of possible controller mapping strategies and identification of directions for future research.
Introduction  The conventional wisdom is that &quot;the Internet is very insecure.&quot; The subtitle of this workshop, namely &quot;deployment obstacles,&quot; implies that network owners, operators, and users could have solved pervasive security problems if they had deployed existing security technology. Is there solid evidence that either of these statements is true?  Clearly, there have been some well publicized Internet security problems (e.g., viruses and distributed denial-of-service attacks) during the past five years, and some loss by individuals and businesses is attributable to them. Does this mean that Internet insecurity is really a significant problem? Is it a more serious problem than it was, say, ten years ago, or is there simply more awareness of it now than there was then? What fraction of Internet activity or potential activity is disrupted or prevented because of actual or perceived insecurity? Is this fraction higher or lower than it was ten years ago?  It is our thesis that better mo
Recent research in airborne oil spill remote sensing [FBFG94] leads towards modular systems that  consist of several distinct sensors to combine the capabilities of the different sensor classes. The  Medusa project [GHW96] is an example of a distributed system. It exhibits a distributed architecture  to provide a maximum of flexibility, concurrency and safety and must clearly be rated as a classical  distributed application from a computer science point of view. This article describes the &quot;sensor  description system&quot; (SDS). SDS allows the developer of sensing systems to minimize the effort of  integrating his particular subsystem into an existing application. By applying formal methods to the  integration process a developer is able to describe the abstract properties of his sensing system like  parameter values, generated data format, applicable methods on the data etc. and can thus rely on  the SDS tools to produce the required software backends automatically: A graphical user interface  for parameter control, an online visualization, data transfer facilities to a database and finally the  evaluation and interpretation facility. This technique puts future sensing enterprises in a position  where different classes of sensors can easily be combined almost off-the-shelf to build powerful  systems in very short turnaround times.
This paper presents an analysis of the performance and behavior of a blind adaptive carrier phase offset recovery scheme based on dispersion minimization (DM). The algorithm, called the DM-derotator, is a kind of simple equalizer that attempts to correct the carrier phase offset by minimizing the dispersion of the projection of the real part of the (complex) data signal. The recent application of this algorithm to digital broadcasting signals motivates our analysis. This paper classifies the stationary points of the DM-derotator for a variety of source signals including digital vestigial side band (VSB) and quadrature amplitude modulation (QAM) and discusses initialization strategies. The analysis is extended to a variety of situations including i) its behavior in the presence of intersymbol interference, ii) its behavior when there is statistical dependence between the in-phase and quadrature components, and iii) its tracking ability.
This paper aims to present the design aspects of electrical motors with soft magnetic  composite (SMC) core. Combined classical and modern analysis procedures are proposed for  developing SMC motors. A permanent magnet claw pole motor using SMC material as the  stator core was firstly designed by the equivalent magnetic circuit method. Threedimensional  finite element magnetic field analysis was conducted to refine the design and  calculate some key parameters. The design and analysis method was validated by the  experimental results on the prototype.
The complexity of embedded controllers is steadily increasing.
Most real-world data is stored in relational form. In  contrast, most statistical learning methods work with  &quot;flat&quot; data representations, forcing us to convert our  data into a form that loses much of the relational structure.
Intelligent agents must function in an uncertain world,  containing multiple objects and relations that change  over time. Unfortunately, no representation is currently  available that can handle all these issues, while allowing  for principled and efficient inference. This paper addresses  this need by introducing dynamic probabilistic  relational models (DPRMs). DPRMs are an extension  of dynamic Bayesian networks (DBNs) where each time  slice (and its dependences on previous slices) is represented  by a probabilistic relational model (PRM). Particle  filtering, the standard method for inference in DBNs,  has severe limitations when applied to DPRMs, but we  are able to greatly improve its performance through a  form of relational Rao-Blackwellisation. Further gains  in efficiency are obtained through the use of abstraction  trees, a novel data structure. We successfully apply  DPRMs to execution monitoring and fault diagnosis of  an assembly plan, in which a complex product is gradually  constructed from subparts.
Techniques for scheduling parallel tasks on to the processors of a multiprocessor architecture must tradeoffthree interrelated factors: 1) scheduling and synchronization costs, 2) load balancing, and 3) memory locality.Current scheduling techniques typically consider only one or twoofthese three factors at a time. We propose a novelSelfAdjusting Scheduling (SAS) algorithm that addresses all three factors simultaneously.This algorithm dedicates a single processor to execute an on-line branch-and-bound algorithm to search for partial schedules concurrent with the execution of tasks previously assigned to the remaining processors. This overlapped scheduling and execution, along with self-adjustment of duration of partial scheduling periods reduces scheduling and synchronization costs significantly.Tosatisfy the load-balancing and locality management, SAS introduces a unified cost model that accounts for both of these factors simultaneously.Wecompare the simulated performance of SAS with the Affinity Scheduling algorithm (AFS). The results of our experiments demonstrate that the potential loss of performance caused by dedicating a processor to scheduling is outweighed by the higher performance produced by SAS&apos;sdynamically adjusted schedules, eveninsystems with a small number of processors. SAS is a general on-line optimization technique that can be applied to a variety of dynamic scheduling problems.
activities to model and analyze Internet worm propagation. In this paper we provide a careful analysis of Code Red propagation by accounting for two factors: one is the dynamic countermeasures taken by ISPs and users; the other is the slowed down worm infection rate because Code Red rampant propagation caused congestion and troubles to some routers. Based on the classical epidemic Kermack-Mckendrick model, we derive a general Internet worm model called the twofactor worm model. Simulations and numerical solutions of the two-factor worm model match the observed data of Code Red worm better than previous models do. This model leads to a better understanding and prediction of the scale and speed of Internet worm spreading.
The objectives of the CATI project (Charging and Accounting Technology for the Internet) include the design, implementation, and evaluation of charging and accounting mechanisms for Internet services and Virtual Private Networks (VPN). They include the enabling technology support for open, Internet-based Electronic Commerce platforms in terms of usage-based transport service charging as well as high-quality Internet transport services and its advanced and flexible configurations for VPNs. In addition, security-relevant and trust-related issues in charging, accounting, and billing processes are investigated. Important application scenarios, such as an Internet telephony application as well as an Electronic Commerce shopping network, demonstrate the applicability and efficiency of the developed approaches. This work is complemented by an appropriate cost model for Internet communication services, including investigations of suitable usage-sensitive pricing models.  
This papers provides two contributions to the problem of Simultaneous Localization and Mapping (SLAM): First we discuss properties of the problem itself and of the intended semantics of an uncertain map representation, with the main idea of &quot;representing certainty of relations despite the uncertainty of positions&quot;. We propose some requirements an ideal solution of SLAM should have concerning uncertainty, memory space and computation time and discuss existing approaches in the light of these requirements. The second part proposes a representation based on sparse information matrices together with some properties that motivate this approach. This is shown to comply to the uncertainty and space requirements. To derive an estimated map from the representation a sparse linear equation system has to be solved. However, an update of the representation itself needs only constant time, making it highly attractive for building a SLAM algorithm.
This paper investigates the problem of providing suitable interpolation for scanline algorithms.  These algorithms are of interest as they are parallelizable. A structure for analysing the problem  is given. The theory in regard to resampling is developed in the context of a scanline algorithm  for image rotation. The theory is compared to results arrived at in practice. Alternative interpolation  schemes are discussed, including the use of a cubic Hermite interpolator. The paper  points to theoretical limitations of scanline algorithms.  Keywords  Resampling, affine transforms, scanline algorithms, parallelization  I. Introduction  Whenever batches of images need to be aligned then an image may need to be rotated. Rapid if not real-time processing of such images may require alternative rotation algorithms that compromise on fidelity. Examples from recent studies are: in Figure 1 Image 1, a Magnetic Resonance (MR) image, is from a study of changes in breast volume during the menstrual ...
In this chapter we outline the importance of facility location decisions in  supply chain design. We begin with a review of classical models  including the traditional fixed charge facility location problem. We then  summarize more recent research aimed at expanding the context of facility  location decisions to incorporate additional features of a supply chain  including LTL vehicle routing, inventory management, robustness, and  reliability.
A general rule of thumb is to tackle the hardest part of a search problem first. Many heuristics therefore try to branch on the most constrained variable. To test their effectiveness at this, we measure the constrainedness of a problem during search. We run experiments in several different domains, using both random and non-random problems. In each case, we observe a constrainedness &quot;knife-edge&quot; in which critically constrained problems tend to remain critically constrained. We show that this knife-edge is predicted by a theoretical lower-bound calculation. We also observe a very simple scaling with problem size for various properties measured during search including the ratio of clauses to variables, and the average clause size. Finally, we use this picture of search to propose some branching heuristics for propositional satisfiability. Introduction  Empirical studies of search procedures usually focus on statistics like the run-time or the total number of nodes visited. It can also be...
Manning reduction on future navy ships is a necessity if a significant reduction in  operating costs and a solution to the availability problem of highly specialized personnel  are to be achieved. One way to accomplish this is to remove knowledge and expertise that  is only required for infrequently occurring tasks (e.g. system malfunction diagnosis) from  the ship to a central facility on shore or on another ship. However, not all tasks are eligible  for &quot;teleknowledge&quot;, and both the crew on board and the teleknowledge operators in the  central facility will need to be supported with the proper tools to ensure the current  performance level. This paper will give a view on the potential applications, requirements  and limitations of teleknowledge on future navy vessels. Issues like the kind of knowledge  that has to be transmitted (the methodology), how this transmission must take place and  what psycho-social boundaries and risks must be taken into account are investigated in a  study performed in co-operation with two TNO institutes. Teleknowledge is not totally  new within the navy. RNLN experiences will be presented and the possibility to improve  the use of the existing teleknowledge equipment and organization structure with the  outcome of this study will be explored.
This paper presents a time-aware admission control and resource allocation scheme in the context of a future generation mobile network. The quality levels (and their respective utility) of the different connections are specified using discrete resource-utility (R-U) functions. The scheme uses these R-U functions for allocating and reallocating bandwidth to connections, aiming to maximise the accumulated utility of the system. However, different applications react differently to resource reallocations. Therefore at each allocation timepoint we take into account the following factors: the age of the connection, a drop (disconnection) penalty and the sensitiveness to reallocation frequency. Finally, we show the superior performance of our approach compared to a recent adaptive bandwidth allocation scheme.
The Neem Platform is a research test bed for Project Neem, concerned  with the development of socially and culturally aware collaborative systems in a  wide range of domains, through development of situated applications that target  specific group cultures and alternative theories. The Neem Platform is a generic  (application neutral) component-based framework that provides functionality that  facilitates building such collaborative applications.
We introduce a charging and accounting (CA) architecture for IP multicast services with QoS guarantees over ATM for QoS-sensitive fair charging. The architecture is based on an extension of the Multicast Integration Server (MIS, [SaCS97]) that efficiently provides IP multicast over ATM, supporting Internet Integrated Services (IS) with receiver heterogeneity and shortcut management. We define requirements for a Charging and Accounting Protocol (CAP) for transporting CA objects, involving CAP clients and a CAP server. We demonstrate that RSVP extensions can be used for transporting CA objects, and propose interworking mechanisms between CAP server, MIS and a Billing Server.
German Aerospace Agency (DLR), Fraunhofer FOKUS.cats, and Tesat-Spacecom have designed a future multimedia ATM-based  LEO satellite network. Part of the development was an adaptive MAC and FEC scheme which are presented in this paper. The MAC
We specify the major characteristics of the Internet under  the headings: heterogeneity, service characteristics, dynamic nature, no  global notions, and unreliability (i.e. security and partial failure). In the  process, we identify  ve categories of Internet services: hosts, active entities,  agents, semistructured data, and passive code. Logic Programming (LP)
In this paper we present SCALEA, which is a performance  instrumentation, measurement, analysis, and visualization tool for parallel  and distributed programs that supports post-mortem and online  performance analysis. SCALEA currently focuses on performance analysis  for OpenMP, MPI, HPF, and mixed parallel/distributed programs.
We present a new algorithm for matching pursuit (MP) dictionary design. This technique uses existing vectorquantization   (VQ) design techniques and an inner-product based distortion measure to learn functions from a set  of training patterns. While this scheme can be applied to many MP applications, we focus on motion compensated  video coding. Given a set of training sequences, data is extracted from the high energy packets of the motion  compensated frames. Dictionaries with different regions of support are trained, pruned, and finally evaluated on  MPEG test sequences. We find that for high bit-rate QCIF sequences we can achieve improvements of up to 0.66 dB  with respect to conventional MP with separable Gabor functions.
Source separation is an important problem at the intersection of several  fields, including machine learning, signal processing, and speech technology.
In this paper, we prove the existence of the product formula for the spherical functions in the complex case and we study properties of the integral kernel of this formula.  
We present a brief overview of an algorithm for interactively animating object grasping and manipulation tasks for human figures. The technique is designed to efficiently generate feasible single-arm manipulation motions given highlevel task commands. For moving an object, the motions necessary for a human arm to reach and grasp the object, reposition it, and return the arm to rest are generated automatically within a few seconds on average.
The amount of wireless communication devices has increased dramatically over the last few years. This has created new kinds of requirements to the technology as the growing number of users want to be able to communicate with each other anywhere and anytime without having to rely on any existing infrastructure or centralized access point. Adhoc network is composed of a collection of mobile nodes co-operating together to form a such network. Every node in ad hoc network acts both as a host and a router, which eliminates the need for existing infrastructure. Ad-hoc Ondemand Distance Vector Routing protocol (AODV) is one of the developed protocols that enable routing with continuously changing topologies. AODV is reactive which means that it builds routes only when they are first needed. It uses extensive flooding of messages when discovering routes but tries to increase the overall bandwidth available by minimizing the use of any periodic advertisements. The increasing popularity of these on-the-fly networks has arisen the question about the efficiency and accuracy of the routing protocols used. This paper presents the performance and scalability of the AODV protocol both in small and large networks.
This paper presents a new approach towards parametric analysis of MINLP models in the context of process synthesis problems under uncertainty. The approach is based on the idea of High Dimensional Model Representation technique which utilize a reduced number of model runs to build an uncertainty propagation model that expresses the variability of optimal solution in the uncertain space. Based on this idea, a systematic procedure is developed where in the first step the possible changes in the optimal design configurations due to parametric uncertainty are identified. In the next step, the variability of optimal solution with parameter uncertainty for each design is captured. Having obtained a parametric expression of optimal objective for each design, the optimal solution can be determined by comparing the solutions for different designs. The proposed approach provides information about variation of the optimal objective and optimal design configuration over the entire uncertain space. This information can then be judiciously utilized in any decision making depending on specific process requirements. The main advantage of the proposed approach is that it does not depend on the nature or existence of a mathematical model to describe the input-output relationship of the process.
Many---indeed most---distributed applications employ some notion of distributed shared state: information required at more than one location. For applications that span the Internet, this state is almost always maintained by means of hand-written, application-specific messagepassing protocols. These protocols constitute a significant burden on the programmer. Rochester&apos;s InterWeave project seeks to eliminate this burden by automating the management of shared state for processes on heterogeneous, distributed machines.  Heterogeneity implies the need for strong typing and for automatic conversion to and from a common wire format when transmitting data and updates between machines. In addition to issues of byte order, alignment, and numeric formats, transparent sharing implies the need for pointers that refer to arbitrary shared locations, and that operate with the speed of ordinary machine addresses when the target is locally cached. To satisfy these constraints, InterWeave employs a typ...
Moments are generic and (usually) intuitive descriptors that can be computed from several kinds of objects defined either from closed contours (continuous object) or a set of points (discrete object). In this paper, we propose to use moments to design a decoupled image-based visual servo. The analytical form of the interaction matrix related to the moments computed for a discrete object is derived, and we show that it is different of the form obtained for a continuous object. Six visual features are selected to design a decoupled control scheme when the object is parallel to the image plane. This nice property is then generalized to the case where the desired object position is not parallel to the image plane. Finally, experimental results are presented to illustrate the validity of our approach and its robustness with respect to modeling errors.
We use a kinematic chain to model human body motion. We estimate the kinematic chain motion parameters using pixel displacements calculated from video sequences obtained from multiple calibrated cameras to perform tracking. We derive a linear relation between the 2D motion of pixels in terms of the 3D motion parameters of various body parts using a perspective projection model for the cameras, a rigid body motion model for the base body, and the kinematic chain model for the body parts. An error analysis of the estimator is provided, leading to an iterative algorithm for calculating the motion parameters from the pixel displacements. We provide experimental results to demonstrate the accuracy of our formulation. We also compare our iterative algorithm to the non-iterative algorithm and discuss its robustness in the presence of noise.
ISO 10303-11 (EXPRESS) and ISO 10303-21 (STEP Files) are the upcoming international standards for the machine independent exchange of product data defined within the STEP (Standard for the Exchange of Product Model Data) effort. The conformance test of STEP files denotes the check whether the product data defined by ISO 10303-21 meets the syntactical requirements as well as interpretation given by the EXPRESS language (ISO 10303-11). We demonstrate that a small network of widely available low-cost workstations, like Sparc1 ELC, is sufficient for checking file sizes up to 800 MByte and presumaly larger files. In the first
We investigate here the performance implications of supporting transaction atomicity in a distributed realtime database system. Using a detailed simulation model of a firm-deadline distributed real-time database system, we profile the real-time performance of a representative set of commit protocols. A new commit protocol that is designed for the real-time domain and allows transactions to &quot;optimistically&quot; read uncommitted data is also proposed and evaluated. The experimental results show that data distribution has a significant influence on the real-time performance and that the choice of commit protocol clearly affects the magnitude of this influence. Among the protocols evaluated, the new optimistic commit protocol provides the best performance for a variety of workloads and system configurations. 1. Introduction  Many real-time database applications, especially in the areas of communication systems and military systems, are inherently distributed in nature [15]. Incorporating distr...
The analysis of biological networks involves the evaluation of the vertices within the connection structure of the network. To support this analysis we discuss five centrality measures and demonstrate their applicability on two example networks, a protein-protein-interaction network and a transcriptional regulation network. We show that all five centrality measures result in different valuations of the vertices and that for the analysis of biological networks all five measures are of interest.
Identity management systems help users to organise their digital profiles in order to communicate parts of them, whenever needed and wanted, to communication partners like internet services or personal contacts. Most current identity management research tries to achieve the highest possible degree of data hiding for best privacy. After sketching some of these projects, this paper presents a different approach where users are assumed to be interested in presenting themselves to selected online communities or internet services for better personalisation, to achieve a consistent reputation, or to establish an application- and serviceindependent internet society. It thereby stresses the aspect of privacy that persons have the option for self-portrayal. To support this thesis, a survey is presented which shows that many users who actively participate in Internet communities would make high use of such a system. Finally, the project &quot;onefC&quot; is presented which prototypically realises this approach.
In this paper, we propose a new design tool for \block encryption  &quot;, allowing the en/decryption of arbitrarily long messages, but  performing en/decryption on only a single block (e.g., 128 bit block),  where the rest of the message is only processed by a good scrambling  function (e.g., one based on an ideal hash function). The design can be  a component in constructing various schemes where the above properties  gives an advantage. A quite natural use of our scheme is for remotely  keyed encryption. We actually solve an open problem (at least in  the relaxed ideal hash model and where hosts are allowed to add randomness  and integrity checks, thus giving a length increasing function),  by demonstrating the existence of a secure remotely keyed encryption  scheme which performs only one interaction with the trusted device.
The flexibility and the wide deployment of IP technologies have driven the development of IP-based solutions for wireless networks, like IP-based Radio Access Networks (RAN). These networks have different characteristics when compared to traditional IP networks, imposing very strict requirements on Quality of Service (QoS) solutions, such as fast dynamic resource reservation, simplicity, scalability, low cost, severe congestion handling and easy implementation. A new QoS framework, called Resource Management in Differentiated Services (RMD), aims to satisfy these requirements. RMD has been introduced in recent publications. It extends the IETF Differentiated Services (Diffserv) architecture with new admission control and resource reservation concepts in a scalable way. This paper gives an overview of the RMD functionality and its performance behavior. Furthermore, it shows that the mean processing delay of RMD signaling reservation messages is more than 1330 times smaller then the mean processing delay of RSVP signaling reservation messages.
KEYWORDS: speaker verification; autoassociative neural network; distribution estimation; matching technique; dimensionality reduction.
Micro moulding process studies have been carried out using a Battenfeld Microsystem moulding machine with a maximum melt injection capacity of 1 cm    . Micro moulds with different designs were used to study melt shear effect during the process. Both amorphous and crystalline polymers, including PMMA, PC, POM, PBT, PP and a PC/LCP blend were evaluated in the study. It has been found that, while the metering size is the dominating factor that affects the process, the mould temperature, melt temperature and injection speed can also affect the mould filling behaviour. Shrinkage and the speed of melt solidification are the two important material factors that need to be considered in the process condition setting. Since micro runner and cavity system solidifies very quickly in the micro moulding process, high melt shrinkage and quick melt solidification may result in short shot or high micro part distortion.
In this paper, we propose a new PDE-based methodology for  deformable surfaces that is capable of automatically evolving its shape  to capture the geometric boundary of the data and simultaneously discover  its underlying topological structure. Our model can handle multiple  types of data (such as volumetric data, 3D point clouds and 2D image  data), using a common mathematical framework. The deformation behavior  of the model is governed by partial differential equations (e.g.
Online mechanism design (MD) considers the problem of providing incentives to implement desired system-wide outcomes in systems with self-interested agents that arrive and depart dynamically. Agents can choose to...
We present an energy-efficient, utility accrual, real-time scheduling algorithm called the Resource-constrained Energy-Efficient Utility Accrual Algorithm (or ReUA). ReUA considers an application model where activities are subject to time/utility function (TUF) time constraints, resource dependencies including mutual exclusion constraints, and statistical performance requirements including activity (timeliness) utility bounds that are probabilistically satisfied. Further, ReUA targets mobile embedded systems where system-level energy consumption is also a major concern. For such a model, we consider the scheduling objectives of (1) satisfying the statistical performance requirements; and (2) maximizing the system-level energy efficiency. At the same time, resource dependencies must be respected. Since the problem is NP-hard, ReUA makes resource allocations using statistical properties of application cycle demands and heuristically computes schedules with a polynomial-time cost. We analytically establish several timeliness and non-timeliness properties of the algorithm. Further, our simulation experiments illustrate the algorithm&apos;s effectiveness.
Simulated tempering and swapping are two families of sampling algorithms in which a parameter representing temperature varies during the simulation. The hope is that this will overcome bottlenecks that cause sampling algorithms to be slow at low temperatures. Madras and Zheng demonstrate that the swapping and tempering algorithms allow efficient sampling from the low-temperature mean-field Ising model, a model of magnetism, and a class of symmetric bimodal distributions [10]. Local Markov chains fail on these distributions due to the existence of bad cuts in the state space.
... importance of group based distributed applications such as media dissemination, computer supported collaborative work or fault tolerance through replication. However, most distributed object based middleware platforms, which are increasingly being used as an implementation environment for such applications, fail to provide suitable support for group applications in their full generality. In this paper we describe a component based approach to the provision of group services in a middleware environment in which application tailored group services can be built by defining particular configurations of components or by incrementally modifying existing configurations. In addition, our approach uses reflective capabilities of the middleware platform to support the run-time reconfiguration of existing and running group applications.
In this article we survey the theoretical background that is required  to build a consistent and continuous setup of dynamic elementary geometry. Unlike in 
We present    n , a multi-agent epistemic logic where each agent can perform uncertain (possibilistic) reasoning. The original feature of this logic is the presence of a distributed belief operator, with the purpose of merging the belief of different agents. Unlike the corresponding operator in the categorical (non-uncertain) case, our distributed belief operator accumulates support for the same fact coming from different agents. This means that opinions shared by different agents can be combined into a stronger distributed belief. This feature is useful in problems like pooling expert opinions and combining information from multiple unreliable sources. We provide a possible worlds semantics and an axiomatic calculus for our logic, and prove soundness, completeness and decidability results. We hint at some possible applications   n in the conclusions.
This paper presents a novel and complementary  technique to optimize an XPath query by minimizing  its wildcard steps. Our approach is based  on using a general composite axis called the layer  axis, to rewrite a sequence of XPath steps (all  of which are wildcard steps except for possibly  the last) into a single layer-axis step. We describe  an efficient implementation of the layer axis  and present a novel and efficient rewriting algorithm  to minimize both non-branching as well as  branching wildcard steps in XPath queries. We  also demonstrate the usefulness of wildcard-step  elimination by proposing an optimized evaluation  strategy for wildcard-free XPath queries that enables  selective loading of only the relevant input  XML data for query evaluation. Our experimental  results not only validate the scalability and efficiency  of our optimized evaluation strategy, but  also demonstrate the effectiveness of our rewriting  algorithm for minimizing wildcard steps in  XPath queries. To the best of our knowledge, this  is the first effort that addresses this new optimization  problem.
We give time lower bounds for the distributed approximation of minimum vertex cover (MVC) and related problems such as minimum dominating set (MDS). In k communication rounds, MVC and MDS can only be approximated by  factorsff    /k) and ffd    /k) for some constant c, where n and ff denote the number of nodes and the largest degree in the graph. The number of rounds required in order to achieve a constant or even only a polylogarithmic approximation ratio is at    log n/ log log n) and ffnd ff/ log log ff). By a simple reduction, the latter lower bounds also hold for the construction of maximal matchings and maximal independent sets.
Speculative execution and instruction reuse are two important strategies that have been investigated for improving processor performance. Value prediction at the instruction level has been introduced to allow even more aggressive speculation and reuse than previous techniques. This study suggests that using compiler support to extend value prediction and reuse to a coarser granularity, such as a basic block, may have substantial performance benefits. We investigate the input and output values of basic blocks and find that these values can be quite regular and predictable. For the SPEC benchmark programs evaluated, 90% of the basic blocks have fewer than 4 register inputs, 5 live register outputs, 4 memory inputs and 2 memory outputs. About 16% to 41% of all the basic blocks are simply repeating earlier calculations when the programs are compiled with the-O2 optimization level in the GCC compiler. Compiler optimizations, such as loop-unrolling and function inlining, affect the sizes of basic blocks, but have no significant impact on their value locality. Based on these results, we evaluate the potential benefit of basic block reuse using a novel mechanism called a block history buffer. This mechanism records input and live output values of basic blocks to provide value prediction and reuse at the basic block level. Simulation results show that using a reasonably-sized block history buffer to provide basic block reuse in a 4-way issue superscalar processor can improve execution time for the tested SPEC programs by 1% to 14% with an overall average of 9% with reasonable hardware assumptions.
This paper answers an open question in the design of complimentary metal-oxide semiconductor VLSI circuits. The question asks whether a polynomial-time algorithm can decide if a given planar graph has a plane embedding      has an Euler trail P = e 1 e 2 . . . em and its dual graph has an Euler trail P ff = e ff 1 e ff 2 . . . e ff m , where e ff i is the dual edge of e i for i = 1, 2, . . . , m. This paper answers this question in the affirmative by presenting a linear-time algorithm.
In a relational indexing approach (see e.g. Farradane&apos;s work), information is carried by a fixed set of relationship  types over an underlying set of terms. The idea is that the essence of the meaning of information is encapsulated  in the relationships between terms. The importance of relationships is now widely recognized within many fields  such as relational databases and knowledge representation formalisms. These fields have substantially improved our  understanding of relationships and the problems involved in trying to formalize them. However, although those relationships  can be correctly represented by almost all the well-known formalisms in such fields, they are not exploited  as much as the objects by concrete operations. In information retrieval, previous attempts at managing relationships  have mainly addressed structural aspects, and exclude the manipulation of index expressions by relational operations.
The paper presents a general architecture for behaviour based control systems for autonomous agents. A number of archi tectural principles are proposed which make it possible to combine reactive control with learning and problem solving in a  coherent way. In particular, I investigate the interaction between reinforcement learning, internal world models and dynamic action selection as well as a number of connections to psychological models and biological systems.
Real-time tasks for command and control systems are too large or too complex for one processor to handle. Simply adding more CPUs does not result in a linear increase in performance. Current comparative analysis of parallel algorithms does not accurately reflect the increased cost of scheduling when more processors are added. A case is made that associative processors effectively handle real-time command and control type problems and avoid most of the difficulties introduced by multiprocessors. These results suggest that when comparing different architectures, comparative analysis should consider the ALU and control unit {CU} separately.
In this paper, we present in detail our approach to constructing a world model in a multi-robot team. We introduce two separate world models, namely an individual world model that stores one robot&apos;s state, and a shared world model that stores the state of the team. We present procedures to effectively merge information in these two world models in real-time. We overcome the problem of high communication latency by using shared information on an as-needed basis. The success of our world model approach is validated by experimentation in the robot soccer domain. The results show that a team using a world model that incorporates shared information is more successful at tracking a dynamic object in its environment than a team that does not use shared information.
We study the problem of representing symmetric Boolean functions as symmetric polynomials over Zm . We show an equivalence between such representations and simultaneous communication protocols. Computing a function f on 0 1 inputs with a polynomial of degree d modulo pq is equivalent to a two player simultaneous protocol for computing  f where one player is given the first dlog p de digits of the weight in base p and the other is given the first dlog q de digits of the weight in base q. This reduces the problem of proving bounds on the degree of symmetric polynomials to proving bounds on simultaneous communication protocols. We use this equivalence to show lower bounds of   on symmetric polynomials weakly representing classes of Mod r and Threshold functions. Previously the best known lower bound for symmetric polynomials weakly representing any function over Zm was n    [1] where t is the number of distinct prime factors of m. We show there exist symmetric polynomials over Zm of degree o(n) strongly representing   Threshold c for c constant, using the fact that the number of solutions of certain exponential Diophantine equations are finite. Conversely, the fact that the degree is o(n) implies that some classes of Diophantine equations can have only finitely many solutions. Our results give simplifications of many previously known results and show that polynomial representations are intimately related to certain questions in number theory.
The Object Constraint Language (OCL) is based on first-order logic and set theory. As the most well-known application, OCL is used to formulate well-formedness rules in the UML metamodel. Here, the transitive closure of a relationship is defined in terms of an OCL invariant, which seems to contradict classical results on the expressive power of first-order logic. In this paper, we give sufficient...
The subject of this paper is the research that aims at efficiency improvement of acquisition of 3D building models from digital images for Computer Aided Architectural Design (CAAD). The results do not only apply to CAAD, but to all applications where polyhedral objects are involved. The research is concentrated on the integration of a priori geometric object information in the modeling process. Parallelism and perpendicularity are examples of the a priori information to be used. This information leads to geometric constraints in the mathematical model. This model can be formulated using condition equations with observations only. The advantage is that the adjustment does not include object parameters and the geometric constraints can be incorporated in the model sequentially. As with the use of observation equations statistical testing can be applied to verify the constraints. For the initial values of orientation parameters of the images we use a direct solution based on a priori object information as well. For this method only two sets of (coplanar) parallel lines in object space are required. The paper
The need for fast parallel table lookups is evident in many modern hardware applications, such as network switches, hard disk controllers, and encryption devices. Typically, most of these table lookups are performed in fast and expensive on-board SRAMs in order to reduce latency. These SRAMs frequently provide dual-ported access at speeds of up to 20 ns. However, for applications demanding many large look-up tables, SRAM&apos;s physical size, density, power requirements, and cost are prohibitive. In this paper, we address this problem through one particularly demanding example: the routing control in a sophisticated ATM switch. We present a design that uses merged memory and logic (MML, a modified form of DRAM) to simulate dual-ported SRAM in performing tens of table lookups in parallel. Our solution fits on one chip instead of over 300 required by an existing design, providing an integrated, low-power solution while still meeting the rigorous timing constraints of the application.
The problem of answering queries using views has been studied extensively, due to its relevance in a wide variety of data-management applications. In these applications, we often need to select a subset of views to maintain, due to limited resources. In this paper, we show that traditional query containment is not a good basis for deciding whether or not a view should be selected. Instead, we should minimize the view set without losing query-answering power. To formalize this notion, we rst introduce the concept of &quot;p-containment.&quot; That is, a view set V is p-contained in another view set W, if W can answer all the queries that can be answered by V. We show that p-containment and the traditional query containment are not related; i.e., one does not imply the other. We then discuss how to minimize a view set while retaining its query-answering power. We develop the idea further by considering p-containment of two view sets with respect to a given set of queries, and consider their relationship in terms of maximally-contained rewritings of queries using the views.  
In this poster we introduce ProThes, a pilot meta-search engine (MSE) for a specific application domain. ProThes combines three approaches: meta-search, graphical user interface (GUI) for query attempts to employ domain-specific knowledge, which is represented by both a conceptual thesaurus and results ranking heuristics. Since the knowledge representation is separated from the MSE core, adjusting the system to a specific domain is trouble free. Thesaurus allows for manual query building and automatic query techniques. This poster outlines the overall system architecture, thesaurus representation format, and query operations. ProThes is implemented on J2EE platform as a Web service.
Using test to validate conformance of Java Card applications needs to take into account objectoriented specificity of the model and the implementation. In this article, we propose a method which integrates test hypotheses in order to build a UML model of Java Card applications. For each hypothesis, we describe how we can use it in order to obtain a model which is test oriented, and as most as possible understandable and maintainable. We use a simplified classical example of smart card application to illustrate our presentation.
A system for recovering 3D hand pose from monocular color sequences is proposed. The system employs a non-linear supervised learning framework, the specialized mappings architecture (SMA), to map image features to likely 3D hand poses. The SMA&apos;s fundamental components are a set of specialized forward mapping functions, and a single feedback matching function. The forward functions are estimated directly from training data, which in our case are examples of hand joint configurations and their corresponding visual features. The joint angle data in the training set is obtained via a CyberGlove, a glove with 22 sensors that monitor the angular motions of the palm and fingers. In training, the visual features are generated using a computer graphics module that renders the hand from arbitrary viewpoints given the 22 joint angles. The viewpoint is encoded by two real values, therefore 24 real values represent a hand pose. We test our system both on synthetic sequences and on sequences taken with a color camera. The system automatically detects and tracks both hands of the user, calculates the appropriate features, and estimates the 3D hand joint angles and viewpoint from those features. Results are encouraging given the complexity of the task.
An important reason for the continued popularity of Artificial Neural  Networks (ANNs) in the machine learning community is that the gradient-descent  backpropagation procedure gives ANNs a locally optimal change procedure and,  in addition, a framework for understanding the ANN learning performance. Genetic  programming (GP) is also a successful evolutionary learning technique that  provides powerful parameterized primitive constructs. Unlike ANNs, though, GP  does not have such a principled procedure for changing parts of the learned system  based on its current performance. This paper introduces Neural Programming,  a connectionist representation for evolving programs that maintains the benefits  of GP. The connectionist model of Neural Programming allows for a regression  credit-blame procedure in an evolutionary learning system. We describe a general  method for an informed feedback mechanism for Neural Programming, Internal  Reinforcement. We introduce an Internal Reinforcement procedure and demonstrate  its use through an illustrative experiment.
With the advent of software agents and assistants, the concept of so called conversational user interfaces evolved, incorporating natural language interaction, dialogue management, and anthropomorphic representations. Today&apos;s challenge is to build a suitable visualization architecture for anthropomorphic conversational user interfaces, and to design believable and appropriate face-to-face interaction imitating human attributes such as emotions. The system is designed as an autonomous agent enabling easy integration into a variety of scenarios. Architecture, protocols, and graphical output are discussed.
CentiJ is a software synthesis system that, until recently, used synchronous, semiautomatic static proxy delegation to help in the automation of the creation of distributed Java programs on NOWS (Networks of Workstations). This paper reports our recent extension to CentiJ so that invocations are asynchronous. Further, we have achieved transparency with respect to local vs. non-local asynchronous invocations so that software can be properly tested in a local mode.
A novel approach is developed for efficient and accurate tracking of vocal tract resonances, which are natural frequencies of the resonator from larynx to lips, in fluent speech. The tracking algorithm is based on a version of the structured speech model consisting of continuous-valued hidden dynamics and a piecewise-linearized prediction function from resonance frequencies and bandwidths to LPC cepstra. We present details of the piecewise linearization design process and an adaptive training technique for the parameters that characterize the prediction residuals. An iterative tracking algorithm is described and evaluated that embeds both the prediction-residual training and the piecewise linearization design in an adaptive Kalman filtering framework. Experiments on tracking vocal tract resonances in Switchboard speech data demonstrate high accuracy in the results, as well as the effectiveness of residual training embedded in the algorithm. Our approach differs from traditional formant trackers in that it provides meaningful results even during consonantal closures when the supra-laryngeal source may cause no spectral prominences in speech acoustics.
Responses of Gabor wavelets in the mid-frequency space build a local spectral representation scheme with  optimal properties regarding the time-frequency uncertainty principle. However, when using Gabor wavelets  we observe a skewness in the mid-frequency space caused by the spreading effect of Gabor wavelets. Though  in most current applications the skewness does not obstruct the sampling of the spectral domain, it affects the  identification and separation of source signals from the filter response in the mid-frequency space. In this paper,  we present a modification of the original Gabor filter, the skew Gabor filter, which corrects skewness so that  the filter response can be described with a sum-of-Gaussians model in the mid-frequency space. The correction  further enables us to use higher-order moment information to separate different source signal components. This  provides us with an elegant framework to deblur the filter response which is not characterized by the limited  spectral resolution of other local spectral representations.
The process chain in optical measurement techniques can be subdivided into four main components: the camera system, the object range, the network design and the analysis system. The included influences (e.g. camera geometry, illumination, algorithms for image measurement) cause remaining deviations on the results due to insufficiently known effects on the photogrammetric system. This article will introduce a simulation technique based on Monte-Carlo-Methods to analyse effects of camera geometry, object space, signalisation and illumination. First two topics will be discussed based on simulation results. It allows a closer look at single system components, their uncertainty and randomly distribution simultaneously to the estimation of their influence on the photogrammetric system. The described Monte-Carlo-Simulation provides an economical process where the effects can be separated and modelled within an acceptable period of time and amount of work. It enables the determination of optimal system components (e.g. signalisation, illumination, camera geometry, analysis) and, in addition, the estimation of their influences on the process chain due to given (fixed) system components.
In this contrx]MJJJ we investigate the use ofdescr0xFV] logics (DLs)for inforVVMMx rnfor al in a multiagent scenarxF Wefir[ descr6 e two advanced DLs andprx]M t therex] ant rM]0]3xF ser0]3x pr videdfor inforVMM1x rnfor al, inpar631xFJ instancerstanc al, instance checking and example-based instancerstanc al. Complete and soundalgor6MVJ existfor each of these tasks in both DLs, but it is shown that a combined DL is undecidable. Inor1M to make use of knowledge bases which usedifferM t DLs, abr1 er1]1]x multiagentinforxFJ]S rnfor al scheme isprM1] ted. The main idea is to pose trJ6330xFJ quer63 to individual agents and combine the answer to obtain a cor[[1 but not necessarFM completermplet TheapprJJ h isillustrM33 with detailed examples.  
: Lossy video compression algorithms, such as those used in the H.261 and MPEG standards, result in quality degradation seen in the form of tiling, edge busyness, and mosquito noise. The number of bits required to encode a scene so as to achieve a given quality objective depends on the scene content; the more complex the scene is, the more bits are required. Therefore, in order to achieve a given video quality at all times, the encoder parameters must be appropriately adjusted according to the scene content. In this paper, we propose a video encoding scheme which maintains the quality of the encoded video at a constant level. This scheme is based on a quantitative video quality measure, and it uses a feedback control mechanism to control the parameters of the encoder. We evaluate this scheme by applying it to test sequences, and compare it with Constant Bit Rate and Open-Loop Variable Bit Rate schemes in terms of quality and rate. We show that our scheme achieves better quality than th...
. A critical concern in the reuse of software is the propagation of changes made to reusable artifacts. Without techniques to manage these changes, multiple versions of these artifacts will propagate through different systems and reusers will not be able to benefit from improvements to the original artifact. We propose to codify the management of change in a software system by means of reuse contracts that record the protocol between managers and users of a reusable asset. Just as real world contracts can be extended, amended and customised, reuse contracts are subject to parallel changes encoded by formal reuse operators: extension, refinement and concretisation. Reuse contracts and their operators serve as structured documentation and facilitate the propagation of changes to reusable assets by indicating how much work is needed to update previously built applications, where and how to test and how to adjust these applications.  1 Introduction  It has become a well-known fact that the...
In infocommunications networks the available bandwidth typically varies in time. The transmission rate of elastic traffic can be tuned according to the actual network state. Assuming such elastic traffic there arises the problem how to allocate resources (bandwidth) to sources in a fair manner and how to protect connections against failures. In recent related works the paths of the demands are given in advance, consequently setting up elastic source rates in fair way leads to suboptimal solution. Better results can be achieved if we determine the bandwidth of elastic sources AND the routes used by these demands simultaneously. In several applications it is meaningful to define minimum and maximum rate for sources. For this case we propose the definition of Relative Fairness (RF). In this paper different resource allocation...
. Mobility is a key concept for network programming; it  has stimulated much research about new programming languages and  paradigms. In the design of programming languages for mobile agents,  i.e. processes which can migrate and execute on new hosts, the integration  of security mechanisms is a major challenge. This paper presents  the security mechanisms of the programming language Klaim (a Kernel  Language for Agents Interaction and Mobility). The language, by  making use of a capability{based type system, provides direct support  for expressing and enforcing policies that control access to resources and  data.  1 Introduction  Most of the dicult issues to face when developing mobile applications running over a network are related to security. A typical example of security property is the requirement that only legitimate mobile agents can be granted access to specic resources, or to specic services. A common solution to this problem is to provide secure communication channels by m...
Nowadays many newspapers and news agencies offer personalized  information access services and, moreover, there is a growing interest in the  improvement of these services. In this paper we present a methodology useful  to improve the intelligent personalization of news services and the way it has  been applied to a Spanish relevant newspaper: ABC. Our methodology  integrates textual content analysis tasks and machine learning techniques to  achieve an elaborated user model, which represents separately short-term needs  and long-term multi-topic interests. The characterization of a user&apos;s interests  includes his preferences about structure (newspaper sections), content and  information delivery. A wide coverage and non-specific-domain classification  of topics and a personal set of keywords allow the user to define his preferences  about content. Machine learning techniques are used to obtain an initial  representation of each category of the topic classification. Finally, we introduce  some details about the Mercurio system, which is being used to implement this  methodology for ABC. We describe our experience and an evaluation of the  system in comparison with other commercial systems.
Much of the existing work in peer to peer networking assumes that users will follow prescribed protocols without deviation. This assumption ignores the user&apos;s ability to modify the behavior of an algorithm for self-interested reasons. We advocate
... applications for robotic manipulators. The artificial neural networks (ANNs) are used for both residual generation and residual analysis. A multilayer perceptron (MLP) is employed to reproduce the dynamics of the robotic manipulator. Its outputs are compared with actual position and velocity measurements, generating the so-called residual vector. The residuals, when properly analyzed, provides an indication of the status of the robot (normal or faulty operation). Three ANNs architectures are employed in the residual analysis. The first is a radial basis function network (RBFN) which uses the residuals of position and velocity to perform fault identification. The second is again an RBFN, except that it uses only the velocity residuals. The third is an MLP which also performs fault identification utilizing only the velocity residuals. The MLP is trained with the classical back-propagation algorithm and the RBFN is trained with a Kohonen self-organizing map. We validate the concepts discussed in a thorough simulation study of a Puma 560 and with experimental results with a 3-joint planar manipulator.
We examine the requirements for a publicly accessible, online collection of three-dimensional biomedical image data, including those yielded by radiological processes such as MRI, ultrasound and others. Intended as a repository and distribution mechanism for such medical data, we created the National Online Volumetric Archive (NOVA) as a case study aimed at identifying the multiple issues involved in realizing a large-scale digital archive. In the paper we discuss such factors as the current legal and health information privacy policy affecting the collection of human medical images, retrieval and management of information and technical implementation. This project culminated in the launching of a website that includes downloadable datasets and a prototype data submission system.
Our contribution in this paper is e3D, a diffusion based routing protocol that prolongs the system lifetime, evenly distributes the power dissipation throughout the network, and incurs minimal overhead for synchronizing communication. We compare e3D with other algorithms in terms of system lifetime, power dissipation distribution, cost of synchronization, and simplicity of the algorithm.
The Farsite distributed file system stores multiple replicas  of files on multiple machines, to provide file access even when some machines  are unavailable. Farsite assigns file replicas to machines so as to  maximally exploit the different degrees of availability of different machines,  given an allowable replication factor R. We use competitive analysis  and simulation to study the performance of three candidate hillclimbing  replica placement strategies, MinMax, MinRand, and RandRand,  each of which successively exchanges the locations of two file replicas. We  show that the MinRand and RandRand strategies are perfectly competitive  for R = 2 and 2/3-competitive for R = 3. For general R, MinRand  is at least 1/2-competitive and RandRand is at least 10/17-competitive. The MinMax
Historically, advances in compiler technology have been driven by the characteristics of applications, particularly those that comprise the SPEC benchmark suite. To achieve high performance, many of the most promising compilation techniques rely on accurate profile information to direct the optimization and instruction scheduling process. Prior studies have shown that for these applications it is possible to generate representative data sets that are suitable for profile -based compilation algorithms. Though we would like to apply sophisticated compiler techniques to operating systems (and eventually other multithreaded software systems such as database management systems), we are presented with two significant problems. First, the process of profiling the operating system is more difficult than profiling an application, because of the complexity of the operating system and the interaction between the operating system and its applications. Second, even when one can profile the operatin...
We address the problem of designing optimal buffer management policies in shared memory  switches when packets already accepted in the switch can be dropped (pushed-out). Our goal is  to maximize the overall throughput, or equivalently to minimize the overall loss probabilityin  the system. For a system with two output ports, weprove that the optimal policy is of pushout  with threshold type (POT). The same result holds if the optimality criterion is the weighted  sum of the port loss probabilities. For this system, we also give an approximate method for the  calculation of the optimal threshold, whichwe conjecture to be asymptotically correct. For the  N-ported  system, the optimal policy is not known in general, but weshow that for a symmetric  system (equal traffic on all ports) it consists of always accepting arrivals when the buffer is not  full, and dropping one from the longest queue to accommodate the new arrival when the buffer  is full. Numerical results are provided which revealaninteresting and somewhat unexpected  phenomenon. While the overall improvement in loss probability of the optimal POT policy over  the optimal coordinate-convex policy is not very significant, the loss probabilityofanindividual  output port remains approximately constant as the load on the other port varies and the optimal  POT policy is applied, a property not shared by the optimal coordinate-convex policy.
In this paper we solve the important problem of relieving link congestions, without the knowledge of  origin-destination traffic demand, in hop-by-hop routed networks with shortest path routing algorithms.
Product classification systems play a major role in searching and comparing offered products on electronic markets. Especially in case of large multi-vendor product catalogs classified data becomes an important asset and success factor. The most known systems are UNSPSC and eCl@ss, however they are still developing, and new systems are emerging as well. Classification systems differ not only in content but also in structure from each other. The management and exchange of the systems between market partners must be able to get along with these differences. A common structure model describing classification systems is missing so far. This paper discusses the design of classification systems and argues to develop standardized messages using XML Schema for the transmission of classification systems.
A generalization of Allen&apos;s interval-based approach to temporal reasoning is presented. The notion of `conceptual neighborhood&apos; of qualitative relations between events is central to the presented approach. Relations between semi-intervals rather than intervals are used as the basic units of knowledge. Semi-intervals correspond to temporal beginnings or endings of events. We demonstrate the advantages of reasoning on the basis of semi-intervals: 1) semi-intervals are rather natural entities both from a cognitive and from a computational point of view; 2) coarse knowledge can be processed directly; computational effort is saved; 3) incomplete knowledge about events can be fully exploited; 4) incomplete inferences made on the basis of complete knowledge can be used directly for further inference steps; 5) there is no trade-off in computational strength for the added flexibility and efficiency; 6) for a natural subset of Allen&apos;s algebra, global consistency can be guaranteed in polynomial time; 7) knowledge about relations between events can be represented much more compactly.
Due to the increasing necessity and availability of information from different sources, information integration is becoming one of the challenging issues in artificial intelligence and computer science. A successful methodology for information integration is based on Federated Databases. However, differently form databases, a completely satisfactory formal treatment of federated databases is still missing. The goal of this paper is to fill this gap by providing a model theoretic semantics, called Local Models Semantics for federated databases. Our basic intuition is that a federated database can be formalized by representing each database as a set of local models. We argue that this perspective is a promising one, as many relevant problems in information integration, such as semantic heterogeneity, interschema dependencies, query distribution, local control over data and processing, and transparency, can be successfully solved by local model semantics. In the paper we provide a formal ...
Web services are an emerging technology that provides a flexible platform  for web interaction. We evaluate Web service performance of handheld  resource-constrained clients using different wireless technologies. Due to the  usage of XML, message sizes in Web services are larger than in traditional web  technologies and therefore, compression of Web service messages is attractive;  as our experiments show in particular for mobile clients with poor connectivity  and high communication costs. However, compression requires CPU time at  both the server and the clients. We present measurement results of a simple  dynamic scheme that provides benefits by compressing responses only when  the required server resources are available.
this paper, we study which of the Bluetooth and Wireless LAN technology features (like radio-signal  strength, device address management, etc.) can be exploited to derive user context, and develop a procedure how low  level sensor data can be brought to application level context information. We introduce a method to automatically  classify heterogeneous sensor data features with supervised or un-supervised classification methods. By defining two  operations, a distance metric and an adaptation operator, any feature can be used as input for the classifier and can  thus contribute to context detection
Recently, reading comprehension tests  for students and adult language learners  have received increased attention  within the NLP community as a means  to develop and evaluate robust question  answering (NLQA) methods. We  present our ongoing work on automatically  creating richly annotated corpus  resources for NLQA and on comparing  automatic methods for answering  questions against this data set. Starting  with the CBC4Kids corpus, we have  added XML annotation layers for tokenization,  lemmatization, stemming,  semantic classes, POS tags and bestranking  syntactic parses to support future  experiments with semantic answer  retrieval and inference. Using this  resource, we have calculated a baseline  for word-overlap based answer retrieval  (Hirschman et al., 1999) on the  CBC4Kids data and found the method  performs slightly better than on the REMEDIA  corpus. We hope that our richly  annotated version of the CBC4Kids corpus  will become a standard resource, especially  as a controlled environment for  evaluating inference-based techniques.
We present a system which is able to visually detect human faces, to track them by controlling a robot-head, and to pursue a detected person by means of driving movements. The detection is based on a multi-modal approach combining color, motion, and contour information. By using a stereo algorithm the position of the person in the scene is determined. Both the path of the person going ahead and a local environment map built by means of range sensor data are used to perform the navigation task. Stationary and dynamic obstacles are avoided during the process of pursuit. 1 Introduction  Acceptance of mobile service robots strongly depends on a man-machine interface which allows the user to communicate and to interact with the robot in an easy and natural way. Speech recognition and generation as well as the visual perception of the environment and in particular the perception of the human user are basic tasks within this context. The problem of integrating several individual basic behavio...
This article introduces the \CAM-Brain Machine&quot; (CBM),  an FPGA based piece of hardware which implements a genetic algorithm  (GA) to evolve a cellular automata (CA) based neural network circuit  module, of approximately 1,000 neurons, in about a second, i.e. a complete  run of a GA, with 10,000s of circuit growths and performance evaluations.
In order to perform effective communication, agents must be able to foresee the effects of their utterances on the addressee&apos;s mental state. In this paper we study the consequences of an utterance on the mental state of a hearer. Given an agent communication language with a STRIPS-like semantics, we propose a set of criteria that allow the binding of the speaker&apos;s mental state to its uttering of a certain sentence. On the basis of these criteria, we give an abductive procedure that the hearer can adopt to partially recognize the speaker&apos;s mental state that led to a specific utterance.
As a conffuence of data mining and WWW technologies, it is now possible to perform data mining on web logrecords collectedfrom the Internet web page access history. The behaviour of the web page readers is imprinted in the web server log ffles. Analyzing and exploring regularities in this behaviour can improve system performance, enhance the quality and delivery of Internet information services to the end user, and identify population of potential customers for electronic commerce. Thus, by observing people using collections of data, data mining can bring considerable contribution to digital library designers.
This paper presents a new approach to dependence testing in the presence of nonlinear and non-closed array index expressions and pointer references. The chains of recurrences formalism and algebra is used to analyze the recurrence relations of induction variables, and for constructing recurrence forms of array index expressions and pointer references. We use these recurrence forms to determine if the array and pointer references are free of dependences in a loop nest. Our recurrence formulation enhances the accuracy of standard dependence algorithms such as the extreme value test and range test. Because the recurrence forms are easily converted to closed forms (when they exist), induction variable substitution and array recovery can be delayed until after the loop is analyzed.
In this paper, we study the problem of pointwise motion tracking in echocardiographic images. We show that decorrelation between tissue motion and intensity variation is inevitable for certain kinds of tissue motion and decorrelation compensation is an ill-posed inverse problem if the decorrelation is beyond a certain correlation threshold. We compare the performance of different features using simulations and phantom examples. We find a threshold value of correlation coefficients below which the B-Mode signal works better than the radio frequency (RF) signal in the analysis of large deformation. We also demonstrate that the introduction of a quantitative reliability measure helps to improve the robustness of displacement estimation.
Introduction  E-catalog portals, such as Expedia.com and Amazon. com, are becoming more and more prominent feature of the Web. They aim to offer one-stop shopping experience for the users. However, the users still need to access a number of portals separately, or to use search engines in order to get complete information they are looking for. It is clearly useful to provide a unified interface to access multiple e-catalog portals. The issue here is that the technology to create, organise, integrate and search these portals has not kept pace with the rapid growth of the available information space. Most existing approaches for providing access to integrated e-catalogs as a portal are based on (i) creating centralised product data repository collected from participating e-catalog providers, (ii) statically linking manually (ad-hoc) identified ecatalogs to the portal. Surely, these are not scalable approaches. First, we cannot expect the integrators to understand underlying schemas of thou
IP (Internet Protocol) Telephony requires the support of guaranteed services and charging to provide a valuable service for potential customers. It has to be considered carefully, that the quality of long-distance telephony calls via the Internet is heavily affected by the load of the links which the call has to traverse. As these different quality requirements of users will exist, the Internet has to support different service classes. Therefore, an advanced services network model is required.  In turn, and that is the main motivation for this work, if at least two traffic classes will exist in the Internet, there must be the right incentive for any user to choose the traffic class which optimally fits his requirements and will be the most efficient one in terms of prices to be payed. Therefore, the integration of charging and Quality-of-Service (QoS) interfaces for IP telephony are important to stimulate future use of the Internet. This papers gives a first overview of OCIT (Open Char...
In this paper, we study energy conservation techniques for disk array-based network servers. First, we introduce a new conservation technique, called Popular Data Concentration (PDC), that migrates frequently accessed data to a subset of the disks. The goal is to skew the load towards a few of the disks, so that others can be transitioned to low-power modes. Next, we introduce a user-level file server that takes advantage of PDC. In the context of this server, we compare PDC to the Massive Array of Idle Disks (MAID). Using a validated simulator, we evaluate these techniques for conventional and two-speed disks and a wide range of parameters. Our results for conventional disks show that PDC and MAID can only conserve energy when the load on the server is extremely low. When two-speed disks are used, both PDC and MAID can conserve significant energy with only a small fraction of delayed requests. Overall, we find that PDC achieves more consistent and robust energy savings than MAID.
This paper proposes an approach to full  parsing suitable for Information Extraction  from texts. Sequences of cascades of  rules deterministically analyze the text,  building unambiguous structures. Initially  basic chunks are analyzed; then argumental  relations are recognized; finally  modifier attachment is performed and  the global parse tree is built. The approach  was proven to work for three languages  and different domains. It was implemented  in the IE module of FACILE,  a EU project for multilingual text classification  and IE.  1 Introduction  Most successful approaches in IE (Appelt et al., 1993; Grishman, 1995; Aone et al., 1998) make a very poor use of syntactic information. They are generally based on shallow parsing for the analysis of (non recursive) NPs and Verbal Groups (VGs). After such step regular patterns are applied in order to trigger primitive actions that fill template(s); meta-rules are applied to patterns to cope with different syntactic clausal forms (e.g...
In this paper we analyse the mapping behavior of an autoassociative neural network (AANN). The mapping in an AANN is achieved by using a dimension reduction followed by a dimension expansion. One of the major results of the analysis is that, the network performs better autoassociation as the size increases. This is because, a network of a given size can deal with only a certain level of nonlinearity. Performanceofautoassociative mapping is illustrated with 2-D examples. We have shown the utility of the mapping feature of an AANN for speaker verification.
This article presents a research agenda for the study of digital reference. The agenda stems from a research symposium held at Harvard in August 2002. The agenda defines digital reference as &quot;the use of human intermediation to answer questions in a digital environment.&quot; The agenda also proposes the central research question in digital reference - &quot;How can human expertise be effectively and efficiently incorporated into information systems to answer user questions?&quot; The definition and question are used to outline a research agenda centered on how the exploration of digital reference relates to other fields of inquiry. Background Digital reference is a distinct and growing practice in libraries today. Academic, public and special libraries are rushing to offer human intermediation services over the Internet. Conference sessions on the topic are filled, workshops are well attended, and major library organizations (RUSA, ACRL, OCLC) are developing digital reference training. However, unlike the interest shown in studying digital libraries, there has been relatively little research interest in digital reference. Aside from the work of a small group of researchers (Lankes 2001, Janes , McClure 2001 and White ) digital reference has remained primarily in the province of the practitioner. Practitioner orientation to digital reference can be demonstrated when one looks at the Virtual Reference Desk (VRD) Conference (http://www.vrd.org/conf-train.shtml). Attendance for the VRD conference, the largest and only international conference dedicated to digital reference, has steadily increased every year since its inception in 1999. The 2001 VRD conference organizers received nearly 80 session proposals, only 3 of which were submitted by Library and Information Science (LIS) faculty me...
We propose a new near-real time technique for 3D face pose tracking from a monocular image sequence obtained from an uncalibrated camera. The basic idea behind our approach is that instead of treating 2D face detection and 3D face pose estimation separately, we perform simultaneous 2D face detection and 3D face pose tracking. Specifically, 3D face pose at a time instant is constrained by the face dynamics using Kalman Filtering and by the face appearance in the image. The use of Kalman Filtering limits possible 3D face poses to a small range while the best matching between the actual face image and the projected face image allows to pinpoint the exact 3D face pose. Face matching is formulated as an optimization problem so that the exact face location and 3D face pose can be estimated effciently. Another major feature of our approach lies in the use of active IR illumination, which allows to robustly detect eyes. The detected eyes can in turn constrain the face in the image and regularize the 3D face pose, therefore the tracking drift issue can be avoided and the processing can speedup. Finally, the face model is dynamically updated to account for variations in face appearances caused by face pose, face expression, illumination and the combination of them. Compared with
In [3], we proposed a first formal and conceptual comparison between  the two important formalizations of context in AI: Propositional Logic of Context  (PLC) [4] and Local Models Semantics/MultiContext Systems (LMS/MCS) [9, 7]. The result
In this paper, we take an availability-centric view on Quality of Service  (QoS) and focus on the issues of providing availability guarantees for widely distributed  systems such as web servers and peer-to-peer (P2P) file sharing systems.
This report investigates two semantic embeddings of Z schemas in Isabelle/HOL. The first represents Z values as elements of a type class with polymorphic type constructors and overloaded operators. In contrast, the second embedding uses a Z universe: all Z values are represented as elements of a single monomorphic HOL type.
Most current-day software engineering tools and environments do not sufficiently allow software engineers to declare or enforce the intended software architecture. On the one hand, architectures are typically described at a too lowlevel, inhibiting their evolution and understanding. On the other hand most tools provide little support to automatically verify whether the source code conforms to the architecture. Therefore, a formalism is needed in which architectures can be expressed at a sufficiently abstract level, without losing the ability to perform automatic conformance checking. We propose to declaratively codify software architectures using virtual software classifications and relationships among these classifications. We illustrate how software architectures can be expressed elegantly in terms of these virtual classifications and how to keep them synchronized with the source code.
In the last years a lot of attention was paid to Multiple Antenna systems with Space-Time Coding (STC) combined with Ultra Wide Band (UWB) transceiver in order to evaluate if these two technology can be considered adequate candidates for next generation WLANs. Merging Ultra Wide Band signals with Multiple Antenna schemes allows us to achieve very high bit rate and error probability matched to QoS requested by 4GWLANs at low SNR. This contribution presents a simple way to combine Ultra Wide Band and Multiple Antenna schemes so to improve performance with respect to the conventional UWB approach in order to promote this approach application scenarios impaired by Rayleigh flat and selective fading are considered.
Although security is an important issue when developing complex computerised systems, very little work has been done in integrating security concerns in the agentoriented methodologies. This paper introduces extensions to the Tropos methodology to accommodate security. A description of new concepts is given along with an explanation of how these concepts are integrated to the current stages of Tropos. The above is illustrated using an agent-based health and social care information system as a case study.
In a P2P free-market resource economy a client peer may want to select multiple server peers for either downloading a file or streaming a stored audio or video object. In general, multiple server peers will make the object available, with each peer offering a different price. The optimal peer selection problem is to select from a subset of the peers those peers that can provide the service at lowest cost. In this paper we formulate and solve the problem of optimally selecting a subset of peers for parallel downloading and for parallel streaming.
The paper presents the case study of the application of the theoretical framework of modelling the processes of information interchange among the members of evolving intelligent agent communities to modelling of one of practically important types of business processes - a planning process. A business process in frame of the presented research is denoted as a set (task) of atomic works. In the frame of the applied modelling approach the members of dynamically formed, scalable and evolving intelligent agent communities act as functional components performing the atomic works of such a task. The principal alterity of the approach is the usage of parametric feedbacks and, alternatively, agent state constraints to atomic works execution sequence control. Case study analysis shows that the framework is practically applicable to modelling of planning processes. Furthermore, the hypothesis that the framework is applicable to another types of business processes is discussed.
There is growing concern that the predictive mathematical models conventionally used in policy analysis are too limiting to serve as tools in futures studies, because they cannot reproduce the sudden changes seen in real societies. The field of complex systems has successfully produced similar changes in simplified model systems, but has been less successful in practical futures work. Some recent scenario exercises (such as the IPCC scenarios, UNEP&apos;s GEO-3 scenarios, the work of the Global Scenario Group and the European VISIONS project) have addressed this issue by combining wide-ranging narratives with quantitative models, demonstrating that a synthesis between qualitative and quantitative approaches is possible. However, there is no consensus on an appropriate methodology. In this paper it is argued that there are essentially two analytical challenges that scenario models must address in order to achieve the goal of more robust planning in the face of both gradual and sudden change. One is to represent complexity, while the other is to represent what might be called &quot;complicatedness.&quot; Complex behavior arises from the interrelatedness of different components of a system, while &quot;complicatedness&quot; as used here means that there are a lot of factors to keep in mind---constraints, actors, resources, etc. It will further be argued that complexity is best dealt with in narratives, and complicatedness is best dealt with using computers. The characteristics of appropriate computer models will be presented, and extant exemplars of appropriate models described.
This paper addresses the problem of channel tracking and equalization for multi-input multi-output (MIMO) time-varying frequency-selective channels. These channels model the effects of inter-symbol interference (ISI), co-channel interference (CCI), and noise. A low-order autoregressive model approximates the MIMO channel variation and facilitates tracking via a Kalman filter. Hard decisions to aid Kalman tracking come from a MIMO finite-length minimum-mean-squared-error decision -feedback equalizer (MMSE-DFE), which performs the equalization task. Since the optimum DFE for a wide range of channels produces decisions with a delay  1 0,  the Kalman filter tracks the channel with a delay. A channel prediction module bridges the time gap between the channel estimates produced by the Kalman filter and those needed for the DFE adaptation. The proposed algorithm offers good tracking behavior for multiuser fading ISI channels at the expense of higher complexity than conventional adaptive algorithms. Applications include synchronous multiuser detection of independent transmitters, as well as coordinated transmission through many transmitter/receiver antennas, for increased data rate.
Current proposals for Web querying systems have assumed a centralized processing architecture where in data is shipped from the remote sites to the user&apos;s site. We present here the design and implementation of DIASPORA, a highly distributed query processing system for the Web. It is based on the premise that several web applications are more naturally processed in a distributed manner, opening up possibilities of significant reductions in network traffic and user response times. DIASPORA is built
In this paper, we propose an approach which can improve  Inductive Logic Programming in multiclass problems. This approach is  based on the idea that if a whole rule cannot be applied to an example,  some partial matches of the rule can be useful. The most suitable class  should be the class whose important partial matches cover the example  more than those from other classes. Hence, the partial matches of  the rule, called partial rules, are first extracted from the original rules.
Large and complex real-time systems can benefit significantly from a component based development approach where new systems are constructed by composing reusable, documented and previously tested concurrent objects. However, reusing objects which execute under real-time constraints is problematic because often application specific time and synchronization constraints are embedded in the internals of these objects. This tight coupling of functionality and real-time constraints make objects interdependent, and consequently unlikely to be reusable in other systems. We propose a model which facilitate separate and modular specification of realtime constraints and functional behavior of objects, and show through a series of examples how this separation is possible. We present our ideas in the context of the actor-model for modelling untimed objects and our real-time synchronizers language for expressing real-time and synchronization constraints. The synchronizers govern the interaction and ...
This paper describes a coniputer program which models in detail singly-salient and doubly-salient. open-loop  and closed-loop. reluctance motor drives. The program is integrated into a larger suite of general modelling and design programs. Saturation. load-dynamics and the limitations of transducers are all included In the program. Validity of the program is demonstrated by results which show a close correlation between the actual system and the modelled sys-  tem. Emphasis is placed on the thoroughness of the modelling and on providing a flexible program. Object-orientated  programming techniques have been used to provide the desired flexibility. Kevwords. Dynamic modelling. reluctance motors. The Authors have been on servo-type performance of reluc-  tance motor drive systems, with emphasis on the total system. In the course of this work it became necessary to model the dynamic of such servo systems and this paper describes the result-  ing program and its implementation. In particularly the authors are interested in reluctance motor based servo systems that incorporate position feedback to synchronize the energization of the motor wind-  ings with the shaft position. Both unipolar and bipolar reluctance motors are of interest, see Fig. 1 and Fig. 2  respectively. reluctance motors, with their  lion synchronised to the shaft position, are often referred to as switched-reluctance motors. has been published on the dynamic modelling of this of motor. with the notable exceptions of Corda and The approach taken by these authors and the use  standard modelling packages were not suitable in this case, as dis-  cussed below. work described in this paper is part of a large ongoing project and  this constrained the design of the program in that it was necessary to Integrate t...
To investigate effects of different land use management practices on carbon fluxes at the regional scale we developed an integrated model by coupling an ecohydrological river basin model SWIM (Soil and Water Integrated Model) and a soil organic matter model SCN (Soil-Carbon-Nitrogen model). The latter is a submodel of the forest growth model 4C. The extended integrated model combines hydrological processes, crop and vegetation growth, carbon, nitrogen, phosphorus cycles and soil organic matter turnover. It is based on a three level spatial disaggregation scheme (basin, subbasin and hydrotopes), whereas a hydrotope is a set of elementary units in the subbasin with a uniform land use and soil type. The direct connection to land use, soil and climate data provides a possibility to use the model for analyses of climate change and land use change impacts on hydrology and soil organic matter turnover. Aim of this study is to test the model performance and its capability to simulate carbon pools and fluxes in right magnitude and temporal behaviour at the regional scale. As a first step, the model was parameterised and validated for conditions in East Germany, incorporating values known from literature and regionally available times series of carbon pools and fluxes. This provides verification of carbon pools and fluxes in the landscape and verifies the correct representation of the environmental processes therein. Based on this, different land management strategies (e.g. soil cultivation techniques, crop residue returns) and land use change options (e.g. conversion of agricultural areas to forest or to set-aside areas) can be simulated to assess the behaviour of water and carbon fluxes as well as carbon sequestration options.
this paper we investigate achievement of server fault tolerance by providing support in the middleware. Specifically, an instance of CORBA is used for implementing generic fault handling
We propose a new 3D kernel for the recovery of 3D orientation signatures. In the Cartesian coordinates, the kernel has a shape of a truncated cone with its axis in the radial direction and very small angular support. In the local spherical coordinates, the angular part of the kernel is a 2D Gaussian function. A set of such kernels is obtained by uniformly sampling the 2D space of azimuth and elevation angles. The projection of a local neighborhood on such a kernel set produces a local 3D orientation signature. In case of spatio-temporal analysis, such a kernel set can be applied either on the derivative space of a local neighborhood or on the local Fourier transform. The well known planes arising from one or multiple motions produce maxima in the orientation signature. The kernel&apos;s local support enables the resulting spatiotemporal signatures to possess higher orientation resolution than 3D steerable filters. Consequently, motion maxima can be detected and localized more accurately. We describe and show in experiments the superiority of the proposed kernels compared to Hough transformation or expectation -- maximization based multiple motion detection.
A novel method for human activity recognition is   presented. Given a video sequence containing human   activity, the motion parameters of each frame are first   computed using different motion parameter models. The   likelihood of these observed motion parameters is   optimally approximated, based directly on a multivariate   Gaussian probabilistic model. The dynamic change of   motion parameter likelihood in a video sequence is   characterized using a continuous density hidden Markov   model. Activity recognition is then posed as a motion   parameter maximum likelihood estimation problem.   Experimental results show that the method proposed here   works well in recognizing such complex human activities   as sitting, getting up from a chair, and some martial art   actions.     1. 
Domain-specific search engines are growing in popularity because they offer increased accuracy and extra functionality not possible with the general, Web-wide search engines. For example, www.campsearch.com  allows complex queries by age-group, size, location and cost over summer camps. Unfortunately these domain-specific search engines are difficult and timeconsuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, identifying informative text segments, and populating topic hierarchies. Using these techniques, we have built a demonstration system: a search engine for computer science research papers. It already contains over 50,000 papers and is publicly available at  www.cora.justresearch.com. 1 Introduction  As the amount of information on the Wor...
New component-based techniques are emerging, leading to new ways to develop software. Industrial component technologies such as COM, JavaBeans, EJB, or CCM are powerful but their extensive use leads to component-based software products that are difficult to understand. This paper discusses several issues in visualizing component-based software products, namely the visualization of the component model itself, the visualization of software components and finally the visualization of software assemblies.
This paper presents the description of an open architecture for the management of environmental content using Web Services. The Web Services technology can be effectively exploited for integrating on one hand the needs for dissemination of analytical data about environment, such as air, noise, traffic, etc., and on the other hand the needs of different users concerning the accessibility requirements of their devices, distributed and heterogeneous systems, remote and mobile control access. The case study of this paper is based on the OASI (Environmental Observatory of Southern Switzerland) project that permits access to the air, noise and traffic measures for Southern Switzerland. Other than through traditional web pages, this access is also made possible thanks to the deployment of software applications based on Web Services. To this aim, a number of Web Services are defined using the UML analisys and developed. They identify a) a group handling the air information, b) a group implementing the access to the noise information and c) a group implementing the access to the traffic information using the OASI&apos;s database. In addition, another group allows users to manage his/her own information, e.g. e-mail, OASI&apos;s news, user&apos;s group, etc. In conclusion, the Web Services Technology could be a good solution for the management of environmental content because it provides open and mobile access to data, interoperability among different client-server nodes, easy extensibility for integrating any kind of device into the system.
This paper introduces a new domain for believable agents (BA) and presents novel methods for dealing with the unique challenges that arise therein. The domain is providing improvisational companionship to a specific musician/user, trading  real-time solos with them in the jazz/blues setting. The ways in which this domain both conflicts with and benefits from traditional BA and interactive computer music system approaches are discussed. Band-out-of-the-Box (BoB), an agent built for this domain, is also presented, most novel in that unsupervised machine learning techniques are used to automatically configure BoB&apos;s aesthetic musical sense to that of its specific user/musician.
Three-dimensional rotational angiography is a very useful  tool for accessing abnormal vascular structures related to a variety of vascular  diseases. Quantitative study of the abnormalities could aid the radiologists  to choose the appropriate apparatuses and endovascular treatments.
Broad-coverage grammars tend to be very ambiguous. When such a grammar has to be used on a restricted domain, it can be desirable to specialize it, i.e. to trade part of the coverage for a reduction in ambiguity. This paper presents a novel formulation of grammar specialization as an optimization problem, in which the search is guided by a global measure combining coverage and ambiguity. The method, applicable to any unification grammar with a phrase-structure backbone, is shown to be effective in specializing a broad-coverage LFG for French.
In recent years, scheduling research has had an increasing impact on practical problems, and a range of scheduling techniques have made their way into real-world application development. Constraint-based models now couple rich representational flexibility with highly scalable constraint management and search procedures. Similarly, mathematical programming tools are now capable of addressing problems of unprecedented scale, and meta-heuristics provide robust capabilities for schedule optimization. With these mounting successes and advances, it might be tempting to conclude that the chief technical hurdles underlying the scheduling problem have been overcome. However, such a conclusion (at best) presumes a rather narrow and specialized interpretation of scheduling, and (at worst) ignores much of the process and broader context of scheduling in most practical environments. In this note, I argue against this conclusion and outline several outstanding challenges for scheduling research.
We consider a joint detection approach for cancellation of co--channel interference in time--division multiple access (TDMA) mobile communications systems like GSM/EDGE (Enhanced Data Rates for GSM Evolution). Concepts from reduced--state equalization of frequency--selective multiple-- input multiple--output (MIMO) channels are applied together  with prefiltering. A novel efficient prefilter computation algorithm is presented. Simulation results demonstrate the high performance of the proposed receiver for GSM/EDGE applications.
In this paper, we present the system MULINEX, a fully implemented system which  supports cross-lingual search of the WWW. Users can formulate, expand and  disambiguate queries, filter the search results and read the retrieved documents by  using only their native language. This multilingual functionality is achieved by the use  of dictionary-based query translation, multilingual document categorisation and  automatic translation of summaries and documents.  The system supports French, German and English and has been installed and tested in  the online services of two European internet content and service provider companies.  This paper focuses on the techniques and algorithms used in the MULINEX system,  explaining how each component works and how it contributes to the overall  functionality of the integrated system. The primary system functionalities are outlined  from the user perspective, followed by a description of the document database used in  the system. The technologies and linguistic resources used in the various system  components are then described in detail.  2  1 
Past attempts to model emotions for speech synthesis have  focused on extreme, &quot;basic&quot; emotion categories. The present paper suggests  an alternative representation of emotional states, by means of emotion  dimensions, and explains how this approach can contribute to making  speech synthesis a useful component of affective dialogue systems.
We present a specialized protocol logic that is built around a process language for describing the actions of a protocol. In general terms, the relation between logic and protocol is like the relation between assertions in Floyd-Hoare logic and standard imperative programs. Like Floyd-Hoare logic, our logic contains axioms and inference rules for each of the main protocol actions and proofs are protocol-directed, meaning that the outline of a proof of correctness follows the sequence of actions in the protocol. We prove that the protocol logic is sound, in a specific sense: each provable assertion about an action or sequence of actions holds in any run of the protocol, under attack, in which the given actions occur. This approach lets us prove properties of protocols that hold in all runs, while explicitly reasoning only about the sequence of actions needed to achieve this property. In particular, no explicit reasoning about the potential actions of an attacker is required.
Imagine distributed knowledge processing with autonomous activities and decentralized control where the  handling of partial knowledge will not result in unclear semantics or failure-prone behavior. In this paper, a  modular approach is taken where autonomous activities, called constraint-based knowledge brokers, process and  generate new knowledge in presence of partial information. We present a
h the failed valves could be located and replaced.  In the event the problem was solved brilliantly by John Eckert and the rest of the ENIAC project team. Thus ENIAC provided an immense  step forward in the provision of devices which could carry out  extremely lengthy and complex pre-designed activities, without human  intervention. However at first the &apos;pre-designed activities&apos;, to which  these early computers were applied were the performing of scientific calculations, and the reliability of the computers was not quite good enough to encourage the unquestioning acceptance of their output. Thus comparatively few people were involved at all directly with computers, and the extent to which they relied on the continuity and the  correctness of the functioning of their computers was quite limited. As the size, performance and reliability of computers has  increased, so has the complexity of the tasks that they are used for, and the extent of the reliance placed upon them. This reliance ma
We present TEP, a system that supports general-purpose shared computation between mutually-distrusting parties. TEP is useful for applications, such as auctions and tax preparation, that use private information from multiple participants. Such applications cannot be run on any one participant&apos;s computer without sacrificing the other participants&apos; privacy. TEP acts as a trusted service that hosts the sensitive parts of such applications. TEP uses
This paper discusses generalized B pictures in the context of the emerging JVT/H.26L compression standard. We focus on reference picture selection and linearly combined motion-compensated prediction signals. We show that bi-directional prediction exploits partially the efficiency of combined prediction signals whereas multihypothesis prediction allows a more general form of B pictures. The general concept of linearly combined prediction signals chosen from an arbitrary set of reference pictures can further improve the emerging JVT/H.26L compression standard.
Kernel PCA as a nonlinear feature extractor has proven powerful as a  preprocessing step for classification algorithms. But it can also be considered  as a natural generalization of linear principal component analysis.  This gives rise to the question how to use nonlinear features for  data compression, reconstruction, and de-noising, applications common  in linear PCA. This is a nontrivial task, as the results provided by kernel  PCA live in some high dimensional feature space and need not have  pre-images in input space. This work presents ideas for finding approximate  pre-images, focusing on Gaussian kernels, and shows experimental  results using these pre-images in data reconstruction and de-noising on  toy examples as well as on real world data.   
Reconstructing a 3D model of a human face from a video sequence is an important problem in computer vision, with applications to recognition, surveillance, multimedia etc. However, the quality of 3D reconstructions using structure from motion (SfM) algorithms is often not satisfactory. One common method of overcoming this problem is to use a generic model of a face. Existing work using this approach initializes the reconstruction algorithm with this generic model. The problem with this approach is that the algorithm can converge to a solution very close to this initial value, resulting in a reconstruction which resembles the generic model rather than the particular face in the video which needs to be modeled. In this paper, we propose a method of 3D reconstruction of a human face from video in which the 3D reconstruction algorithm and the generic model are handled separately. A 3D estimate is obtained purely from the video sequence using SfM algorithms without use of the generic model. The final 3D model is obtained after combining the SfM estimate and the generic model using an energy function that corrects for the errors in the estimate by comparing local regions in the two models. The optimization is done using a Markov Chain Monte Carlo (MCMC) sampling strategy. The main advantage of our algorithm over others is that it is able to retain the specific features of the face in the video sequence even when these features are different from those of the generic model. The evolution of the 3D model through the various stages of the algorithm is presented.
Keeping data in a secure and safe space is a real problem for many users of mobile devices because those devices are more vulnerable and fragile than the stationary ones. To solve that problem many commercial products are appearing that oer storage space in the Internet. In this paper, we present a new solution to the mentioned problem, based on the use of the agent technology, called the Locker Rental Service that goes a step further with respect to existing ones. Our service provides two speci  c advantages: autonomy and mobility. In terms of autonomy, data in the lockers are managed by agents working on behalf of the users, representing them in the network even while the users are actually disconnected. These agents take care of obtaining requested data, optimizing their format and their transmission, taking into account the communication resources available, the user preferences and the speci  c mobile device resources and characteristics. With respect to mobility, lockers can follow the users in their movements to remain close to the users&apos; actual physical location.
We present a simple generic framework to solve constraints  on any domain (finite or infinite) which has a lattice structure. The approach  is based on the use of a single constraint similar to the indexicals  used by CLP over finite domains and on a particular definition of an  interval lattice built from the computation domain. Weprovide the theoretical  foundations for this framework, aschematic procedure for the  operational semantics, and numerous examples illustrating howitcanbe  used both over classical and new domains. We also showhow lattice combinators  can be used to generate new domains and hence new constraint  solvers for these domains from existing domains.
In this paper, we present a system for the image indexing and retrieval using speech annotations based on a pre-defined structured syntax. In addition to the introduction of N-best lists for index generation, a query expansion technique is explored to enhance the query terms and to improve retrieval effectiveness. By adding the most probable substitutions for the query terms, more relevant images are distinguished from the data collection. This approach is particularly helpful to deal with those less frequently used words, including out-of-vocabulary (OOV) words, which are very common for names of people and places. Experiments on a collection of 1,200 photos show that the retrieval effectiveness is increased considerably for segment of individual domain on People, Location and Event. With this method, the average value of precision versus recall over a  combination of segments has improved significantly, from 50% to 72.4%.
We propose a new space priority mechanism, and analyze its performance in a single Constant Bit Rate (CBR) server. The arrival process is derived from the superposition of two types of traffics, each in turn results from the superposition of homogeneous ON-OFF sources that can be approximated by means of a two-state Markov Modulated Poisson Process (MMPP). The buffer mechanism enables the Asynchronous Transfer Mode (ATM) layer to adapt the quality of the cell transfer to the Quality of Service (QoS) requirements and to improve the utilization of network resources. This is achieved by &quot;Selective-Delaying and Pushing-In&quot; (SDPI) cells according to the class they belong to. The scheme is applicable to schedule delay-tolerant non-real time traffic and delaysensitive real time traffic. Analytical expressions for various performance parameters and numerical results are obtained. Simulation results in term of cell loss probability conform with our numerical analysis.
this document, the Humbold Heroes only had reinforcements from Bremen and Darmstadt. The two other universities will actively participate with the beginning of the winter semester
This paper is a user&apos;s guide to a set of Gauss procedures developed at the Bank of Canada for estimating regime-switching models. The procedures can estimate relatively quickly a wide variety of switching models and so should prove useful to the applied researcher. Sample program listings are included. Rsum  La prsente tude constitue un guide d&apos;utilisation d&apos;un ensemble de procdures de Gauss mises au point la Banque du Canada en vue de l&apos;estimation des modles changement de rgime. Ces procdures permettent d&apos;estimer de faon assez rapide une vaste gamme de modles changement de rgime et devraient s&apos;avrer utiles pour la recherche applique. Des chantillons de programmes sont inclus dans l&apos;tude.   Contents  1.0 Introduction..............................................................................................................1  1.1 What Is New in This Version ...............................................................................................1  2.0 Simple Switching ...........
Virtual prototyping as an embedded system design technique has the potential to significantly increase efficiency of the design process. An environment for automatic generation of virtual prototypes (VPs) directly from algorithmiclevel descriptions is presented here. It is implemented as part of a unified design methodology and produces VSIA compliant VPs. When applied to an industrial design flow of a UMTS receiver, this environment for automatic generation of VPs produced significant speedups over traditional manual VP creation, with savings in the order of hundreds to thousands of person-hours.
We compared the performance of 5 AES candidates, with a new performance  evaluation tool that we have developed. This tool automatically evaluates the results of a  tune-up implementation without any manual tune-up so that it figures out the lower bounds  of performance on real platforms. With this tool, we evaluated the performance of the 5 AES  candidates on Pentium II, UrtraSPARC and Itanium systems. Rijndael and Twofish  attained the highest performances across all of these platforms, so we consider these two  algorithms as good candidates for AES algorithms from the point of view of performance.
In this paper we discuss some of the problems of the current Web and show how the introduction of object-orientation provides flexible and extensible solutions. Web resources become encapsulated as objects, with well-defined interfaces through which all interactions occur. The interfaces and their implementations can be inherited by builders of objects, and methods (operations) can be redefined to better suit the object. New characteristics, such as concurrency control and persistence, can be obtained by inheriting from suitable base classes, without necessarily requiring any changes to users of these resources. We describe the W3Object model which we have developed based upon these ideas, and show, through a prototype implementation, how we have used the model to address the problems of referential integrity and transparent object (resource) migration. We also give indications of future work.
Despite extensive development over many years and significant demonstrated benefits, formal methods remain poorly accepted by industrial practitioners. Many reasons have been suggested for this situation such as a claim that they extent the development cycle, that they require difficult mathematics, that inadequate tools exist, and that they are incompatible with other software packages. There is little empirical evidence that any of these reasons is valid. The research presented here addresses the question of why formal methods are not used more widely. The approach used was to develop a formal specification for a safety-critical application using several specification notations and assess the results in a comprehensive evaluation framework. The results of the experiment suggests that there remain many impediments to the routine use of formal methods. 
Image-based 3D modelling is often an underconstrained and noise-sensitive process. Incorporation of geometrical constraints increases the robustness of this process, especially when dealing with small image sets. This relies on a human operator and his intuitive knowledge of the main properties of the objects commonly present in scenes. In this thesis we propose various approaches for the camera and model constraints satisfaction. In particular, we propose an original method for the satisfaction of constraints in an exact way. Contrary to most existing work, each proposed method is completed by algorithms dealing with problems engendered by insufficient user-provided input. The introduced methods are validated by reconstruction of 3D models from small image sets, and even from single images. The images used for reconstructions are taken from various sources, such as the Internet, postcards or architectural drawings.
Phylogenetic trees are built by examining differences in the biological traits of a set of species. An example of such  a trait is a biological network such as a metabolic pathway, common to all species but with subtle differences in  each. Phylogenetic trees of metabolic pathways represent multiple aspects of similarity and hypothetical evolution  in a single, yet complex structure that is difficult to understand and interpret. We present a visualization method  that facilitates analysis of such structures by presenting multiple coordinated perspectives simultaneously. Each  of these perspectives constitutes a useful visualization in its own right, but it is only together that they unfold their  full explorative power.
this article which does not hide patterns (such as repetitions) in the plaintext. Usage of this mode should be strongly discouraged. In the past the ECB mode was sometimes 1  recommended for the encryption of keys; however, authenticated encryption would be much better for this application (or the AES key wrapping algorithm proposed by NIST)
During the development of software systems the number of faults can significantly be reduced by prototype-based validation and verification. Therefore, if prototypes can be made available early in the software development process, the costly removal of faults in later stages of development can be reduced. In this paper, a prototyping approach for supporting the development of reactive systems is presented. This approach proposes the generation of prototypes from semi-formal development documents, which are predominant during the requirements engineering phase. To perform this generation step in a systematic manner, a formal model is employed that describes all types of development products and their relations for a given requirements engineering method.
The Java Environment for Distributed Invocation (JEDI) is efficient, dynamic, and easier to use than alternative communication systems for distributed Java objects. Existing state-of-the-art mechanisms for remote method calls on Java objects, such as RMI, require users to perform a complicated series of steps. Furthermore, the compiled static interfaces these systems use limit their functionality. This paper presents the design and implementation of JEDI&apos;s simpler approach utilizing dynamic proxies. We discuss a means of integrating JEDI with a publicly available CORBA ORB, followed by the tests used to ensure the robustness of the JEDI system. Comparing this system&apos;s performance with that of other communication facilities such as UDP, TCP, and RMI demonstrates the efficiency of JEDI. A calendar program illustrates the flexibility and usability tradeoffs of employing JEDI in distributed client-server applications.
We recently proposed a range-based Service Level Agreement (SLA) [15] approach and edge provisioning in DiffServ capable Virtual Private Networks (VPNs) to customers that are unable or unwilling to predict load between VPN endpoints exactly. With range-based SLAs customers specify their requirements as a range of quantitative values rather than a single one. Various suitable policies and algorithms dynamically provision and allocate resources at the edges for VPN connections. However, we also need to provision the interior nodes of a transit network to meet the assurances offered at the boundaries of the network. Although a deterministic guaranteed service (single quantitative value approach) provides the highest level of QoS guarantees, it leaves a significant portion of network resources on the average unused. In this paper, we show that with range-based SLAs providers have the flexibility to allocate bandwidth that falls between a lower and upper bound of the range only, and therefore, take advantage of this to make multiplexing gain in the core that is usually not possible with a deterministic approach. But dynamic and frequent configurations of an interior device is not desired as this will lead to scalability problems and also defeats the purpose of the DiffServ architecture which suggests to drive all the complexities towards edges. We, therefore, propose virtual core provisioning that only requires a capacity inventory of interior devices to be updated based on VPN connection acceptance, termination or modification at the edges.
We study the query language BQL: the extension of the relational algebra with for-loops. We also study FO(FOR): the extension of first-order logic with a for-loop variant of the partial xpoint operator. In contrast to the known situation with query languages which include while-loops instead of for-loops, BQL and FO(FOR) are not equivalent. Among the topics we investigate are: the precise relationship between BQL and FO(FOR); inflationary versus non-inflationary iteration; the relationship with logics that have the ability to count; and nested versus unnested loops.
A new ASIC capable of computing rank order filters, weighted rank order &quot;lters, standard erosion and dilation, soft erosion and dilation, order statistic soft erosion and dilation, fuzzy erosion and dilation and fuzzy soft erosion and dilation is presented in this paper. Its function is based on local histogram and a successive approximation technique and performs on 3ff3-pixel image windows. The hardware complexity of the proposed structure is linearly related to both image data window size and pixel resolution. The dimensions of the core of the proposed ASIC are 2.88 mmff 2.8 mm&quot;8.06 mmff and its die size dimensions are 3.72 mmff3.64 mm&quot;13.54 mmff. It executes 3.5ff10ff non-linear filter operations per second. 
this paper, we will abbreviate this to decomposable. For example, an all-different constraint is decomposable into a clique of binary not-equals constraints. As a second example, a monotonicity constraint is decomposable into a sequence of ordering constraints on pairs of variables. Not all non-binary constraints are decomposable into binary constraints on the same set of variables. For example, the parity constraint even(x 1 +x 2 +x 3 ) cannot be represented as a binary constraint satisfaction problem without the introduction of additional variables. In this paper, we compare theoretically the levels of consistency which are achieved on non-binary constraints compared to that achieved on their binary decomposition. We correct an error in [16] that suggested that neighborhood inverse consistency on the binary decomposition is an upper bound on the level of consistency achieved by generalized arc-consistency
The goal of this paper is to propose a new methodology for designing coordination between human agents and software agents and, ultimately, among software agents. The methodology is based on two key ideas. The first is that coordination should be designed in steps, according to a precise software engineering methodology, and starting from the specification of early requirements. The second is that coordination should be modeled as dependency between actors. Two actors may depend on one another because they want to achieve goals, acquire resources or execute a plan. The methodology used is based on Tropos, an agent oriented software engineering methodology presented in earlier papers. The methodology is presented with the help of a case study.
While there has already been significant research in support of openness and programmability in networks, this paper argues that there remains a need for generic support for the integrated development, deployment and management of programmable networking software. We further argue that this support should explicitly address the management of run-time reconfiguration of systems, and should be independent of any particular programming paradigm (e.g. active networking or open signaling), programming language, or hardware/ operating system platform. In line with these aims, we outline an approach to the structuring of programmable networking software in terms of a ubiquitously applied software component model that can accommodate all levels of a programmable networking system from low-level system support, to in-band packet handling, to active networking execution environments to signaling and coordination.
This paper presents a new scalable buffer-management scheme for IP Differentiated Services. The scheme consists of a Differentiated Random Drop (DRD) algorithm using feedback from a virtual scheduler. DRD choses a queue to perform an early packet drop to avoid congestion according to a specific probability function. First it will be shown that DRD in conjunction with first-come first-served scheduling is able to support relative service differentiation. The virtual scheduler is introduced to enable service differentiation in terms of bandwidth and delay at the same time. A virtual scheduler runs in parallel to the real scheduler and maintains virtual queue lengths that are being used by the congestion avoidance scheme as a feedback for packet drop decisions. Scheduling packets for transmission is performed by the real scheduler only.
Transactions ensure simple and correct handling of concurrency and failures but are often considered too expensive for use in file systems. This paper argues that performance is not a barrier to running transactions. It presents a simple mechanism that substantially lowers the cost of read-only transactions (which constitute the bulk of operations in a file system). The approach is inexpensive: it requires modest additional storage, but storage is cheap. It causes read-only transactions to run slightly in the past, but guarantees that they nevertheless see a consistent state.
This paper proposes a basic middleware service intended to be used as a generic background dissemination service for distributed self-organizing applications and protocol services in mobile ad-hoc networks. The basic idea is the combination of a device discovery service needed in ad-hoc networks and a dissemination service based on epidemic message distribution. This service is intended to be used by different competing applications and protocols for permanent information dissemination, while consuming only a small fraction of the available limited network bandwidth in a mobile environment. A prototype of this service is implemented and studied in a simulation environment. The paper concludes that information dissemination based on a background dissemination service is attractive if time constraints for the information distribution are low.
With the invention of microarrays, researchers are capable of measuring thousands of gene expression levels in parallel at various time points of the biological process. To investigate general regulatory mechanisms, biologists cluster genes based on their expression patterns. In this paper, we propose a new memetic co-clustering algorithm for expression profiles, which incorporates a priori knowledge in the form of Gene Ontology information. Ontologies offer a mechanism to capture knowledge in a shareable form that is also processable by computers. The use of this additional annotation information promises to improve biological data analysis and simplifies the identification of processes that are relevant under the measured conditions.
A peer-to-peer system is a distributed system with no physical or logical central authority. We give a formal model of a peer-to-peer system where agents communicate through read-modify-write registers that can be accessed by exactly two agents. For this model, we study so-called ordering decision tasks for wait-free agents. We show how agents can determine their position in a total linearizable order in the peer-to-peer model. We also show that electing a leader among the agents and finding a predecessor agent in the total ordering cannot be implemented without a central authority. Our peer-to-peer model is related to other models of distributed computing, specifically to concurrent objects in asynchronous shared memory and to switching (counting) networks.
Tropical algebraic geometry is the study of piecewise-linear objects which behave like algebraic varieties. We give an introduction to this theory, with an emphasis on tropical varieties in dimensions two and three, and we present several new results on incidence theorems and families of quadrics.
Human Emotion establishes a set of functional interactions with Cognition. Contrary to what was advocated for a long time, these interactions were identified as essential for the emergence of intelligent behavior in complex environments with real-time requirements. The present work strives to provide Autonomous Agent Architectures with mechanisms with functionalities similar to those Emotions have in Human Cognition. This would enhance Autonomous Agents behavior in complex environments, as the ones we live in. Such Architecture may be called an Emotion-based Agent Architecture.
A key challenge in peer-to-peer computing systems is to provide a decentralized and yet reliable service on top of a network of loosely coupled, weakly connected and possibly unreliable peers. This paper presents an effective dynamic passive replication scheme designed to provide reliable service in PeerCQ, a decentralized and self-configurable peer-to-peer Internet information monitoring system. We first describe the design of a distributed replication scheme, which enables reliable processing of long-running information monitoring requests in an environment of inherently unreliable peers. Then we present an analytical model to discuss its fault tolerance properties. A set of initial experiments is reported, showing the feasibility and the effectiveness of the proposed approach.
In this paper we address the problem of reducing the amount of data to be analyzed when indexing audiovisual secuences. The reduction is based in selecting those parts of the video sequence where it is likely that there is a face talking. This can be useful since usually this kind of scenes contain important and reusable information such as interviews. The proposed technique is based on our a priori knowledge of the editing techniques used in news sequences. The results show that with this algorithm it is possible to discard arround    of the news sequence with minimal processing. 1. 
. Converting sequential programs to execute on parallel computers is difficult because of the need to globally optimize for both parallelism and data locality. The choice of which loop nests to parallelize, and how, drastically affects data locality. Similarly, data distribution directives, such as DISTRIBUTE in High Performance Fortran (HPF), affects available parallelism and locality. What is needed is a systematic approach to converting programs to parallel form, based upon analysis that identifies opportunities for both parallelism and locality in one representation. This paper presents a global framework for optimizing parallelism and locality, based upon constraint solving for locality between potentially parallel loop nests. We outline the theory behind the framework, and provide a global algorithm for parallelizing programs while optimizing for locality. We also give results from applying the algorithm to parallelizing the Perfect benchmarks, targeted at the KSR-1, and analyze ...
This article discusses methods for avoiding that the reconstruction of the acceleration data structure becomes a  bottleneck in animated or interactive ray tracing. Situations in which this could occur include trying to increase the  frame rate by parallelization of the ray tracing phase or by techniques such as frameless rendering. Specifically,  we explore a method for avoiding unnecessary reconstruction in rigid-body animated scenes. The method builds  a hierarchy of oriented bounding boxes containing recursive grids by applying these to the rigid bodies found in  different transforms in the scene graph. The oriented bounding boxes containing gridded objects are then kept  intact during the complete animation. Before performing intersection tests, rays are transformed to the local  coordinate system of an oriented bounding box. Using this technique, the reconstruction of the data structure can  be performed an order of magnitude faster as compared to using a recursive grid that has to be rebuilt completely  between each frame.
In this paper we investigate effcient strategies for supporting on-demand information dissemination and gathering in large-scale wireless sensor networks. In particular, we propose a &quot;comb-needle&quot; discovery support model resembling an ancient method: use a comb to help find a needle in sands or a haystack. The model combines push and pull for information dissemination and gathering. The push component features data duplication in a linear neighborhood of each node. The pull component features a dynamic formation of an on-demand routing structure resembling a comb. The comb-needle model enables us to investigate the cost of a spectrum of push and pull combinations for supporting discovery and query in large scale sensor networks. Our result shows that the optimal routing structure depends on the frequency of query occurrence and the spatial-temporal frequency of related events in the network. The benefit of balancing push and pull for discovery in large scale geometric networks are demonstrated. We also raise the issue of query coverage in unreliable networks and investigate how redundancy can improve the coverage via both theoretical analysis and simulation. Last, we study adaptive strategies for the case where the frequencies of query and events are unknown a priori and time-varying.
The computation required for kernel machines with N training samples  is O(N    ). Such computational complexity is significant even for moderate  size problems and is prohibitive for large datasets. We present an  approximation technique based on the improved fast Gauss transform to  reduce the computation to O(N). We also give an error bound for the  approximation, and provide experimental results on the UCI datasets.
This paper presents a new algorithm for mobile robot localization, called Monte Carlo Localization (MCL). MCL is a version of Markov localization, a family of probabilistic approaches that have recently been applied with great practical success. However, previous approaches were either computationally cumbersome (such as grid-based approaches that represent the state space by high-resolution 3D grids), or had to resort to extremely coarse-grained resolutions. Our approach is computationally efficient while retaining the ability to represent (almost) arbitrary distributions. MCL applies sampling-based methods for approximating probability distributions, in a way that places computation &quot; where needed.&quot; The number of samples is adapted on-line, thereby invoking large sample sets only when necessary. Empirical results illustrate that MCL yields improved accuracy while requiring an order of magnitude less computation when compared to previous approaches. It is also much easier to implement...
Hyperheuristics can be defined to be heuristics which choose between heuristics in order to solve a given optimisation problem. A number of hyperheuristics have been developed over the past few years. Here we propose a new hyperheuristic framework within which heuristics compete against one another. The rules for competition are motivated by the principles of reinforcement learning. We analyse the differences between a previously published choice function hyperheuristic and the new hyperheuristic. We demonstrate how the new hyperheuristic can make further improvements when a number of features are incorporated, including a dynamic tabu list which forbids the use of certain heuristics at certain times. The result is an algorithm which is competitive with the choice function hyperheuristic when applied to a comprehensive suite of nurse scheduling problems at a major UK hospital, featuring a wide variety of solution landscapes.
With the continuing emergence of multithreaded computation as a powerful  vehicle for science and engineering, the need for an introduction to  multithreaded programming for scientists and engineers is high. All popular  operating systems already support multithreaded programming and the popular  POSIX Pthreads standard has been approved. It is the right time to teach  students this new technology. This paper presents the problems and difficulties  we encountered and a set of comprehensive and flexible course materials for  a multithreaded programming course for sophomore and junior students. This  paper also presents the design of pedagogical tools for the students to visualize  and experiment with various concepts in multithreaded programming. These  concepts include program behavior and execution visualization, deadlock and  race condition detection, and software metrics for measuring the complexity  of students&apos; programs.
In this paper, we present automated techniques for bootstrapping and populating specialized domain ontologies by organizing and mining a set of relevant overlapping Web sites provided by the user. We develop algorithms that detect and utilize HTML regularities in the Web documents to turn them into hierarchical semantic structures encoded as XML. Next, we present tree-mining algorithms that identify key domain concepts and their taxonomical relationships. We also extract semi-structured concept instances annotated with their labels whenever they are available. Experimental evaluation for the News, Travel, and Shopping domains indicates that our algorithms can bootstrap and populate domain specific ontologies with high precision and recall.
In this paper, we develop a general technique for truncating  Petri net unfoldings, parameterised according to the level of information  about the original unfolding one wants to preserve. Moreover,  we propose a new notion of completeness of a truncated unfolding. A  key aspect of our approach is an algorithm-independent notion of cut-off  events, used to truncate a Petri net unfolding. Such a notion is based  on a cutting context and results in the unique canonical prefix of the  unfolding. Canonical prefixes are complete in the new, stronger sense,  and we provide necessary and suffcient conditions for its finiteness, as  well as upper bounds on its size in certain cases. A surprising result is  that after suitable generalisation, the standard unfolding algorithm presented  in [5], and the parallel unfolding algorithm proposed in [8], despite  being non-deterministic, generate the canonical prefix. This gives an alternative  correctness proof for the former algorithm, and a new (much  simpler) proof for the latter one.
Discretizations of two-fluid flow problems inconservx/fiV formulation generally exhibit pressure oscillations. In this work we show that these pressure oscillations are induced by the loss of apressure-invKvv property under discretization, and we introduce a non-oscillatoryconservllat method for barotropic two-fluid flows. TheconservKx/T formulation renders the two-fluid flow problem suitable to treatment by aGodunovfi[/S method. We present a modified Osher scheme for the two-fluid flow problem. Numerical results are presented for a translating-interface test case and a shock/interface--collision test case.
An environment for general research into and prototyping of algorithms for reliable constrained and unconstrained global nonlinear optimization and reliable enclosure of all roots of nonlinear systems of equations, with or without inequality constraints, is being developed. This environment should be portable, easy to learn, use, and maintain, and sufficiently fast for some production work. The motivation, design principles, uses, and capabilities for this environment are outlined. The environment includes an interval data type, a symbolic form of automatic differentiation to obtain an internal representation for functions, a special technique to allow conditional branches with operator overloading and interval computations, and generic routines to give interval and non-interval function and derivative information. Some of these generic routines use a special version of the backward mode of automatic differentiation. The package also includes dynamic data structures for exhaustive sear...
the results of multiple clusterings. Initially, n d-dimensional data is decomposed into a large number of compact clusters; the K-means algorithm performs this decomposition, with several clusterings obtained by N random initializations of the K-means. Taking the cooccurrences of pairs of patterns in the same cluster as votes for their association, the data partitions are mapped into a co-association matrix of patterns. This n    n matrix represents a new similarity measure between patterns. The final clusters are obtained by applying a MST-based clustering algorithm on this matrix. Results on both synthetic and real data show the ability of the method to identify arbitrary shaped clusters in multidimensional data.
Three-dimensional (3-D) models of outdoor scenes are widely used for object recognition, navigation, mixed reality, and so on. Because such models are often made manually with high costs, automatic 3-D modeling has been investigated. A 3-D model is usually generated by using a stereo method. However, such approaches cannot use several hundreds images together for dense depth estimation because it is difficult to accurately calibrate a large number of cameras. In this paper, we propose a 3-D modeling method that first estimates extrinsic camera parameters of a monocular image sequence captured by a moving video camera, and then reconstructs a 3-D model of a scene. We can acquire a 3-D model of an outdoor scene accurately by using several hundreds input images.
In a graph G=(V;E), the eccentricity e(v) of a vertex v is max{d(v; u): u  ffV}.
In everyday life we are often faced with the need to monitor the contents of some kind of container e.g. the contents of a medicine cabinet or a tool box. In this paper we show that this entire class of scenarios has similar features and properties. To remove the annoyance associated by the user with this monitoring task we propose a smart box concept. In this concept the monitoring is facilitated by any automatic identification technology. RFID technology proved to be a suitable means to invisibly monitor the contents of the box without requiring the user to adapt the way he interacts with the smart objects. We describe the characteristics of the smart box concept and then demonstrate the usability of the smart box concept with three applications: the smart toolbox, the smart medicine cabinet and the smart surgical kit.
In this paper we present an efficient general simulation strategy for computations designed for fully operational BSP machines of n ideal processors, on n-processor dynamic-fault-prone BSP machines. The fault occurrences are failstop and fully dynamic, i.e., they are allowed to happen on-line at any point of the computation, subject to the constraint that the total number of faulty processors may never exceed a known fraction. The computational paradigm can be exploited for robust computations over virtual parallel settings with a volatile underlying infrastructure, such as a NETWORK OF WORKSTATIONS (where workstations may be taken out of the virtual parallel machine by their owner).
We describe representation, inference strategies, and control procedures employed  in an automated conversation system named the Bayesian Receptionist. The  prototype is focused on the domain of dialog about goals typically handled by receptionists  at the front desks of buildings on the Microsoft corporate campus. The system  employs a set of Bayesian user models to interpret the goals of speakers given evidence  gleaned from a natural language parse of their utterances. Beyond linguistic features, the  domain models take into consideration contextual evidence, including visual findings. We  discuss key principles of conversational actions under uncertainty and the overall architecture  of the system, highlighting the use of a hierarchy of Bayesian models at different  levels of detail, the use of value of information to control question asking, and application  of expected utility to control progression and backtracking in conversation.
This paper presents the design of a Windows 95 brain mapping application named brainmap.
Community networks are community-oriented information and  communication systems that are generally patterned after the public library&apos;s  model of free, inclusive service and commitment to universal access. To serve  the community network objectives it is therefore important to have easy and  widespread information access. In this paper we present the Campiello system  that proposes both enhanced information services and complementary user  interfaces to better serve the community network objectives. Enhancement of  the services is obtained by introducing collaborative filtering functions to  support easier navigation in the information space of the community. To extend  access to the community network, a paper-based interface is used, that supports  exchange of information with the network, from physical locations spread in  town. A large screen based interface is also used, which provides collective  easy entry points to the most recent and relevant community information.
Needed narrowing is a complete operational principle for modern declarative languages which integrate the best features of (lazy) functional and logic programming. We define a transformation methodology for functional logic programs based on needed narrowing. We provide (strong) correctness results for the transformation system w.r.t. the set of computed values and answer substitutions and show that the prominent properties of needed narrowing -- namely, the optimality w.r.t. the length of derivations and the number of computed solutions -- carry over to the transformation process and the transformed programs. We illustrate the power of the system by taking on in our setting two well-known transformation strategies (composition and tupling). We also provide an implementation of the transformation system which, by means of some experimental results, highlights the benefits of our approach.
This paper applies the graph conditioning algorithm called the approximate cycle extrinsic message degree (ACE) algorithm to design high-rate (R    1/2) irregular LDPC codes. The algorithm was shown to be an effective tool to lower the error floors of lower-rate (R    1/2) LDPC codes. However, for high-rate LDPC codes, due to the large number of degree-2 variable nodes in the optimal degree distribution, the error floor is high and it is more difficult to condition the graph. By constraining the number of degree-2 nodes, we found that the ACE algorithm can dramatically lower the error floor with little compromise of the threshold. A rate-3/4, length-10688 LDPC code is proposed whose AWGN channel performance is within 0.67 dB of the Shannon limit at BER =10 -5 anditserrorflooris  lower than 10 -7 . Compared to existing semi-regular codes which lower the floor by adopting non-optimal degree distributions, our graph-conditioned codes provides 0.38 dB of performance improvement at BER=10 -5 . The same design criteria also apply well to the medium-length LDPC code design and are suitable for rate-compatible applications using the information-nulling technique. The rate-compatible scheme has consistently good thresholds and low error floors for 1/2      8/9.
In this white paper we will describe how mobile phones can be used for remote monitoring and control. By augmenting an existing or new product with an embedded information-server, the product can be remotely controlled in a very cost effective way. We will describe the many different benefits of using embedded information-servers by a set of example scenarios. The scenarios will illustrate that increased product value, decreased development cost, decreased per-unit cost, as well as, decreased cost for service and support can be achieved by equipping products with information-servers.
Accurate yet simple methods for traffic engineering are important for efficient dimensioning of broadband networks. The goal of this paper is to apply and evaluate large deviation techniques for traffic engineering. In particular, we employ the recently developed theory of effective bandwidths,  where the effective bandwidth depends not only on the statistical characteristics of the traffic stream, but also on a link&apos;s operating point through two parameters, the space  and time parameters, which are computed using the many sources asymptotic. We show that this effective bandwidth definition can accurately quantify resource usage. Furthermore, we estimate and interpret values of the space and time parameters for various mixes of real traffic demonstrating how these values can be used to clarify the effects on the link performance of the time scales of burstiness of the traffic input, of the link parameters (capacity and buffer), and of traffic control mechanisms, such as traffic shaping...
A (t, n) threshold proxy signature scheme enables an original signer to  delegate the signature authority to a proxy group of n member such that  t or more than t proxy signers can cooperatively sign messages on behalf  of the original signer. In the paper, we review the security of some nonrepudiable  threshold proxy signature schemes with known signers. We show  that Sun&apos;s threshold proxy scheme, Yang et al.&apos;s threshold proxy signature  scheme and Tzeng et al.&apos;s threshold proxy signature scheme are insecure  against an original signer&apos;s forgery. We also show that Hsu et al.&apos;s threshold  proxy signature scheme suffers from the conspiracy of the original signer and  the secret share dealer SA, and that Hwang et al.&apos;s threshold proxy signature  scheme is universally forgeable. In a word, none of the above-mentioned  threshold proxy signature schemes can provide non-repudiation.
Auctions are a widely used approach for determining the current market price for congested resources. However, using them for real systems, such as the Internet, the auction algorithms require modifications as soon as connections cover links from more than one Internet Service Provider. This paper presents work in progress on two new auction schemes that have been designed to overcome this problem.  1 Introduction  As current developments are driving the Internet more and more towards commercial use, the question of charging users for using the Internet has become of rapidly increasing importance. The last years have seen a couple of proposals for pricing models for communication services (for a summarizing overview cf., e.g., [1]). Unfortunately, most of them remain on a rather theoretical level, without paying too much attention to practical problems, such as implementability with minimized overhead or scalability in tomorrow&apos;s Internet architecture, and as they are tackled, e.g., wi...
The positive skewness of Gaborwavelets in the parameter space aects the identi  cation  of source signals. In this paper, we present a new  lter to correct the skewness and  describe the result in the parameter space with a sum-of-Gaussians model. We further  use higher order moment information to separate dierent source signal components.
In this paper we present DCI (Direct Count &amp; Intersect), a new data mining algorithm for frequent  set counting. We also discuss the parallelization strategies used in the design of ParDCI, a distributed and  multi-threaded version of DCI. DCI adopts a classical level-wise approach based on candidate generation  to extract frequent sets, but uses a hybrid method to determine the supports of candidate itemsets.
Ever increasing complexity and heterogeneity of SoC platforms require diversified on-chip communication schemes beyond the currently omnipresent shared bus architectures. To prevent time consuming design changes late in the design flow, we propose the early exploration of the on-chip communication architecture to meet performance and cost requirements. Based on SystemC 2.0.1 we have defined a modular exploration framework, which is able to capture the effect on performance for different on-chip networks like dedicated point-to-point, shared bus, and crossbar topologies. Monitoring of performance parameters like utilization, latency and throughput drives the mapping of the inter-module traffic to an efficient communication architecture. The effectiveness of our approach is demonstrated by the exemplary design of a high performance Network Processing Unit (NPU), which is compared against a commercial NPU device.
In contrast to classical assumptions in Video on Demand (VoD) research, the main requirements for VoD in the Internet are adaptiveness, support of heterogeneity, and last not least high scalability. Hierarchically layered video encoding is particularly well suited to deal with adaptiveness and heterogeneity support for video streaming. A distributed caching architecture is key to a scalable VoD solution in the Internet. Thus, the combination of caching and layered video streaming is promising for an Internet VoD system, yet, requires thoughts about some new issues and challenges, e.g., how to keep layered transmissions TCP-friendly. In this paper, we investigate one particular of these issues: how can a TCP-friendly transmission exploit its fair share of network resources taking into account that the constrained granularity of layer encoded video inhibits an exact adaptation to actual transmission rates. We present a new technique that makes use of retransmissions of missing segments for a cached layered video to claim the fair share within a TCP-friendly session. Based on simulative experiments the potential and applicability of the technique, which we also call fair share claiming is shown. Moreover, a design for the integration of fair share claiming in streaming applications which are supported by caching is devised.
The paper consists of two parts. In the first part we introduce  a model for structured document databases where we propose Booleanvalued  attribute grammars (BAGs) as a query facility. In the second  part we show that DOOD technology offers a natural platform on top  of which this model as a whole can be conveniently implemented. Each  structured document database is mapped to an OO-database and each  BAG is translated into deductive rules. Our translation is such that the  well-founded semantics and the naive bottom-up fixpoint procedure of  the deductive rules capture the evaluation of the BAG. We also present  a modification of the translation suited to the inflationary semantics.
The role of computers in aviation is extensive and growing. Many  crucial systems, both on board and on the ground, rely for their correct operation  on sophisticated computer systems. This dependence is increasing as more and  more functionality is implemented using computers and as entirely new systems  are developed. Several new concepts are being developed specifically to address  current safety issues in aviation such as runway incursions. This paper  summarizes some of the system issues and the resulting challenges to the safety  and software engineering research communities.
We explore to what extent and how efficiently constraint programming  can be used in the context of automated reasoning for modal  logics. We encode modal satisfiability problems as constraint satisfaction  problems with non-boolean domains, together with suitable constraints.
. The optimization of rough set based classication models  with respect to parameterized balance between a model&apos;s complexity  and condence is discussed. For this purpose, the notion of a parameterized  approximate inconsistent decision reduct is used. Experimental  extraction of considered models from real life data is described.  1 Introduction  While reasoning about a domain specied by our needs, we usually base on the information gathered by the analysis of a sample of objects. The rough set theory ([3]) assumes that a universe of known objects is the only source of knowledge, which can be applied to construct models of reasoning about new cases. Reasoning can be stated, e.g., as a classication problem, concerning prediction of values of a decision attribute under information provided over conditional attributes. For this purpose, one stores data within decision tables, where each training case drops into one of predened decision classes.  Classication of new objects is perfo...
There is a very rich variety of systems of autonomous agents, be it software or robotic agents. In particular, multi-agent systems can include agents that may be part of a team and need to coordinate their actions during their distributed task execution. This coordination requires an agent to observe, i.e., to monitor, the other agents in order to detect a possible coordination failure of the team. Several researchers have addressed the problem of monitoring for single or multiple agent systems and have contributed successful, but mainly application-specific, approaches. In this paper, we aim at contributing a unifying, domain-independent statement of the distributed multi-agent monitoring problem. We define the problem in terms of a pre-defined desirable joint state and an observation-state mapping. Given a concrete joint observation during execution, we show how an agent can detect a possible coordination failure by processing the observation-state mapping and the desirable joint state. To illustrate the generality of our formalism, one of the main contributions of the paper, we represent several previously studied examples within our formalism. We note that basic failure detection algorithms can be computationally expensive. We further contribute an efficient method for failure detection that builds upon an off-line compilation of the principled relations introduced. We show empirical results that demonstrate this effectiveness.
The ever increasing complexity and heterogeneity of modern System-on-Chip designs demands early consideration and exploration of architectural alternatives, which is hardly practicable on the low abstraction level of implementation models. In this paper, a system level design methodology based on the SystemC 2.0 library is proposed, which enables the designer to reason about the architecture on a much higher level of abstraction. Goal of this methodology is to define a system architecture, which provides sufficient performance, flexibility and cost efficiency as required by demanding applications like broadband networking or wireless communications. The methodology also provides capabilities for co-simulating multiple levels of abstraction simultaneously. This enables reuse of the simulation environment for functional verification of synthesizable implementation models against the abstract architecture model.
Two string comparison measures, edit distance and n-gram co-occurrence, are tested for automatic evaluation of translation quality, where the quality is compared to one or several reference translations. The measures are tested in combination for diagnostic evaluation on segments. Both measures have been used for evaluation of translation quality before, but for another evaluation purpose (performance) and with another granularity (system). Preliminary experiments showed that the measures are not portable without redefinitions, so two new measures are defined, WAFT and NEVA. The new measures could be applied for both purposes and granularities.
The emergence of the Grid computing model is important for disciplines  that are computing and data storage-intensive. A generic web-based interface  to a computing grid is presented. The package allows the user to submit a  high energy physics simulation to Grid resources, track the progress of the job  and upon job completion, display graphical results.
This paper presents an efficient constrained global optimization approach based on mean field annealing (MFA) theory to the problem of contour energy minimization with a contour interior constraint for object boundary extractions. In the method, with a given contour energy function, different target boundaries can be modeled as constrained global optimal solutions under different constraints expressed as a set of parameters characterizing the target contour interior structures. To search for the constrained global optimal solutions, a fast and efficient global approach based on MFA is employed to avoid local minima, which has been very difficult to achieve in most deformable contour methods. As an illustrative example, three target boundaries in a synthetic image are modeled as constrained global energy minimum contours with different constraint parameters and are successfully located using the derived algorithm. A conventional variational based deformable contour method [1] with the same energy function and constraint fails to achieve the same task. Experimental evaluations and comparisons with other methods on ultrasound pig heart, MRI knee, and CT kidney images where gaps, blur contour segments having complex shape and inhomogeneous interiors have been conducted with most favorable results.
Precision achieved by stochastic sampling algorithms for Bayesian networks typically deteriorates in face of extremely unlikely evidence. To address this problem...
The objective of the AQUADAPT Project (www.aquadapt.net) is to develop strategic tools to inform adaptive integrated water resource policy using a co-evolutionary approach. In Spain a  methodology has been developed in the framework of an integrative study to identify evidence of coevolutionary  processes between the hydrological system and the water-using communities of the Marina  Baixa over a period of 50 years. The Marina Baixa is comprised of 18 municipalities each with radically  different land-use patterns spread over an area of 671km    . The research task is complicated by the fact  that Spain is a country that is currently debating the merits of a move from demand management to  supply augmentation for this water-using region. Models, processes and assessments are applied to reflect  a co-evolutionary perspective of the relationships between water resources, ecological quality and  sustainable development. A variety of mosaics have been assembled with which to identify couplings of  elements that could have spawned a co-evolving process. The paper discusses both the challenges and the merits associated with designing new methodologies in an integrative study framework for dealing with  the possibilities, probabilities and uncertainties of adaptive integrated water resource management.
In traditional IP network IP address has both the meaning of network locater as well as host identity. This paradigm is fundamentally incompatible with mobility, with exception of data link layer mobility, such as WLAN and GPRS. Mobile IP
This paper describes a class of probabilistic approximation algorithms based on bucket elimination which offer adjustable levels of accuracy and efficiency. We analyze the approximation for several tasks: finding the most probable explanation, belief updating and finding the maximum a posteriori hypothesis. We identify regions of completeness and provide preliminary empirical evaluation on randomly generated networks.  1 Overview  Bucket elimination, is a unifying algorithmic framework that generalizes dynamic programming to enable many complex problem-solving and reasoning activities. Among the algorithms that can be accommodated within this framework are directional resolution for propositional satisfiability, adaptive consistency for constraint satisfaction, Fourier and Gaussian elimination for linear equalities and inequalities, and dynamic programming for combinatorial optimization [ 7 ] . Many algorithms for probabilistic inference, such as belief updating, finding the most proba...
We present a system that extracts and tracks 2D midsagital tongue surfaces from ultrasound image sequences produced by a Head and Transducer Support System (HATS). Such a system can be a valuable and practical tool for speech and swallowing research. We extend our previous work by introducing a deformable contour model to impose restrictions of spatiotemporal coherency by assuming that the 2D mid-sagital tongue surface contours sweep a coherent 3D structure in spatiotemporal 3D space. Another novel contribution of this paper is a dynamic programming minimization method to optimize the new energy functional, which can be employed in general snake minimization tasks. We tested this system on a number of speech and swallowing sequences each containing 24 to 30 frames. We verified with speech experts and found that our system produces promising results, especially for the swallowing sequences, which are more problematic.
We describe a binding schema markup language (BSML) for describing data interchange between scientific codes.
Goal models have been used in Computer Science in order to represent  software requirements, business objectives and design qualities. In previous  work we have presented a formal framework for reasoning with goal models, in a  qualitative or quantitative way, and we have introduced an algorithm for forward  propagating values through goal models. In this paper we focus on the qualitative  framework and we propose a technique and an implemented tool for addressing  two much more challenging problems: (1) find an initial assignment of labels  to leaf goals which satisfies a desired final status of root goals by upward value  propagation, while respecting some given constraints; and (2) find an minimum  cost assignment of labels to leaf goals which satisfies root goals. The paper also  presents preliminary experimental results on the performance of the tool using the  goal graph generated by a case study involving the Public Transportation Service  of Trentino (Italy).
Object-oriented languages come with pre-defined composition mechanisms, such as inheritance, object composition, or delegation, each characterized by a certain set of composition properties, which do not themselves individually exist as abstractions at the language level. However, often non-standard composition semantics is needed, with a mixture of composition properties, which is not provided as such by any of the standard composition mechanisms. Such non-standard semantics are simulated by complicated architectures that are sensitive to requirement changes and cannot easily be adapted without invalidating existing clients.
Statistical data protection can be viewed as a heir of the work that was started on statistical database protection in the 70s and 80s. Massive production of computerized statistics by government agencies combined with an increasing social importance of individual privacy has led to a renewed interest in this topic. This paper summarizes recent activity in statistical data protection, then outlines the current areas of work and finally lists some issues which are likely to deserve the attention of researchers and practitioners in the near future.
This paper addresses the issue of guidance and control of an autonomous  underwater vehicle (AUV) for a cable tracking problem. A linear quadratic Gaussian  controller with loop transfer recovery (LQG/LTR) is developed because of its strong  robustness properties. The vehicle is guided towards the target using a combination of  different guidance algorithms. The vehicle speed is used to formulate the guidance  problem. Simulation results are presented and a comparison is made between fix and  variable AUV speeds.  
It had been thought that it is diffcult to provide receiptfreeness  in mixnet-based electronic voting schemes. Any kind of user chosen  randomness can be used to construct a receipt, since a user can prove  to a buyer how he had encrypted the ballot. In this paper we propose  a simple and effcient method to incorporate receipt-freeness in mixnetbased  electronic voting schemes by using the well known re-encryption  technique and designated verifier re-encryption proof (DVRP). In our  scheme a voter has to prepare his encrypted ballot through a randomization  service provided by a tamper resistant randomizer (TRR), in such  a way that he finally loses his knowledge on randomness. This method  can be used in most mixnet-based electronic voting scheme to provide  receipt-freeness.
This paper proposes an alternate perspective to current research in terms of understanding the dynamics of an ERP software selection decision-making process, in that this paper builds on the theoretical framework proposed by Sammon and Adam (2002). We use the proposed model of the ERP Community (Sammon and Adam 2002) and attempt to understand how the substantive, institutional, and political factors, proposed by Caldas and Wood (1998) and Wood and Caldas (2000, 2001), impact an ERP software selection decisionmaking process. As a result, we highlight the existence of Non-Decision Making (NDM) within the ERP Community under study and identify various forms of Category Manipulation, in the context of relationships and interactions observed between the ERP Community actors. Furthermore, we present the actual outcomes of the ERP software selection decision making processes within the ERP Community under study, highlighting the impact of NDM. In conclusion, this research paper goes someway to validating the model proposed by Sammon and Adam (2002) and extends our thinking in terms of understanding the complexities of an ERP software selection process.
A central problem of co-operative information systems is  the ability to integrate information from multiple sources. Although this  problem has been studied for several decades, we argue that there is a  need for a more re  ned approach in those cases where the original sources  form a loose federation, with each one wishing to maintain its own independent  view of the world. In particular, we motivate with examples  the utility of directed non-injective mappings between the individuals  in the domains of multiple IS. We then extend the logical formalism  of Description Logics, which is used in semantic-web ontolgies and has  served well in many other aspects of IS design and integration, to handle  such mappings. The result is called Distributed Description Logics, and  we consider some of its desirable properties, as well as some theorems  concerning its computational aspects.
We prove that claw-free graphs, containing an induced dominating path, have a Hamiltonian path, and that 2-connected claw-free graphs, containing an induced doubly dominating cycle or a pair of vertices such that there exist two internally disjoint induced dominating paths connecting them, have a Hamiltonian cycle. As a consequence, we obtain linear time algorithms for both problems if the input is restricted to (claw,net)-free graphs. These graphs enjoy those interesting structural properties.
We present a geometric interpretation of the problem of motion recovery from three weak-perspective images. Our interpretation is based on reducing the problem of estimating the motion to a problem of finding triangles on a sphere whose angles are known. Using this geometric interpretation, a simple method to completely recover the motion parameters using three images is developed. The results of running the algorithm on real images are presented. In addition, we describe which of the various motion parameters can be recovered already from two images. 1 Introduction  Determining the relative position and orientation of two or more cameras from images of an unknown scene taken by those cameras is known as the motion estimation problem in computer vision. In this paper we address the problem of determining the motion parameters between two and three weak-perspective (or scaled-orthographic) views. The weak-perspective model is appropriate when the observed object is far from the camera r...
Traffic engineering (TE) has been widely investigated in recent years. Though, the scope of these works were predominantly limited to intra-domain TE and single administrative domains. Not even the long term TE with monitoring and analysis of traffic exchange points has been adequately solved, not to mention the short term, real-time performance related inter-domain TE tasks. This is in spite of the fact that today&apos;s operators acknowledge that the edges of their networks where they pass traffic to other domains is the source of their greatest costs, which demands effective management of their inter-domain traffic. In this article, the authors argue that the first simplest step towards inter-domain TE could be the distribution of load towards one destination prefix among the possible alternative egress links. This is achieved by having more than one connection in the domain to the outside world; commonly known as multi-homing. The authors propose a network model and several algorithmic approaches to capture, incorporate and utilise the advantages given by multi-homing into the intra-domain TE objectives. Numerical evaluation of the different proposals compared to traditional shortest path first routing is also performed.
This paper presents an arc-length preserving axial deformation along a B-spline curve based on arc-length parameterization of the axial curve. Space spanned by arc length and rotation minimizing frame on the axis is taken as the embedded space. As in real life, the length of an object&apos;s skeleton usually remains constant when it is axially deformed such as a swimming fish, a swaying tree etc, An length preserving axial curve deformation is presented. Keyframe skeleton B-spline curves are approximated by polylines after adaptive subdivisions. The edge lengths and the directional vertex angles of the keyframe polylines(or unit edge vectors) are then interpolated to generate the intermediate polylines. These interpolated polylines are intermediate axes in the discreted form. Experiments show our method is very useful, intuitive and easy to control.
We present a theoretically justified pricing scheme for ABR services which utilizes mechanisms provided by rate-based flow control as defined by the ATM Forum. As a result, the scheme imposes no additional communication overhead, while the added complexity at the switches and end-systems is minimal. Our approach complements ABR&apos;s rate-based flow control and leads to economically efficient utilization of network resources. According to the scheme, a connection is charged based on the sum of the price per unit of bandwidth on all links along its route. Prices depend on the demand for bandwidth and are adjusted in a decentralized and iterative manner. Simulation results show that prices converge reasonably fast and do not have a negative effect on the convergence properties of flow control.  1 Introduction  The Available Bit Rate (ABR) service class is one of the five service classes identified by the ATM Forum for ATM-based integrated services networks, [1, 2]. It is intended for &quot;best-e...
Opus is a new programming language designed to assist in coordinating the execution of multiple, independent program modules. With the help of Opus, coarse grained task parallelism between data parallel modules can be expressed in a clean and structured way. In this paper we address the problems of how to build a compilation and runtime support system that can efficiently implement the Opus constructs. Our design considers the often-conflicting goals of efficiency and modular construction through software re-use. In particular, we present the system requirements for an efficient Opus implementation, the Opus runtime system and describe how they work together to provide the underlying services that the Opus compiler needs for a broad class of machines. 1 Introduction  Data parallel languages, such as High Performance Fortran (HPF) [23], have been successfully applied to a wide range of numerical applications. For single programs, significant parallelism can be achieved using high-level ...
Predictability -- the ability to foretell that an implementation will not violate a set of specifiedreliability and timeliness requirements -- is a crucial, highly desirable property of responsive embedded systems. This paper overviews a development methodology for responsive systems, which enhances predictability by eliminating potential hazards resulting from physically-unsound specifications. The backbone of our methodology is a formalism that restricts expressiveness in a way that allows the specification of only reactive, spontaneous, and causal computation. Unrealistic systems -- possessing properties such as clairvoyance, caprice, infinite capacity, or perfect timing -- cannot even bespecified. We argue that this &quot;ounceofprevention&quot;atthe  specification level is likely to sparealotoftimeand  energy in the development cycle of responsive systems -- not to mention the elimination of potential hazards that would have gone, otherwise, unnoticed.
Lookup of services is an important issues in many distributed systems. This paper  deals with lookup in service-oriented architectures, such as Web services, P2P systems, GRIDs,  or spontaneous networks. Service-oriented architectures impose specific requirements onto the  lookup service, for instance regarding the runtime extensibility of lookup models, runtime extensibility  of lookup queries, construction of complex lookup queries, scalability, and fault tolerance.
We present the results of user studies that were performed on sighted people to test their ability to detect simple shapes with SoundView. SoundView is an experimental vision substitution system for the blind. Visual images are mapped onto a virtual surface with a fine-grained color dependent roughness texture. The user explores an image by moving a pointer device over the image which creates sounds. The current prototype uses a Wacom graphics tablet as a pointer device. The pointer acts like a virtual gramophone needle, and the sound produced depends on the motion as well as on the color of the area explored. An extension of SoundView also allows haptic feedback and we have compared the performance of users using auditory and/or haptic feedback.
The recently proposed multiplicative masking countermeasure  against power analysis attacks on AES is interesting as it does not  require the costly recomputation and RAM storage of S-boxes for every  run of AES. This is important for applications where the available space  is very limited such as the smart card applications. Unfortunately, it is  here shown that this method is in fact inherently vulnerable to dierential  power analysis. However, it is also shown that the multiplicative  masking method can be modi  ed so as to provide resistance to dierential  power analysis of nonideal but controllable security level, at the  expense of increased computational complexity. Other possible random  masking methods are also discussed.
Flow control in high speed networks requires distributed routers to make fast decisions based only on local information in allocating bandwidth to connections. While most previous work on this problem focuses on achieving local objective functions, in many cases it may be necessary to achieve global objectives such as maximizing the total flow. This problem illustrates one of the basic aspects of distributed computing: achieving global objectives using local information.  Papadimitriou and Yannakakis [PY93] initiated the study of such problems in a framework of solving positive  linear programs by distributed agents. We take their model further, by allowing the distributed agents to acquire more information over time. We therefore turn attention to the tradeoff between the running time and the quality of the solution to the linear program.  We give a distributed algorithm that obtains a (1+ffl) approximation to the global optimum solution and runs in a polylogarithmic number of distrib...
Flexible personal communications may require dynamically discovering,  using, and combining a number of services to support the activities of  a mobile user. However, many service discovery and service control protocol  frameworks are not designed with requirements for ad-hoc and group communication  in a changing environment in mind. In this paper, we motivate the case  for personalized group communications based upon a (static) office application  scenario featuring simple remote device control and then enhance the scope towards  service location and dynamic establishment of group communications for  mobile users: ad-hoc multiparty peering. We particularly explore the issues relating  to group communication setup and robustness in the presence of changing  connectivity and present a framework for mobile multiparty ad-hoc cooperation.
We present an algorithm for symbolic model checking temporal-epistemic properties of  multi-agent systems, expressed in the formalism of interpreted systems. We  rst introduce a  technique for the translation of interpreted systems into boolean formulae, and then present  a model-checking algorithm based on this translation. The algorithm is tailored for the use  of OBDDs, as they oer a compact and ecient representation for boolean formulae.
We address the problem of reducing power dissipation on the time multiplexed address buses employed by contemporary DRAMs in SOC designs. We propose address encoding techniques to reduce the transition activity on the time-multiplexed address buses and hence reduce power dissipation. The reduction in transition activity is achieved by exploiting the principle of locality in address streams in addition to its sequential nature. We consider a realistic processor-memory architecture and apply the proposed techniques on the address streams derived from time-multiplexed DRAM addresses. Although the techniques by themselves are not new, we show that a judicious combination of the existing techniques yield significant gains in power reductions. Experiments on SPEC95 benchmark programs show that our encoding techniques yield as much as 82% in transition activity compared to binary encoding. We show that these reductions amount to as much 60% reduction in the off-chip address bus power. Also since the encoder/decoder add some power overhead, we calculate the minimum off-chip bus capacitance to the internal node capacitance ratio needed to achieve power reductions.
An efficient, network-flow-theory based model with an algorithm is proposed for configuration of  networks. The configuration results in a structure of paths which can survive any single link failure.
Smart object interfaces enable a computer to respond to a group of users&apos; manipulations of a physical  environment. This unobtrusive interface is especially well suited for providing guidance as students  attempt to solve mathematical and scientific puzzles. This paper introduces a formalism for describing  arrangements of smart objects on a 2D surface, and suggests a strategy for efficiently representing such  arrangements in a computer application. It then shows how these techniques are implemented in a  Tangram with a smart objects interface, which provides multimedia feedback as children play with the  puzzle pieces.  Keywords: tangible user interface, spatial relationships, computer vision, multimodal interface,  graphical interaction, multimedia, tracking, educational application, Tangram  INTRODUCTION  Graphical user interfaces rely on metaphors to achieve a familiarity that allows people to learn to use them with minimal effort. Yet nothing is as easy as manipulating things in ...
Semiconductor fabrication trends indicate that emerging process technologies will experience an increase in the number of noise induced errors and device defects. While past work in fault tolerance has focused on static testing, modular redundancy in combinational logic, and error correcting codes in memories, the prevalence of transient errors may make these techniques less effective. In this project, we introduce the NanoBox, a field-programmable gate array (FPGA) style lookup table with fault tolerance coding applied to the static random access memory (SRAM) array within the lookup table. In this way, we contain and selfcorrect errors within the lookup table, thereby presenting a robust logic block to higher levels of logic design. Our results show that triplicating the SRAM array within the lookup table gives excellent error coverage, has a 1.9x area overhead, 2.2x power overhead, and a 1.3x delay overhead, as compared to a design with no fault tolerance coding. We also investigate NanoBox implementations using information coding, but we find these implementations have significantly higher overhead, and worse error coverage, than the triplicated SRAM bit implementation.
Safe primes are prime numbers of the form p = 2q + 1 where  q is prime. This note introduces a simple method for doubling the speed of safe prime generation. The method is particularly suited to settings where a large number of RSA moduli must be generated. keywords : safe primes, key-generation, prime-generation, RSA. 1 
This paper describes the University Course Timetabling Problem  (UCTP) used in the International Timetabling Competition 2003  organized by the Metaheuristics Network and presents a state-of-the-art  heuristic approach towards the solution of the competition instances.
A novel method of point correspondence recovery between planar curves is presented in this paper where motion between the curves is nonrigid. Fourier transformation is used to decompose planar curves into a set of ellipses, each at a different frequency level. The point correspondences between two planar curves is based on the correspondences between two ellipses in the same frequency level. At each level, a simple method is implemented to get the correspondences between the two ellipses through their shape information. This way, nonrigid point correspondence problem between two planar curves is decomposed to a set of simple subproblems -- the correspondence between ellipses. We have conducted comprehensive experiments on synthetic and  real data sets and found that the algorithm is quite effective and efficient.
The documentation of software artifacts in general, and object-oriented frameworks in  particular, has always been problematic. In this paper, we advocate the use of a declarative  meta-programming environment to document software artifacts. In particular, we show how  a significant and important part of the design of a framework can be adequately and concisely  documented in such an environment, and how this allows us to use this documentation in an  active way.
We consider the problem of predicting time series originating from nonstationary and from mixed dynamical systems. It is shown that the complexity of finding representations for the dynamics of such systems can be drastically reduced if their composite nature is taken into account. Two paradigmatic cases are discussed and their solutions presented: jump processes and stationary mixtures. Examples demonstrate that divisive approaches can substantially improve predictions of time series compared to methods that model the dynamics globally.
Design is a complex activity that can be analysed from a wide variety of perspectives. This paper attempts to look at the individual problem solving process, taking into account psychological arguments. We characterise some of the phases involved in the design process, namely the constraints identification, the optimisation of solution space and the reuse process. We highlight a three-dimensional framework of how the constraints identification impacts on the solution space which, in turn, determines the range of the components that will be eligible for reuse. We discuss this argument through examples from both inside and outside the software engineering field.
Many real-time database applications arise in electronic financial services, safety-critical installations and  military systems where enforcing security is crucial to the success of the enterprise. We investigate here the performance  implications, in terms of killed transactions, of guaranteeing multilevel secrecy in a real-time database  system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of  this issue. Our main
In this paper, we present our development of a document management and retrieval tool, which is named Ontalk. Our system provides a semi-automatic metadata generator and an ontology-based search engine for electronic documents. Ontalk can create or import various ontologies in RDFS or OWL for describing the metadata. Our system that is built upon .NET technology is easily communicated with or flexibly plugged into many different programs.
. McMillan has recently proposed a new technique to avoid the state explosion problem in the verification of systems modelled with finite-state Petri nets. The technique requires to construct a finite initial part of the unfolding of the net. McMillan&apos;s algorithm for this task may yield initial parts that are larger than necessary (exponentially larger in the worst case). We present a refinement of the algorithm which overcomes this problem.  Keywords: unfolding, partial-order semantics, Petri nets  1. Introduction  In a seminal paper [12], McMillan has proposed a new technique to avoid the state explosion problem in the verification of systems modelled with finite-state Petri nets. The technique is based on the concept of net unfoldings, a well known partial-order semantics of Petri nets introduced in [15], and later described in more detail in [4] under the name of branching processes. The unfolding of a net is another net, usually infinite but with a simpler structure. McMillan prop...
From a software engineering perspective, agent systems are a  specialization of object-oriented (OO) systems, inwE2]  individual objects have  theirow  threads of control and their  ow  goals or sense of purpose. Engineering such systems is  most naturally approached as an extension of object-oriented  systems engineering. In particular, the Unified Modeling  Language (UML) can be naturally extended to support the  distinctive requirements of multi-agent systems. One such  requirement results from the increasing emphasis on the  correspondencebetwsp  multi-agent systems and social  systems. Sociological analogies are proving fruitful models  for agent-oriented  constructions,wons  sociologists  increasingly use agents as a modeling tool for studying social  systems. We combine several existing organizational models  for agents, including AALAADIN, dependency theory,  interaction protocols, and holonics, in a general theoretical  framework, and show how  UML can be applied and extended to  capture constructions in  that framework.
This paper describes a new approach to managing a stream of questions about mathematics by integrating a text categorization framework into a relational database management system. The corpus studied is based on unstructured submissions to an ask-an-expert service in learning mathematics. The classification system has been tested using a Nave Bayes learner built into the framework. The performance results of the classifier are also discussed. The framework was integrated into a PostgreSQL database through the use of procedural trigger functions.
In this paper we present a new approach to high quality 3D object reconstruction by using well known computer vision techniques. Starting from a calibrated sequence of color images, we are able to recover both the 3D geometry and the texture of the real object. The core of the method is based on a classical deformable model, which defines the framework where texture and silhouette information are used as external energies to recover the 3D geometry. A new formulation of the silhouette constraint is derived, and a multi-resolution gradient vector flow diffusion approach is proposed for the stereo-based energy term.
This work presents a sample of what evolved neural net circuit modules using the socalled &quot;CoDi-1Bit&quot; neural net model [5] can do. This work is part of an 8 year research project at ATR which aims to build an artificial brain containing a billion neurons by the year 2001, that will be used to control the behaviors of a kitten robot &quot;Robokoneko&quot; [2][3][4]. It looks as though the figure is more likely to be 40 million, but the numbers are not of great concern. What is more important is the issue of evolvability of the cellular automata (CA) based neural net circuits which grow and evolve in special FPGA (Field Programmable Gate Array) hardware, at hardware speeds (e.g. updating 150 billion CA cells per second, and performing a complete run of a genetic algorithm, i.e. tens of thousands of circuit growths and fitness evaluations, to evolve the elite neural net circuit in about 1 second). The specialized hardware which performs this evolution is labeled the CAM-Brain Machine (CBM) [6]. It ...
Modern multiprocessor systems offer advanced synchronization primitives, build in hardware, to support the development of efficient parallel algorithms. In this paper we develop a simple and efficient algorithm of atomic registers (variables) of arbitrary length. The simplicity and better complexity of the algorithm is achieved via the utilization of two such common synchronization primitives. In this paper we also evaluate the performance of our algorithm and the performance of a practical previously know algorithm that is based only on read and write primitives. The evaluation is performed on 3 well-known, parallel architectures. This evaluation clearly shows that both algorithms are practical and that as the size of the register increases our algorithm performs better accordingly to its complexity behavior.
Today there are many services which provide information over the phone using a prerecorded or synthesized voice. These voices are invariant in speed. Humans giving information over the telephone, however, tend to adapt the speed of their presentation to suit the needs of the listener. This paper presents a preliminary model of this adaptation. In a corpus of simulated directory assistance dialogs the operator &apos;s speed in number-giving correlates with the speed of the user&apos;s initial response and with the user&apos;s speaking rate. Multiple regression gives a formula which predicts appropriate speaking rates, and these predictions correlate (.46) with the speeds observed in good dialogs in the corpus. An experiment with 18 subjects suggests that users prefer a system which adapts its speed to the user in this way.
Disjunctive Logic Programming (DLP) under the answer set semantics  is an advanced formalism for knowledge representation and reasoning. It is  generally considered more expressive than normal (disjunction-free) Logic Programming,  whose expressiveness is limited to properties decidable in NP. However,
Genetic Programming uses a tree based representation to express solutions to problems. Trees are constructed from a primitive set which consists of a function set and a terminal set. An extension to GP is the ability to define modules, which are in turn tree based representations defined in terms of the primitives. The most well known of these methods is Koza&apos;s Automatically Defined Functions. In this paper it is proved that for a given problem, the minimum number of nodes in the main tree plus the nodes in any modules is independent of the primitive set (up to an additive constant) and depends only on the function being expressed. This reduces the number of user defined parameters in the run and makes the inclusion of a hypothesis in the search space independent of the primitive set.
Distributed Denial-of-Service attacks are an effective means to make a service unavailable, mask other attack activities and generally degrade or disrupt network functionality. The key characteristic is that analysis of and defence against this attack type is dicult because of the high number of attacking hosts and large amount of attack trac that can be generated. The emerging Peer-to-Peer  lesharing systems have characteristics that turn them into an attractive infrastructure that can be used as attack platform. Attackers that can compromise a P2P system can expect bene  ts such as a large number of participants, easy hiding of attack control trac and good, global distribution of participating hosts. This gives attackers high exibility and at the same time a smal risk of being identi ed. This paper explains these characteristics in detail and concludes that further research into this threat and into possible countermeasures is urgently needed.
Closed-loop control systems are dynamic systems subject to perturbations. One of the main concerns of the control is to design controllers to correct or limit the deviation that transient perturbations cause in the controlled system response. The smaller and shorter the deviation, the better the achieved performance. However, such controllers have been traditionally implemented using fixed timing constraints (periods and deadlines). This precludes controllers to execute dynamically, accordingly to the system dynamics, which may lead to sub-optimal implementations: although higher execution rates may be preferable when reacting to perturbations in order to minimize the response deviations, they imply wastage of resources when the system is in equilibrium.
With the increase of complexity in automotive control systems, the amount of data that needs to be managed is also increasing. Using a real-time database management system (RTDBMS) as a tightly integrated part of the software development process can give significant benefits with respect to data management. However, the variability of data management requirements in different systems, and the heterogeneousness of the nodes within a system may require a distinct database configuration for each node. In this paper we introduce a software engineering approach for generating RTDBMS configurations suitable for resource-constrained automotive control systems, denoted the COMET development suit. Using software engineering tools to assist developers with design and analysis of the system under development, different database configurations can be generated from pre-fabricated components. Each generated COMET database contains only functionality required by the node it is executing on.
The 9/11 tragedy triggered an increased interest in biometric  passports. According to several sources [2], the electronic ID market is  expected to increase by more than 50% per annum over the three coming  years, excluding China.
Recent emerging technologies such as internetworking and the World Wide  Web (WWW) have significantly expanded the types, availability, and volume  of data accessible to an information management system. In this new environment  it is imperative to view an information source at the level of its relevant  semantic concepts. We propose that these semantic concepts be chosen from  pre-existing domain specific ontologies. Domain specific ontologies are used as  tools/mechanisms for specifying the ontological commitments or agreements between  information users and providers on the information infrastructure. We use  domain specific ontologies to tackle the information explosion by the: (a) Re-use  and organization of knowledge in pre-existing real world ontologies, achieved by  mapping semantic concepts in the ontologies to data structures in the underlying  repositories; and (b) Knowledge integration and development of mechanisms to  translate information requests across ontologies. We thus provide support for multiple  domain specific ontologies as alternate world views on the vast amounts of  data. Semantic information brokering is implemented by brokering across domain  ontologies based on interontology relationships such as synonyms, hyponyms and  hypernyms defined between terms in different ontologies. Information requests  are rewritten using these relationships to obtain translations across ontologies.
We present solutions to statically load-balance scatter operations in parallel codes run on grids. Our load-balancing strategy is based on the modification of the data distributions used in scatter operations. We study the replacement of scatter operations with parameterized scatters, allowing custom distributions of data. The paper presents: 1) a general algorithm which finds an optimal distribution of data across processors; 2) a quicker guaranteed heuristic relying on hypotheses on communications and computations; 3) a policy on the ordering of the processors. Experimental results with an MPI scientific code illustrate the benefits obtained from our load-balancing.
this documentwe propose a new mode of operation for symmetric key block cipher algorithms. The main feature distinguishing the proposed mode from existing modes is that along with providing confidentiality of the message, it also provides message integrity. In other words, the new mode is not just a mode of operation for encryption, but a mode of operation for authenticated encryption. As the title of the document suggests, the new mode achieves the additional property with little extra overhead, as will be explained below. The new mode is also highly parallelizable. In fact, it has critical path of only two block cipher invocations. By one estimate, a hardware implementation of this mode on a single board (housing 1000 block cipher units) achieves terabits/sec (10  12  bits/sec) of authenticated encryption. Moreover, there is no penalty for doing a serial implementation of this mode. The new mode also comes with proofs of security, assuming that the underlying block ciphers are secure. For confidentiality,themode achieves the same provable security bound as CBC. For authentication, the mode achieves the same provable security bound as CBC-MAC. The new parallelizable mode removes chaining from the well known CBC mode, and instead does an input whitening (as well an output whitening) with a pairwise independent sequence. Thus, it becomes similar to the ECB mode. However, with the input whitening with the pairwise independent sequence the new mode has provable security similar to CBC (Note: ECB does not have security guarantees like CBC). Also, the output whitening with the pairwise independent sequence guarantees message integrity. The pairwise independent sequence can be generated with little overhead. In fact, the input and output whitening sequence need only be pairwi...
A method for object recognition and pose estimation for robotic bin picking is presented. The approach discussed is a variant on current approaches to eigenimage analysis. Compared to traditional approaches which use object geometry only (shape invariants), the implementation described uses the eigenspace determined by processing the eigenvalues and eigenvectors of the image set. The image set is obtained by varying pose whilst maintaining a constant level of illumination in space, and the eigenspace is computed for each object of interest. For an unknown input image, the recognition algorithm projects this image to each eigenspace and the object is recognised using space partitioning methods which determine the object and the position in space. Several experimental results have been obtained to demonstrate the robustness of this method when applied to the robotic bin picking task.
We describe a new selection scheme for steady-state evolution strategies, median selection. In steady-state algorithms, only one individual is generated and evaluated at each step and is immediately integrated into the population. This is especially well suited for parallel fitness evaluation in a multiprocessor environment. Previous steady-state selection schemes resembled (m + l) selection, which has a disadvantage in self-adaptation of the mutation step length. Median selection is similar to (m, l) selection. Median selection is compared with other steady-state selection schemes and with (m, l) selection on a uniprocessor and on a multiprocessor. It achieves equally good or better results as the best other selection scheme for a number of benchmark functions.
We describe a formal tool based on a symbolic semantics  for Full LOTOS, where specifications without restrictions in their data  types can be executed. The reflective feature of rewriting logic and the  metalanguage capabilities of Maude make it possible to implement the  whole tool in the same semantic framework, and have allowed us to implement  the LOTOS operational semantics, to integrate it with ACT  ONE specifications, and to build an entire environment with parsing,  pretty printing, and input/output processing of LOTOS specifications.
In this paper, we present a fast watershed algorithm based on the rainfalling simulation. We  present the various techniques and data structures utilized in our approach. Throughout this work,  the processing of large data sets (images as well as volume data) is especially emphasized. The  results&apos; correctness, the fast execution time, and the memory requirements are discussed in detail.
A problem which statistical offices and research institutes are faced with by releasing micro-data is the preservation of confidentiality. Official statistics are not allowed to pass on to external users outside the office, unless disclosure limitation is guaranteed. The same holds for survey-data, conducted by private or official research institutes, if confidentiality is promised to the respondents. Traditional methods to avoid disclosure often destroy the structure of data, i.d., information loss is more or less high. In this paper I discuss an alternative technique of creating scientificuse -files, which reproduce the characteristics of the original data quite well. It is based on an idea of Fienberg (1997 und 1994) [2], [3] to estimate and resample from the empirical multivariate cumulative distribution function of the data to get synthetic data. For estimation of the empirical cumulative distribution function non-parametric and semi-parametric methods are most useful, like kernel density estimators or a Bayesian approach (Fienberg, 1997 [2]). The procedure should create datasets which have the same charateristics as the original survey data. Means, variances, covariances, correlation and percentiles should not significantly differ. As the elements of the resample are drawn from the cumulative distribution function and do not necessarily correspond to any of those individuals in the original sample survey, an identification of true values is not possible. Nevertheless one cannot rule out the possibility of disclosure, as synthetic datasets could be very similar to real characteristics of observations. Especially, extreme values are at risk. To increase data protection the sample size of the resample could be raised until outliers repeatedly appear. The
The functionality in object-oriented programs is fragmented in its objects.
Abstract. Resources on the Semantic Web are described by metadata based on some formal or informal ontology. It is a common situation that casual users are not familiar with a domain ontology in detail. This makes it diffcult for such users (or their user tools) to formulate queries to find the relevant resources. Users consider the resources in their specific context, so the most straightforward solution is to formulate queries in an ontology that corresponds to a user-specific view. We present an approach based on multiple views expressed in ontologies simpler than the domain ontology. This allows users to query heterogeneous data repositories in terms of multiple, relatively simple, view ontologies. Ontology developers can define such view ontologies and the corresponding mapping rules. These ontologies are represented in Semantic Web ontology languages such as RDFS, DAML+OIL, or OWL. We present our approach with examples from the e-learning domain using the Semantic Web query and transformation language TRIPLE. 1
Evolutionary algorithms have been appied to the synthesis of neural architectures...
Given two--dimensional images T [1::n; 1::n] and P [1::m; 1::m] where m ! n, we develop a  fast rotation invariant filtration algorithm for finding the locations of approximate occurrences  of P in T . In such an occurrence, each pixel of T should match the corresponding  intervals of pixels defined by P . Our filter works by extracting linear sequences of intervals,  called features, from P , and by searching these from T to find candidate positions  and orientations for a whole occurrence of P . We give exact combinatorial analysis of the  problem. The number of relevant orientations for P is O(m  3  ) and number of different  features of length u is O(u  2  ). These can be reduced to O(m) and O(u) respectively, with  heuristics. We show that the optimal feature length is O(log m) and give O(jT j log m)  worst case time filtering algorithm for the problem. We conclude with experimental results.  Keywords: rotation invariance; combinatorial string matching; image comparison  1 Introdu...
In this paper, we examine methods for comparing  human and agent behavior. The results of such  a comparison can be used to validate a computer  model of human behavior, score a Turning test, or  guide an intelligent tutoring system. We introduce  behavior bounding, an automated model-based approach  for behavior comparison. We identify how  this approach can be used with both human and  agent behavior. We demonstrate that it requires  minimal human effort to use, and that it is efficient  when working with complex agents. Finally, we  show empirical results indicating that this approach  is effective at identifying behavioral problems in  certain types of agents and that it has superior performance  when compared against two benchmarks.
An important and intriguing aspect of e-entrepreneurship is the formation of new ventures in the domain of open source software (OSS). Previous research on these ventures has primarily looked at the design of business models, yet has neglected other key questions relating to the management of these firms, despite clear indications that some existing insights on venture management cannot be applied to new ventures in OSS. The purpose of this paper is to explore how three key challenges of venture management -- the liabilities of newness and smallness of start-ups and market entry barriers -- affect new ventures in OSS. Based on empirical data from personal interviews and a large scale survey we find that many of the liabilities that are typically discussed in the entrepreneurship literature are much less of a challenge for new ventures in OSS. Our findings have interesting implications for the emerging theory on e-entrepreneurship, and for entrepreneurs considering to exploit business opportunities in OSS, and more generally business opportunities based on open innovations.
To address the environmental and economic challenges of the 21  st  century it is incumbent upon the global community to evolve and sustain a global observation network. These observations serve as the foundation for the models that are used to describe Earth processes and can be used to predict the effect of different forcings on the planet (natural and anthropogenic). As this observational data accumulates in global archives new opportunities become available for knowledge discovery about the Earth sy stem. However, access to these observational data is optimized for the science teams for whom the instruments were launched and access by operational users may be problematic. Also, sensor fusion and data mining algorithms are generally not considered as necessary exploration modes of the archives. This paper will address the need for global observations from a variety of vantage points, some descriptions of global observation archives within the United States, recommendations for better access to image archives to facilitate image mining, and the research agenda for data archiving and distribution systems being developed by the IEEE Geoscience and Remote Sensing Society.
In regional Australia there is a growing interest and investment in community capacity building and this is beginning to be formalised in a desire to integrate information communications technology opportunities with other forms of community development. This paper explores the opportunity for greater social integration based on the formation of community-based information communication technology (ICT) driven organizations, using a case study approach. It is suggested that whether disseminating information, collaborating with other communities, assisting the development of new industries, or simply by sharing the lessons learned along the way, community-based IT can assist and support a community&apos;s economic and Copyright 2004, Idea Group Inc. Copying or distributing in print or electronic forms without written  permission of Idea Group Inc. is prohibited.
Let A(q; n; d) denote the maximum size of a q- ary code of length n and distance d. We study the minimum asymptotic redundancy (q; n; d) = n log q A(q; n; d) as n grows while q and d are fixed. For any d and q  d 1; long algebraic codes are designed that improve on the BCH codes and have the lowest asymptotic redundancy  (q; n; d) . ((d 3) + 1=(d 2)) log q n  known to date. Prior to this work, codes of fixed distance that asymptotically surpass BCH codes and the Gilbert-Varshamov bound were designed only for distances 4; 5; and 6.
Since computing is a physical activity, all forms of computing must obey locality constraints imposed by physics. Unknowingly, many software abstractions violate locality constraints because they represent high dimensional topologies that have higher degrees of freedom than is uniformly implementable by the underlying physical architecture. This semantic gap between abstractions implemented in the virtual architecture and the physical machine resources results in poor performance for certain classes of computing problems. This paper will discuss and analyze the impact of locality constraints and dimensionality limits upon software and architecture trends with the specific goals of improved performance, lower cost, and the longevity of architectural investments. 1.0 Importance of Locality to Architecture Much work in the physics and computing community focuses on physical limits to computing, but very little effort is spent on the applying the consequences of those limits to architect...
Prosody in a single speaking style -- often read speech -- has been studied extensively in acoustic speech. During the past few years we have expanded our interest in two directions: 1.) Prosody in expressive speech communication and 2.) Prosody as an audiovisual expression. Understanding the interactions between visual expressions (primarily in the face) and the acoustics of the corresponding speech presents a substantial challenge. Some of the visual articulation is for obvious reasons tightly connected to the acoustics (e.g. lip and jaw movements), but there are other articulatory movements that do not show up on the outside of the face. Furthermore, many facial gestures used for communicative purposes do not affect the acoustics directly, but might nevertheless be connected on a higher communicative level in which the timing of the gestures could play an important role. In this presentation we will give some examples of recent work, primarily at KTH, addressing these questions. We will report on methods for the acquisition and modeling of visual and acoustic data, and some evaluation experiments in which audiovisual prosody is tested. The context of much of our work in this area is to create an animated talking agent capable of displaying realistic communicative behavior and suitable for use in conversational spoken language systems, e.g. a virtual language teacher.
In this paper we present an analysis of the minimal hardware precision required to implement Support Vector Machine (SVM) classification within a Logarithmic Number System architecture. Support Vector Machines are fast emerging as a powerful machine-learning tool for pattern recognition, decision-making and classification. Logarithmic Number Systems (LNS) utilize the property of logarithmic compression for numerical operations. Within the logarithmic domain, multiplication and division can be treated simply as addition or subtraction. Hardware computation of these operations is significantly faster with reduced complexity. Leveraging the inherent properties of LNS, we are able to achieve significant savings over double-precision floating point in an implementation of a SVM classification algorithm.
Summarizing web pages have recently gained much attention from researchers. Until now two main types of approaches have been proposed for this task: content- and context-based methods. Both of them assume fixed content and characteristics of web documents without considering their dynamic nature. However the volatility of information published on the Internet argue for the implementation of more time-aware techniques. This paper proposes a new approach towards automatic web page description,  which extends the concept of a web page by the temporal dimension. Our method provides a broader view on web document summarization and can complement the existing techniques.
We present an approach to the interpretation of non-sentential utterances  like B&apos;s utterance in the following mini-dialogue: A: &quot;Who came to the  party?&quot; B: &quot;Peter.&quot; Such utterances pose several puzzles: they convey `sentence-type&apos; messages (propositions, questions or request) while being of nonsentential  form; and they are constrained both semantically and syntactically  by the context. We address these puzzles in our approach which is compositional,  since we provide a formal semantics for such fragments independent  of their context, and constraint-based because resolution is based on collecting  contextual constraints.
We present a means of comparing texts  to highlight their informational differences.
This paper presents and compares strategies for collecting data about moving objects in a  distributed system composed of servers in charge of recording events that occur in particular  partitions of a geographic space. This work shows that strategies based on the number of objects  or the number of move in events in partitions are practical alternatives for collecting large  amount of data about moving objects per time unit. Our study was performed on a simulated  environment with different distributions of servers in the space, in particular, uniform, normal  and cluster distributions. Among these distributions, collecting data on a uniform distribution  of servers outperforms the collection of data on a normal or cluster distribution of servers.
Storage consumption continues to grow rapidly, especially with the popularity of multimedia files. Worse, current disk technologies are reaching physical media limitations. Storage hardware costs represent a small fraction of overall management costs, which include backups, quota maintenance, and constant interruptions due to upgrades to incrementally larger storage. HSM systems can extend storage lifetimes by migrating infrequently-used files to less expensive storage. Although HSMs can reduce overall management costs, they also add costs due to additional hardware. Our key
this paper, we have presented constructive induction techniques recently added to the EITHER theory refinement system. Intermediate concept utilization employs existing rules in the theory to derive higherlevel features for use in induction. Intermediate concept creation employs inverse resolution to introduce new intermediate concepts in order to fill gaps in a theory than span multiple levels. These revisions allow EITHER to make use of imperfect domain theories in the ways typical of previous work in both constructive induction and theory refinement. As a result, EITHER is able to handle a wider range of theory imperfections than any other existing theory refinement system
ii Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv Table of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiv 1 
The purpose of this quantitative/qualitative evaluation study was to analyze the impact of the Success for All (SFA) program on reading achievement, attendance, and academic self-efficacy. Robert Slavin (1996) and his colleagues at Johns Hopkins University developed the Success for All program, which incorporates a comprehensive school restructuring approach. This program focuses on improving achievement of at-risk children and aims to have every child reading on or above grade level by grade three (Slavin, 1996). Two urban, schoolwide Title I elementary schools were compared using a non-equivalent matched group, evaluation design. Stanford 9 reading comprehension scores and attendance data were analyzed through an Analysis of Variance. Results yielded positive effects for group membership (SFA, non-SFA) in reading achievement and reading selfefficacy with mean scores of 58.6 NCEs vs 33.6 NCEs and 86.6 vs 68.7 respectively. Focus group results showed strong parental and staff support for the program. Implications are presented along with suggested future avenues of research such as the SFA program&apos;s impact over time and the investigation of the program&apos;s impact on other measures of achievement. Key Words: Reading, Title I, Efficacy, At-Risk iii  DEDICATION Dedicated to my husband Terrence Lynn Atkinson and our sons Taylor Lynn, Alexander Jenkins and Austin Howell Atkinson, I love you! iv  ACKNOWLEDGMENTS First giving honor to God, from whom all Blessings flow, I give thanks! This accomplishment is truly a blessing from above. Thanks to my committee co-chairpersons, Robert R. Richards and Mary E. Yakimowski, who encouraged me, critiqued my work, and supported me through this endeavor. Thanks to my other committee members, Stephen R. Parson and Christina M. Dawson, for ...
Declarative multi-paradigm languages combine the main features  of functional and logic programming, like laziness, logic variables  and non-determinism. The operational semantics of these languages is  based on a combination of narrowing and residuation. In this article,  we introduce a non-standard memoizing semantics for multi-paradigm  declarative programs and prove its equivalence with the standard operational  semantics. Both pure functional and pure logic programming have  for long time taken advantage of tabling or memoizing schemes [15,19,7],  which motivates the interest in the adapation of this technique to the  integrated paradigm.
This project demonstrated that enough information  can be retrieved from MEI, an XML format for musical  information representation, to transform it into  music notation with good fidelity. The process involved  writing an XSLT script to transform files into  Mup, an intermediate format, then processing the  Mup into PostScript, the de facto page description  language for high-quality printing. The results show  that the MEI format represents musical information  such that it may be retrieved simply, with good recall  and precision.
We give a new proof of the theorem of Joszef Denes: If L 1 and L 2  are distinct latin squares of order n    2, n /    6}, that satisfy the  quadrangle criterion, then L 1 and L 2 differ in at least 2n entries.
We explore the application of a state space compression algorithm to spin. Compression  techniques are notably unpredictable: we tend to use them when they work  well for us (e.g. zip),but we tend to forget that, according to the theory, they can also  result in an expansion (e.g., ziptwice).
The aim of this paper is to focus on the issues connected with the diagrammatic notations and tools, which should support developers when moving from the design phase towards the implementation phase of MASs. The paper deals with three main research lines: the representation of the interactions between agents and their relationship with the internal architecture of the single agent, the deployment of MASs, and the pattern reuse.
In this work we present the analysis and decissions taken to  develop a distributed raytracing system. We will detail the  options analyzed, as well as the solutions adopted to get a  multiplatform and flexible system under the client-server  paradigm. In this way, the system will improve the performance  of computer based animation workers that make use of  raytracing techniques. Moreover, due to modular architecture,  the system could be utilized in many other types of calculations.
A configuration management concept is presented for software projects using Lyee  methodology. To show this concept an introduction in configuration management is  given. Then, the structure of Lyee programs is redefined by sets and their dependencies. From this
Introduction  Converging evidence from experimental, clinical, and neuroimaging data suggests that a number of cortical areas are involved in human pain processing [1, 2, 3]. Among those are the primary (SI) and secondary (SII) somatosensory cortex. With regard to the representation of pain in SI and SII two issues have not been investigated so far:  1. The temporal characteristics of nociceptive processing in these cortices have remained largely unexplored. Especially, it is unknown whether SI and SII are activated in a serial or a parallel mode.  2. SI is not a homogeneous area but consists of four cytoarchitectonically distinct fields arranged from rostral to caudal and referred to as areas 3a, 3b, 1, and 2 [4, 5]. Whereas the parietal representations of tactile stimuli have been extensively investigated the representation of nociceptive stimuli within SI is unclear.  We, therefore, used whole-head magnetoencephalography (MEG) to investigate the time course of cortical responses in 
Suppose G is r-colorable and P  V (G) is such that the components of G[P ]  are far apart. We show that any (r + s)-coloring of G[P ] in which each component  is s-colored extends to an (r + s)-coloring of G. If G does not contract to K 5 or is  planar and s  2, then any (r + s 1)-coloring of P in which each component is  s-colored extends to an (r + s 1)-coloring of G. This result uses the Four Color  Theorem and its equivalence to Hadwiger&apos;s Conjecture for k = 5. For s = 2 this  provides an armative answer to a question of Thomassen. Similar results hold for  coloring arbitrary graphs embedded in both orientable and non-orientable surfaces.
Popular articles and manufacturers&apos; literature assert that full-spectrum fluorescent lighting improves cognitive performance, vision, and mood. Previous experimental investigations have failed to demonstrate these effects. This paper reports an attempt to replicate work by Veitch, Gifford, and Hine (1991) concerning the effects of information sets about lighting on performance and mood. In this 2 (lamp type) x 4 (information sets) x 2 (gender) factorial experiment, 104 male and 104 female participants were led to expect good, poor, or indifferent outcomes of working under full-spectrum or cool-white fluorescent lighting, or they were not given any information. There were no effects of lamp type or information set on performance or mood. The results are discussed in relation to other evidence that cognitive processes mediate lighting-behaviour relationships. 
To support enport1 human users in robust,mixed-inNBX1GFX speech dialogueingue1&quot;L3&apos; which reach beyon curren capabilities in dialogue systems, the DARPA Commun&quot;NB1G program [1] isfun&apos;FU the developmen of a distributed message-passin inage-passin for dialogue systems which all Commun1GF3X participanM areusinN In thispresenXX1GF&apos; we describe the features ofan requiremenM for a genMM31G useful software inare1&quot;&apos;L&quot;1G for this purpose. KeywCBT Spoken dialogue, speechinch1&apos;FL&quot; 1.  
Organizations depend on regular meetings to carry out their everyday tasks. When carried  out successfully, meetings offer a common medium for participants to exchange ideas and make decisions.
The availability of multiple transmit antennas allows for two-dimensional channel codes that exploit the spatial transmit diversity. These codes were referred to as space--time codes by Tarokh et al. Most prior works on space--time code design have considered quasi-static fading channels. In this paper, we extend our earlier work on algebraic space--time coding to block-fading channels. First, we present baseband design criteria for space--time codes in multi-input multi-output (MIMO) block-fading channels that encompass as special cases the quasi-static and fast fading design rules. The diversity advantage baseband criterion is then translated into binary rank criteria for phase shift keying (PSK) modulated codes. Based on these binary criteria, we construct algebraic space--time codes that exploit the spatial and temporal diversity available in MIMO block-fading channels. We also introduce the notion of universal space--time codes as a generalization of the smart--greedy design rule. As a part of this work, we establish another result that is important in its own right: we generalize the full diversity space--time code constructions for quasi--static channels to allow for higher rate codes at the expense of minimal reductions in the diversity advantage. Finally, we present simulation results that demonstrate the excellent performance of the proposed codes.
There is currently a high activity in the transportation tunnelling industry in the countries of the Union with the highest seismicity. In this work, and in order to assure the safety of vulnerable tunnel cross-sections or cross-sections where very high standards of safety are required, an integrated package is being developed that includes a deformation monitoring system that can provide real-time measurements of tunnel deformation during an earthquake and a decision support system that processes these measurements to assess the lining&apos;s ability to survive expected aftershocks.  1. INTRODUCTION  There is currently a high activity in the transportation tunnelling industry it in the countries of the Union with the highest seismicity where a good part of the tunnel kilometres will be under densely populated areas and require very high standards of safety.  During an earthquake or an ensemble of earthquakes and aftershocks tunnels may exceed their functional or structural limits: Even mod...
Hybrid rendering of volume and polygonal model is an interesting feature of visualization systems, since it helps users to better understand the relationships between internal structures of the volume and fitted surfaces as well as external surfaces. Most of the existing bibliography focuses at the problem of correctly integrating in depth both types of information. The rendering method proposed in this paper is built on these previous results. It is aimed at solving a different problem: how to efficiently access to selected information of a hybrid model. We propose to construct a decision tree (the Rendering Decision Tree), which together with an auxiliary runlength representation of the model avoids visiting unselected surfaces and internal regions during a traversal of the model.
Interest in synthesis of Application Specific Instruction Processors or ASIPs has increased considerably and a number of methodologies have been proposed in the last decade. This paper attempts to survey the state of the art in this area and identifies some issues which need to be addressed. We have identified the five key steps in ASIP design as application analysis, architectural design space exploration, instruction set generation, code synthesis and hardware synthesis. A broad classification of the approaches reported in the literature is done. The paper notes the need to broaden the architectural space being explored and to tightly couple the various subtasks in ASIP synthesis.
We consider the Contextual Recursive Cascade Correlation  model (CRCC), a model able to learn contextual mappings in structured  domains. We propose a formal characterization of the &quot;context window&quot;,  i.e., given a state variable, the &quot;context window&quot; is the set of state variables  that directly or indirectly contribute to its determination. On the  basis of this definition, a formal and compact expression describing the  &quot;context windows&quot; for the CRCC, and RCC model, are derived.
We provide here a framework for studying Prolog programs with various built-in&apos;s that include arithmetic operations, and such metalogical relations like var and ground. To this end we propose a new, declarative semantics and prove completeness of the Prolog computation mechanism w.r.t. this semantics. We also show that this semantics is fully abstract in an appropriate sense. Finally, we provide a method for proving termination of Prolog programs with built-in&apos;s which uses this semantics. The method is shown to be modular and is illustrated by proving termination of a number of programs including the unify program of Sterling and Shapiro [SS86]. Keywords and Phrases: Prolog programs, built-in&apos;s, declarative semantics, termination.  1985 Mathematics Subject Classification: 68Q40, 68T15,  CR Categories: F.3.2., F.4.1, H.3.3, I.2.3. Note: This research was done during the second and third authors&apos; stay at Centre for Mathematics and Computer Science, Amsterdam. The work of K.R. Apt was par...
We motivate, derive and implement a multilevel approach to the travelling salesman problem. The  resulting algorithm progressively coarsens the problem, initialises a tour and then employs either the LinKernighan  (LK) or the Chained Lin-Kernighan (CLK) algorithm to refine the solution on each of the coarsened  problems in reverse order. In experiments on a well established test suite of 79 problem instances  we found multilevel configurations that either improved the tour quality by over 25% as compared to the  standard CLK algorithm using the same amount of execution time, or that achieved approximately the  same tour quality over 7 times more rapidly. Moreover the multilevel variants seem to optimise far better  the more clustered instances with which the LK &amp; CLK algorithms have the most difficulties.
Some memory writes have the particular behaviour of not modifying  memory since the value they write is equal to the value before the write.These  kind of stores are what we call Redundant Stores. In this paper we study the  behaviour of these particular stores and show that a significant saving on  memory traffic between the first and second level caches can be avoided by  exploiting this feature. We show that with no additional hardware (just a simple  comparator) and without increasing the cache latency, we can achieve on  average a 10% of memory traffic reduction. 1. 
Several recent price-based congestion control schemes require relatively accurate path price estimates for successful operation. The proposed addition of the two-bit Explicit Congestion Notification (ECN) field in the IP header provides routers with a mechanism for conveying price information. Recently, two proposals have emerged for probabilistic packet marking at the routers; the proposals allow receivers to estimate path price from the fraction of marked packets. In this paper we introduce an alternative deterministic marking scheme for encoding path price. Under our approach, each router quantizes the price of its outgoing link to a fixed number of bits. We then make use of the IP identification (IPid)fieldtomapdata  packets to different probe types, and each probe type calculates a partial sum of the path price bits. A router deduces its marking behaviour according to the IPid and the TTL (Time To Live) field of each packet. We evaluate the performance of our algorithm in terms of its error in representing the end-toend price, and compare it to probabilistic marking. We show that based on empirical Internet traffic characteristics, our algorithm performs better when estimating path price using small blocks of packets. We also derive the probability distribution of the error for our scheme, and provide a relatively simple bound on its maximum mean-squared error.
A naive Bayes classifier was used to analyze gene behavior based on text data and presented as an entry for the 2002 KDD Cup, a data mining exercise to predict the behavior of the yeast S. Cerevisiae. The solution presented was based on the multinomial event model for text classification(McCallum &amp; Nigam 1998) with a feature selection mechanism added. Despite this simple model, performance close to that of the best entries in the competition could be obtained, which were using more sophisticated techniques. It appears that seemingly minor effort in using prior knowledge to conflate the gene classes, as well as the previously described effectiveness of the naive Bayes method contributed to this success.
Among the most promising modern approaches to creativity, a very  important one makes use of non-standard problem solving techniques of  artificial intelligence for addressing methodological descriptions of creativity. The purpose of this
 We discuss the emergence of giant components in two random graph models (one directed, one undirected). Our study of these models was motivated by an interest in finding a random model of the Internet. 
In this paper we propose the application of a new transform-based coding method[1] in conjunction with Golomb-Rice ( ) codes to lower significantly the complexity, which can be used in various applications, e.g. the Multiple Description coding[2]. The theoretical evaluations predict no important loss in compression performance, while the complexity is considerably reduced. Since   codes are very fast and well suited for exponentially decaying distributions, they were implemented during the last decade in image and audio compressors. In all these schemes, the selection of the code parameter is performed presuming Laplacian distribution of prediction errors. We derive the selection method for the   code parameter also for the case of Gaussian inputs. 1. 
Web Services constitute a set of technologies that many believe will change the web communication landscape within the next few years. They offer standardized and easy communications for distributed systems over the Internet. However their dynamic and distributed nature requires a well-managed system, and pending security issues prevent their widespread adoption. Meanwhile there is a big rage toward the use of Virtual Private Networks (VPNs) to secure communications in a cost-effective environment like the Internet. In this paper we explain how to merge these two technologies in a new powerful hybrid model that: (1) enables an easy management of web services, (2) provides web services security thanks to the use of dynamic and programmable VPNs, and (3) remains simple and fully integrated.
this paper is to evaluate the performance gains provided by VIA, when compared to TCP/IP, a traditional, multilayered communication protocol. To achieve this purpose we run five distinct applications using the same network card and switch, and just change the communication protocol: Water from the SPLASH benchmark suite [SIN 92]; 3-D FFT, IS and EP from NAS benchmark [BAI 91]; and SOR, from [LU 95]. We ran SOR and Water with two different input sets: SOR internal elements of the matrix initialized to either zero (SOR-Z) or nonzero values (SOR-NZ); and Water with 288 and 1728 molecules. For all but one application, the speedup of TCP/IP was between 28% and 97% that of achieved by VIA. TCP/IP outperformed VIA in just one application, SOR, with both input sets
This chapterdiscussesthedesirable featuresoflanguagesandprotocolsfor communicationamongintelligentinformationagents.These desiderata aredividedintosevencategories:form,content,semantics,implementation,networking, environment,and reliability.TheKnowledgeQueryandManipulation LanguageffKQMLffisanewlanguageandprotocolforexchanginginformation andknowledge.Thisworkispartofalargereffort,theARPAKnowledgeSharingE ffort,whichisaimedatdevelopingtechniquesandmethodologiesforbuildinglarge ffscaleknowledgebasesthataresharableandreusable.KQMLisbotha messageformatandamessageffhandlingprotocoltosupportrunfftimeknowledge sharingamongagents.KQML is describedandevaluatedasanagentcommunicationlanguage relativetothedesiderata. ToappearinIntelligentAgentsVolumeIIffProceedingsofthe1995Workshopon  AgentTheories,Architectures,andLanguages.M.Wooldridge,J.P.Mullerand  M.Tambeffedsff.LectureNotesinArtiffcialIntelligence,Springer-Verlag,1996. 1 
We present Structural Logistic Regression, an extension of logistic  regression to modeling relational data. It is an integrated approach to building  regression models from data stored in relational databases in which potential predictors,  both boolean and real-valued, are generated by structured search in the  space of queries to the database, and then tested with statistical information criteria  for inclusion in a logistic regression. Using statistics and relational representation  allows modeling in noisy domains with complex structure. Link prediction  is a task of high interest with exactly such characteristics. Be it in the domain  of scientific citations, social networks or hypertext, the underlying data are extremely  noisy and the features useful for prediction are not readily available in a  &quot;flat&quot; file format. We propose the application of Structural Logistic Regression to  building link prediction models, and present experimental results for the task of  predicting citations made in scientific literature using relational data taken from  the CiteSeer search engine. This data includes the citation graph, authorship and  publication venues of papers, as well as their word content.
We show that partial evaluation can be usefully viewed as a programming model for realizing mixed-initiative functionality in interactive applications. Mixed-initiative interaction between two participants is one where the parties can take turns at any time to change and steer the flow of interaction. We concentrate on the facet of mixed-initiative referred to as `unsolicited reporting&apos; and demonstrate how out-of-turn interactions by users can be modeled by `jumping  ahead&apos; to nested dialogs (via partial evaluation). Our approach permits the view of dialog management systems in terms of their support for staging and simplifying interactions; we characterize three different voice-based interaction technologies using this viewpoint. In particular, we show that the built-in form interpretation algorithm (FIA) in the VoiceXML dialog management architecture is actually a (well disguised) combination of an interpreter and a partial evaluator.
Withmo del-based develo pment beingo n the vergeo f beco ming an industrial standard, theto pico f researcho f statically checking the co sistencyo f a mo del made upo f several submo dels has already received increasing attentio . The evo lutio omo dels withinsoff ware engineering requires suppoF fo incremental c o sistency analysis techniques o a new versio o f themo del after evoA tio , thereby avo iding a coz lete reiteratio o f all co sistency tests. In this paper, we discuss thepro blemo f preserving co nsistency within mo del-based evo lutio n fo cusingo n UML-RTmo dels. We intro duce the co ncepto f amo del transfo rmatio n rule that captures an evo lutio n step. CoE oE io o f several evoA tio steps leads to acoE lex evo lutio o a mo del. Fo r each evo lutio n step, we study the effectso n the co nsistency o f theo verallmo del andpro vide lo calized co nsistency checks fo rtho se partso f themo del that have changed. Fo r a co mplex evo lutio no f a mo del, co nsistency can then be established by incrementally perfo rming tho se lo calized co nsistency checks asso ciatedto the transfo rmatio n rules applied within the evo lutio n. 1 
Understanding and measuring end-to-end service performance perceived by the clients is a challenging task. Client-perceived web site responses are downloaded web pages. Typically, a web page is composed from multiple objects: a main HTML file and several embedded objects such as images. However, HTTP does not provide any means to delimit the beginning or the end of a web page to effectively measure the overall response time for web page retrieval. This paper presents, EtE monitor, a novel approach to measuring web site performance. Our system passively collects packet traces from a server site to determine service performance characteristics. We introduce a two-pass heuristic and a statistical filtering mechanism to accurately reconstruct different client page accesses and to measure performance characteristics integrated across all client accesses. Relative to existing approaches, EtE monitor offers the following benefits: i) a latency breakdown between the network and server overhead of retrieving a web page, ii) longitudinal information for all client accesses, not just the subset probed by a third party, iii) characteristics of accesses that are aborted by clients, and iv) quantification of the benefits of network and browser caches on server performance. Our initial implementation and performance analysis across three different commercial web sites confirm the utility of our approach.
In this paper, organized in essay style, I first assess the situation of Machine Translation, which is characterized, on the one hand, by unsatisfied user expectations, and, on the other hand, by an ever increasing need for translation technology to fulfil the promises of the global knowledge society, which is promoted by almost all governments and industries worldwide. The assessment is followed by an outline of the design of a blueprint that describes possible steps of an MT evolution regarding short term, mid term and long term developments. Although some user communities might aim at an MT revolution, the evolutionary implementation of the different aspects of the blueprint fit seamless with the foundation that we are faced with in the assessment part. With the blueprint the thesis of this MT evolution essay is established, and the stage is opened for the antithesis in which I develop the points for an MT revolution. Finally, in the synthesis part I develop a combined view which then completes the discussion and the establishment of a blueprint for MT evolution.
This video illustrates three distortion-oriented displays as novel ways of providing awareness of others within groupware. These displays employ magnification lenses and fisheye view techniques to show global context and local detail within a single window, providing both peripheral and detailed awareness of other participants&apos; presence, location, and actions. The head-up lens uses a transparent interface, showing local detail in the foreground and an overview in the background. The offset lens allows a person to magnify a region of the overview. Finally, the fisheye text viewer displays multiple focal points, one for each participant in the conference .
A (t, n) threshold signature scheme allows t or more group  members to generate signatures on behalf of a group with n members,  while any t    or less members cannot do the same thing. In 2001, based  on a variant of ElGamal digital signature scheme, Li et al. proposed two  (t, n) threshold signature schemes with traceable signers. One of their  schemes needs the assistance of a mutually trusted center, while the other  does not. In this paper, we present a security analysis on their schemes.
This paper develops a novel slotted ALOHA protocol (Direction-Of-Arrival ALOHA) for use in ad hoc networks where nodes are equipped with smart antennas. The protocol relies on the ability of the antenna and DOA algorithms to identify the direction of the desired signal and the direction of the interferers to maximize SINR (Signal to Interference and Noise Ratio) at the receiver. The performance of the protocol is evaluated using joint simulation in OPNET and Matlab. We compare the performance of our new protocol against recent directional MAC (Medium Access Control)[3] protocol. We show that DOA-ALOHA achieves significantly higher throughput than [3] despite its simplicity. The impact of using different number of antenna elements is also studied for this environment.
We present an algorithm that uses partitioning and gluing to compress large triangular meshes which are too complex to fit in main memory. The algorithm is based largely on the existing mesh compression algorithms, most of which require an &apos;in-core&apos; representation of the input mesh. Our solution is to partition the mesh into smaller submeshes and compress these submeshes separately using existing mesh compression techniques. Since a direct partition of the input mesh is out of question, instead, we partition a simplified mesh and use the partition on the simplified model to obtain a partition on the original model. In order to recover the full connectivity, we present a simple scheme for encoding/decoding the resulting boundary structure from the mesh partition. When compressing large models with few singular vertices, a negligible portion of the compressed output is devoted to gluing information. On desktop computers, we have run experiments on models with millions of vertices, which could not be compressed using standard compression software packages, and have observed compression ratios as high as 17 to 1 using our technique.
In this paper it is argued that it is legitimate to talk about mental imagery, and thus, to claim that spatial information is coded in an analogue mode. Evidence from research with blind people indicate that they can perform mental rotation and that analogue spatial cognition does not depend on visual information. It is therefore proposed that, if the blind can perform mental rotation in an analogue mode, there exists a common mode, for processing spatial information, which is not modality specific. The results of the presented study, which was conducted on eight blind subjects, and was intended to extend previous findings, was not conclusive. Reaction time of a haptic mental rotation task could not directly be shown to be a linear function of angular disparity. An alternative theory, based on two different cognitive strategies, is considered as an alternative explanation of the experimental results.
To meet the challenges of application management, our research group in Hewlett Packard Laboratories is prototyping software agents made up of &quot;parts&quot;---lightweight threads that communicate using message-passing. Specifications for these agents are described in CIM and stored in Microsoft&apos;s CIMOM object repository. To facilitate the design of these agents, a simple macro language called Agent Generation Tool (AGT) has been developed. Rather than specifying agents directly in MOF, agents can be specified using AGT. The AGT macro processor can then generate the appropriate MOF. AGT allows super-parts, combinations of parts that communicate in a given pattern, to be defined and reused multiple times. The design considerations behind AGT are discussed.
We give a simple combinatorial algorithm that computes a piecewise-linear approximation of a smooth surface from a finite set of sample points. The algorithm uses Voronoi vertices to remove triangles from the Delaunay triangulation. We prove the algorithm correct by showing that for densely sampled surfaces, where density depends on &quot;local feature size&quot;, the output is topologically valid and convergent (both pointwise and in surface normals) to the original surface. We describe an implementation of the algorithm and show example outputs. 1 Introduction  The problem of reconstructing a surface from scattered sample points arises in many applications such as computer graphics, medical imaging, and cartography. In this paper we consider the specific reconstruction problem in which the input is a set of sample points S  drawn from a smooth two-dimensional manifold F embedded in three dimensions, and the desired output is a triangular mesh with vertex set equal to S that faithfully represen...
Neural network ensemble can significantly improve the generalization ability of neural network based systems. However, its comprehensibility is even worse than that of a single neural network because it comprises a collection of individual neural networks. In this paper, an approach named REFNE is proposed to improve the comprehensibility of trained neural network ensembles that perform classification tasks. REFNE utilizes the trained ensembles to generate instances and then extracts symbolic rules from those instances. It gracefully breaks the ties made by individual neural networks in prediction. It also employs specific discretization scheme, rule form, and fidelity evaluation mechanism. Experiments show that with different configurations, REFNE can extract rules with good fidelity that well explain the function of trained neural network ensembles, or rules with strong generalization ability that are even better than the trained neural network ensembles in prediction.
We present the architecture, technology and experimental applications of a real-time, multi-site, interactive and collaborative environment called Distributed Immersive Performance (DIP). The objective of DIP is to develop the technology for live, interactive musical performances in which the participants - subsets of musicians, the conductor and the audience - are in different physical locations and are interconnected by very high fidelity multichannel audio and video links. DIP is a specific realization of broader immersive technology - the creation of the complete aural and visual ambience that places a person or a group of people in a virtual space where they can experience events occurring at a remote site or communicate naturally regardless of their location. The DIP experimental system has interaction sites and servers in different locations on the USC campus and at several partners, including the New World Symphony of Miami Beach, FL. The sites have different types of equipment to test the effects of video and audio fidelity on the ease of use and functionality for different applications. Many sites have high-definition (HD) video or digital video (DV) quality images projected onto wide screen wall displays completely integrated with an immersive audio reproduction system for a seamless, fully three-dimensional aural environment with the correct spatial sound localization for participants. The system is capable of storage and playback of the many streams of synchronized audio and video data (immersidata), and utilizes novel protocols for the low-latency, seamless, synchronized realtime delivery of immersidata over local area networks and widearea networks such as Internet2. We discuss several recent interactive experiments using the system and many technical cha...
This paper provides a brief overview of main adaptive navigation support techniques and analyzes the results of most representative empirical studies of these techniques. It demonstrates an evidence that different known techniques work most efficiently in different context. In particular, the studies summarized in the paper have provided evidence that users with different knowledge level of the subject may appreciate different adaptive navigation support technologies. The paper argues that more empirical studies are required to help the developers of adaptive hypermedia systems in selecting most relevant adaptation technologies. It also attempts to build a case for meta-adaptive hypermedia systems, ie, systems that are able to adapt the very adaptation technology to the given user and context
This paper presents the design, implementationand evaluation of Mingle, a secure distributed search system. Each participating host runs a Mingle server, which maintains an inverted index of the local file system. Users initiate peerto -peer keyword searches by typing keywords to lightweight Mingle clients. Central to Mingle are its access control mechanisms and its insistence on user convenience. For access control, we introduce the idea of access-right mapping, which provides a convenient way for file owners to specify access permissions. Access control is supported through a single sign-on mechanism that allows users to conveniently establish their identity to Mingle servers, such that subsequent authentication occurs automatically, with minimal manual involvement. Preliminary performance evaluation suggests that Mingle is both feasible and scalable.
When solving large nonsymmetric systems of linear equations with the restarted GMRES algorithm, one is inclined to select a relatively large restart parameter in the hope of mimicking the full GMRES process. Surprisingly, cases exist where small values of the restart parameter yield convergence in fewer iterations than larger values. Here, two simple examples are presented where GMRES(1) converges exactly in three iterations, while GMRES(2) stagnates. One of these examples reveals that GMRES(1) convergence can be extremely sensitive to small changes in the initial residual.
This thesis deals with the analysis and application of evolutionary algorithms for optimization problems with multiple objectives. Many application problems involve (i) a system model that is not given in closed analytical form and (ii) multiple, often conflicting optimization criteria. Both traits hamper the application of classical optimization techniques, which require a certain structure of the problem and are mostly designed to handle only a single objective. For this problem domain, the class of randomized search heuristics, to which evolutionary algorithms also belong, have become popular. Due to their population concept, evolutionary algorithms can process multiple solutions in parallel and can therefore cope with different objectives more naturally.
this paper makes some initial steps in that direction. We start with the definition of the measure. Let G = (VG ; EG ) be a finite graph. We assume that G is connected and bipartite. Let  v 0 2 VG be a specified vertex of G. Let X G;v0 denote the set of all mappings f : VG ! Z with the property that (i) f(v 0 ) = 0, and (ii) jf(u) \Gamma f(v)j = 1 for all u; v 2 VG such that fu; vg 2 EG  (property (ii) asserts that f is a graph homomorphism from G to Z). Let P G;v0 be the uniform probability measure on X G;v0 , i.e
On the basis of the ordering of bare complements, modifying adjectives and certain adverbs in French, we show that certain constituents are more constrained than others, and we explain this situation in terms of weight, as one of the factors which determine word order. In addition to the distinction between heavy and non-heavy constituents, we propose that there exists a distinction among non-heavy constituents between &apos;light&apos; and &apos;middle-weight&apos; ones. We formalise this distinction in the feature based HPSG framework with a two-value (lite vs non-lite) feature WEIGHT, which is appropriate both for lexical items and phrases. Finally, we suggest that the lite vs non-lite distinction is universally available, although other word order properties make it more or less apparent in a given language.
Application or web services are increasingly being used across organisational boundaries. Moreover, new services are being introduced at the network and storage level. Languages to specify interfaces for such services have been researched and transferred into industrial practice. We investigate end-to-end quality of service (QoS) and highlight that QoS provision has multiple facets and requires complex agreements between network services, storage services and middleware services. We introduce SLAng, a language for defining Service Level Agreements (SLAs) that accommodates these needs. We illustrate how SLAng is used to specify QoS in a case study that uses a web services specification to support the processing of images across multiple domains and we evaluate our language based on it.
In the process of becoming engineering educators, most professors have successfully developed some fundamental skills that are necessary (although not sufficient) to become a successful researcher.However, theyoften have noclear idea of howtopass this knowledge along to their students beyond the general approach of carefully guiding their students&apos; individual research efforts. A sense of not knowing where to begin this type of mentoring process can be especially acute in newfaculty.This paper presents several techniques, organized around a set of guiding principles, for helping newfaculty teach the engineering research process. 1. 
em is primarily being designed for is the classic &apos;user to root&apos;. This is where a user tries to get additional privileges such as access to log files or the ability to masquerade as another user. We would like to know if the user is trying to misuse the system. Once a user has gained &apos;root&apos; access they generally have the ability to go in and change log files in order to &apos;cover their tracks&apos;. Therefore the need to determine if a user is attempting to gain root access needs to be closely monitored. A method of tracking users and how they use the system that they are on is by inspecting audit files. Auditing is the process by which records are maintained of a users activity on a system, and typically cannot be readily accessed for editing by anyone but the superuser or chief security officer. Through reconstructing these audit logs it becomes possible to trace what the user did. One of the benefits of an auditing program is that a system can be designed and created which can run real-time
In the field of agents, there are a wide variety of them, such as  learning agents, planning agents or communicative agents. One of the youngest  members in the family are mobile agents, which provide us with the interesting  feature of mobility in order to perform their tasks in different machines. In this  paper, we will see some of the current uses of mobile agents and we will  suggest how we could use these agents along with Virtual Environments in  order to enhance them and open a new world of possibilities for the users of  these applications.
Specifications of XML documents typically consist of typing  information (for example, a DTD), and integrity constraints (for example,  keys and foreign keys). We show that combining the two may lead  to seemingly reasonable specifications that are nevertheless inconsistent:  there is no XML document that both conforms to the DTD and satisfies  the constraints. We then survey results on the complexity of consistency  checking, and show that, depending on the classes of DTDs and  constraints involved, it ranges from linear time to undecidable. Furthermore,  we show that for some of the most common classes of specifications  checking consistency is intractable.
Let g and h be binary morphisms defined on A =   b}, and exactly one of them be periodic. It is well known that their equality language is generated by at most one non-empty word. We show that this word is up to the symmetry of the letters a and  b equal to a     , with i, j   0. A word of that form is an equality word for all values of  i, j. 1. 
We develop the theory of weak conditioning initiated in [2, 5] in a general framework for semimartingales with jumps. An example is given based on Levy processes, for which the link to pure jump Schrodinger processes is exhibited. AMS 2000 Subject Classification : Primary: 60G07, 60G44; secondary: 60G51.
this paper, we will focus on the study of the unique completion of partial latin squares. Such ideas are important for the determination of critical sets in latin squares [2], [5]
We consider the requirements for uniform pseudo-random number generators on modern  vector and parallel machines; consider the pros and cons of various popular classes of methods  and some new methods; and outline what is currently available. We then make a proposal  for a class of random number generators which have good statistical properties and can  be implemented efficiently on vector processors and parallel machines. A proposal regarding  initialization of these generators is made. We also discuss the results of a trial implementation  on a Fujitsu VP 2200/10 vector processor.
UML provides a variety of diagram types for specifying both  the structure and the behavior of a system. During the development  process, models specified by use of these diagram types have to be transformed  into corresponding code. In the past, mainly class diagrams and  state diagrams have been considered for an automatic code generation. In this paper,
The goal of this paper is to propose a new methodology for designing coordination between human agents and software agents and, ultimately, among software agents. The methodology is based on two key ideas. The first is that coordination should be designed in steps, according to a precise software engineering methodology, and starting from the specification of early requirements. The second is that coordination should be modeled as dependency between actors. Two actors may depend on one another because they want to achieve goals, acquire resources or execute a plan. The methodology used is based on Tropos, an agent oriented software engineering methodology presented in earlier papers. The methodology is presented with the help of a case study.
The media monitoring activity is undergoing a large expansion as a consequence of the different emerging  media sources. This is pushing the development of automatic systems for selective dissemination of multimedia  information. In this paper we present the development of a prototype system able to scan multimedia data, specifically TV broadcasts, and to generate alert messages to users about the relevant information to them. The system makes use of advanced processing technologies for content-based indexing of multimedia information. We use large vocabulary speech recognition system, associated with  audio segmentation, and automatic topic indexing and segmentation, to generate category information as semantic markup of multimedia data. The system service is based on a web interface design offering new views of these marked documents and providing useful end-user services based on the content multimedia exploitation.
A high-speed high-resolution sample-and-hold amplifier (SHA) is designed for time-interleaved analog-todigital converter applications. Using the techniques of precharging and output capacitor coupling can mitigate the requirements for the opamp, resulting in low power dissipation. Implemented in a standard 0.25 m CMOS technology, the SHA achieves 73 dB SFDR for 2 Vpp input at 100 MHz sampling rate. The performance is not degraded for input&apos;s frequency up to the Nyquist frequency. Power consumption is 33 mW from a single 2.5 V supply.
Partial redundancy elimination (PRE) techniques play an  important role in optimizing compilers. Many optimizations, such as  elimination of redundant expressions, communication optimizations, and  load-reuse optimizations, employ PRE as an underlying technique for improving  the efficiency of a program. Classical approaches
A new class of biometrics based upon ear features is introduced for use in the  development of passive identification systems. The viability of the proposed biometric  is shown both theoretically in terms of the uniqueness and measurability  over time of the ear, and in practice through the implementation of a computer  vision based system. Each subject&apos;s ear is modeled as an adjacency graph built  from the Voronoi digram of its curve segments. We introduce a novel graph  matching based algorithm for authentication which takes into account the erroneous  curve segments which can occur due to changes (e.g., lighting, shadowing,  and occlusion) in the ear image. This new class of biometrics is ideal for passive  identification because the features are robust and can be reliably extracted from  a distance.
This paper presents some analytic results concerning the Pivot Interval Routing  (PIR) strategy of [8]. That strategy allows message routing on every weighted n-node  network along paths whose stretch (namely, the ratio between their length and the  distance between their endpoints) is at most  ve, and whose average stretch is at  most three, with routing tables of size O(      n) bits per node. In addition, the  route lengths are at most 2D (d1:5De for uniform weights) where D is the weighted  diameter of the network. The PIR strategy can be constructed in polynomial time  and can be implemented so that the generated scheme is in the form of an interval  routing scheme (IRS), using at most O(    n log n) intervals per link. Here it is shown  that there exists an unweighted n-node graph G and an identity assignment ID for its  nodes such that for every R 2 PIR1 on G with a set of pivots computed by a greedy  cover algorithm (respectively, a randomized algorithm), AvStrG (R) &gt; 3 o(1) (resp.,  with high probability). Also, it is shown that for almost every unweighted n-node  graph G, and for every R 2 PIR1 on G, AvStrG (R) = 1:875  o(1). A comparison  between PIR and HCP k , the hierarchical routing strategy presented in [3] is also  given.
We describe an architecture for efficient and accurate linear decomposition of an image into scale and orientation subbands. The basis functions of this decomposition  are directional derivative operators of any desired order. We describe the construction and implementation of the transform.
In this paper, we consider the problem of information multicast, namely transmitting common information from a sender s to a set of receivers T , in a communication network. Conventionally, in a communication network such as the Internet, this is done by distributing information over a multicast distribution tree. The nodes of such a tree are required only to replicate and forward, i.e., route, information received. Recently, Ahlswede et al. [1] demonstrated that it is in general suboptimal to restrict the network nodes to perform only routing. They show that the multicast capacity, which is defined as the maximum rate that a sender can communicate common information to a set of receivers, is given by the minimum C = min tffT C t of max-flows C t = maxflow(s, t) between the sender and each receiver. Moreover, they showed that while the multicast capacity cannot be achieved in general by routing, it can be achieved by network coding. Network coding refers to a scheme where coding is done at the interior nodes in the network, not only at the sender and receivers. Li, Yeung, and Cai [2] showed that it is suffcient for the encoding functions at the interior nodes to be linear. Koetter and Medard[3] gave an algebraic characterization of linear encoding schemes and proved existence of linear timeinvariant codes achieving the multicast capacity. Jaggi, Sanders, et al. [4][5][6] showed for acyclic networks how to find the encoding and decoding coeffcients in polynomial time. Chou, Wu, and Jain [7][8] proposed a distributed scheme for practical network coding in real packet networks achieving throughput close to capacity with low delay that is robust to random packet loss and delay as well as robust to any changes to network topology or capacity
This paper presents an unsupervised method to segment multispectral images, involving a correlated non-Gaussian noise. The effciency of the Markovian quadtree-based method we propose will be illustrated on a satellite image segmentation task with multispectral observations, in order to update nautical charts. The proposed method relies on a hierarchical Markovian modeling and includes the estimation of all involved parameters. The parameters of the prior model are automatically calibrated while the estimation of the noise parameters is solved by identifying generalized distribution mixtures [P. Rostaing, J.-N. Provost, C. Collet, Proc. International Workshop EMMCVPRff99: Energy Minimisation Methods in Computer Vision and Pattern Recognition, Springer Verlag, New York, 1999, p. 141], by means of an iterative conditional estimation (ICE) procedure. Generalized Gaussian (GG) distributions are considered to model various intensity distributions of the multispectral images. They are indeed well suited to a large variety of correlated multispectral data. Our segmentation method is applied to Satellite Pour lffObservation de la Terre (SPOT) remote multispectral images. Within each segmented region, a bathymetric inversion model is then estimated to recover the water depth map. Experiments on different real images have demonstrated the effciency of the whole process and the accuracy of the obtained results has been assessed using ground truth data. *  Corresponding author. Present address: Universite Louis Pasteur, ENSPS-LSIIT UMR CNRS 7005, Bd S. Brant, 67400 Illkirch, France. Fax: +33-3-90-24-43-42.
tained last year by optimizing the model to the Earth Simulator, improving the quasi-conservative semi-Lagrangian scheme and introducing a two-time level integration to the semi-Lagrangian scheme. Improvements of physical processes and adjustments of parameters in the parameterizations led to a better performance in simulating the Baiu front and tropical cyclones. Multi-year time integrations at TL959L60 (horizontal grid sizes of about 20 km with 60 layers) and 20 years integrations at T213L40 (horizontal grid sizes of about 60 km with 40 levels) were performed successfully, which demonstrated that stable long-term integrations are possible by the global model on the Earth Simulator.  As for the subproject 2, the cloud-resolving NHMs for the Earth Simulator were further optimized. To make long-term simulations of the NHMs, the spectral boundary coupling method was included and the physical processes were also improved. The 70-day simulations were performed in the East Asia areas (horiz
We present a feature-based technique for morphing 3D objects represented by light fields. Our technique enables morphing of imagebased objects whose geometry and surface properties are too difficult to model with traditional vision and graphics techniques. Light field morphing is not based on 3D reconstruction; instead it relies on ray correspondence, i.e., the correspondence between rays of the source and target light fields. We address two main issues in light field morphing: feature specification and visibility changes. For feature specification, we develop an intuitive and easy-to-use user interface (UI). The key to this UI is feature polygons, which are intuitively specified as 3D polygons and are used as a control mechanism for ray correspondence in the abstract 4D ray space. For handling visibility changes due to object shape changes, we introduce  ray-space warping. Ray-space warping can fill arbitrarily large holes caused by object shape changes; these holes are usually too large to be properly handled by traditional image warping. Our method can deal with non-Lambertian surfaces, including specular surfaces (with dense light fields). We demonstrate that light field morphing is an effective and easy-to-use technqiue that can generate convincing 3D morphing effects.
Analytical and numerical integration methods of the Biot-Savart law are presented  for the three-dimensional computation of the magnetic field generated by  stationary currents. For complex-shaped conductors a finite volume approximation  is proposed, based on a composite Gauss-Legendre quadrature on tetrahedral subdomains  of the conductors. For circular coils with rectangular cross section, a modified  semi-analytical Urankar&apos;s method is considered, expressed in terms of elementary  functions, Jacobian elliptic functions and complete/incomplete elliptic integrals of  the first, second and third kind. As alternative, numerical integration can be applied  to sequences of one-dimensional non-singular integrals in azimuthal coordinate.
This report is the first in which HMPV was the only pathogen identified from postmortem specimens from a patient with a fatal respiratory tract disease
We present FUSE observations of O VI  1032; 1038 emission from non-radiative and radiative shocks in the Cygnus Loop supernova remnant. The velocity pro  les of the lines and the spatial variation of their uxes can be used to study the distribution of the emitting components and the shock geometry. The ratio between the uxes of the two components can be used to evaluate the eects of both resonance scattering and absorption by material along the line of sight. The O VI emission varies systematically with the H behind smooth nonradiative shocks but the two are distributed very dierently in complex shock-cloud interactions. O VI traces gas at about 300,000 K, hotter than the optical  laments and cooler than the X-ray regions, and thus provides a crucial diagnostic for multi-wavelength studies of SNR shocks.
How should digital design be taught to Computing Science students in a single onesemester  course? This paper advocates the use of state of the art design tools and programmable  devices and presents a series of laboratory exercises to help students learn digital  logic. Each exercise introduces new concepts and produces the complete design of a standalone  apparatus that is fun and interesting to use. These exercises lead to the most challenging  capstone designs for a single semester course of which the authors are aware. Fast progress is  made possible by providing students with pre-designed input/output modules. Student feedback  demonstrates that the students approve this methodology. An extensive set of slides,  support teaching material, and lab exercises are freely available for downloading.
We have made long-slit spectrophotometric observations in the optical and near infrared of 15 H II regions in dierent spiral galaxies (NGC 628, NGC 925, NGC 1232 and NGC 1637). These spectrophotometric observations were performed with a wide spectral coverage and at a resolution high enough to detect and measure both weak auroral forbidden lines and Wolf-Rayet features. Electron temperatures have been derived in order to investigate the ionization structure and to derive the chemical composition of the gas in these regions. Therefore, we have selected from the literature (Van Zee et al. 1998) those H II regions with solar or oversolar abundance, as deduced from empirical calibrations based on the optical oxygen forbidden lines.
Digital fingerprinting is a technique for identifying users who might try to use multimedia content for unintended purposes, such as redistribution. These fingerprints are typically embedded into the content using watermarking techniques that are designed to be robust to a variety of attacks. A cost-effective attack against such digital fingerprints is collusion, where several differently marked copies of the same content are combined to disrupt the underlying fingerprints. In this paper, we investigate the problem of designing fingerprints that can withstand collusion and allow for the identification of colluders. We begin by introducing the collusion problem for additive embedding. We then study the effect that averaging collusion has upon orthogonal modulation. We introduce an effcient detection algorithm for identifying the fingerprints associated with K colluders that requires log(n/K)) correlations for a group of n users. We next develop a fingerprinting scheme based upon code modulation that does not require as many basis signals as orthogonal modulation. We propose a new class of codes, called anti-collusion codes (ACC), which have the property that the composition of any subset of K or fewer codevectors is unique. Using this property, we can therefore identify groups of K or fewer colluders. We present a construction of binary-valued ACC under the logical AND operation that uses the theory of combinatorial designs and is suitable for both the on-off keying and antipodal form of binary code modulation. In order to accommodate n users, our code construction requires only ff n) orthogonal signals for a given number of colluders. We introduce four different detection strategies that can be used with our ACC for identifying a suspect set of colluders. We demonstrate th...
The fundamental matrix is a basic tool in the analysis of scenes taken with two uncalibrated cameras, and the 8-point algoritm is a frequentffe citff3 metff9 d for computff10 t he fundament al ma tff ix from a set of 8 or more point mat ches. It hast he advant age of simplicit y of implement at ion. The prevailing view is, however,tff(9 it isextff3791( susceptff-43 t o noise and hence virtually useless for most purposes. This paper challengestffen view, by showing tffng by precedingt he algorit hm wit h a very simple normalizat ion(t ranslat  ion and scaling) oft he coordinat es oft he mat ched pointff( resultff are obtff ined comparable witff t he best itffff at ive algoritff209 This improved performance is justff690 bytff1082 and verified byextff259( e experiment  s on real images.
of temporarily and globally ambiguous sentences in Korean. The head-final and pro-drop nature of Korean can generate both temporary and global ambiguity. Two experiments were conducted to investigate the effect of prosody on these ambiguous strings. Results showed that prosody can influence the meaning recovered from ambiguous string, and can also influence the processing difficulty of a particular syntactic analysis.
According to Leibniz&apos; principle, two individuals a and b are indiscernible, if they share the same properties. Indiscernibility of objects provides a potential for optimization in deductive systems, and has e.g. been exploited in the area of active database systems. In this paper, we address the issue of indiscernibility in logic programs and outline possible benefits for computation. After a formal definition of the notion of indiscernibility, we investigate some basic properties. The main contribution is then an analysis of the computational cost of checking indiscernibility of individuals (i.e. constants) in logic programs without function symbols, which we pursue in detail for ground logic programs. For the concern of query optimization, they show that online computation of indiscernibility is expensive, and thus suggest to adopt an offline strategy, which may pay off for certain computational tasks.
A family of interpolating 3-point ternary subdivision schemes is shown to exist and have C   -continuity. A family of interpolating 4-point ternary subdivision schemes is shown to exist and have    -continuity. An approximating 3-point ternary scheme has been found and shown to have C   binary scheme is derived and shown to have C   continuity. The generating function formalism is used to analyze the continuity properties of these schemes. These are compared with the established schemes. 1.
The constraint language for lambda-structures (CLLS) allows for a simple, integrated, and underspecified treatment of scope, ellipses, anaphora, and their interaction. CLLS features constraints for dominance, lambda binding, parallelism, and anaphoric links. In the case of antecedent contained deletion (ACD), the definition of parallelism in the original version of CLLS is slightly too restrictive due to an overly weak notion of quantifier identity. We show how to extend CLLS with an appropriate notion of quantifier identity such that ACD can be naturally analysed. This sheds some light on conflicting requirements on quantifier representations as needed for ACD and Hirschbuhler sentences.
This paper provides an overview of challenges that security poses to testing and describes the role of testing in the engineering of trustworthy systems
We investigate the application of classification techniques to  the problem of information extraction (IE). In particular we  use support vector machines and several different feature-sets  to build a set of classifiers for information extraction. We  show that this approach is competitive with current state-ofthe  -art information extraction algorithms based on specialized  learning algorithms. We also introduce a new technique  for improving the recall of IE systems called convergent  boundary classification. We show that this can give significant  improvement in the performance of our IE system  and gives a system with both high precision and high recall.
We present the first randomized O(log n) time and O(m+n) work EREW PRAM algorithm for finding a  spanning forest of an undirected graph G = (V; E) with n vertices and m edges. Our algorithm is optimal  with respect to time, work and space. As a consequence we get optimal randomized EREW PRAM  algorithms for other basic connectivity problems such as finding a bipartite partition, finding bridges  and biconnected components, finding Euler tours in Eulerian graphs, finding an ear decomposition,  finding an open ear decomposition, finding a strong orientation, and finding an st-numbering.
We report the detection of long, thin H I tails emanating from molecular clouds in the Perseus Arm of the Galaxy. The hypothesis that they are driven by stellar winds from one or both of the massive stars HD 17603 and WR 5 places strong constraints on the stellar wind environment: we find that a heavily mass-loaded wind bubble is required.
The implementation of a RAKE receiver for a quasisynchronous DS-CDMA (direct sequence code division multiple access) system requires the estimation of the dominant path delays for each user. Presented here is a blind method to estimate these parameters. The algorithm takes advantage of various signal space invariances in the frequency domain to isolate the subspace of interest for each user, then uses ESPRIT on these subspaces to estimate the delays. The method processes each user independently of the others, is near-far resistant, and allows several delays per user. Simulations indicate a fair accuracy.
In this paper, we provide tools for convergence and performance analysis of an agreement  protocol for a network of integrator agents with directed information ow. Moreover, we analyze  algorithmic robustness of this consensus protocol for the case of a network with mobile  nodes and switching topology. We establish a connection between the Fiedler eigenvalue of  the graph Laplacian and the performance of this agreement protocol. We demostrate that a  class of directed graphs, called balanced graphs, have a crucial role in solving average-consensus  problems. Based on the properties of balanced graphs, a group disagreement function (i.e. Lyapunov  function) is proposed for convergence analysis of this agreement protocol for networks  with directed graphs. This group disagreement function is later used for convergence analysis  for the agreement problem in networks with switching topology. We provide simulation results  that are consistent with our theoretical results and demonstrate the eectiveness of the proposed  analytical tools.
In this paper a number of improvements are suggested that can be applied to most k-medoids-based algorithms. These can be divided into two categories - conceptual / algorithmic improvements, and implementational improvements. These include the revisiting of the accepted cases for swap comparison and the application of partial distance searching and previous medoid indexing to clustering. We propose extensions to the problem of nearest neighbor search, by combining the previous medoid index with triangular inequality elimination and partial distance searching. An improved k-medoids algorithm using simulated annealing, CLASA, is also discussed, as is a novel mechanism for managing memory usage. Various hybrids of these search approaches are then applied to a number of k-medoids-based algorithms and we show that the method is generally applicable. For example, experimental results based on various datasets, including both artificial and real datasets, demonstrate that when applied to  CLARANS the number of distance calculations can be reduced by up to 98% with similar average distance per object. Importantly, these search approaches can also be applied to nearest neighbor searching and other clustering algorithms.
One fundamental problem in predicting the subjective quality of a degraded video is that the perceived quality depends on the properties of the video itself, or the context of the degradation. In this paper, we present the results for a series of experiments designed to measure the detection thresholds and annoyance values of small regions of MPEG-2 artifacts inserted into mostly uncorrupted video sequences. In previous work, we found that the detection threshold contains much, but not all, of the information needed to remove the dependence of quality on the context. In this paper, we report the result of two experiments. In one experiment, we varied the type of MPEG-2 artifacts inserted into the test sequences. In the other experiment, we varied the location, size, and duration of the corrupted regions in the test sequences. From each set of data, we estimated detection thresholds and fitted the parameters of a quality function. The experimental results demonstrated that, under a wide set of test conditions, the detection threshold is still very useful for the estimation of quality as context varies. In fact, the detection threshold was the only factor necessary to model the changes in the quality function parameters with artifact type. The experimental data showed that the detection threshold and the quality function parameters do depend on the size and duration of the degraded region. However, the effects of size and duration are minor relative to the effect of artifact location.  Keywords:ff video, quality, MPEG-2, metrics, annoyance, appearance, duration, size, location  1.ff 
This paper studies the relative proof complexity of variations of a tableau method for Boolean  circuit satisfiability checking obtained by restricting the use of the cut rule in several natural ways. The  results show that the unrestricted cut rule can be exponentially more effective than any of the considered  restrictions. Moreover, there are exponential differences between the restricted versions, too. The  results also apply to the Davis-Putnam procedure for conjunctive normal form formulae obtained from  Boolean circuits with a standard linear size translation.
We present a novel approach to parameterised curve detection.
This paper proposes a novel method for video-based real time face recognition. The proposed method uses motion information to detect the face region, and the region is processed in    color space to determine the location of the eyes. The system extracts only the gray level features relative to the location of the eyes. Autoassociative Neural Network (AANN) model is used to capture the distribution of the extracted gray level features. Experimental results show that the proposed system gives an average recognition rate of	  75 in real time for 25 subjects. The performance of the proposed method is invariant to size, tilt of the face and is also not sensitive to natural lighting conditions.
Acyclic conjunctive queries form a polynomially evaluable fragment  of definite nonrecursive first-order Horn clauses. Labeled graphs, a special class  of relational structures, provide a natural way for representing chemical compounds.
This paper proposes a method to analyze Japanese anaphora, in which zero pronouns (omitted obligatory cases) are used to refer to preceding entities (antecedents). Unlike the case of general coreference resolution, zero pronouns have to be detected prior to resolution because they are not expressed in discourse. Our method integrates two probability parameters to perform zero pronoun detection and resolution in a single framework. The first parameter quantifies the degree to which a given case is a zero pronoun. The second parameter quantifies the degree to which a given entity is the antecedent for a detected zero pronoun. To compute these parameters effciently, we use corpora with/without annotations of anaphoric relations. We show the effectiveness of our method by way of experiments.
Software systems are often highly structured, consisting of artifacts: types, methods, variables, and packages; and relationships between these artifacts. Domain and meta models, and software design documentation provide additional artifacts such as roles, associations, use cases, and paragraphs of text. This paper outlines a tool for software structure understanding. The tool consists of a knowledge base containing software artifacts, relationships between artifacts, and rules for generating new relationships. The knowledge base is then explored using formal concept analysis (FCA). We shall refer to the method of exploring software structure via a knowledge base and FCA as Conceptual Analysis of Software Structure (CASS). Exploration of
This paper describes an algorithm for visualising rooted labeled trees on a lattice, described by Daida [1]. The method for generating points on the lattice is described and previous work is extended with a technique for visualising populations of trees in three dimensions and an accompanying measure for comparing populations based on their three dimensional structure, which encapsulates many key features of a population&apos;s structural properties.
This paper attempts to evaluate the performance of an environmental mitigation banking system operating under different regulatory. Pricing and subsidization policies using system dynamics modeling and computer simulation. Pricing of credits is an important aspect of the banking system and complex engineering methods connecting cost to price and market have been proposed as pricing criteria. Also, subsidization of the mitigation system by the government is often advocated by environmental groups. The analysis of this paper suggests that the market is able to yield an optimal price with or without inputs from engineering methods connecting price to cost. Also, the system operates best without yielding overshoot in infrastructure development when operated without any subsidies. The experimental process used to test the efficacy of the mitigation banking system is seen in general to be important to the design of social innovations direly needed for the smooth functioning of the modern day complex societal system.
this article, we have utilized a least-squares approximation to arrive at a more high level descriptor (described below).
nd or grout to the cores of the blocks simply increases the weight; the increase in STC can be  Controlling Sound Transmission through Concrete Block Walls Some Basic Concepts --- Transmission Loss (TL) and Sound Transmission Class (STC)  For significant noise reduction between two rooms, the wall (or floor) separating them must transmit only a small fraction of the sound energy that strikes it. The ratio of the sound energy striking the wall to the transmitted sound energy, expressed in decibels (dB), is called the transmission loss (TL). The less sound energy transmitted, the higher the transmission loss. In other words, the greater the TL, the better the wall is at reducing noise. Sound transmission class (STC) is a singlenumber rating that summarizes transmission loss data. It is obtained by fitting a standard reference contour to the data (see Figures 2 and 3). estimated from Figure 1. Adding soundabsorbing materials to the cores is not effective because the sound bypasses the ins
On-line, spatially localized information about internal network performance can greatly assist dynamic routing algorithms and traffic transmission protocols. However, it is impractical to measure network traffic at all points in the network. A promising alternative is to measure only at the edge of the network and infer internal behavior from these measurements. In this paper we concentrate on the estimation and localization of internal delays based on end-to-end delay measurements from sources to receivers. We develop an EM algorithm for computing MLEs of the internal delay distributions in cases where the network dynamics are stationary over the observation period. For time-varying cases, we propose a sequential Monte Carlo procedure capable of tracking non-stationary delay characteristics. Simulations are included to demonstrate the promise of these techniques.
Introduction  Many problems in machine learning require a data classification algorithm to work with a set of discrete objects. Common examples include biological sequence analysis where data is represented as strings (Durbin et al., 1998) and Natural Language Processing (NLP) where the data is given in the form of a string combined with a parse tree (Collins and Duffy, 2001) or an annotated sequence (Altun et al., 2003).  In order to apply kernel methods one defines a measure of similarity between discrete structures via a feature map ff : X    F. Here X is the set of discrete structures (eg. the set of all parse trees of a language) and F is a Hilbert space. Since ff(x)    F we can define a kernel by evaluating the scalar products  k(x, x ff ) =  ffff(x),  ff(x ff )ff (1.1) where x, x ff    X. The success of a kernel method employing k depends both on the faithful representation of discrete data and an effcient means of computing k.  Recent research effort has focussed on defining meaningful ker
The concept of replacing threads with flexible wires and sensors in a fabric to provide an underlying platform for integrating electronic components is known as e-textiles. This concept can be used to design applications involving different types of electronic components including sensors, digital signal processors, microcontrollers, color-changing fibers, and power sources. The adaptability of the textiles to the needs of the individual and the functionality of electronics can be integrated to provide unobtrusive, robust, and inexpensive clothing with novel features. This thesis focuses on the design of e-textiles for acoustic signal processing applications. This research examines challenges encountered when developing e-textile applications involving distributed arrays of microphones. A framework for designing such applications is presented. The design process and the performance analysis of two e-textiles, a large-scale beamforming fabric and a speech-processing vest, are presented.
An algorithm is presented for fitting an expression composed of continuous and discontinuous primitive functions to real-valued data points. The data modeling problem comes from the need to infer task structure for making coordination decisions for multi-agent systems. The presence of discontinuous primitive functions requires a novel approach.
In this note it is proved that if a graph G of order n has an irreducible covering  of its vertex set by n k cliques, then its clique number !(G)  k + 1 if k = 2 or 3  and !(G)       if k  4. These bounds are sharp if n  k + 1 (for k = 2 or 3)  and n  k +      (for k  4).
The Mouse Brain Web, a federated database, provides for the construction of anatomically correct models of mouse brain networks. Each web page in this database  provides the position, orientation, morphology, and putative synapses for each biologically observed neuron. The Mouse Brain Web has been designed to support (1) mapping of the spatial distribution and morphology of neurons by type; (2) wiring of the network -- synaptic assembly; (3) projection of neuron morphology and synapses to geometric multi-compartmental models; (4) search for motifs and canonical circuits in the brain networks using customized web-crawlers; and (5) the mapping of anatomically correct networks to physiologically correct network simulations.
In this article I review the dominant physical mechanisms in the extended emission line regions (EELR) of powerful radio galaxies at low redshifts and, in particular, the balance between jet-induced shocks and AGN photoionization. I consider the evidence for jet-induced shocks based on morphological, kinematical and ionization (diagnostic diagram) information. Although each of these three types of information separately provides some evidence for the eects of jet-induced shocks, this evidence is often ambiguous. The major advance in recent years has been to combine morphological, kinematical and diagnostic diagram approaches. In this way it has been possible to show that jet-induced shocks dominate the ionization and acceleration of the EELR along the radio axes of many powerful radio galaxies. However, AGN photoionization is likely to remain the dominant mechanism in the nuclear regions of radio galaxies in which the radio sources extend well beyond the emission line regions.
Recognizing speech, gestures, and visual features are important interface capabilities for embedded mobile systems. Perception algorithms have many traits in common with more conventional media processing applications. The primary motivation for this work is that applications such as real-time, speaker-independent, large-vocabulary, domain-independent continuous speech recognition systems require more performance than is currently available on embedded processors. Even on modern highperformance processors the performance is just barely able to keep up with real-time demands while consuming power at a rate that is well beyond what can be sustained on mobile systems. The solution to this dilemma has traditionally been to design a special ASIC. ASIC design however is both expensive and lacks the generality needed to support different phases of a complex algorithm or even evolutionary improvements to base method. This paper introduces an execution cluster based coprocessor architecture and its CMOS implementation. This is compared against software implementations of algorithms running on a general purpose processor and also against custom ASICs. The cluster achieves an order of magnitude improvement in energy consumption over a conventional processor while retaining a reasonable level of generality. The architecture is evaluated on several important perception applications where energy consumption is shown to improve by a factor of 12-55 times and energy-delay product improves by a factor of 3.8 - 40 times over conventional processor approaches.
. Because computation speed and memory size are both increasing, the latency of memory, in basic machine cycles, is also increasing. As a result, recent compiler research has focused on reducing the effective latency by restructuring programs to take more advantage of high-speed intermediate memory (or cache, as it is usually called). The problem is that many real-world programs are non-trivial to restructure, and current methods will often fail. In this paper, we present some encouraging preliminary results of a project to determine how much restructuring is possible with automatic techniques. 1. Introduction. Over the past decade we have seen dramatic reductions in the cycle times of microprocessors, while memories for the same processors have been growing in size. These two trends have yielded computer systems in which memory latency is quite large in terms of basic machine cycles---latencies of 10 to 20 cycles are not unusual. To address this problem, system designers have incorpor...
Roberts, Massoulié and co-authors have introduced and studied a flow level model of Internet congestion control, that represents the randomly varying number of flows present in a network where bandwidth is dynamically shared between elastic file transfers. In this paper we consider a generalization of the model to include streaming traffic as well as file transfers, under a fairness assumption that includes TCP-friendliness as a special case. We establish stability, under conditions, for a fluid model of the system. We also assess the impact of each traffic type on the other: file transfers are seen by streaming traffic as reducing the available capacity, whereas for file transfers the presence of streaming traffic amounts to replacing sharp capacity constraints by relaxed constraints. The integration of streaming traffic and file transfers has a stabilizing effect on the variability of the number of flows present in the system.
The limitations of the deterministic formulation of scheduling are outlined  and a probabilistic approach is motivated. A number of models are reviewed  with one being chosen as a basic framework. Response-time analysis is extended  to incorporate a probabilistic characterisation of task arrivals and execution times.
We introduce a new class of dithering methods called N-candidate methods. The main idea is that the output color is randomly chosen among several candidate colors so that the estimated color average would be preserved. The dithering process is pixelwise without any interaction with the neighboring pixels. The N-candidate methods are thus location invariant, which has two benefits: (1) the algorithm can be fully parallelized; and (2) the image can be partially processed without effecting the pixels outside the processed part. The proposed approach allows more efficient dithering than error diffusion but at the cost of a slightly lower image quality. 
Under normal viewing conditions, humans find it easy to distinguish between objects made out of different materials such as plastic, metal, or paper. Untextured materials such as these have different surface reflectance properties, including lightness and gloss. With single isolated images and unknown illumination conditions, the task of estimating surface reflectance is highly underconstrained, because many combinations of reflection and illumination are consistent with a given image. In order to work out how humans estimate surface reflectance properties, we asked subjects to match the appearance of isolated spheres taken out of their original contexts. We found that subjects were able to perform the task accurately and reliably without contextual information to specify the illumination. The spheres were rendered under a variety of artificial illuminations, such as a single point light source, and a number of photographically-captured real-world illuminations from both indoor and outdoor scenes. Subjects performed more accurately for stimuli viewed under real-world patterns of illumination than under artificial illuminations, suggesting that subjects use stored assumptions about the regularities of real-world illuminations to solve the ill-posed problem.
We develop a stochastic model of a simple protocol for the self-configuration of IP network interfaces. We describe the mean cost that incurs during a selfconfiguration phase and describe a trade-off between reliability and speed. We derive a cost function which we use to derive optimal parameters. We show that optimal cost and optimal reliability are qualities that cannot be achieved at the same time.
This paper presents a floorplan for designing FIR filter arrays, using enhanced Fermat ALUs, complementing previous work done in the VLSI Group. The structure is based on polynomial mapping of the Modulus Replication RNS, with computational modulus 257. It exploits the redundancy in the input representation, thus reducing the coefficient growth due to the polynomial multiplication, leading to a smaller probability of overflow error. The silicon area overhead for the input/output mappings is less than 10%. This a great reduction compared to conventional and recent RNS designs of inner product processor array for DSP applications. A 0.35 m process implementation of a 53 tap filter is detailed.
We show that if the codimension one Anosov flow \Phi on a compact n-manifold M  satisfies the so called condition (L), then there is a continuous Lyapunov function  g : R  n  ! R, where R  n  is the universal covering space of M , such that g strictly increases along the orbits of the lift of \Phi and is constant on the leaves of the lift of the strong stable foliation of the &quot;synchronization&quot; (i.e. suitable reparametrization) of \Phi. We also give a sufficient condition in terms of g for \Phi to admit a global cross section. 
The paper presents a collection of 93 different bugs, detected in formal verification of 65 student designs that include: 1) singleissue pipelined DLX processors; 2) extensions with exceptions and branch prediction; and 3) dual-issue superscalar implementations. The processors were described in a high-level HDL, and were formally verified with an automatic tool flow. The bugs are analyzed and classified, and can be used in research on microprocessor testing.
We discuss a difficult optimization problem on a chess-board,  requiring equal numbers of black and white queens to be placed on the  board so that the white queens cannot attack the black queens. We show  how the symmetry of the problem can be straightforwardly eliminated  using SBDS, allowing a set of non-isomorphic optimal solutions to be  found. We present three different ways of modelling the problem in constraint  programming, starting from a basic model. An improvement on  this model reduces the number of constraints in the problem by introducing  ancillary variables representing the lines on the board. The third  model is based on the insight that only the white queens need be placed,  so long as there are sufficient unattacked squares to accommodate the  black queens. We also discuss variable ordering heuristics: we present a  heuristic which finds optimal solutions very quickly but is poor at proving  optimality, and the opposite heuristic for which the reverse is true.
Architecture&quot;, then &quot;component&quot;, became buzzwords in the last decade. The precise meanings of these terms have been evolving over time, and vary among different research communities. Traditionally the reengineering community has focused on recovering the architecture of unstructured or modular software. Recently, significant amount of work has been dedicated to the integration of the reengineering and object-oriented worlds. In this paper we claim that the next step could be the integration of reengineering and component-based development. Our collaboration with Dassault Systmes, the world leader in the CAD/CAM market, shows that time has come to investigate this issue.
The model studied concerns a simple first-order hyperbolic system. The solutions in which one is most interested have discontinuities which persist for all time, and therefore need to be interpreted as weak solutions. We demonstrate existence and uniqueness for such weak solutions, identifying a canonical `exact&apos; solution which is everywhere defined. The direct method used is guided by the theory of measure-valued diffusions. The method is more effective than the method of characteristics, and has the advantage that it leads immediately to the McKean representation without recourse to Ito&apos;s formula. We then conduct computer studies of our model, both by integration schemes (which do use characteristics) and by `random simulation&apos;.
In recent years, disk-based approaches to the analysis of Markov models have proved to be an effective method of combating the state space explosion problem. Coupled with parallel and symbolic techniques, disk-based methods have demonstrated impressive performance for numerical solution. In an earlier paper, we presented a novel, symbolic out-of-core algorithm which used MTBDD-based data structures for matrix storage in RAM and disk-based storage for solution vectors. This extended the size of models...
The bulk of the research on Data Warehousing is concentrated on logical and  physical issues, since they mainly determine the system performance. As to relational Data  Warehouse, the logical issues concern the definition of the relational structure of the DW and  those optimizations that can be carried out by tuning such structures, while physical issues  mainly deal with the index selection problem. The research carried out in the last ten years  produced a lot of interesting results that are essential to reduce the query response time. The  scope of this paper is to survey the main results in the literature as a base for the research in the  ongoing project.
unication that establishes a framework for multimodal &quot;language&quot; and &quot;dialog&quot;, much like the framework we have evolved for spoken exchange.  Another important aspect is the development of Human-Centered Information Systems. The most important issue here is how to achieve synergism between man and machine. The term &quot;Human-Centered&quot; is used to emphasize the fact that although all existing information systems were designed with human users in mind, many of them are far from being user friendly. What can the scientific/engineering community do to effect a change for the better?  Information systems are ubiquitous in all human endeavors including scientific, medical, military, transportation, and consumer. Individual users use them for learning, searching for information (including data mining), doing research (including visual computing), and authoring. Multiple users (groups of users, and groups of groups of users) use them for communication and collaboration. And either single or multipl
We give the  rst polynomial time algorithm to learn any function of a constant number  of halfspaces under the uniform distribution to within any constant error parameter. We also  give the  rst quasipolynomial time algorithm for learning any function of a polylog number of  polynomial-weight halfspaces under any distribution. As special cases of these results we obtain  algorithms for learning intersections and thresholds of halfspaces. Our uniform distribution  learning algorithms involve a novel non-geometric approach to learning halfspaces; we use Fourier  techniques together with a careful analysis of the noise sensitivity of functions of halfspaces. Our  algorithms for learning under any distribution use techniques from real approximation theory  to construct low degree polynomial threshold functions.
This paper discusses the representation and implementation of the  Observer design pattern using aspect-oriented techniques.
We study the problem of finding a Fourier representation R of B terms for a given discrete signal A of length N . The Fast Fourier Transform (FFT) can find the optimal N-term representation in O(N log N)  time, but our goal is to get sublinear algorithms for B ! N , typically, B  N . Suppose kAk2  M kRoptk 2 , where Ropt is the optimal output. The previously best known algorithms output R such that kA \Gamma Rk    poly(B; log(1=ffi); log N; log M; 1=ffl): Even though this is sublinear in the input size, the dominating term is the polynomial factor in B which is B   . In our experience, this is a limitation in practice. Our main result is a significantly improved algorithm for this problem. Our algorithms output R  such that kA \Gamma Rk    B \Delta poly(log(1=ffi); log N; log M; 1=ffl): We also obtain improvements for higher dimensional Fourier transforms. We need two crucial ideas to achieve this bound: bulk sampling and estimation for multipoint polynomial evaluation using an unevenly-spaced Fourier tranform, and construction and use of arithmeticprogression independent random variables. Our improved algorithms are likely to find many applications. 1 
We present a Switching Kalman Filter Model for the realtime inference of hand kinematics from a population of motor cortical neurons. Firing rates are modeled as a Gaussian mixture where the mean of each Gaussian component is a linear function of hand kinematics. A &quot;hidden state&quot; models the probability of each mixture component and evolves over time in a Markov chain. The model generalizes previous encoding and decoding methods, addresses the non-Gaussian nature of firing rates, and can cope with crudely sorted neural data common in on-line prosthetic applications.
This paper describes a method for risk analysis based on the approach used in CRAMM, but instead of using discrete measures for threats and vulnerabilities and lookup tables to derive levels of risk, it uses subjective beliefs about threats and vulnerabilities as input parameters, and uses the belief calculus of subjective logic to combine them. Belief calculus has the advantage that uncertainty about threat and vulnerability estimates can be taken into consideration, and thereby reflecting more realistically the nature of such estimates. As a result, the computed risk assessments will better reflect the real uncertainties associated with those risks.
Active network technology envisions deployment of virtual execution environments within network elements, such as switches and routers. As a result, inhomogeneous processing can be applied to network traffic. To use such technology safely and efficiently, individual nodes must provide mechanisms to enforce resource limits. This implies that each node must understand the varying resource requirements for specific network traffic. This paper presents an approach to model the CPU time requirements of active applications in a form that can be interpreted among heterogeneous nodes. Further, the paper demonstrates how this approach can be used successfully to control resources consumed at an active-network node and to predict load among nodes in an active network, when integrated within the Active Virtual Network Management Prediction system.  1. 
We survey a set of algorithmic techniques that make it possible  to build a high performance storage server from a network of cheap  components. Such a storage server oers a very simple programming  model. To the clients it looks like a single very large disk that can handle  many requests in parallel with minimal interference between the requests.
this paper I analyse how landscape structure and dynamics over the last thirty years have affect- ed carabid beetles in hedgerows
Development and projections in the field of PDAs and wireless communication architectures let us consider new applications which exploit short range and direct exchanges. In a close future we may imagine mobile users equipped with wireless PDAs dynamically and spontaneously  exchanging rich information in an ad hoc manner. In such a context, a connection between two PDAs may be broken at any time because of the short communication range and the unconstrained mobility of the users. Thus, such spontaneous data exchanges become a real challenge. To perform spontaneous and relevant transmissions compatible with these volatile connections, we propose a method that progressively downloads information according to its importance. Then, we show an implementation of such a mechanism in a Web context by extending the actual browsers capabilities.
The rapid e-commerce growth has made both business community and customers face a new situation. Due to intense competition on one hand and the customer&apos;s option to choose from several alternatives business community has realized the necessity of intelligent marketing strategies and relationship management. Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, creating adaptive Web sites, business and support services, personalization, network traffic flow analysis and so on. The study of ant colonies behavior and their self-organizing capabilities is of interest to knowledge retrieval/management and decision support systems sciences, because it provides models of distributed adaptive organization, which are useful to solve difficult optimization, classification, and distributed control problems, among others [17][18][16]. In this paper, we propose an ant clustering algorithm to discover Web usage patterns (data clusters) and a linear genetic programming approach to analyze the visitor trends. Empirical results clearly shows that ant colony clustering performs well when compared to a selforganizing map (for clustering Web usage patterns) even though the performance accuracy is not that efficient when comparared to evolutionary-fuzzy clustering (iminer) [1] approach.
The category of locally compact locales over any elementary topos is characterised by means of the axioms of abstract Stone duality (monadicity of the topology, considered as a selfadjoint exponential ff   , and Scott continuity, Fff =  ffff.
The idea described in this paper is to use the built-in cameras of consumer mobile phones as sensors for 2-dimensional visual codes. Such codes can be attached to physical objects in order to retrieve object-related information and functionality. They are also suitable for display on electronic screens. The proposed visual code system allows the simultaneous detection of multiple codes, introduces a position-independent coordinate system, and provides the phone&apos;s orientation as a parameter. The ability to detect objects in the user&apos;s vicinity offers a natural way of interaction and strengthens the role of mobile phones in a large number of application scenarios. We describe the hardware requirements, the design of a suitable visual code, a lightweight recognition algorithm, and present some example applications.
Baryshnikov [3] and Gravner, Tracy &amp; Widom [14] have shown that the largest eigenvalue of a random matrix of the G.U.E. of order d has the same distribution as  1fft1  fffftd-1ff0          +W d (t d-1 )] , where W = (W 1 ,     , W d ) is a d-dimensional Brownian motion. We provide a generalization of this formula to all the eigenvalues and give a geometric interpretation. For any Weyl chamber a + of an Euclidean finite-dimensional space a, we define a natural continuous path transformation   which associates to a path w in a a path   w in a+ . This transformation occurs in the description of the asymptotic behaviour of some deterministic dynamical systems on the symmetric space G/K where G is the complex group with chamber a + . When a = R   , a + =       , x d ); x 1 &gt; x 2 &gt;     &gt; x d   if W is the Euclidean Brownian motion on a then   W is the process of the eigenvalues of the Dyson Brownian motion on the set of Hermitian matrices and (T W )(1) is distributed as the eigenvalues of the G.U.E.
This paper develops a local controllability result for Multiple Model Driftless Affine (MMDA) control systems. The controllability result can be interpreted as a non-smooth extension of Chow&apos;s theorem, and uses a set-valued Lie Bracket. These results are interpreted in terms of an illustrative example involving an overconstrained wheeled vehicle.
We present a generative model and its associated stochastic filtering algorithm for simultaneous tracking of 3D position and orientation, non-rigid motion, object texture, and background texture. The model defines a stochastic process that belongs to the class of conditionally Gaussian processes [1]. This allows partitioning the filtering problem into two components: a linear component for texture that is solved using a bank of Kalman filters with time-varying parameters, and a nonlinear component for pose (rigid and non-rigid motion parameters) whose solution depends on the states of the Kalman filters. When applied to the 3D tracking problem, this results in an inference algorithm from which existing optic flow-based tracking algorithms and tracking algorithms based on texture templates emerge as special cases. Flow-based tracking emerges when the pose of the object is certain but its appearance is uncertain. Template-based tracking emerges when the position of the object is uncertain but its texture is relatively certain. In practice, optimal inference under this model integrates optic flow-based and template-based tracking, dynamically weighting their relative importance as new images are presented.
We present an algorithm for the verification of multiagent systems specified by means of a modal logic that includes a temporal, an epistemic, and a deontic operator. Verification is performed by model checking on OBDD&apos;s. We present an implementation of the algorithm and report on experimental results for the bit transmission problem with faults.
In many industrial applications inversion of the Laplace transform is necessary to reconstruct a searched for object function. In X-ray diractometry, a method from non-destructive testing, we want to recover the stress tensor of a specimen with the help of X-ray measurements. The mathematical model is described by the Laplace transform of the stress tensor at  nitely many scanning points that are not equally distributed. In this article we present an inversion method for the Laplace transform using a non-equispaced sampling grid that applies the approximate inverse to this transform. The approximate inverse is a regularization technique for inverse problems based on evaluations of the given data with so called reconstruction kernels. Each kernel solves a system of linear equations de  ned by the adjoint of the Laplace transform and dilation invariant molli  ers, which are designed particularly for this operator. An error estimate is given and point-wise convergence of the approximate inverse to the exact solution is proved. The paper ends with some numerical results.
The World Wide Web is increasingly becoming the preferred repository of  information. The strength of this information infrastructure is also its weakness.
this paper. Speci  cally, it is shown that the singular value decomposition of the transformation matrix between the power and Bernstein bases is important in determining both the numerical condition of the transform between the bases and the number of roots of a polynomial that lie in the interval I = fx : 0  x  1g
Double R Model (Referential and Relational Model) is a model of language comprehension  intended for use in the development of software agents with NLP capabilities. Double R Model is fairly unique in  adopting a cognitively plausible approach to modeling language comprehension, while at the same time attempting  to support the development of large-scale, functional models. The key claim is that adhering to well-established  cognitive constraints on language comprehension serves to prune the search space for possible solutions and may  actually facilitate the development of functional NLP systems.
One of the most fundamental tasks any automatic parallelization and optimization tool is confronted with is to find an optimal domain decomposition for an application at hand. For regular domain problems (such as simple matrix manipulations) this task may seem trivial. However, communication costs in message passing programs often significantly depend on the capabilities and particular behavior of the applied communication primitives. As a consequence, straightforward domain decompositions may deliver non-optimal performance.
This paper considers communication over coherent multiple-input multiple-output (MIMO)  flat fading channels where the channel is only known at the receiver. For this setting, we introduce  the class of LAttice Space-Time (LAST) codes. We show that these codes achieve  the optimal diversity-vs-multiplexing tradeoff defined by Zheng and Tse under generalized  minimum Euclidean distance lattice decoding. Our scheme is based on a generalization of  Erez and Zamir mod-ff scheme to the MIMO case. In our construction the scalar &quot;scaling&quot;  of Erez-Zamir and Costa Gaussian &quot;Dirty-Paper&quot; schemes is replaced by the minimum  mean square error generalized decision-feedback equalizer (MMSE-GDFE). This result  settles the open problem posed by Zheng and Tse on the construction of explicit coding and  decoding schemes that achieve the optimal diversity-vs-multiplexing tradeoff. Moreover,  our results shed more light on the structure of optimal coding/decoding techniques in delay  limited MIMO channels, and hence, opens the door for novel approaches for space-time  code constructions. In particular; 1) we show that MMSE-GDFE plays a fundamental role  in approaching the limits of delay limited MIMO channels in the high SNR regime, unlike  the AWGN channel case and 2) our random coding arguments represent a major departure  from traditional space-time code designs based on the rank and/or mutual information  design criteria.
A set of second order statistics collectively called normalized mean square covariance (NMSV) is defined to characterize the frequency and/or the time selectivity of wide-sense stationary uncorrelated scattering (WSSUS) channels. Normalized frequency mean square covariance (NFMSV) quantifies the frequency selectivity, while normalized time mean square covariance (NTMSV) characterizes the time selectivity. Normalized frequency-time mean square covariance is defined to characterize the combined effect of frequency and time selectivities. The NMSV&apos;s of a WSSUS channel can easily be computed from the scattering function. We show that there is a very close relationship between the NMSV of a WSSUS channel and the performance of various diversity combing scheme. Also we discuss, with practical system design problems, how useful the parameters are for efficient system design.
  We consider the problem of characterizing whether a Coons map is a diffeomorphism from the unit square onto a planar domain delineated by four given curves. We aim primarily at having not only theoretically correct conditions but also practically efficient methods. Throughout the paper we suppose that the given four boundary curves are presented in Bezier forms. We will prove three sufficient conditions: the first one is based upon the tangents of the boundary curves, the second one exploits the representation of the Jacobian in Bezier surface with a degree elevation when relevant, and the last one invokes the subdivision and polar forms techniques. Further, we will prove that the last condition is also necessary for sufficiently many subdivisions. We present a way of adaptive subdivision so as to make it efficient. Numerical results are reported in order to illustrate the approaches.
This article  describes the overall approach to SOE-driven automation that was demonstrated,  identifies gaps in SOE definitions and project profiles that hamper automation, and  provides detailed measurements of the knowledge engineering effort required for  automation
hymns, poetry or the libretto of an opera. A plausible point of departure for writing  melodies for a text is to study the prosody of the text as spoken either naturally or  dramatically. In this paper we introduce PROSE: a system for aiding such study. The  system extracts the prosody of a spoken signal and (re)synthesises it at various  resolutions. The main advantages of using PROSE over simply listening to the spoken  signals are that composers can focus on prosodic auditory information detached from  the meaning of the text and can assess this information at various resolutions. Also, the  analysis data can be plotted for visual assessment and/or mapped onto musical  parameters.
For selling spectrum licenses economists have designed new auction types proceeding over several rounds and offering several licenses simultaneously. Communication between bidders usually is forbidden to prevent collusions (i.e., through separate compartments and supervision). We investigate these auctions from the cryptographic point of view and identify that the usual implementation by a succession of (traditional) sealed-bid auctions where the auctioneer announces at least winner and winning bid of each round offers a covert channel to the bidders. The announcement should be limited to the minimum a bidder needs to know for taking part in the next round. We suggest that the bids made are kept private and she only gets to know which items she currently wins. Only at the end, overall winners and winning bids are revealed. We present a protocol based on a special sealed-bid auction that implements this idea.
This paper proposes a practical approach employing n-gram models and error-correction rules for Thai key prediction and Thai-English language identification. The paper also proposes rule-reduction algorithm applying mutual information to reduce the error-correction rules. Our algorithm reported more than 99% accuracy in both language identification and key prediction.
In this paper I present an automatic technique for transforming a program by changing the data types in that program to ones which are more appropriate for the task. Programs are synthesised by proving modified synthesis theorems in the proofs-as-programs paradigm. The transformation can be verified in the logic of type theory. Transformations are motivated by...
In this report, we consider the local convergence properties of the computational algorithms for the elementwise weighted total least squares (EW-TLS) estimator, proposed in [MRP   estimator and linear local convergence of the algorithms are proven. The results are stated in a stochastic framework and are asymptotic in nature.  
In this paper we will present a novel approach of using surface patches for Image Based Rendering. Based on image sequences acquired with a freely moving portable multi-camera-rig we can extrapolate novel views of complex real scenes in realtime. The cameras are calibrated from the image sequence itself and dense depth maps are computed for each camera view using an improved multiview depth estimation technique. The depth maps are then approximated with quad surface patches for efficient rendering.
Cost and efficiency concerns can force distributed embedded systems to use a  single network for both critical and non-critical messages. Such designs must protect  against masquerading faults caused by defects in and failures of non-critical network  processes. Cyclic Redundancy Codes (CRCs) offer protection against random bit errors  caused by environmental interference and some hardware faults, but typically do not defend  against most design defects. A way to protect against such arbitrary, non-malicious faults is  to make critical messages cryptographically secure. An alternative to expensive,  full-strength cryptographic security is the use of lightweight digital signatures based on  CRCs for critical processes. Both symmetric and asymmetric key digital signatures based  on CRCs form parts of the cost/performance tradeoff space to improve critical message  integrity.
CLaRK is an XML-based software system  for corpora development. It incorporates  several technologies: XML  technology; Unicode; Regular Cascaded  Grammars; Constraints over XML Documents.
We show that the electroweak symmetry can be broken in a natural  and phenomenologically acceptable way by a neutrino condensate.
A problem of great interest in the control of hybrid systems is the design of least restrictive controllers for reachability specifications. Controller design typically uses game theoretic methods to compute the region of the state space for which there exists a control such that for all disturbances, an unsafe set is not reached. In general, the computation of the controllers requires the steady state solution of a Hamilton-Jacobi partial differential equation which is very diffcult to compute, if it exists. In this paper, we show that for special classes of hybrid systems where the continuous vector fields are linear, the controller synthesis problem is semi-decidable: There exists a computational algorithm which, if it terminates in a finite number of steps, will exactly compute the least restrictive controller. This result is achieved by a very interesting interaction of results from mathematical logic and optimal control.
The robustness of the Internet relies heavily on the robustness of BGP routing. BGP is the glue that holds the Internet together: it is the common language of the routers that interconnect networks or Autonomous Systems(AS). The robustness of BGP and our ability to manage it effectively is hampered by the limited global knowledge and lack of coordination between Autonomous Systems. One of the few efforts to develop a globally analyzable and secure Internet is the creation of the Internet Routing Registries (IRRs). IRRs provide a voluntary detailed repository of BGP policy information. The IRR effort has not reached its full potential because of two reasons: a) extracting useful information is far from trivial, and b) its accuracy of the data is uncertain. In this
The identification of objects in natural images and the semantic interpretation of the image is an important topic of research. For a collection of images, image analysis typically involves a number of steps that employ predetermined tools. The choice of image segmentation and texture analysis algorithms and their parameters for processing more than one image is based on the a priori experience of the researcher and what in their opinion seems to work the best for a given application. It  has been recently shown that the optimisation of image processing tools on a per image basis is likely to lead to high quality classification results for object identification. In this paper we develop a `bank of classifiers&apos; approach to image object recognition and evaluate both selective and classifier combination approaches against a baseline approach that works with a single classifier.
This paper focuses on advanced software research This paper focuses on advanced software research This paper focuses on advanced software research This paper focuses on advanced software research directions which are important to overcome a lot of bottlenecks directions which are important to overcome a lot of bottlenecks directions which are important to overcome a lot of bottlenecks directions which are important to overcome a lot of bottlenecks already present in distributed heterogeneous environments. Special already present in distributed heterogeneous environments. Special already present in distributed heterogeneous environments. Special already present in distributed heterogeneous environments. Special attention is given to remote programming attention is given to remote programming attention is given to remote programming attention is given to remote programming, a new communication , a new communication , a new communication , a new communication paradigm based on mobile intelligent agents and active networks as a paradigm based on mobile intelligent agents and active networks as a paradigm based on mobile intelligent agents and active networks as a paradigm based on mobile intelligent agents and active networks as a building blocks of future client/server environments. building blocks of future client/server environments. building blocks of future client/server environments. building blocks of future client/server environments.
I tackle the problem of naming and sharing resources across administrative boundaries. Conventional systems manifest the hierarchy of typical administrative structure in the structure of their own mechanism. While natural for communication that follows hierarchical patterns, such systems interfere with naming and sharing that cross administrative boundaries, and therefore cause headaches for both users and administrators. I propose to organize resource naming and security, not around administrative domains, but around the sharing patterns of users. The dissertation is organized...
We present &apos;malicious  insider attacks&apos; on chip-card personalization  processes and suggest  an improved way to securely generate  secret-keys shared between  an issuer and the user&apos;s smart  card. Our procedure which results  in a situation where even the  card manufacturer producing the  card cannot determine the value  of the secret-keys that he personalizes  into the card, uses public  key techniques to provide integrity  and privacy of the generated  keys with respect to the  complete initialisation chain. Our  solution, which provides a noninteractive  alternative to authenticated  key agreement protocols,  achieves provable security in the  random oracle model under standard  complexity assumptions. Our  mechanism also features a certain  genericity and, when coupled  to a cryptosystem with fast encryption  like RSA, allows low-cost  intrusion-secure secret key generation.
Measurements from polarimetric radar systems are often represented in the form of a pair of polarization response curves, one showing the co-polarized response, the other the cross-polarized. This representation is effectively a latitude--longitude plot of the Poincare sphere. Despite its familiarity in the literature, this presentation of the polarimetric response fails to present optimally the data in an intuitive and straightforward manner. As an alternative, this work describes a polar projection that presents the data in a manner that is more effective at communicating the key aspects of the polarization response. The key advantages are that the orientation and helicity are immediately apparent, and that the equal area projection does not bias the visual emphasis towards circular polarizations.
The application range of memory-based collaborative filtering (CF) is limited due to CF&apos;s high memory consumption and long runtime. The approach presented in this paper removes redundant and inconsistent instances (users) from the data. Our work shows that a satisfactory accuracy can be achieved by using only a small portion of the original data set, thereby alleviating the storage and runtime cost of the CF algorithm. In our approach, we consider instance selection as the problem of selecting informative data that increase the a posteriori probability of the optimal model. We evaluate the empirical performance of our approach on two realworld data sets and attain very promising results. Data size and prediction time are significantly reduced, while the prediction accuracy is on a par with results achieved by using the complete database.
Market research companies predict a huge market  for services to be delivered to mobile users. Services
The task of named entity annotation of unseen  text has recently been successfully automated  with near-human performance.
We describe the XenoSearch system for performing expressive resource discovery searches in a distributed environment. We represent server meta-data, such as their locations and facilities, as points in a multi-dimensional space and then express queries as predicates over these points. Each XenoSearch node holds a portion of this space and the key goal of XenoSearch is to direct queries to those nodes containing the meta-data of matching XenoServers. Communication between these XenoSearch nodes is based on the self-organizing Pastry peer-to-peer routing substrate. Our initial performance evaluation on a wide-area prototype shows that queries are only a factor of 3-5 times longer than basic Pastry routing, while supporting multi-dimensional searches of arbitrary shapes.
We formalize a potentially rich new streaming model, the semi-streaming model, that we believe is necessary for the fruitful study of efficient algorithms for solving problems on massive graphs whose edge sets cannot be stored in memory. In this model, the input graph, G = (V, E), is presented as a stream of edges (in adversarial order), and the storage space of an algorithm is bounded by O(n &amp;middot; polylog n), where n = |V|. We are particularly interested in algorithms that use only one pass over the input, but, for problems where this is provably insufficient, we also look at algorithms using constant or, in some cases, logarithmically many passes. In the course of this general study, we give semi-streaming constant approximation algorithms for the unweighted and weighted matching problems, along with a further algorithm improvement for the bipartite case. We also exhibit log n/log log n semi-streaming approximations to the diameter and the problem of computing the distance between specified vertices in a weighted graph. These are complemented by &amp;Omega;(log^(1-e) n) lower bounds.
We present a parallel computational method for the QR decomposition with column pivoting of a sparse matrix by means of Modified Gram-Schmidt orthogonalization. Nonzero elements of the matrix M to be decomposed are stored in a one-dimensional doubly linked list data structure. We discuse a strategy to reduce fill-in in order to get memory savings and decrease the computation times. As an application of QR decomposition, we describe the least squares problem. This algorithm was designed for a message passing multiprocessor and has been evaluated on a Cray T3D, using the Harwell-Boeing sparse matrix collection.
The design of modern embedded systems require automated modeling tools for faster design and for the study of various design tradeoffs. Such tools put together constitute an integrated environment where the designer can write the high level design specifications in a language and use these tools for automatic generation of system specific tools. In this work we have designed a Retargetable Functional Simulator (Fsimg) for our integrated environment where the Sim-nML language is used as a base language for writing processor models. Sim-nML is an extension of nML machine description formalism and is powerful enough to describe a processor at instruction level. The
The need to understand how temporal factors shape work and interaction has been acknowledged. This position paper presents the notion of temporal tension: time is seen as the subjectively felt tension between the task deadline and the current speed or pace of work. Five tension of time are distinguished: acceleration, balance, hurrying, deceleration, and waiting---all associated with different cognitive and social factors relevant to interaction. We discuss how these temporal tensions could and have been taken into account in the design of interactive systems.
This paper reviews some of the history of automated visual surveillance, from the second and third generation VMD days of the early 1990s, to the current state of the art. It discusses the inherent limitations that resulted in an nearly negligible &quot;increase&quot; in performance throughout the 1990s and still exist in commercially available systems. Then we review an approach that overcomes these limitations -- active visual surveillance with geo-spatial rules.
This paper is about a variant of k nearest neighbor classification on large many-class high dimensional datasets.
Introduction. Ron Book&apos;s interest in string-rewriting systems was stimulated by Maurice Nivat [12], who, in the 1970&apos;s, investigated Thue systems [15] and semi-Thue systems for applications to formal languages and algebra. The collection of research problems that Book was to focus on in the 1980&apos;s was, to a large extent, an outgrowth of the collection of problems that Nivat and his collaborators had focused on in the 1970&apos;s (see Berstell&apos;s 1977 paper [1]). During most of the 1980&apos;s Book was intensively interested in research in this area. He is to be lauded for carrying out his research on a broad front, maintaining an interest in several different research questions, developing his own thoughts and paying careful attention to the results of others. He had many research collaborators, including several doctoral students and people who spent some fruitful post-doctoral years at Santa Barbara. He was, in effect, the leader of a group that included all or most of these. Part of ou
We present in this paper a technique allowing to  choose the parsing granularity within the same  approach relying on a constraint-based formalism. Its main
Optimization of timing behaviour of manufacturing systems can be regarded as a scheduling problem in which tasks model the various production processes. Typical for many manufacturing systems is that tasks or collections of tasks can be associated with manufacturing entities, which can be structured hierarchically. Execution of production processes for several instances of these entities results in nested finite repetitions, which blows up the size of the task graph that is needed for the specification of the scheduling problem, and, in an even worse way, the number of possible schedules. We present a subclass of UML activity diagrams which is generic for the number of repetitions, and therefore suitable for the compact specification of task graphs for these manufacturing systems. The approach to reduce the complexity of the scheduling problem exploits the repetitive patterns extracted from the activity diagrams. It reduces the original problem to a problem containing some minimal number of identical repetitions, and after scheduling of this much smaller problem the schedule is expanded to the original size. We demonstrate our technique on a real-life example from the semiconductor industry.
Performance-intensive software, such as that found in high-performance computing systems and distributed real-time and embedded systems, increasingly executes on a multitude of platforms and user contexts. To ensure that performance-intensive software meets its quality of service (QoS) requirements, it must often be fine-tuned to specific platforms/contexts by adjusting many (in some cases hundreds of) configuration options. Developers who write these types of systems must therefore try to ensure that their additions and modifications work across this large configuration space. In practice, however, time and resource constraints often force developers to assess performance on very few configurations and to extrapolate from these to the entire configuration space, which allows many performance bottlenecks and sources of QoS degradation to escape detection until systems are fielded.
We provide a game-theoretic foundation of Forchheimer&apos;s model of dominant-firm price leadership based on quantity-setting games with one large firm and many small firms.
This paper describes techniques for unsupervised word sense disambiguation of English and German medical documents using the Unified Medical Language System (UMLS). We present both monolingual techniques which rely only on the structure of UMLS, and bilingual techniques which also rely on the availability of parallel corpora. The best results are obtained using relationships between terms given by UMLS, a method which achieves 74% precision, 66% coverage for English and 79% precision, 73% coverage for German on evaluation corpora and over 83% coverage over the whole corpus. The success of this technique for German shows that a lexical resource giving relationships between concepts used to index an English document collection can be used for high quality disambiguation in another language. Document
This paper describes an agent-based artificial market system whose underlying interaction protocols provide advanced features. Using the system, actors (i.e., customers and merchants) can delegate a variety of tasks to personal intelligent agents that act as their artificial employees. Contrary to other approaches, where a new agent is launched when their associated actors intend to perform a buying or selling transaction and &quot;lives&quot; only while this transaction is processed, our approach builds on a personalization of agents that permanently &quot;live&quot; in the market representing their actors&apos; interests. Beyond just requesting and proposing an offer, agents in our system maintain a profile of their owners, which is updated upon the actor-agent interaction type. Furthermore, they can proactively ask their owners&apos; permission to initiate a transaction (e.g., when a new product, which match one&apos;s profile, appears in the market). The system is also enabled with a highly interactive multiple criteria decision making tool that can handle ill-structured information during a purchase transaction, and perform a progressive synthesis and comparative evaluation of the existing proposals.
A Maximum Entropy statistical treatment of an inverse problem concerning frame  theory is presented. The problem arises from the fact that a frame is an overcomplete  set of vectors that defines a mapping with no unique inverse. Although any vector in  the concomitant space can be expressed as linear combination of frame elements, the  coefficients of the expansion are not unique. Frame theory guarantees the existence of a  set of coefficients which is &quot;optimal&quot; in a Minimum Norm sense. Weshow here that these  coefficients are also &quot;optimal&quot; from a Maximum Entropy viewpoint.
Within a group of cooperating agents the decision making of an individual agent depends on the actions of the other agents. In dynamic environments, these dependencies will change rapidly as a result of the continuously changing state. Via a context-specific decomposition of the problem into smaller subproblems, coordination graphs offer scalable solutions to the problem of multiagent decision making. We will apply coordination graphs to the continuous domain by assigning roles to the agents and then coordinating the different roles. Finally, we will demonstrate this method in the RoboCup soccer simulation domain.
We show here a natural extension of finite graph automata, by allowing each  node of a network to store in its memory some pieces of information that are  only bounded by the size of the underlying network (like a unique address). Depending  of the power of the new local transition function, we show results about  the power of the global function computed by the graph automata. The main  result is that the global power is always less than the local computing power  and even with very powerful local function (non recursive) we can not compute  all global functions (even some primitive recursive ones): Distributed  computing is limited by its own structure.
Previous work has demonstrated that genetic programming can  automatically create analog electrical circuits, controllers, and other  devices that duplicate the functionality and, in some cases, partially or  completely duplicate the exact structure of inventions that were patented  between 1917 and 1962. This paper reports on a project in which we  browsed patents of analog circuits issued after January 1, 2000 on the  premise that recently issued patents represent current research that is  considered to be of practical and scientific importance. The paper  describes how we used genetic programming to automatically create  circuits that duplicate the functionality or structure of five post-2000  patented inventions. This work employed four new techniques (motivated  by the theory of genetic algorithms and genetic programming) that we  believe increased the efficiency of the runs. When an automated method  duplicates a previously patented human-designed invention, it can be  argued that the automated method satisfies a Patent-Office-based variation  of the Turing test.
Auctions define games of incomplete information for which it is often too hard to compute the exact Bayesian-Nash equilibrium. Instead, the infinite strategy space is often populated with heuristic  strategies, such as myopic best-response to prices. Given these heuristic strategies, it can be useful to evaluate the strategies and the auction design by computing a Nash equilibrium across the restricted strategy space. First, it is necessary to compute the expected payoff for each heuristic strategy profile. This step involves sampling the auction and averaging over multiple simulations, and its cost can dominate the cost of computing the equilibrium given a payoff matrix. In this paper, we propose two information theoretic approaches to determine the next sample through an interleaving of equilibrium calculations and payoff refinement. Initial experiments demonstrate that both methods reduce error in the computed Nash equilibrium as samples are performed at faster rates than naive uniform sampling. The second, faster method, has a lower metadeliberation cost and better scaling properties. We discuss how our sampling methodology could be used within experimental mechanism design.
The eXtensible Markup Language (XML) is well accepted in many different  application areas. As a consequence, there is an increasing need for persistently  storing XML documents. As soon as many users and applications work concurrently  on the same collection of XML documents --- i.e. an XML base --- isolating  accesses and modifications of different transactions becomes an important  issue. We discuss six
Subband adaptive filters suffer degraded performance when high input energy occurs at frequencies coincident with subband boundaries. This is seen as increased error in critically sampled systems and as reduced asymptotic convergence speed in oversampled systems. To address this problem a dynamic frequency decomposition scheme is presented which aims to control the frequency of subband boundaries such that they avoid spectral regions of high input energy. An efficient structure for this is described, which maintains the low complexity advantage of subband systems. Simulation results show reductions in MSE of around 5-10dBs in the critical case and convergence improvement in the oversampled case, in addition to increased robustness to coloured inputs in both cases.
Information reusability refers to a concept where processed information at various levels of granularity can be derived efficiently from information derived earlier at finer granularities. Moreover, reusable information elements can also fulfil different information requirements.
The work outlined in this paper explores the influence of prior knowledge and related experience (held in the form of weights) on the generalisation performance of connectionist models. Networks were trained on simple classification and associated tasks. Results regarding the transfer of related experience between networks trained using back-propagation and recurrent networks performing sequence production, are reported. In terms of prior knowledge, results demonstrate that experienced networks produced their most pronounced generalisation performance advantage over nave networks when a specific point of difficulty during learning was identified and an incremental training strategy applied at this point. Interestingly, the second set of results showed that knowledge learnt about in one task could be used to facilitate learning of a different but related task. However, in the third experiment, when the network architecture was changed, prior knowledge did not provide any advantage and indeed when learning was expanded, even found to deteriorated.
this article is not simply to reiterate or update these works, but to outline the progression of research with respect to malaria epidemiology, so that an understanding can be gained of how remote-sensing techniques have been adopted. This background enables us to anticipate and plan for the opportunities made available by forthcoming advances in satellite-sensor technology    
For cervical cancer detection, the performance of multispectral texture (MST) features extracted from multispectral Pap smear images is evaluated. In this study we carried out pairwise comparisons between different image features, including MST versus average spectral texture features (AST, without spectral information), and MST versus multispectral intensity features (MSI, without texture information). We demonstrate, experimentally, that well-selected MST features combining both multispectral and texture information can achieve better classification results (ROC curves) for cervical cancer detection from multispectral Pap smear images. Furthermore, we investigate which type of wavelet texture features (orthogonal, bi-orthogonal or non-orthogonal) individually or in combination is most effective.
In this paper, we address the general problem of identifying convolutive channels when several training sequences are inserted in the transmitted data symbols stream. We analyze the general situation where the training sequences differ from each other. We consider quasi-static channels (i.e. the sampling period is several orders of magnitude below the coherence time of the channel). There are no requirements on the length of the training sequence and all the received symbols that contain contributions from the training symbols are used for the identification. We first propose an iterative method that quickly converges to the Maximum Likelihood (ML) channel estimate. We also derive a simple closed form expression that approximates the ML channel estimate.
It has recently been shown that Artificial  Immune Systems are not only capable of  performing classification, but that AIRS, a  resource limited Artificial Immune System, is  competitive with some of the best classifiers in  the world on a broad variety of classification  problems. This paper explores some of the issues  that affect the performance of AIRS. These  include modifications to the algorithm for  resource allocation, a policy for handling ties and  approaches to ARB pool organization.
This paper concerns the need for methodological support in providing Knowledge Management (KM) IT solutions. Due to the distributed nature of knowledge, the support of KM often requires complex, distributed IT systems, which are inherently difficult to design. We propose an agent-oriented methodology based on Tropos for the analysis and design of KM systems that offers appropriate abstractions for modeling and designing the characteristics of the organizational setting of the system. The method is illustrated using a fictitious scenario where a newcomer in a knowledge organization decides to join an existing Community of Practice (CoP) in order to share knowledge and adjust to his new working environment.
Using polarized brightness (pB) measurements made by the High Altitude Observatory (HAO) Mauna Loa  Mk III K-coronameter, we investigate the daily changes of path-integrated density at 1.15 R , . During 1996,  when simultaneous pB and helioseismology data were available, we find that the correlation of pB (at zero time  lag and 20ff latitude lag) varies with latitude in the same way that the subsurface differential rotation inferred  from helioseismology does. The association is such that bands of higher pB correlation are associated with  retrograde subsurface rotation and that lower pB correlation bands are associated with prograde subsurface rotation.
We describe a methodology for verifying system-on-chip designs. In our methodology, the problem of verifying system-on-chip designs is decomposed into three tasks. First, we verify, once and for all, the standard bus interconnecting IP Cores in the system . The next task is to verify the glue logic, which connects the IP Cores to the buses. Finally, using the verified bus protocols and the IP core designs, the complete system is verified. To illustrate our methodology, we verify the PCI Local Bus, a widely used bus protocol in system-on-chip designs. We demonstrate various modeling and verification techniques for buses by modeling the PCI Local Bus with the symbolic model checker SMV. We have found two potential bugs in the PCI bus protocol specification that await confirmation of the PCI Special Interest Group (PCI-SIG).
We attack the problem of perfect equalizability of multi-user channels, in which the usual linear time-invariant assumption is dismissed. In the linear, time-invariant case, condition for perfect equalizability is plain and expressed in terms of the column rank of the channel&apos;s transfer matrix. Using the module-theoretic approach developped by Fliess, in which the transfer matrix of a time-varying channel as well as the rank of a non-linear channel are clearly defined, we show how the condition obtained in the linear time-invariant case naturally extends to the time-varying and the non-linear cases.
In this paper we argue that content distribution in the face of censorship is a compelling and feasible application of active networking. In the face of a determined and powerful adversary, every fixed protocol can become known and subsequently monitored, blocked, or its member nodes identified and attacked. Frequent and diverse protocol change is key to allowing information to continue to flow. Typically, decentralized and locally-customized protocol evolution is also an important aspect in providing censor-resistance.
This work gives an overview of analytic tools to the design, analysis, and modelling of communication systems which can be described by linear vector channels such as y = Hx+z where the number of components in each vector is large. Tools from probability theory, operator algebra, and statistical physics are reviewed. The survey of analytical tools is complemented by examples of applications in communications engineering.
It i s often assumed that knowledge of both the source and target languages is necessary in order to evaluate the output of a machine translation (MT) system. This paper reports on an experimental evaluation of Chinese-English MT and Spanish-English MT from output specifically designed for evaluators who do not read or speak Chinese or Spanish. An outline of the characteristics measured and evaluation follows.
Many software engineering research tools are stand-alone applications that have trouble interoperating with other development tools and do not fit well into the software developers&apos; established work processes. Our main hypothesis is that in order for new tools to be adopted effectively, they must be compatible with both existing users and existing tools.
Electronic bilingual lexicons are crucial for machine translation, cross-lingual information retrieval and speech recognition. For low-density languages, however, the availability of electronic bilingual lexicons is questionable. One solution is to acquire electronic lexicons from printed bilingual dictionaries. While manual data entry is a possibility, automatic acquisition of lexicons from scanned images of bilingual dictionaries would expedite the prototyping process of cross-language systems. Printed dictionaries have a logical model that defines the syntax of the dictionary entries -- i.e. order of the dictionary entry, its part of speech, its pronunciation and its definition. In this article we propose an algorithm to automatically extract bilingual dictionary entries based on stochastic language models. We demonstrate this algorithm on a printed Chinese-English dictionary. This work can be easily used for extracting information from other tabular structures like telephone books, catalogs, etc.
Many natural computational problems have input consisting of two or more parts. For example,
Among existing aggregate production planning (APP) approaches, the spreadsheet solver approach is found to be the most applicable for industries due to the following reasons: (1) the solver on spreadsheet software is readily available on virtually all personal computers, (2) the APP model is relatively easy to formulate in a spreadsheet format, and (3) the results are easy to interpret. This paper presents an APP model and a guideline to develop an optimal aggregate production plan using the spreadsheet solver approach. A manufacturing case study is presented to demonstrate how the guideline can be applied. The developed APP model is also evaluated whether it is satisfactory and can lead to immediate implementation.
This paper proposes a number of type-system and language  extensions to natively support relational and hierarchical data within a  statically typed object-oriented setting. In our approach SQL tables and  XML documents become first class citizens that benefit from the full  range of features available in a modern programming language like C    or Java. This allows objects, tables and documents to be constructed,  loaded, passed, transformed, updated, and queried in a unified and typesafe  manner.
Web browsing is often a collaborative activity. Users involved in a  joint information gathering exercise will wish to share knowledge about the  web pages visited and the contents found. Magpie is a suite of tools supporting  the interpretation of web pages and semantically enriched web browsing. By  automatically associating an ontology-based semantic layer to web resources,  Magpie allows relevant services to be invoked as well as remotely triggered  within a standard web browser. In this paper we describe how Magpie trigger  services can provide semantic support to collaborative browsing activities.
In this paper, we develop a model for evaluating the blocking probability of various connection management protocols for wavelengthrouted optical networks with dynamic lightpath establishment. The model characterizes both the blocking due to insufficient resources, as well as the blocking due to multiple interfering connection requests. We then use the analytical model to compare two connection management schemes, one which utilizes source-initiated reservation, and another which utilizes destination-initiated reservation.
This article describes the flight configuration of the SBV sensor, its incorporated technologies, its preflight status, and the surveillance experiments to be performed
this paper we are in particular interested in how we can conserve mass in fluid flow. The mass conservation for an incompressible fluid reads div v   0, where v is the velocity of the fluid. In a two dimensional situation one can easily associate v  to a stream function, a Hamiltonian which asks for special numerical treatment in order to have conservation. If we have a three dimensional problem such a formulation is not possible. Yet, in cases of symmetry we can often reformulate the problem as a two dimensional problem. With some appropriate change of variables this then results in a (Hamiltonian) stream function. This paper is built up as follows. In Section 2 we consider the relationship between conservation, the stream function as a Hamiltonian and how this can be applied in a three dimensional axisymmetric case. Then in Section 3 we briefly describe the use of the midpoint rule, a simplectic numerical method that conserves quantities in a time stepping procedure. In Section 4 we give two examples to illustrate this conservation. Finally, in Section 5 we consider an application of the method in a practical simulation: the pressing of glass in a mould. 2 Conservation and Hamiltonian Systems  If we have an incompressible fluid with density ff, moving with velocity v then the conservation of mass can be expressed as   ffv   0. Since ff is constant this simplifies to     0. (2.1) This law implies that a certain volume, V   (t) say, remains constant, i.e. is conserved. For a two dimensional flow this has an interesting consequence. Let us denote a vector x     (t) as      , (2.2) and the velocities in x and y direction by u and v respectively  v(x, y)   u x (x, y), u y (x, y)    . (2.3) Then (2.1) implies        0. (2.4) As is well-known we can associate a stream function ...
We extend Kobayashi and Sumii&apos;s type system for the deadlockfree   -calculus and develop a type reconstruction algorithm. Kobayashi  and Sumii&apos;s type system helps high-level reasoning about concurrent programs  by guaranteeing that communication on certain channels will eventually  succeed. It can ensure, for example, that a process implementing a  function really behaves like a function. However, because it lacked a type  reconstruction algorithm and required rather complicated type annotations,  applying it to real concurrent languages was impractical. We have  therefore developed a type reconstruction algorithm for an extension of  the type system. The key novelties that made it possible are generalization  of usages (which specifies how each communication channel is used)  and a subusage relation.
Warping can be used to reduce interindividual structural variations of 3D image datasets of brains by generating a standard brain and subsequent matching of individual datasets to this reference system. Point-based warping uses structural information (landmarks) to construct the spatial correspondence between the datasets. For this we compare the performance of three landmark detection algorithms. The first two approaches use a threshold-based definition of landmarks, the third spatial derivations of voxels. The warping is based on a distance-weighted method with an exponential weighting function. All methods tested are able to reduce structural variations, best results are obtained by the derivation approach.
To provide mutimedia applications with new functionalities, such as content-based interactivity and scalability, the new video coding standard MPEG-4 relies on content based representation. This means a prior decomposition of a video sequence into semantically meaningful, physical objects. We formulate this problem as one of separating foreground objects from the background based on motion information. We present in detail an unsupervised video segmentation technique which uses the watershed transformation for spatial segmentation. We first give an outline of the algorithm and then proceed to explain each of the steps in detail.
Introduction With the release of the open-source GIS GRASS 5.0 in early 1999, opportunities are presented for integration with the open-source R statistical data analysis programming environment (Ihaka and Gentleman, 1996, code obtained from [2]). In the examples presented, R is run interactively within the GRASS 5.0 environment, transfering data by writing and reading temporary text files; the operating system here is Linux. The note describes the implementation in R of functions needed to move data between GRASS and R, providing the user with a basic interface between the two environments. Development of the leading Open Source GIS --- GRASS --- has been moved to Baylor University in Texas, where work on a new release incorporating floating-point raster cell values and NULL values different from zero is now in beta testing (Byars and Clamons, 1998, Linux binary obtained from [3]). In parallel with this, the R statistical and data analysis language, also Open Source, is maturing very 
The impact of device independent authoring on software engineering manifests itself mainly at the middleware level. Until recently middleware platforms were targeted at vertical coverage of specific scenarios. Consumer devices with integrated Internet-access are becoming more popular and their diversity grows with their market penetration and with the extension of the mobile communication infrastructure. This requires software architectures that are capable of supporting horizontal coverage of a wide range of devices and scenarios. This paper presents the Multi User Interface, Single Application project. It provides a feasable approach for multiplatform support through the introduction of an adaptable and abstract interaction-oriented user interface language.
The GermanTeam is a joint project of several German universities in the Sony Legged Robot  League. This report describes the software developed for the RoboCup 2001 in Seattle. It  presents the software architecture of the system as well as the methods that were developed  to tackle the problems of motion, image processing, object recognition, self-localization, and  robot behavior. The approaches for both playing robot soccer and mastering the challenges  are presented. In addition to the software actually running on the robots, this document will  also give an overview of the tools the GermanTeam used to support the development process.
This report presents an unusual event recognition approach in the field of traffic surveillance. Such  events are unusual traffic behaviour like traffic jams, accidents or ghost drivers. An interest-point  based tracking algorithm (KLT-tracker) is discussed which pursues features on vehicles through  a static camera scene. Tracking data can be collected by observing normal traffic. Then, this data  is used to learn a spatio-temporal model of normal traffic behaviour. Thereby, training samples  are generated in a learning space by the tracking data. Thus, the spherical probability density  function (p.d.f.) of the space can be estimated. We use a Growing Neural Gas in combination  with a MDL-based pruning algorithm for unsupervised learning. The former method belongs to  the class of soft-competitive algorithms which overcome the problems of &quot;stranded&quot; reference  vectors. In contrast to other works, the number of reference vectors has not to be constant. The algorithm finds
This paper proposes a new theory of the relationship between the sentence processing  mechanism and the available computational resources. This theory -- the Syntactic Prediction  Locality Theory (SPLT) -- has two components: an integration cost component and a component  for the memory cost associated with keeping track of obligatory syntactic requirements. Memory cost is
this paper we give sound and complete proof systems  for the useful and expressive quanti  ed propositional temporal  logic, both with and without past temporal operators. Until now an  axiomatization has only existed for the version with the inclusion of  the past operators and this axiomatization relied on the past operators  in subtle and complicated ways. In certain situations, such as  in branching time extensions of the linear time logic, it is important  to avoid using the past time operators. Our completeness proof proceeds  mostly by using deterministic Rabin automata but the main  step is via a new and interesting proof of correctness for an optimal  complementation procedure for nondeterministic Buchi automata
Privacy of personal location information is becoming an increasingly important issue. This paper refines a method, called the mix zone, developed to enhance user privacy in location-based services. We improve the mathematical model, examine and minimise computational complexity and develop a method of providing feedback to users.
Changes in the end-to-end path between two hosts can lead to sudden changes in the round-trip time and available bandwidth, or even the complete loss of connectivity. Determining the reason for the routing change is crucial for diagnosing and fixing the problem, and for holding a particular domain accountable for the disruption. Active measurement tools like traceroute can infer the current path between two end-points, but not where and why the path changed. Analyzing BGP data from multiple vantage points seems like a promising way to infer the root cause of routing changes. In this paper, we explain the inherent limitations of using BGP data alone and argue for a distributed approach to troubleshooting routing problems. We propose a solution where each AS continuously maintains a view of routing changes in its own network, without requiring additional support from the underlying routers. Then, we describe how to query the measurement servers along the AS-level forwarding path from the source to the destination to uncover the location and the reason for the routing change.
this article should be addressed to M. F. Damian, University of Bristol, Department of Experimental Psychology, 8 WoodlandRoad, Bristol BS8 1TN, England (e-mail: m.damian @bristol.ac.uk)
An algorithm for rapid computation of Richards&apos;s smooth molecular surface is described.  The entire surface is computed analytically, triangulated, and displayed at interactive rates. The  faster speeds for our program have been achieved by algorithmic improvements, parallelizing the  computations, and by taking advantage of the special geometrical properties of such surfaces.  Our algorithm is easily parallelizable and it has a time complexityofOffklog  kffover n  processors,  where n is the number of atoms of the molecule and k is the average number of neighbors per  atom.  1 Introduction  The smooth molecular surface of a molecule is deffned as the surface which an exterior probe-sphere touches as it is rolled over the spherical atoms of that molecule. This deffnition of a molecular surface was ffrst proposed by Richards ff16ff. This surface is useful in studying the structure and interactions of proteins, in particular for attacking the protein-substrate docking problem. An example of suc...
Grouping or clustering genes based on their expression behavior (expression proles) is  an important step preceding further analysis of the interaction between these genes. Based  on the hypothesis that similarity in expression implies similarity in regulatory mechanisms  (coregulation), cluster algorithms only have to group signicant amounts of genes with  a high degree of expression similarity (coexpression). Genes not showing this similarity  have to be excluded from further analysis.
Reinforcement learning means learning a policy---a mapping of observations into actions--- based on feedback from the environment. The learning can be viewed as browsing a set of policies while evaluating them by trial through interaction with the environment. We present an application of gradient ascent algorithm for reinforcement learning to a complex domain of packet routing in network communication and compare the performance of this algorithm to other routing methods on a benchmark problem.
Spatial indexes play a major role in fast access to  spatial and location data. Most commercial applications  insert new data in bulk: in batches or arrays.
A fully integrated 1.9-GHz CMOS low-noise amplifier (LNA) has been implemented in a 0.8-m CMOS technology. For low-noise performance, the amplifier employs high-quality spiral inductors with a quality factor of 8.5--12.5, and device layout and bias condition of the active devices were optimized for low-noise conditions. This amplifier showed a noise figure of 2.8 dB with a forward gain of 15 dB at current consumption of 15 mA. To the authors&apos; knowledge, this represents the lowest noise figure reported to date for a fully integrated CMOS LNA operating at 1.9 GHz.
This paper presents an overview of recent experimental progress by the Duke DNA NanoTech Group in our efforts to utilize novel DNA nanostructures for computational self-assembly as well as for templates in the fabrication of functional nano-patterned materials. We have prototyped a new DNA tile type known as the 4x4 (a cross-like structure composed of four four-arm junctions) upon which we have deposited metal to form highly conductive nanowires and also are adapting multi-tile 4x4 sets for a variety of computational applications. We have recently described a DNA barcode lattice composed of DX tiles assembled on a long scaffold strand; the system propagates 1-dimensional (1D) information encoded on the scaffold strand into a specific and reprogrammable barcode pattern which is visible in 2D by atomic force microscopy. We have succeeded in demonstrating the first highly parallel computation via DNA tile self-assembly by using a singlelayer superstructure made of DX tiles which computes the entire lookup table of pairwise XOR calculations up to a modest size input string length. We have prototyped a 2-state DNA lattice assembly containing actuator components and demonstrated its ability to be controllably switched between the two states. We are currently working on a molecular robotics experiment aimed at demonstrating unidirectional motion of a small DNA fragment along a track constructed of DNA. We have demonstrated a diverse set of novel structures and applications which extend the inherent information carrying capacity of DNA in a variety of novel directions
Introduction  Ultrasound, whose frequency is over 20 kHz, is not a sound mainly to hear in general use. However, in 1948, Gavreau has reported that it could be perceived as sound when stimulated by bone conduction [1]. There are some interesting characteristics of bone-conducted ultrasound that are quite different from those of audible sound, those are presumed to occur due to the difference of the mechanism of the hearing. However, the mechanism of ultrasonic hearing is not completely made clear and there has been a controversy over it. One predicts that the certain biomechanical demodulation transform ultrasound into low frequency audible sound [2], and others hypothesize contribution of the cochlear hair cells [3, 4, 5], or the vestibular hair cells [6, 7]. To provide conclusive evidence relevant to the debate mentioned above, we had measured with magnetoencephalography (MEG) and positron emission tomography (PET) to elucidate the mechanism of ultrasonic hearing and revealed the cor
We consider control problems that arise in the context of the IRVS. Speci  cally,  we study the dynamics of systems of fully autonomous agents that are expected to  optimize a global potential function of which they have only partial knowledge. Our  ultimate goal is to characterize mathematically the global performance of agent-based  systems in which the inter-agent cooperation is based on local exchange of information.
Eighty-two bacillus strains were isolated from Thai fermented soybeans (thua nao), of which  thirty-nine were identified as Bacillus subtilis. Crude proteins from these B. subtilis strains were  investigated for their proteolytic activity. When tested on skim milk agar, the crude proteins of B.
Increased competition, complex service provision chains and integrated service offerings require effective techniques for the rapid integration of telecommunications services and management systems over multiple organisational domains. This paper presents some of the results of practical development work in this area, detailing the technologies and standards used, the architectural approach taken and the application of this approach to specific services. This work covers the integration of multimedia services, broadband networks, service management and network management, though the detailed examples given focus specifically on the integration of services and service management.
Janos is an operating system for active network nodes whose primary focus is strong resource management and control of untrusted active applications written in Java. Janos includes the three major components of a Javabased active network operating system: the low-level NodeOS, a resource-aware Java Virtual Machine, and an active network protocol execution environment. Each of these components is separately usable. This article lays out the Janos design and its rationale.
In the past, very few types of primitives have been directly supported by graphics accelerators, in spite of the fact that a great variety of complex primitives are routinely used for high-quality rendering. We explore the issues associated with the support of arbitrary procedural primitives, and describe a machine-independent language as well as an extension to OpenGL designed to support userdefined procedural primitives. We have created a prototype implementation on PixelFlow, a highperformance graphics accelerator.
The paper explores how the structure of Webspace can be used for accelerated Web prefetch. We have conducted experiments based on a novel hyperspace aware prefetch proxy and have studied the prefetch performance on several dominant hyperspace patterns. The study assesses the system&apos;s responsiveness and background loads for various user interaction duration, surfing and prefetch sequences. This paper also draws attention to the emergence of some regular patterns in contemporary webspace. The results show that webspace awareness can help in improving prefetch performance. This study also provides an interesting insight toward a framework where the professional content developers can gain more control towards authoring prefetch friendly collection for increased site responsiveness.
Nonmonotonic logic programming (NMLP) and inductive logic  programming (ILP) are two important extensions of logic programming.
this paper, we consider the following way of describing the approximation of positive real numbers by rational numbers which is speci  cally rooted in actual calculation. The rational numbers used in this context are given by the decimal expansions   = b m b m 1    b 0 :a 1 a 2    an  where n will be called the order ord of . Such an  will be considered as an approximation to a real number  if  belongs to the open interval (      ), the precision of the approximation then evidently measured by ord. Further, in accordance with this view, we put      i ( 10          );  expressing the notion that  is a better approximation than   for any  it approximates. As a suggestive example we note that     whenever ord  ord   and  is an extension of  , say  b m b m 1    b 0 :a 1 a 2    an+1  b m b m 1    b 0 :a 1 a 2    an ;  but of course there are other instances, such as 0:12  0:2
Web sites are exposed to high rates of incoming requests. Since web sites are sensitive to overload, admission control mechanisms are often implemented. The purpose of such a mechanism is to prevent requests from entering the web server during high loads. This paper presents how admission control mechanisms can be designed and implemented with a combination of queueing theory and control theory. Since web servers behave non-linear and stochastic, queueing theory can be used for web server modelling. However, there are no mathematical tools in queueing theory to use when designing admission control mechanisms. Instead, control theory contains the needed mathematical tools. By analysing queueing systems with control theoretic methods, good admission control mechanisms can be designed for web server systems. In this paper we model an Apache web server as a GI/G/1-system. Then, we use control theory to design a PI-controller, commonly used in automatic control, for the web server. In the paper we describe the design of the controller and also how it can be implemented in a real system. The controller has been implemented and tested together with the Apache web server. The server was placed in a laboratory network together with a traffic generator which was used to represent client requests. Measurements in the laboratory setup show how robust the implemented controller is, and how it correspond to the results from the theoretical analysis.
A brain-computer interface (BCI) is a system that should in its ultimate form translate a subject&apos;s intent into a technical control signal without resorting to the classical neuromuscular communication channels. By using that signal to, e.g., control a wheelchair or a neuroprosthesis, a BCI could become a valuable tool for paralyzed patients. One approach to implement a BCI is to let users learn to self-control the amplitude of some of their brain rhythms as extracted from multi-channel EEG. Here we present a method that estimates subject-specific spatial filters which allow for a robust extraction of the rhythm modulations. The effectiveness of the method was proved by achieving the minimum prediction error on data set IIa in the BCI Competition 2003, which consisted of data from three subjects recorded in 10 sessions.
This paper describes the ideas, advantages and methods maintaining a Multiple Representation Database (MRDB) for storing and serving spatial data. A Multi-resolution/representation-database (MRDB) can be described as a spatial database, which can be used to store the same real-world-phenomena at different levels of precision, accuracy and resolution. Furthermore these phenomena can be stored in different ways of presentation or symbolisation. There are several reasons for introducing an MRDB: On the one hand it allows for a multi-scale analysis of the data: Information in one resolution can be analysed with respect to information given on another resolution. A major reason for National Mapping Agencies to investigate and implement an MRDB is the possibility of propagating updates between the scales with the advantage that only the level with the highest resolution must be updated manually. Two projects using an MRDB will be presented. The EU-project GiMoDig develops methods for delivering geospatial data to a mobile user in real-time and for small displays. The other project, called WIPKA, aims at establishing a consistent representation of all topographic data sets in Germany (ATKIS) in an MRDB to enable an automatic incremental update process for the data sets.
this paper, introduce &quot;Mutual Modeling Teammate Behavior&quot; (MMTB). This model simulates executed action of a teammate. combination with the known world dynamics action used determine teammate&apos;s future state. model, assume that teammates homogeneous and follow a policy that common knowledge among them. Furthermore, describe MMTB implemented RoboCup soccer simulation Trilearn empirical results about effectiveness. Mutual Modeling Teammate Behavior CONTENTS Contents 1 
Database applications and technologies are of central importance in many information systems a person may encounter. To obtain services, the end-users are required a smart card (plastic card containing a microcomputer), which is a device providing information about the user&apos;s identity and some related personal data. It can be updated and loaded with new data that will be used during further sessions. Moreover the data contained into the smart card can be used by other information systems, the data are carried away from a site to another. The individual mobility increases the need for a person to carry information about himself anywhere and at any time. For services providers, such as health professionals, it is essential to access to this information stored on several information systems. In many applicative areas, to provide different information systems linked and networked is a real challenge. Based on personal information about the bearer, the smart card is a key to access to different information systems and a mean to share and interchange data. The smart cards are evolving towards personal database functions. We briefly present the technology of smart cards, then we introduce a new approach : the CQL card (for Card Query Language). This card integrates the concepts of the Database Management Systems. Database engine is carried out by the card microcomputer, the card is a new database machine. It manages &quot;users&quot; entities which handle different &quot;objects&quot; according to their &quot;privileges&quot;. CQL, a subset of SQL, is used to communicate with the card. Views enable sharing data among information systems. Access rights and privileges guarantee the data privacy. To ease the integration of this portable database we have implemented an ODBC driver enabling smart card connect...
We present new attacks on key schedules of block ciphers.
Many phenomena of flow and transport are governed by the heterogeneous structure of the material. A notorious problem in soil physics is to predict preferential flow along highly conductive regions. In many cases, the key for a quantitative understanding of such processes is the spatial structure of hydraulic properties which lead to complex flow fields of water. Direct measurements of the hydraulic structure at the scale of a soil column of some 10-30 cm in diameter are difficult. As an alternative, we assume that the soil bulk density within a given soil can be used as a suitable proxy.
In this paper, we present a syntax analyser tool for Verilog programs which can be used as a front end to debugging and program verification tools.
Current image indexing methods are based on measures of visual content. However, this approach provides only a partial solution to the image retrieval problem. For example, an artist might want to retrieve an image (for use in an advertising campaign) that evokes a particular &quot;feeling&quot; in the viewer. One technique for measuring evoked feelings, which originated in Japan, indexes images based on the inner impression (i.e. the kansei) experienced by a person while viewing an image or object -- impressions such as busy, elegant, romantic, or lavish. The aspects of the image that evoke this inner impression in the viewer are called kansei factors. The challenge in kansei research is to enumerate those factors, with the ultimate goal of indexing images with the &quot;inner impression&quot; that viewers experience. Thus, the focus is on the viewer, rather than on the image, and similarity measures derived from kansei indexing represent similarities in inner experience, rather than visual similarity. This paper presents the results of research that indexes images based on a set of kansei impressions, and then looks for correlations between that indexing and traditional content-based indexing. The goal is to allow the indexing of images based on the inner impressions they evoke, using visual content.
Planning systems rely on knowledge about the problems they  have to solve: The problem description and in many cases advice  on how to find a solution. This paper is concerned with a  third kind of knowledge which we term domain knowledge:  Information about the problem that is produced by one component  of the planner and used for advice by another. We first  distinguish domain knowledge from the problem description  and from advice, and argue for the advantages of the explict  use of domain knowledge. Then we identify three classes  of domain knowledge for which these advantages are most  apparent and define a language, DKEL, to represent these  classes. DKEL is designed as an extension to PDDL.
Business process re-engineering has overcome the hype of the nineties of the last century and has become an engineering discipline. With the advent of e-business a new era of uncertainty in systems&apos; development has arrived. Do current approaches extend to e-business engineering, or are there too many essential differences to prevent this? In this paper we discuss the similarities and difference between business process engineering and e-business engineering, based on research and cases that we have carried out over the last seven years. From this analysis we digest what support is needed for engineers in e-business and relate this to current approaches in e-business.
Fuzzy CoCo is a methodology, combining fuzzy logic and evolutionary  computation, for constructing systems able to accurately predict  the outcome of a human decision-making process, while providing an  understandable explanation of the underlying reasoning. Fuzzy logic  provides a formal framework for constructing systems exhibiting both  good numeric performance (precision) and linguistic representation (interpretability)  . However, fuzzy modeling---meaning the construction  of fuzzy systems---is an arduous task, demanding the identification of  many parameters. To solve it, we use evolutionary computation techniques,  specifically cooperative coevolution, which are widely used to  search for adequate solutions in complex spaces. We have successfully  applied the algorithm to model the decision processes involved in two  breast-cancer diagnostic problems: the WBCD problem and the Catalonia  mammography interpretation problem, obtaining systems both  of high performance and high interpretability. For the Catalonia problem,  an evolved system was embedded within a web-based tool---called  COBRA---for aiding radiologists in mammography interpretation.
this paper, we examine the vulnerabilities of wireless networks and argue that we must include intrusion detection  in the security architecture for mobile computing environment. We have developed such an architecture and evaluated  a key mechanism in this architecture, anomaly detection for mobile ad-hoc network, through simulation experiments
FORMA MEnTIS is an interactive software tool able to encourage and support the diffusion and the dissemination of scientific culture through an innovative approach based on the integration of multimedia tools, Web and mathematical software. In this paper we will explore the main design and implementation issues of the FORMA MEnTIS project. In particular we will focus on tools to be used and how to use them in order to obtain a similar environment from scratch.
ase of mesh generation and quality  of solution.  Oxford University Computing Laboratory  Numerical Analysis Group  Wolfson Building  Parks Road  Oxford, England OX1 3QD May, 2000 Contents  1 INTRODUCTION 3 2 NUMERICAL SCHEME DESCRIPTION 4 3 MIXER CONFIGURATION 6 4 GRID GENERATION 7 5 RESULTS 9 5.1 Planar shear layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 5.2 Convoluted shear layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 6 CONCLUSIONS 18 1 INTRODUCTION  Lobed mixers are passive mixing devices providing augmentation of the mixing between two coflowing streams. These devices, which have demonstrated much benefit in terms of improved mixing with minimum pressure losses, have become a common component of a modern gas turbine engine. The lobed mixer consists of a splitter plate separating the two streams with a corrugated trailing edge. The corrugations create radial deflections of each stream through the lobes but in opposite direct
In this paper we propose a numerical method for computing minimal  surfaces with fixed boundaries. The level set method is used to evolve  a codimension-1 surface with fixed codimension-2 boundary in    under  mean curvature flow. For n = 3 the problem has been approached in  [3] using the level set method, but with a more complicated boundary  condition. The method we present can be generalized straightforward  to arbitrary dimension, and the framework in which it is presented is  dimension independent. Examples are shown for n = 2, 3, 4.
Due to the increasing necessity and availability of information from dierent sources,  information integration is becoming one of the challenging issues in arti  cial intelligence  and computer science. A successful methodology for information integration is based  on Federated Databases (FDB). However, dierently form databases (DB) a completely  satisfactory formal treatment of FDB is still missing. The goal of this paper is to  ll  this gap. Our basic intuition is that an FDB can be formalized by considering each DB  of the federation as a context. We argue that this perspective is a promising one, as  some of the relevant problems in the area of information integration, such as semantic  heterogeneity, can be successfully solved using contexts. In the paper we provide a  formal notion of FDB schema, and a context based semantics for such a schema, called  Local Models Semantics for FDBs. We show by means of examples that Local Models  Semantics overcomes the drawbacks of the previous approaches.
Due to our familiarity with how fluids move and interact, as well as their complexity, plausible animation of fluids  remains a challenging problem. We present a particle interaction method for simulating fluids. The underlying  equations of fluid motion are discretized using moving particles and their interactions. The method allows simulation  and modeling of mixing fluids with different physical properties, fluid interactions with stationary objects, and  fluids that exhibit significant interface breakup and fragmentation. The gridless computational method is suited  for medium scale problems since computational elements exist only where needed. The method fits well into the  current user interaction paradigm and allows easy user control over the desired fluid motion.
The goal of this experience report is to identify some of the key problems  of today&apos;s enterprises that have to deal with managing their large business  critical software systems. Our motivation to do so is based on preliminary findings  from the ARRIBA project. The work we present here form our preliminary  conclusions of the first 6 months of the project, where we visited some of these  enterprises, to identify their main needs of today.
Few tools exist that address the challenges facing researchers in the Textual Data Mining (TDM) field. Some are too specific to their application, or are prototypes not suitable for general use. More general tools often are not capable of processing large volumes of data.
Surface geometry is often modeled with irregular triangle meshes. The process of remeshing refers to approximating such geometry using a mesh with (semi)-regular connectivity, which has advantages for many graphics applications. However, current techniques for remeshing arbitrary surfaces create only semi-regular meshes. The original mesh is typically decomposed into a set of disk-like charts, onto which the geometry is parametrized and sampled. In this paper, we propose to remesh an arbitrary surface onto a completely regular structure we call a geometry image. It captures geometry as a simple 2D array of quantized points. Surface signals like normals and colors are stored in similar 2D arrays using the same implicit surface parametrization --- texture coordinates are absent. To create a geometry image, we cut an arbitrary mesh along a network of edge paths, and parametrize the resulting single chart onto a square. Geometry images can be encoded using traditional image compression algorithms, such as wavelet-based coders.
In this paper we consider a first attempt to deal with re-identification of individuals when the variables in the two data files to be matched are not exactly the same but &quot;similar&quot;.
To construct a secure operating system with high assurance, it is essential that the security architecture of the operating system can be analyzed vigorously and that the architecture can be easily understood by engineers who translate the design into code. In this paper we describe a partial model of the security policies of an operating system which implements a variant of the Bell-LaPadula model. In particular, we describe the privileges of trusted subjects and how they are used in granting accesses. We use a combination of an object-oriented modeling technique, the Unified Modeling Language (UML), and a mathematically-based formal method called Higher-Order Logic (HOL). UML provides a visual, intuitive model that is easy to write and easily understood by engineers. HOL provides a rigorous model whose properties can be mechanically proved, thus allowing the correctness of the model to be established. UML models provide the structure for natural language descriptions and HOL models. HOL models add precise semantics to both text descriptions and UML models.
In this paper, we present a neural network model that realizes a dynamic version of the distance transform algorithm (used for path planning in a stationary domain). The novel version is capable of performing path generation for highly dynamic environments. The neural network has discrete-time dynamics, is locally connected, and, hence, computationally efficient. No preliminary information about the world status is required for the planning process. Path generation is performed via the neural-activity landscape, which forms a dynamically-updating potential field over a distributed representation of the configuration space of a robot. The network dynamics guarantees local adaptations and includes a set of strict rules for determining the next step in the path for a robot. According to these rules, planned paths tend to be optimal in a L_1 metric. Simulation results in a series of experiments for various dynamical situations prove the effectiveness of the proposed model.
Relevant to the uplink of a VSG-CDMA system, a technique part of 3G standards, this work seeks power and data rate allocations for each of N terminals, so that the network weighted throughput is maximized. The weights admit various interpretations, including levels of importance, &quot;utility&quot;, and price. We have learned that at least one terminal should operate at the highest available data rate. Our analysis leads to allocations in which terminals not operating at the highest data rate operate at the same signal-to-interference ratio (SIR). This value is determined by the physical layer through the function that gives, in terms of the received SIR, the probability that a data packet is received correctly. Other factors held constant, lowering the highest available data rate increases the number of terminals which should operate at maximum data rate. This analysis conforms to classical optimization theory. Our model should accommodate a wide variety of physical layer configurations.
this paper, we provide a general overview of Helios, by focusing on the role of peer ontologies and matching techniques for knowledge sharing (Section 2). In Section 3, we discuss future research works in Helios
The tourism industry is among the top three industries worldwide in terms of volume and economic significance. Many countries depend part of their annual GDP on tourism and especially Europe which is the major destination of worldwide tourism. According to World Tourism Organization (WTO) [1] Europe holds more that half of the global tourism traffic. The structure of tourism industry includes a lot of different involved actors and the value chains developed are in general long and complex. This is due to the special attributes of the tourist product which  involve a lot of decisions with limited amount of information at hand. In fact the industry is very  information intensive. On the other hand as mobile commerce evolves; content delivery is  revolutionized by offering content anytime, anywhere. Ubiquitous computing though, did not only offer new channels of information distribution but paved the road for old applications to evolve into a completely new and different service offering. One of these applications that are going to be more and more popular in the coming few years is auctioning of tourism products through mobile devices. Of course, auctioning of tourism products is not a new idea, but the new modes of operation (e-commerce, m-commerce) radically affect both the back-office and the front-desk operations. In this paper we are going to present the solution developed as part of the IST-ESTIA project to support auctioning and reverse auctioning of tourism products and services over  mobile devices, considering various aspects such as special attributes of the product, limitations  of the communication channel, business requirements etc.
The local defect correction (LDC) method is used to solve a convection-diffusion-reaction problem that contains a high activity region in a relatively small part of the domain. The improvement of the solution on a coarse grid is obtained by introducing a correction term computed from a local fine grid solution. This paper studies problems where the high activity region is covered with a rectangular fine grid not aligned with the axes of the global domain. This study shows that the resulting method is less expensive than both a uniform refinement and tensor product grid method.
Self-compression of visible optical pulse in bulk     medium has been  demonstrated taking the advantage of negative group-velocity dispersion of  tilted pulses.
We present an information-theoretic approach to obtain an estimate of the number of bits that can be hidden in compressed image sequences. We show how addition of the message signal in a suitable transform domain rather than the spatial domain can significantly increase the data hiding capacity. We compare the data hiding capacities achievable with different block transforms and show that the choice of the transform should depend on the robustness needed. While it is better to choose transforms with good energy compaction property (like DCT, Wavelet etc.) when the robustness required is low, transforms with poorer energy compaction property (like Hadamard or Hartley transform) are preferable choices for higher robustness requirements. 1. INTRODUCTION Data hiding or Steganography, is a rapidly growing field with potential applications for copyright protection (watermarking), hiding executables (e.g., for access control of digital multimedia data), embedded captioning, secret communica...
To query XML data over the Web, query engines need to be able to resolve semantic differences between heterogeneous attributes that are conceptually similar. This demo presents a mapping tool and method to resolve semantic heterogeneity at the schema and value levels for data sets that are part of a Web-based information system. The mapping tool automatically produces agreement files. We enhanced a base prototype XML Web query system to include an ontology subsystem that generates subqueries using the agreement information. Other contributions include the use of minimal metadata to locate data sets, a formal language construct to support query re-write called a GeoSpace, and post-query aggregate statistics and spatial display.
We propose DR-nets, Data-Reconstruction networks, to construct massively parallel disk systems with large capacity, wide bandwidth and high reliability. Each node of a DR-net has disks, and is connected by links to form an interconnection network. To realize the high reliability, nodes in a sub-network of the interconnection network organize a group of parity calculation proposed for RAIDs. Inter -node communication for calculating parity keeps the locality of data transfer in DR-nets, and it inhibits bottlenecks from occurring, even if the size of the network becomes very large. Overlapped two types of parity groups on the network make the system able to handle multiple disk-drive failures. A 5  5 torus DR-net recovers data 100% with two damaged disk drives located in any place, 95% with four damaged drives, and can recover with up to nine damaged drives.
Mobile IP is the dominant Internet standard for the provision  of Internet mobility support. However, Mobile IP does not  provide any considerations for simultaneous use of multiple  points of attachment. This feature is considered very important  for Internet real-time communications. Filters for Mobile IP  Bindings is a protocol extension of Mobile IP that allows  mobile nodes to associate filters with simultaneous bindings  for a single home IP address. Flows that match a filter are  tunnelled to the point of attachment indicated by the binding.
This report presents the results of 2 full-scale fire resistance tests conducted on load-bearing gypsum board protected, wood stud shear wall assemblies. The wall assembly had an asymmetrical installation (1x2) with a layer of gypsum board on both exposed and unexposed sides and a shear membrane as a base layer on the unexposed side of the wood stud frame. The gypsum board used was 12.7 mm thick Type X and the shear membranes used were of 12.7 mm and 8.5 mm thick plywood. The insulation used were of cellulose and glass fibre type. These two tests were conducted to determine the effect of insulation and of shear membrane thickness on the fire resistance of gypsum board protected, wood stud shear wall assemblies. A discussion of these results in relation to other similar tests that were conducted previously, are presented in the paper. i WOOD STUD SHEAR WALLS V. K. R. Kodur, M. A. Sultan, J.C.Latour, P. Leroux and R.C.Monette ACKNOWLEDGEMENTS This research is part of a consortium project on the fire resistance and sound performance of wall assemblies - Phase II, among the following partners:  . Canadian Wood Council  . Canadian Home Builders Association  . Canadian Sheet Steel Building Institute  . Forintek Canada Corp.
Introduction Fewer than twenty years after the invention of the steam engine in 1785, the first steam driven locomotives were constructed. During the first decades of the nineteenth century, rail traffic increased drastically, especially in coal mining, and for freight transportation. However, the railroad companies quickly started public passenger transportation. In the 1830&apos;s, most European railroads were already being built in order to transport passengers. For the efficient use of the new technology in this early age of railroad systems, it was soon necessary to develop detailed plans for the schedules of the public transportation services. The planning process started with the definition of the routes and lines of the railroad network. To attract customers, regularly serviced routes connecting stations had to be guaranteed. Assignment and dispatch of locomotives, railroad carriages, and personnel became more and more involved. These problems of the early ages of rail traf
The goal of the presentation is to give an overview about how to build a commodity  PC based GNU/Linux cluster for High Performance Computing (HPC) in  a research environment. Due to the extreme flexibility of the GNU/Linux operating  system and the large variety of hardware components, building a cluster for  High Performance Computing (HPC) is still a challenge in many cases. At the  Division of Intelligent Systems at the Norwegian University of Science and Technology  (NTNU), we have build a 40 node HPC cluster for research purposes using  the source-based GNU/Linux distribution Source Mage.
We report new spectroscopic observations of the recently discovered transiting  planet OGLE-TR-56b with the Keck/HIRES instrument. Our radial velocity  measurements with errors of 100 m s    show clear variations that are in excellent  agreement with the phasing (period and epoch) derived from the OGLE  transit photometry, con  rming the planetary nature of the companion. The  new data combined with measurements from the previous season allow an improved  determination of the mass of the planet, M p = 1:45  0:23 M Jup . All  available OGLE photometry, including new measurements made this season,  have also been analyzed to derive an improved value for the planetary radius  of R p = 1:23  0:16 R Jup . We discuss the implications of these results for the  theory of extrasolar planets.
In this note, we give a 1.47-approximation algorithm for the preemptive scheduling of jobs  with release dates on a single machine so as to minimize the weighted sum of job completion  times; this problem is denoted by 1jr j ; pmtnj    j w j C j in the notation of Lawler et al. [6]. Our  result improves on a 2-approximation algorithm due to Hall, Schulz, Shmoys and Wein [4], and  also yields an improved bound on the quality of a well-known linear programming relaxation of  the problem.
The University of Michigan Business School went through a substantial redesign in 2002-2003 of their site of over 3,000 pages, with a focus on increased ease of use, clean, professional design, improved branding, reduction of Web sprawl, and integration of their public site with their intranet, called  iMpact. The site serves a range of users with varying needs, including faculty, students, staff, alumni, prospective students, media, and recruiters. Our redesign process was grounded in a metrics-based user testing approach that set targets for various measurements, such as task completion rate, and compared these to users performance on their previous site and the sites of their competitors. Task completion rate, for instance, went from 61.7% on their former site to 92.5% on the final redesigned site. Continuous testing provided detailed feedback, and measurement enabled us to reduce project risks while demonstrating substantial improvements versus competitors Web sites. Keywords Brand...
In this paper we show that, paradoxically, what looks like  a &quot;universal improvement&quot; or a &quot;straight-forward improvement&quot; which  enables better security and better reliability on a theoretical level, may  in fact, within certain operational contexts, introduce new exposures  and attacks, resulting in a weaker operational cryptosystem. We demonstrate  a number of such dangerous &quot;improvements&quot;. This implies that  careful considerations should be given to the fact that an implemented  cryptosystem exists within certain operational environments (which may  enable certain types of tampering and other observed information channels  via faults, side-channel attacks or behavior of system operators).
A new potential smoothing method, the shifted-tophat (or  stophat) is presented. This method uses a tophat function as the  smoothing kernel, instead of the gaussian used in conventional methods.
Current developments in digital image coding tend to involve more and more complex algorithms, and require therefore an increasing amount of computation. To improve the overall system performance, some schemes apply a different coding algorithms to separate parts of an image according to the content of this subimage. Such schemes are referred to as dynamic coding schemes. Applying the best suited coding algorithm to a part of an image will lead to an improved coding quality, but implies an algorithm selection phase. Current selection methods require the computation of the reconstructed image after coding and decoding with all the selected algorithms in order to choose the best method. Some other schemes use ways of pruning the search in the algorithm space. Both approaches suffer from a heavy computational load. Furthermore, the computational complexity is increased even more if the parameters have to be adjusted for a given algorithm during the search. This paper describes a way to pr...
For all ane Toda  eld theories we propose a new type of generic boundary  bootstrap equations, which can be viewed as a very speci  c combination of elementary  boundary bootstrap equations. These equations allow to construct generic solutions for  the boundary reection amplitudes, which are valid for theories related to all simple Lie  algebras, that is simply laced and non-simply laced. We provide a detailed study of these  solutions for concrete Lie algebras in various representations.
The combinatorial Dirichlet problem is formulated, and an algorithm for solving it is presented. This provides an eective method for interpolating missing data on weighted graphs of arbitrary connectivity. Image processing examples are shown, and the relation to anisotropic diusion is discussed.
This paper discusses several distributed poweraware routing protocols in wireless ad-hoc networks (especially sensor networks). We seek to optimize the lifetime of the network. We have developed three distributed power-aware algorithms and analyzed their efficiency in terms of the number of message broadcasts and the overall network lifetime modeled as the time to the first message that can not be sent. These are: (1) a distributed min Power algorithm (modeled on a distributed version of Dijkstra&apos;s algorithm), (2) a distributed max-min algorithm, and (3) the distributed version of our the centralized online max-min zP min algorithm presented in [12]. The first two algorithms are used to define the third, although they are very interesting and useful on their own for applications where the optimization criterion is the minimum power, respectively the maximum residual power. The distributed max-min zP min algorithm optimizes the overall lifetime of the network by avoiding nodes of low power, while not using too much total power.
The &quot;schooling industry&quot; is faced with an inescapable demand to redefine its endeavors in terms of producing learning, rather than providing instructions. We propose a hybrid learning model to attract distance as well as traditional students. One of the major components of this model is the web-based learning and interaction. We review the available avenues of distance learning and offer a model that we prepared as an example of web-based distance learning and training. This model combines video-conferencing, interactive classroom, web-based lectures and traditional instructions in an optimized way to achieve the goal of high quality technical education. The availability of lecture notes and other course material on the web makes it a 24x7 hours classroom. Conducting classes via videoconferencing ensures student participation from all locations and optional interactive conventional teaching strengthens studentteacher relationship and reinforces the concepts in the minds of students. Therefore, we believe this model offers one of the best opportunities in web-based distance learning. This paper presents the hybrid learning model and discusses the opportunities and challenges of the web technologies in the education of Computer Science and Engineering. Through this hybrid model we have already successfully offered several courses in the Computer Science program at SUNY Fredonia with students registering from across the New York State.
A complementarity problem with a continuous mapping f from  the n-dimensional Euclidean space R    into itself can be written as the system of  equations  F (xff y)=0 and (xff y)  0:  Here F is the mapping from R    into itself defined by  F (xff y)=(x 1 y 1 ffx 2 y 2 ff \Delta\Delta\Deltaffx n y n ff y ; f(x)) for every (xff y)  0:  Under the assumption that the mapping f is a P 0 -function, we study various aspects  of homotopy continuation methods that trace a trajectory consisting of solutions  of the family of systems of equations  F (xff y)=t(aff b) and (xff y)  0  until the parameter t  0 attains 0. Here (aff b) denotes a 2n-dimensional constant  positive vector. We establish the existence of a trajectory which leads to a solution  of the problem, and then present a numerical method for tracing the trajectory. We also
Evidence suggests that in Iwaija, a non-Pama-Nyungan language of northern Australia, the Intonation Phrase (IP) is an integrated entity, one which is typical of a cursus language (Pulgram,1970) like French, in which words in connected speech give up some of the properties they exhibit in isolation. A salient feature of the prosody of this language is syllabification across content word boundaries. The paper examines the way in which this phenomenon relates to two influential versions of the prosodic hierarchy, both of which suggest that syllabification is subsumed, one way or another, within the boundaries of word-sized units. On the basis of acoustic analysis, the paper suggests that the IP, rather than the Phonological or Prosodic Word may form the domain for syllabification in Iwaija.
Introduction  The use of adaptive antennas on handheld radios is a new area of research. In 1988, Vaughn [9.1] concluded that with then-current technology, adaptive beamforming would work for units moving at pedestrian speeds but would be difficult to implement for high-speed mobile units. No further reports of research in this area during the following ten years were found. In 1999, Braun, et al. [9.2] reported indoor experiments in which data were recorded using a single stationary narrowband transmitter and a two-element handheld antenna array, and processed using diversity and optimum beamforming techniques. The channels measured were primarily non line-of-sight because the transmitter was deliberately obstructed with a large metallic screen. Data were recorded as the receiver was carried along 10 different paths. Two handset prototypes with different antenna configurations were used. One had a monopole and a shorted patch antenna and the other had a monopole and a planar meander l
as a banking application, about three decades ago. Prior to the advent of the relational data model, business data was managed and queried using customized programs/scripts that were developed for each application. Reusing programs, and the algorithms for querying the data, involved rewriting application program and logic, which was very time consuming and expensive. In addition, the querying programs were closely tied to the format that was used to represent the data. Any change in the format of the data representation often would break the querying programs. Furthermore, writing complex queries, such as querying over multiple data sets or posing complex analytical queries, was a daunting task. One of the critical contributions of the relational data model (Codd, 1970) was the introduction of a declarative querying paradigm for business data management, instead of the previously used procedural paradigm. In a declarative querying paradigm, the user expresses the query in a high-level
Defined as the apparent motion in a sequence of images, the optical flow is very important in the Computer Vision community where its accurate estimation is strongly needed for many applications. It is one of the most studied problem in Computer Vision. In spite of this, not much theoretical analysis has been done. In this article, we first present a review of existing variational methods. Then, we will propose an extended model that will be rigorously justified on the space of functions of bounded variations. Finally, we present an algorithm whose convergence will be carefully demonstrated. Some results showing the capabilities of this method will end that work.
infefl0: forlarge circuitswhe compare with that of fault-simulation -base{ staten-base{{{ tea geten-ba such asStrate.L;R 4PropteR.   andSpeC3   In thisarticle we precle ane.C:30; logicsimulation -base tem gemulati thatet.C333 significantly  more quickly than its fault-simulationbase countemulati This tes ge.CflHRR.L fault cove.flC compare favorably with that ofthe latet te.flH{:{. forlarge seeFHR.L circuits.  Motivation and concept  Hard faults gets .CR{ rets . rets .C hardto -reflH{ statefl during teing   TheC;.L:R PomeC;.L and Re .  5deHfl.eR  ate{ ge{fl: ator that aims to drive the fault-fre circuit to as manyne state aspossible Howeble beebl large seeleLH circuitshave manystate reateFF byteCR00{ the faultcove.C: can  Efficient Sequential Test Generation Based on Logic Simulation   A simple and highly efficient logic-simulationbased test generator uses a genetic algorithm to achieve both high fault coverage and short test generation times.  Shuo Sheng  Rutgers University  Miversi 
Detection of tubular structures in 3D images is an important issue for vascular detection in medical  imaging. We present in this paper a new approach for centerline detection and reconstruction of 3D  tubular structures. Several models of vessels are introduced for estimating the sensivity of the image  second order derivatives according to elliptical cross-section, to curvature of the axis, or to partial volume  effects. Our approach uses a multiscale analysis for extracting vessels of different sizes according to the  scale. For a given model of vessel, we derive an analytic expression of the relationship between the radius  of the structure and the scale at which it is detected. The algorithm gives both centerline extraction  and radius estimation of the vessels allowing their reconstruction. The method has been tested on both  synthetic and real images, with encouraging results. This work was done in collaboration with GEMS    .
We present a memory management scheme for Java based on thread-local heaps. Assuming most objects are created and used by a single thread, it is desirable to free the memory manager from redundant synchronization for thread-local objects. Therefore, in our scheme each thread receives a partition of the heap in which it allocates its objects and in which it does local garbage collection without synchronization with other threads. We dynamically monitor to determine which objects are local and which are global. Furthermore, we suggest using profiling to identify allocation sites that almost exclusively allocate global objects, and allocate objects at these sites directly in a global area. We have 
This article presents a series of experiments on concept formation. These experiments compared four different categorisation rules. The rules presume that the stimuli to be classified can be modelled psychologically as points in a dimensional space (Shepard 1987, Grdenfors 1990a, 1990b). Two of the rules are based on the assumption that there are prototypical representatives of a concept, a third rule is a &quot;nearest neighbour&quot; model, and the fourth is based on average distances (Reed 1972), where distances are measured in the dimensional space. Before giving a more precise description of the categorisation rules and the experiments, we will present the general theoretical approach that is adopted
A microscopic description of packet transport in the Internet by using a simple cellular automaton model is presented. A generalised exclusion process is introduced which allows to study travel times of the particles (`data packets&apos;) along a ffxed path in the network. Computer simulations reveal the appearance of a free ffow and a jammed phase separated by a (critical) transition regime. The power spectra are compared with empirical data for the round trip time (RTT) obtained from measurements in the Internet. We ffnd that the model is able to reproduce the characteristic statistical behaviour in agreement with the empirical data for both phases (free ffow and congested). The occurrence of different phases is therefore due to jamming properties and are not related to the structure of the network. Moreover the model shows, as observed in reality, critical behaviour (1=f-noise) for paths with critical load. c    2001 Elsevier Science B.V. All rights reserved.
The focus of this thesis is fast and robust adaptations of logistic regression (LR) for data mining and high-dimensional classification problems. LR is well-understood and widely used in the statistics, machine learning, and data analysis communities. Its benefits include a firm statistical foundation and a probabilistic model useful for ``explaining&apos;&apos; the data. There is a perception that LR is slow, unstable, and unsuitable for large learning or classification tasks. Through fast approximate numerical methods, regularization to avoid numerical instability, and an efficient implementation we will show that LR can outperform modern algorithms like Support Vector Machines (SVM) on a variety of learning tasks. Our novel implementation, which uses a modified iteratively re-weighted least squares estimation procedure, can compute model parameters for sparse binary datasets with hundreds of thousands of rows and attributes, and millions or tens of millions of nonzero elements in just a few seconds. Our implementation also handles real-valued dense datasets of similar size.
This paper presents our research on devising a dependability assessment method for the upcoming OGSA 3.0 middleware using network level fault injection. We compare existing DCE middleware dependability testing research with the requirements of testing OGSA middleware and derive a new method and fault model for testing OGSA middleware. From this we have implemented an extendable fault injector framework and undertaken some small proof of concept experiments with a simulated OGSA middleware system based around Apache SOAP and Apache Tomcat. We also present results from our initial experiments, which uncovered a discrepancy with our simulated OGSA system. We finally detail future research, including plans to adapt this fault injector framework from the stateless environment of a standard web service to the statefull environment of an OGSA service. 
This paper describes an exercise in object-oriented modelling  where Formal Concept Analysis is applied to a formal specification case  study using Object-Z. In particular, the informal description from the  case study is treated as a set of use-cases from which candidate classes  and objects are derived. The resulting class structure is contrasted with  the existing Object-Z design and the two approaches are discussed.
This paper describes an algorithm which can be used to detect and measure device setup errors in radiotherapy without the need of human interaction. The algorithm operates automatically with an accurance that is limited by image quality and processing speed only, because the required computation power is still quite huge for most of today&apos;s low and medium level computers
Available grid technologies like the Globus Toolkit [18] make possible for one to run a parallel application on resources distributed across several administrative domains. Most grid computing users, however, don&apos;t have access to more than a handful of resources onto which they can use this technologies. This happens mainly because gaining access to resources still depends on personal negotiations between the user and each resource owner of resources. To address this problem, we are developing the OurGrid resources sharing system, a peer-to-peer network of sites that share resources equitably in order to form a grid to which they all have access. The resources are shared accordingly to a network of favors model, in which each peer prioritizes those who have credit in their past history of interactions. The emergent behavior in the system is that peers that contribute more to the community are prioritized when they request resources. We expect, with OurGrid, to solve the access gaining problem for users of bag-of-tasks applications (those parallel applications whose tasks are independent).
Include file for user and kernel space. * * Environment: * Kernel and user modes * * Revision History: * *--------------------------------------------------------------- */ //This is the maximum buffer length required for DMA transfers. //Since this is the amount of memory that will actually be //allocated and since it does need to be contiguous, please make //sure that you do not allocate more memory than you need. //Version 1.0 of the driver requires that this option be changed //at compile time. ffdefine PPCIDMA_MAX_DMA_BUFFER_LENGTH 4000 * 1024 //The vendor ID and device ID of the PCI device ffdefine PCIDMA_VENDORID 0x010e8 ffdefine PCIDMA_DEVICEID 0x04750 /* * Define the various device type values. Note that values used * by Microsoft Corporation are in the range 0-32767, and 32768- * 65535 are reserved for use by customers. */ ffdefine FILE_DEVICE_SKELETON 0x0000CBFC /* * Macro definition for defining IOCTL and FSCTL function control * codes. Note that function codes 0-2047 are reserved for * Microsoft Corporation, and 2048-4095 are reserved for * customers. */ ffdefine SKELETON_IOCTL_BASE 0x800  PPCIDMA_IOCTL.H 175  /* StartIo subroutine fields. Used by QueryVideo() */ KDPC StartIoDpc; //Used by the query video routine KTIMER StartIoTimer; //Timer used in StartIo subroutine ULONG StartIoState; //Current state of StartIo subroutine PVOID StartIoBuffer; //Current buffer we are using /* Some DMA related fields */ PADAPTER_OBJECT AdaptorObj; //Pointer to DMA adaptor object ULONG DmaMapRegisters; //Number of DMA map registers PVOID MapRegisterBase; //DMA map register base //DMA buffer related fields PVOID VirtualAddress; //Virtual address of MDL for DMA //transfer, return from //HalAllocateCommonBuffer ULONG BufferLength; //Length of the DMA buffer PHYSICAL_ADDRESS Log...
Airline crew scheduling gives rise to many difficult and interesting  optimisation problems. With straightforward approaches to the crew  pairing problem one specifically needs to solve very large set-covering  problems. One suggested approach to these problems is column generation.
In this paper we present a concept for automated testing of object-oriented applications and a tool called SeDiTeC that implements these concepts for Java applications. SeDiTeC uses UML sequence diagrams, that are complemented by test case data sets consisting of parameters and return values for the method calls, as test specification and therefore can easily be integrated into the development process as soon as the design phase starts. SeDiTeC supports specification of several test case data sets for each sequence diagram as well as to combine several sequence diagrams to so-called combined sequence diagrams thus reducing the number of diagrams needed. For classes and their methods whose behavior is specified in sequence diagrams and the corresponding test case data sets SeDiTeC can automatically generate test stubs thus enabling testing right from the beginning of the implementation phase. Validation is not restricted to comparing the test case data sets with the observed data, but can also include validation of pre- and postconditions.
We study the speed of convergence of the explicit and implicit space-time discretization schemes of the solution u(t, x) to a parabolic partial differential equation in any dimension perturbed by a space-correlated Gaussian noise. The coeffcients only depend on u(t, x)  and the influence of the correlation on the speed is observed; in the limit case, corresponding to the space-time white noise in dimension 1, we recover the speeds obtained by I. Gyöngy.
In this paper, a simple and efficient solution for combining shear-warp volume rendering and the hardware graphics pipeline is presented. The approach applies an inverse warp transformation to the Z-Buffer, containing the rendered geometry. This information is used for combining geometry and volume data during compositing. We present applications of this concept which include hybrid volume rendering, i.e., concurrent rendering of polygonal objects and volume data, and volume clipping on convex clipping regions. Furthermore, it can be used to efficiently define regions with different rendering modes and transfer functions for focus+context volume rendering. Empirical results show that the approach has very low impact on performance. 1 
this paper, we address the question how to create low-dimensional PDA map using information about the limb in the external space instead of full description in the configurational space. It will be argued that for configurational modulation a simple low-dimensional representation may satisfy the constraints of speed-field tracking. The novelty of our computational scheme is in the low-dimensional external space description that can be viewed as a gelatinous medium surrounding the limb. Computer simulations on multi-segment robotic arm will be used to demonstrate the idea. This solution to the control problem can be scaled. We shall argue that configurational modulation required for the low dimensional description may correspond to the clustering property of motor experience data
Domains and metric spaces are two central tools for the study of denotational semantics in computer science, but are otherwise very different in many fundamental aspects. A construction that tries to establish links between both paradigms is the space of formal balls, a continuous poset which can be defined for every metric space and that reflects many of its properties. On the other hand, in order to obtain a broader framework for applications and possible connections to domain theory, generalized ultrametric spaces (gums) have been introduced. In this paper, we employ the space of formal balls as a tool for studying these more general metrics by using concepts and results from domain theory. It turns out that many properties of the metric can be characterized by conditions on its formal-ball space. Furthermore,
A good contract is often the key to a successful IT outsourcing relationship. The contract defines the rights, liability, and expectations of both the outsourcing vendor and the outsourcing customer concerned and is often the only solid mechanism for regulating the relationship of the parties. Outsourcing contracts are often of high value and last a relatively long time. It is therefore of particular importance to get them right first time. Research on IT outsourcing contractual issues tends to be mostly theoretical and very limited in scope. There is relatively little literature available on the systematic and practical treatment of issues relating to IT outsourcing contracts. Lacity and Hirschheim&apos;s seminal work (1993) represents a major step in that direction, offering important lessons learnt in contract negotiation. This article builds further on that direction by presenting a practical and systematic overview of some key IT outsourcing contractual issues, exploring and highlighting management implications where appropriate. Issues such as service level, transfer of assets, staffing, pricing and payment, warranty and liability, dispute resolution mechanism, termination, intellectual property matters, and information security are discussed in this article. Practical advice on pre-contractual negotiation and postcontractual management is also given. By discussing these issues systematically from a management and practical perspective, this article contributes to bridging the gap between theory and practice and offers useful information to management considering IT outsourcing.
Traditional measures of species diversity and spatially-explicit measures of landscape diversity (derived from Romme 1982) are used to compare biotic diversity in six landscapes across Georgia, USA; two each from the Appalachian Highlands, Piedmont, and Coastal Plain. Species richness and evenness of plots generally increased from the Coastal Plain to the Appalachian Highlands. Community richness, landscape contrast, and landscape heterogeneity increased from the Appalachian Highlands to the Coastal Plain, opposite the gradient of topographic complexity. Coastal Plain landscapes possessed greater contrast and heterogeneity than landscapes in the other two physiographic provinces. This high level of landscape diversity is interpreted as a response to two factors: the increased role of human activity in shaping landscape structure, and the increased range of soil moisture regimes encountered in the sand-rich substrates of the Coastal Plain (from permanently flooded hydric communities to well drained xeric uplands only a few meters higher in elevation). 1. 
We consider a cluster architecture in which dynamic content is generated by a database back-end and a collection of Web and application server front-ends.
Revising beliefs given new information has long been a problem in artificial intelligence (AI). Much of this work has assumed little about the source of the information. However, such pedigree information is often readily accessible and useful in determining how to incorporate the new input. We should treat very dierently the information &quot;It is raining&quot; when we receive it from the weather reporter, from a notorious liar, or from our own eyes. The goal of this work is to enable the use of such pedigree information in semantically-justified ways. One source
We present a general wavelet-based denoising scheme for functional magnetic resonance imaging (fMRI) data and compare it to Gaussian smoothing, the traditional denoising method used in fMRI analysis. One-dimensional WaveLab thresholding routines were adapted to two-dimensional images, and applied to 2D wavelet coefficients. To test the effect of these methods on the signal-to-noise ratio (SNR), we compared the SNR of 2D fMRI images before and after denoising, using both Gaussian smoothing and wavelet-based methods. We simulated a fMRI series with a time signal in an active spot, and tested the methods on noisy copies of it. The denoising methods were evaluated in two ways: by the average temporal SNR inside the original activated spot, and by the shape of the spot detected by thresholding the temporal SNR maps. Denoising methods that introduce much smoothness are better suited for low SNRs, but for images of reasonable quality they are not preferable, because they introduce heavy deformations. Wavelet-based denoising methods that introduce less smoothing preserve the sharpness of the images and retain the original shapes of active regions. We also performed statistical parametric mapping (SPM) on the denoised simulated time series, as well as on a real fMRI data set. False discovery rate control was used to correct for multiple comparisons. The results show that the methods that produce smooth images introduce more false positives. The less smoothing wavelet-based methods, although generating more false negatives, produce a smaller total number of errors than Gaussian smoothing or wavelet-based methods with a large smoothing effect.
A multi-level interior penalty method is used as an efficient preconditioner for the Schur complement of the local discontinuous Galerkin (LDG) discretization of Poisson&apos;s problem. Then, this is used in a block-triangular preconditioner of the LDG saddle point system. The block preconditioner is of the same eciency as the Schur complement version. Finally, the block preconditioner is extended to the discretization of Stokes&apos; problem by the LDG method. Again, the preconditioned saddle point problem can be solved in about as many steps as the Schur complement. The influence of several parameters on the performance of these methods is investigated.
Current approaches to agent system design are generally goaldriven.
There are known methods for discovering pure linear measurement models given as input just the joint distribution of the observed variables and assumptions about the number of pure indicators per latent and normality. This paper extends this approach by describing a variation that is more computationally ecient, as well as providing a proof of its correctness.
Software product families are an emerging and important  area of software engineering, whereas product  configuration of traditional products, i.e.,  mechanical and electronic, has a slightly longer history  as a specific area of research and business. This  paper presents a preliminary comparison of concepts  for modelling variety and evolution in both fields.
This paper introduces Quantian, a scientific computing environment. Quantian  is directly bootable from cdrom, self-configuring without user intervention,  and comprises hundreds of applications, including a significant number of  programs that are of immediate interest to quantitative researchers. Quantian  is available at http://dirk.eddelbuettel.com/quantian.html.
Speed and availability of information, delivered in past years by Internet technologies, made it easier for any company to outsource primary activities, which resulted in unbundling of many companies&apos; traditional value constellations into networks of different companies. In the electricity power sector decomposition of the value constellation is not only enabled by technological advances such as smallscale electricity generation devices and Internet-based interconnections, but it is also enforced by government regulations and crucial ecological issues like CO2 reduction. In this paper
In this paper, a novel data--aided stochastic gradient algorithm for adjustment of the widely linear (WL) minimum mean--squared error (MMSE) filter for multiple access interference (MAI) suppression for direct--sequence code--division multiple access (DS--CDMA) is introduced and analyzed. We give analytical expressions for the steady--state signal--to--interference-- plus--noise ratio (SINR) of the proposed WL least--mean--square (LMS) algorithm, and we also investigate its speed of convergence. Both analytical considerations and simulations show in good agreement  the superiority of the novel WL adaptive algorithm. Nevertheless, the computational complexity of the proposed algorithm is lower than that of the linear LMS algorithm.
In general, graph search can be described in terms of a sequence of searchers&apos; moves on a graph, able to capture a fugitive resorting on its vertices/edges. Several variations of graph search have been introduced, differing on the abilities of the fugitive as well as of the search. In this paper, we examine the case where the fugitive is inert, i.e., it moves only whenever the search is about to capture it. Mainly, there are two variants for &quot;clearing&quot; an edge during a search: when a sliding of a searcher occurs along the edge or when both its endpoints are simultaneously occupied by searchers. These variants define the inert edge search and the inert node search respectively. A third search variant, the inert mixed search, is defined when both ways of clearing an edge are possible. As we show, inert search and inert mixed search are equivalent (surprisingly, this is not the case if we discard the inertness property). Moreover, we prove that, in any case, by restricting the searches to only those that always reduce further the fugitive&apos;s possible resorts, does not give any advantage to the fugitive (this monotonicity property is usually expressed as: &quot;recontamination does not help&quot;). So far, the only monotonicity result on inert search concerns inert node search and our results yield a much simpler proof of that result as well. Furthermore, we define a new graph-theoretic parameter, the proper-treewidth, in analogy to the parameter proper-pathwidth, and prove it equivalent to the inert mixed search game. Last, we prove that proper-treewidth, in turn, is equivalent to a known graph theoretic parameter related to treewidth.
In this paper we have looked at two issues. We have reconsidered some of the evidence on the growth-finance nexus, taking into account both different sources of growth and different national characteristics. Secondly, we have considered the implications of different growth-finance models in for the process of EU integration, as defined with respect to the targets set at the Lisbon summit, in March 2000. Our estimation results show that: a) finance affects growth though different channels (GDP, investment, productivity, technology) all of which are relevant in EU integration; b) EU membership has played a role in boosting growth through productivity enhancement; c) Both banks and markets have an impact on growth; d) The rise of an innovation related bubble at the end of the 80&apos;s has increased the importance of market based finance in boosting technology driven growth, but credit finance has maintained a significant role in supporting investment driven growth (which may be associated in part with enhanced process innovation, itself related to IT); e) While there is evidence of similar growth finance relations across countries the growth-finance nexus is far from homogeneous. National specificities matter both because growth is driven by different factors with different intensity in different countries and because the relative weight of credit and market finance varies across countries. f) In general market finance is more relevant in countries where technology driven growth is more important. 
We have developed a 3D sound system for headphones that allows real-time sound source and user displacement in a virtual acoustic environment. Because of a flexible design that uses different sets of pre-selected, physically modeled filters, the complexity level of the simulation can be chosen, making the system adaptable both to available CPU power and to application requirements. No extensive signal processing knowledge is required in order to select the appropriate simulation complexity. A preliminary evaluation involving 4 users showed that the system provides a satisfying localization of sounds and users (even with limited memory and CPU power) while also giving access to low-level control over simulation complexity.
We introduce a characterisation of probabilistic transition  systems (PTS) in terms of linear operators on some suitably de  ned  vector space representing the set of states. Various notions of process  equivalences can then be re-formulated as abstract linear operators related  to the concrete PTS semantics via a probabilistic abstract interpretation.
User-driven innovation regards users as a resource in the innovation process. Taking prototypes of novel technology as a starting point, a dialogue with users becomes a springboard to generate new ideas. The user group is often highly specialized, and not necessarily the intended end users of the technology. The Future Applications Lab has successfully used this approach in several recent projects. In Pin&amp;Play, we pushed the development of novel surfacebased networking in collaboration with the staff of a film festival. In context photography we engaged a group of photographers with a unique outlook on the process of picture taking.
An effcient technique to solve precision problems consists in using exact computations. For geometric predicates, using systematically expensive exact computations can be avoided by the use of fflters. The predicate is ffrst evaluated using rounding computations, and an error estimation gives a certiffcate of the validityof the result. In this note, we studies the statistical effciency of fflters for cosphericity predicate with an assumption of regular distribution of the points.
Due to the complexity and sophistication of the skills needed in real world tasks, the development of autonomous robot controllers requires an ever increasing application of learning techniques. To date, however, learning steps are mainly executed in isolation and only the learned code pieces become part of the controller. This approach has several drawbacks: the learning steps themselves are undocumented and not executable. In this paper, we extend an existing control language with constructs for specifying control tasks, process models, learning problems, exploration strategies, etc. Using these constructs, the learning problems can be represented explicitly and transparently and, as they are part of the overall program implementation, become executable. With the extended language we rationally reconstruct large parts of the action selection module of the AGILO2001 autonomous soccer robots.
This paper contains preliminary observations concerning the properties of the intonational phrase, the nuclear melody, and pauses in semi-spontaneous texts of short narratives in Polish. 1. 
In this paper, we introduce the value-and-criterion filter structure and give an example of a filter with the structure. The value-and-criterion filter structure is based on morphological opening (or closing), which is actually two filters applied sequentially: the first assigns values based on the original image values, and the second assigns  values based on the results of the first. The value-and-criterion structure is similar, but includes an additional step  in parallel to the first that computes a different set of values to use as criteria for selecting a final value.
Programmable shading is a common technique for production animation, but interactive programmable shading is not yet widely available. We support interactive programmable shading on virtually any 3D graphics hardware using a scene graph library on top of OpenGL. We treat the OpenGL architecture as a general SIMD computer, and translate the high-level shading description into OpenGL rendering passes. While our system uses OpenGL, the techniques described are applicable to any retained mode interface with appropriate extension mechanisms and hardware API with provisions for recirculating data through the graphics pipeline. We present two demonstrations of the method. The first is a constrained shading language that runs on graphics hardware supporting OpenGL 1.2 with a subset of the ARB imaging extensions. We remove the shading language constraints by minimally extending OpenGL. The key extensions are color range (supporting extended range and precision data types) and pixel texture (using framebuffer values as indices into texture maps). Our second demonstration is a renderer supporting the RenderMan Interface and RenderMan Shading Language on a software implementation of this extended OpenGL. For both languages, our compiler technology can take advantage of extensions and performance characteristics unique to any particular graphics hardware.
This paper studies lower bounds for classical multicolor Ramsey numbers, first  by giving a short overview of past results, and then by presenting several general  constructions establishing new lower bounds for many diagonal and off-diagonal  multicolor Ramsey numbers. In particular, we improve several lower bounds for  R k (4) and R k (5) for some small k, including 415  R 3 (5), 634  R 4 (4), 2721   R 4 (5), 3416  R 5 (4) and 26082  R 5 (5). Most of the new lower bounds are  consequences of general constructions.
This paper presents an integrated systolic array design for implementing full-search block matching, 2-D discrete wavelet transform, and full-search vector quantization on the same VLSI architecture. The above three functions are prime essential but with large amount of computation in video compression. To meet real-time requirement, many systolic arrays are designed for individually performing each of them. In fact, these functions contain the similar computational procedure. The matrix-vector product forms of them are quite similar. In this paper, with carefully extracting the common computation component, an integrated systolic array design that can perform above three functions is presented. A utilization of 100% to 97% is achieved for executing full-search block matching and full-search vector quantization. When performing 2-D discrete wavelet transform, the utilization is about 32%. The proposed integrated architecture spends lower hardware cost and has a regular hardware structure. It befits the VLSI implementation for video compression.
Search tree data structures like van Emde Boas (vEB) trees are a theoretically attractive  alternative to comparison based search trees because they have better asymptotic  performance for small integer keys and large inputs. This paper studies their  practicability using 32 bit keys as an example. While direct implementations of vEBtrees  cannot compete with good implementations of comparison based data structures,  our tuned data structure signi  cantly outperforms comparison based implementations  for searching and shows at least comparable performance for insertion and deletion.
Expansion Postponement is a tantalisingly simple conjecture about Pure Type Systems, which has so far resisted all attempts to prove it for any interesting class of systems. We prove the property for all normalising Pure Type Systems, and discuss the connection with typechecking.
A recent frequency-domain, subspace-based algorithm is used in the identification of two power transformers. The results indicate that the subspace-based identification algorithms can be used without modification even when the dynamic range of frequency response data is large.  Key words: Identification, frequency response data, power transformer, subspace-based algorithm.  I. Introduction  Frequency response methods are often used in practice to obtain a nonparametric model of a linear system. This identification may be performed without significant a priori  knowledge of the plant. Further, if the excitation of the system is well-designed, e.g., periodic input or stepped sine, each transfer function measurement, compiled from a large number of time-domain measurements, is of high quality. Also, data obtained from different experiments can easily be combined in the frequency domain. The problem of fitting a real-rational model to a given frequency response data set has been addressed ...
Simultaneous multithreading (SMT) improves processor throughput by processing instructions from multiple threads each cycle. This is the first work to explore soft real-time scheduling on an SMT processor. Scheduling with SMT requires two decisions: (1) which threads to run simultaneously (the co-schedule), and (2) how to share processor resources among co-scheduled threads. We explore algorithms for both for soft-real time multimedia applications, focusing more on co-schedule selection. We examine previous multiprocessor co-scheduling algorithms, including partitioning and global scheduling. We propose new variations that consider resource sharing and try to utilize SMT more effectively by exploiting application symbiosis.We find (using simulation) that the best algorithm uses global scheduling, exploits symbiosis, prioritizes high utilization tasks, and uses dynamic resource sharing. This algorithm, however, imposes significant profiling overhead and does not provide admission control. We propose alternatives to overcome these limitations, but at the cost of schedulability. 1
Today&apos;s Palm computers face practical restrictions when playing complex games. This article deals with both the functional as well as the hardware limitations of a Palm computer and their impact on game design. It is observed that the designers of complex games refuse to take sufficiently into account the imposed limitations. Seven design problems are identified and examples of these problems in actual games are given, including ways of solving them. Consequently we propose a list of five design rules which may alleviate the problems. Finally, we conclude that now the time is ripe for one-person Palm game businesses. We expect this to last only a few years.
The article addresses the technical principles of a new high-performance haptic device, called HapticMaster, and gives an overview of its haptic performance. The HapticMaster utilizes the admittance control paradigm, which facilitates high stiffness, large forces, and a high force sensitivity. On top of that the HapticMaster has a large workspace, and a huge haptic resolution. Typical applications for the HapticMaster are therefore found in virtual reality, haptics research, and rehabilitation.
Automatic word segmentation is a basic requirement  for unsupervised learning in morphological  analysis. In this paper, we formulate a  novel recursive method for minimum description  length (MDL) word segmentation, whose  basic operation is resegmenting the corpus on  a prefix (equivalently, a suffix). We derive a  local expression for the change in description  length under resegmentation, i.e., one which depends  only on properties of the specific prefix  (not on the rest of the corpus). Such a formulation  permits use of a new and efficient algorithm  for greedy morphological segmentation of the  corpus in a recursive manner. In particular, our  method does not restrict words to be segmented  only once, into a stem+affix form, as do many  extant techniques. Early results for English and  Turkish corpora are promising.
The objective of this work is to provide a simple and yet efficient tool to detect human faces in video sequences. This information can be very useful for many applications such as video indexing and video browsing. In particular the paper will focus on the significant improvements made to our face detection algorithm presented in [1]. Specifically, a novel approach to retrieve skin-like homogeneous regions will be presented, which will be later used to retrieve face images. Good results have been obtained for a large variety of video sequences.
Volume illustration is a developing trend in volume visualization, focused on conveying volume information effectively by enhancing interesting features of the volume and omitting insignificant data. However, the calculations involved have limited the illustration process to noninteractive rendering. We have developed a new interactive volume illustration system (IVIS) that harnesses the power of programmable graphics processors, and includes a novel approach for feature halo enhancement. This interactive illustration system is a powerful tool for exploration and analysis of volumetric datasets.
This thesis describes how to add first-class generic types---including mixins---to strongly-typed object-oriented languages with nominal subtyping such as Java and Cff. A generic type system is &quot;first-class&quot; if generic types can appear in any context where conventional types can appear. In this context, a mixin is simply a generic class that extends one of its type parameters, e.g., a class C&lt;T&gt; that extends T. Although mixins of this form are widely used in C++ (via templates), they are clumsy and error-prone because C++ treats mixins as syntactic abbreviations (macros), forcing each mixin instantiation to be separately compiled and type-checked. The abstraction embodied in a mixin is never separately analyzed.
Compatible meshes are isomorphic meshing of the interiors of two polygons having a correspondence between their vertices. Compatible meshing may be used for constructing sweeps, suitable for finite element analysis, between two base polygons. They may also be used for meshing a given sequence of polygons forming a sweep. We present a method to compute compatible triangulations of planar polygons with a very small number of Steiner (interior) vertices. Being close to optimal in terms of the number of Steiner vertices, these compatible triangulations are usually not of high quality, i.e., do not have well-shaped triangles. We show how to increase the quality of these triangulations by adding Steiner vertices in a compatible manner, using several novel techniques for remeshing and mesh smoothing. The total scheme results in high-quality compatible meshes with a small number of triangles. These meshes may then be morphed to obtain the intermediate triangulated sections of a sweep, if needed.
The distribution of center self-embeddings and extrapositions in German is assumed  to reflect a universal performance strategy of minimizing memory load during  parsing. Self-embedded relative clauses of embedding depth 2 were semi-automatically  analysed in a treebank of German newspaper texts. Clause length and especially extraposition  distance are found as the main distinctive parameters between center  embeddings and extrapositions.
File system consistency frequently involves a choice between raw performance and integrity guarantees. A few software-based solutions for this problem have appeared and are currently being used on some commercial operating systems; these include log-structured file systems, journaling file systems, and soft updates. In this paper, we propose meta-data snapshotting as a low-cost, scalable, and simple mechanism that provides file system integrity. It allows the safe use of write-back caching by making successive snapshots of the meta-data using copy-onwrite, and atomically committing the snapshot to stable storage without interrupting file system availability. In the presence of system failures, no file system checker or any other operation is necessary to mount the file system, therefore it greatly improves system availability. This paper describes meta-data snapshotting, and its incorporation into a file system available for the Linux and K42 operating systems. We show that metadata snapshotting has low overhead: for a microbenchmark, and two macrobenchmarks, the measured overhead is of at most 4%, when compared to a completely asynchronous file system, with no consistency guarantees. Our experiments also show that it induces less overhead then a write-ahead journaling file system, and it scales much better when the number of clients and file system operations grows.
Service composition in pervasive environments enables users to utilize services in the environment to solve complex queries. Current work in development of service composition architectures focuses on wired-networked environments where solutions are centralized and tailored towards a reliable network and fixed service topology. In this paper, we present an alternate and novel design architecture of a broker-based distributed service composition protocol for pervasive environments. We present simulation results by comparing our protocol to a centralized architecture for composition. Results show that our distributed broker-based composition architecture perform better than the centralized solution in terms of composition efficiency, broker arbitration efficiency and composition radius.
The use of feedback control techniques has been gaining importance  in real-time scheduling as a means to provide predictable performance in the face  of uncertain workload. In this paper, we propose and analyze a feedback scheduling  algorithm, called double-loop feedback scheduler, for distributed real-time  systems, whose objective is to keep the deadline miss ratio near the desired value  and achieve high CPU utilization. This objective is achieved by an integrated design  of a local and a global feedback scheduler. We provide the stability analysis  of the double-loop system. We also carry out extensive simulation studies to evaluate  the performance and stability of the proposed double-loop scheduler. Our  studies show that the proposed scheduler achieves high CPU utilization with low  miss ratio and stays in steady state after a step change in workload, characterized  by change in actual execution time of tasks.
With the recent advent of the DNA microarray technology [1], methods for organising and analysing gene expression data are rapidly emerging. Several references can be found in the literature in the two basic approaches for dealing with the problem of organising gene expression data: clustering procedures and ordering methods. Clustering procedures define groups of genes with similar expression patterns, while ordering methods have the goal of finding the best linear order for gene expression data considering a distance criterion. In this sense, the ordering problem becomes equivalent to the travelling salesman problem (TSP). Several clustering techniques have been applied to gene expression data. In spite of their early successes, these methods present a series of limitations, and may not be appropriate for certain configurations of data. Most clustering techniques require the number of clusters to be defined a priori, and often assume a predefined structure for the data with well-defined clusters. Besides, the data set to be analysed may present a distribution of points in the space that does not admit an inherent separation of its elements. In these situations, there is no natural cluster in the data, and clustering techniques may provide biased results. Among the clustering methods, the most popular is hierarchical clustering [5]. A few strategies have been proposed to enhance the solutions obtained by hierarchical clustering procedures, such as reordering the leaves of the generated hierarchical tree [2].
In Floudas and Visweswaran (1990), a new global optimization algorithm (GOP) was proposed for solving constrained nonconvex problems involving quadratic and polynomial functions in the objective function and/or constraints. In this paper, the application of this algorithm to the special case of polynomial functions of one variable is discussed. The special nature of polynomial functions enables considerable simplification of the GOP algorithm. The primal problem is shown to reduce to a simple function evaluation, while the relaxed dual problem is equivalent to the simultaneous solution of two linear equations in two variables. In addition, the one-to-one correspondence between the x and y variables in the problem enables the iterative improvement of the bounds used in the relaxed dual problem. The simplified approach is illustrated through a simple example that shows the significant improvement in the underestimating function obtained from the application of the modified algorithm. The...
Design by contract, as introduced by B.Meyer, is of increasing  importance to the OO community in the specification, reuse, and monitoring  of classes. We strongly feel that class libraries of all programming languages  should be equipped with contracts, insofar as these constitute a powerful  and simple interface definition. Very powerful and expressive contracts  can be written using the OCL language, although for operations with many  effects on the system state, these contracts can become unmanageable and  incomprehensible. In order to maintain contracts at a manageable level of  complexity, we claim that the OCL powerful mechanism of navigation  through associations should be used moderately when building contracts,  and that the effects of non-query operations should be allowed to be referred  to within pre- and post-conditions. To achieve that purpose, we propose an  extension to OCL and present a formal semantics for it.
The aim of this paper is to present the multi-linear approach being developed among the ProDiGE research group at the Institut de Phontique of the Laboratoire Parole &amp; Langage (Universit de Provence) in Aix-en-Provence. We first introduce the conceptual bases of this approach, before focusing our attention on the prosodic levels which constitute one of the lines in our formal and functional representation. In the last part, we present a sample analysis grid of part of our spontaneous speech corpus, detailing each level in turn.
Maintainability is perhaps one of the most important aspects of software development in that maintenance costs account for at least 50% of software system lifetime costs. A common maintenance practice is adding new capabilities to satisfy the evolving system dynamics, thus contributing to the ever-growing expenses. This paper discusses some lessons learned from system modifiability that affect software maintainability from the design standpoint. Deciding which design approach is most appropriate for the underlying system requirements plays a major role in software product operability and extensibility.
The simple network management protocol (SNMP) is a widely used standard for management of devices in Internet protocol networks. Part of the protocol great success is due to its simplicity; all the managed information is kept in a management information base (MIB) that can be accessed using SNMP queries to a software agent. In this paper, we develop a general model that abstract the data retrieval process in SNMP. In particular, we study the amount of queries (communication) and time needed to randomly access an element in this model. It turns out that this question has practical importance.
This paper presents `Mobile ELDIT&apos; (m-ELDIT), a system under development which  goal is to offer access from PDAs to the learning materials of ELDIT -- an adaptable language  learning platform. The ELDIT system, which is developed at the European Academy of Bolzano,  consists of a learner&apos;s dictionary and a text corpus. Exercises, a tandem system and a tutor module  are planned. The system works online and the adaptation of the content is done dynamically  according to interactions with the learner. We analyze the requirements for adaptation and  transformation of the data, the user interfaces and also the architecture of the `Mobile ELDIT&apos;. We  discuss the necessity of specific user modeling in order to provide both online and offline access to  the learning materials from mobile devices.
There has been increased attention and focus on the importance of intellectual capital disclosure. Several Scandinavian companies have ventured forward by publishing intellectual capital statements. However, despite the global appeal and changing beliefs surrounding the value of intellectual capital, it continues to be excluded from Canadian corporate annual reports. This paper outlines a study in which content analysis was conducted on the annual reports of 10,000 Canadian corporations. A list of intellectual capital related terms was searched within the annual reports yielding a significantly small number of instances in which intellectual capital disclosure took place. A major recommendation for corporations that are concerned with their relationship with the capital markets is to develop strategic and tactical initiatives that provide for voluntary disclosure of intellectual capital. These initiatives may initially be used for internal management purposes only; however, an external stakeholder-focus report will more than likely be the ultimate goal. Copyright 2002, Bontis. Version: April 14, 2002. All rights reserved. This paper is open for comment and is targeted for publication in the  Journal of Human Resource Costing &amp; Accounting.
We present an algorithm for a magic trick. Given a polygon with holes P , our  algorithm determines a folding of a rectangular sheet of paper such that a single straight  cut suces to cut out P. This paper is a simplification and improvement of a paper first published in Fun with Algorithms [10].
Providing guaranteed QoS, be it statistical or deterministic, necessarily requires allocation of scarce resources. This might happen on a session or on an aggregate basis, nevertheless, it is conceivable that at least at system edges scarcity of resources, exposed in the form of non-negligible (virtual) costs, will prevail to necessitate explicit allocation of resources as opposed to pure overdimensioning. An example of this logic is constituted by the Differentiated Services (DiffServ) architecture which is largely based on explicit bilateral Service Level Agreements (SLA) between peering providers. Often such resource allocation decisions are done on a multi-period basis because resource allocation decisions at a certain point in time may depend on earlier decisions and thus it can turn out sub-optimal to look at decisions in an isolated fashion. In earlier papers we discussed the general class of optimization problems that are applicable in these scenarios. We call the class MPRASE (Multi-Period Resource Allocation at System Edges). In this paper we present a taxonomy for all the MPRASE problems. 4  1 
In this paper we describe the architecture of the MIND system for federating  multimedia digital libraries. MIND integrates heterogeneous, multimedia  non-co-operating digital libraries and gives the user the impression of a single  coherent system. The architecture consists of a single mediator and one proxy  (composed of several proxy components) for every connected library. These specialised,  distributed components are connected via SOAP. This architecture with  clearly defined responsibilities perfectly fits the needs of a distributed research  project, and allows for integrating different platforms and programming languages.
We consider the problem of solving numerically the stationary incompressible Navier-Stokes equations in an exterior domain in two dimensions. This corresponds to studying the stationary fluid flow past a body. The necessity to truncate for numerical purposes the infinite exterior domain to a finite domain leads to the problem of finding appropriate boundary conditions on the surface of the truncated domain. We solve this problem by providing a vector field describing the leading asymptotic behavior of the solution. This vector field is given in the form of an explicit expression depending on a real parameter. We show that this parameter can be determined from the total drag exerted on the body. Using this fact we set up a self-consistent numerical scheme that determines the parameter, and hence the boundary conditions and the drag, as part of the solution process. We compare the values of the drag obtained with our adaptive scheme with the results from using traditional constant boundary conditions. Computational times are typically reduced by several orders of magnitude.  
Artificial intelligence research has had great success in many clasic games such as chess, checkers, and othello. In these perfect-information domains, alpha-beta search is sufficient to achieve a high level of play. However Artificial intelligence research has long been criticized for focusing on deterministic domains of perfect information -- many problems in the real world exhibit properties of imperfect or incomplete information and non-determinism. Poker, the archetypal game studied by...
Query processing over XML data sources has emerged as a popular topic. XML is an ordered data model and XQuery expressions return results that have a well-defined order. However little work on how order is supported in XML query processing has been done to date. In this paper we study the challenges related to handling order in the XML context, namely challenges imposed by the XML data model, by the variety of distinct XML operators and by incremental view maintenance. We have proposed an efficient solution that addresses these issues. We use a key encoding for XML nodes that supports both node identity and node order. We have designed order encoding rules based on the XML algebraic query execution data model and on node encodings that does not require any actual sorting for intermediate results during execution. Our approach supports more efficient incremental view maintenance as it makes most XML operators distributive with respect to bag union. Our approach is implemented in the context of Rainbow [26], an XML data management system developed at WPI. We prove the correctness of our order encoding approach, namely that it ensures order handling for query processing and for view maintenance. We also show, through experiments, that the overhead of maintaining order in our approach is indeed neglectible.
We state precisely and demonstrate two conjectures of R. Thomas following which a) the existence of a positive circuit in the oriented interaction graph of a dioeerential system is a necessary condition for the existence of several steady states, and b) the existence of a negative non-oriented circuit of length at least two is a necessary condition for the existence of a stable periodic orbit.
Individual mobility plays a crucial role in our everyday life. Since people spend a lot of time being mobile, making mobility more efficient is a demand of most of them. Due to the latest developments in the fields of computer technology and wireless communication, computer applications became possible to meet the needs of mobile individuals. Since also positioning techniques have reached an acceptable accuracy, the location of users can be exploited by mobile applications, too. Thus, location-based services (LBS) evolved. They are preparing information and services according to a user&apos;s position. The data and operations they provide very much coincide with those available in GIS. For this reason, LBS can be understood as mobile GIS applications. At the
In statistical machine translation, there are many source words that  can present different translations. The usual approach to disambiguating these  words is to use a target language model. These models are based on local phenomena  and in many cases are not capable of properly translating some of these  words. To deal with this problem, a new approach is proposed that is based on  the so-called naive Bayesian classifier. After a process of automatic selection of  ambiguous words, an adequate binary feature set that maximizes the information  gain is chosen. A Bayesian classifier can be used to choose the most adequate  translation of a word, using this feature set. Experimental results on a  parallel corpus (approximately 650,000 Spanish-Catalan sentence pairs) show  the benefits that the proposed method can achieve.
The Wizard of Oz (WOz) technique is an experimental evaluation  mechanism. It allows the observation of a user operating an apparently  fully functioning system whose missing services are supplemented by a  hidden wizard. From our analysis of existing WOz systems, we observe that  this technique has primarily been used to study natural language interfaces.
The best lower bound known on the crossing number of the complete bipartite  graph is :        In this paper we prove that:      for suffciently large m and n.
This paper presents the implementation of a  modified particle filter for vision-based  simultaneous localization and mapping of an  autonomous robot in a structured indoor  environment. Through this method, artificial  landmarks such as multi-coloured cylinders can  be tracked with a camera mounted on the robot,  and the position of the robot can be estimated at  the same time. Experimental results in simulation  and in real environments show that this approach  has advantages over the extended Kalman filter  with ambiguous data association and various  levels of odometric noise.
The unsteady Navier--Stokes equations offer a large variety of instantaneous solutions even for rather simple boundary conditions once the relevant spatial and temporal scales of the problem at hand are adequately resolved. This is especially true for DNS (Direct Numerical Simulation) of laminar-turbulent transition, where an initially laminar  flow, disturbed by some unsteady fluctuations &quot;breaks down&quot; into small-scale high-frequency turbulence. When trying to study and understand this process, one faces the problem of identifying the relevant structures to describe the flow which consist of vortices and shear layers, and on their isolation for further investigation. Here, we present a postprocessing method based on the vortex definition proposed by Jeong &amp; Hussain [5] and on Feature Extraction similar to Silver [13] that allows to successfully identify, isolate and track vortical features in a transitional boundary layer.
The arisal of experimental systems to transfer value (digital cash) over the Internet is of interest to monetary policy circles. These systems claim to turn the International Financial System back to the days of free banking, with uncontrolled and rampant issue of currency by private banks. This paper argues that, in actuality, Internet cash issuance will not be a strong force, neither against the tools of monetary policy, nor for its own mercantile purposes. Three models are used to develop an understanding of how digital cash systems will fit in the financial system. Fractional banking describes the effect of such cash on the money supply. The Baumol-Tobin model provides insights into how digital cash will function in terms of balances, and thus how it effects the rest of the financial system. Finally, a look at potential participancy reveals the scale of effect on monetary policy. 1. 
Individual organisations as well as industry consortia are currently defining application and domain-specific languages using the eXtended Markup Language (XML) standard of the World Wide Web Consortium (W3C). This trend introduces new challenges for version and configuration management. We show that configuration management for XML languages is considerably more complicated for an XML Schema or DTD than it is for traditional software engineering artifacts. In addition to internal consistency of the language definition, also consistency between the language and its instance XML documents needs to be preserved when evolving the language definition. We propose a definition for compatibility between versions of XML languages that takes this additional need into account. Compatibility between XML languages in general is undecidable. We argue that the problem can become tractable using heuristic methods if the two languages are related in a version history.
Cache misses for which data must be obtained from a remote cache (cache-to-cache transfer misses) account for  an important fraction of the total miss rate. Unfortunately, cc-NUMA designs put the access to the directory information into the critical path of 3-hop misses, which significantly penalizes them compared to SMP designs. This work studies the use of owner prediction as a means of providing cc-NUMA multiprocessors with a more efficient support for cache-to-cache transfer misses. Our proposal comprises an effective prediction scheme as well as a coherence protocol designed to support the use of prediction. Results indicate that owner prediction can significantly reduce the latency of cache-to-cache transfer misses, which translates into speed-ups on application performance up to 12%. In order to also accelerate most of those 3-hop misses that are either not predicted or mispredicted, the inclusion of a small and fast directory cache in every node is evaluated, leading to improvements up to 16% on the final performance.
The proposals in this document are experimental. While they may be deployed in the current Internet, they do not represent a consensus that this is the best method for high-speed congestion control. In particular, we note that alternative experimental proposals are likely to be forthcoming, and it is not well understood how the proposals in this document will interact with such alternative  proposals.
This document provides instructions for preparing the required 7 camera-ready copies  of VERY-HIGH-QUALITY of your paper for the CS 426 / BMI 226 Course. When you are  finished, your final camera-ready paper should end up looking very much like this  document.
ie injectee par unite de surface) sous le seuil de formation de plasma : (i) spallation resultant de l&apos;instabilite de la cible solide apres le passage d&apos;une onde de pression tensile; (ii) explosion de phase resultant de la nucleation de bulles de gaz au sein d&apos;un solide surchauffe; (iii) fragmentation d&apos;un fluide surcritique fortement contraint; et (iv) evaporation complete de la couche de surface de la cible. Jusqu&apos;a trois de ces mecanismes peuvent etre actifs en meme temps, causant une ablation a des profondeurs differentes sous la surface. L&apos;existence d ces mecanismes est liee aux caracteristiques du chemin de relaxation thermodynamique suivi par le materiau apres l&apos;absorption de l&apos;impulsion laser.  1 Introduction  When a laser is shined on a solid target, material can be ejected following the intense heating caused by the light absorption. This process --- laser ablation --- plays an incresingly important role in many different technological applications. These include thin film gr
Superresolution produces high quality, high-resolution images from a set of  degraded, low-resolution images where relative frame to frame motions provide  different looks at the scene. Superresolution translates data temporal  bandwidth into enhanced spatial resolution. If considered together on a reference  grid, given low-resolution data are nonuniformly sampled. However,  data from each frame are sampled regularly on a rectangular grid. This special  type of nonuniform sampling is called interlaced sampling. We propose a  new wavelet-based interpolation-restoration algorithm for superresolution. Our  efficient wavelet interpolation technique takes advantage of the regularity and  structure inherent in interlaced data, thereby significantly reducing the computational  burden. We present 1-D and 2-D superresolution experiments to  demonstrate the effectiveness of our algorithm.
UML-RT is an extension of UML for modelling embedded reactive  and real-time software systems. Its particular focus lies on system  descriptions on the architectural level, defining the overall system structure.
A dynamic optimizer is a runtime software system that groups a program&apos;s instruction sequences into traces, optimizes those traces, stores the optimized traces in a softwarebased code cache, and then executes the optimized code in the code cache. To maximize performance, the vast majority of the program&apos;s execution should occur in the code cache and not in the different aspects of the dynamic optimization system. In the past, designers of dynamic optimizers have used the SPEC2000 benchmark suite to justify their use of simple code cache management schemes. In this paper, we show that the problem and importance of code cache management changes dramatically as we move from SPEC2000, with its relatively small number of dynamically generated code traces, to large interactive Windows applications. We also propose and evaluate a new cache management algorithm based on generational code caches that results in an average miss rate reduction of 18% over a unified cache, which translates into 19% fewer instructions spent in the dynamic optimizer. The algorithm categorizes code traces based on their expected lifetimes and groups traces with similar lifetimes together in separate storage areas. Using this algorithm, short-lived code traces can easily be removed from a code cache without introducing fragmentation and without suffering the performance penalties associated with evicting long-lived code traces.
Pushing monotone constraints in frequent pattern mining  can help pruning the search space, but at the same time it can also reduce  the effectiveness of anti-monotone pruning. There is a clear tradeoff. It is better
Designing a rich repertoire of behaviors for virtual humans is an important problem for virtual environments and  computer games. One approach to designing such a repertoire is to collect motion capture data and pre-process it  to form a structure that can be walked in various orders to re-sequence the data in new ways. In such an approach  identifying the location of good transition points in the motion stream is critical. In this paper, we evaluate the  cost function described by Lee et al.
paper,  weinvesffEI&quot;JE@  interactive learning between  humansffanI33K and robot experimentally,  andits  esffsff4@3K  characterisffE4J  are  examinedusffmi  the dynamicals  fflI@@O approach. Ourresff3@:3 concentrated on  the  navigationsffviga  of a  sff3:N4KI&quot;  developed humanoid  robot called Robovie andsffdI3 humansffmanI4J  whosff eyes  were covered, making them dependent on  the robot for  directionsff  We compared theusffI4 feedforward  neural network (FFNN) withoutrecursff44 connections  and the recurrent neural network (RNN). Although  theperformances  obtained with both the RNN  and the FFNN improved in the early sffrlyI of learning,  as  thesffeIKKO changed the operation by learning  onits  own, all  performances  gradually becameunsffmeI4 and  failed. Resffed. of  aquesffKI&quot;4KN4J  given to thesffeI4JK4  confirmed that the  FFNNgives  better mental impresff  sffpres esffesff:I&quot;4  from theasffI3: of operability. When  the robot usff3 a  consff3I&quot;4EO4JIsffIsff4:E algorithmusff  ing  therehearsffI outputs  of the RNN, the performance  improved even when interactive learning continued for  a long time.
We prove that a random cubic graph almost surely is not homomorphic to a cycle of size 7. This
The abundance of errors in genome databases is a well-known fact. Major problems are errors in genome annotation. Performing biological experiments to eliminate them is time consuming and expensive. As a viable alternative, we introduce novel data cleansing methods for (semi-)automatic detection and correction of erroneous entries. Using a simple example we show the applicability of this cleansing approach. Our approach forms a sound basis for the solution of many open questions such as the efficient identification of erroneous entries, the specification of the cleansing process, management of alternative solutions until the correct one is identified, and efficient management of dependencies to react on changes to base data and avoid outdated data.
This paper focuses on the first stage of the City project that concerns the design of a mixed reality system that may support co-visiting for local and remote museum visitors. We discuss the initial visitor studies, the prototype system and the user trials, with a focus on the role direct interaction  and peripheral awareness have in the shaping of the visitor experience. The paper concludes with reflections on the use of the system and future plans.
In previous work, we experimented with different techniques of vector-based call routing, using the transcriptions of the queries to compare algorithms. In this paper, we base the routing decisions on the recogniser output rather than transcriptions and examine the use of confidence measures (CMs) to combat the problems caused by the &quot;noise&quot; in the recogniser output. CMs are derived for both the words output from the recogniser and for the routings themselves and are used to investigate improving both routing accuracy and routing confidence. Results are given for a 35 route retail store enquiry-point task. They suggest that although routing error is controlled by the recogniser errorrate, confidence in routing decisions can be improved using these techniques.
This paper focuses on the distinction between methods which are mathematically &quot;clever&quot;, and those which are simply crude, typically repetitive and computer intensive, approaches for &quot;crunching&quot; out answers to problems. Examples of the latter include simulated probability distributions and resampling methods in statistics, and iterative methods for solving equations or optimisation  problems. Most of these methods require software support, but this is easily provided by a PC. The  paper argues that the crunchier methods often have substantial advantages from the perspectives of  user-friendliness, reliability (in the sense that misuse is less likely), educational efficiency and realism. This means that they offer very considerable potential for simplifying the mathematical syllabus underlying many areas of applied mathematics such as management science and statistics: crunchier methods can provide the same, or greater, technical power, flexibility and insight, while requiring only a fraction of the mathematical conceptual background needed by their cleverer brethren.
In this paper we describe the fusion of various data and knowledge sources for intelligent SAR sea ice classification, thereby addressing the weaknesses of each information source while improving the overall reasoning power of the classifier. We equip our ice classification system, ARKTOS, with the capability of analyzing and classifying images unsupervised by emulating how a human geophysicist or photo-interpreter classifies SAR images. To imitate human visual inspection of raw images, we have designed and implemented a data mining application that first categorizes pixels into regions, and then extracts for each region a complex feature set of more than 30 attributes. In addition, we have incorporated other sea ice data and knowledge products such as ice concentration maps, operational ice charts, and land masks. Finally, we solicited human sea ice expertise as classification rules through interviews, and collaborative refinements during the earlystage evaluations. Using a Dempster-Shafer belief system, we are able to perform multisource data and knowledge fusion in ARKTOS&apos; rule-based classification. ARKTOS has been installed at the National Ice Center and Canadian Ice Service.
To study the properties of the Java Virtual Machine(JVM) and Java programs, our research group has produced a series of JVM models written in a functional subset of Common Lisp. In this paper, we present our most complete JVM model from this series, namely, M6, which is derived from a careful study of the J2ME KVM[16] implementation. On the  one
LRU is the de facto standard page replacement strategy. It is well-known, however, that there are many situations where LRU behaves far from optimal. We present a replacement policy that approximates the optimal algorithm OPT more closely by predicting the time each page will be referenced again and by evicting the page that has the largest predicted time of next reference. Experiments using several benchmarks from the SPEC 2000 benchmark suite show that our algorithm is superior to LRU, in some cases by as much as 25%-30% and in one case by more than 100%.
When they are available, safe state abstractions improve the efficiency of  reinforcement learning algorithms by allowing an agent to ignore irrelevant  distinctions between states while still learning an optimal policy.
A &quot;trip&quot; is a triple (g; u; v) where g is, in general, a graph  and u and v are nodes of that graph. The trip is from u to v on the graph  g. For the special case that g is a tree (or even a string) we investigate  ways of specifying and implementing sets of trips. The main result is  that a regular set of trips, specified as a regular tree language, can be  implemented by a tree-walking automaton that uses marbles and one  pebble.
Our research presented in this paper concerns the problem of fusing the results returned by the underlying systems to a mediating retrieval system, also called meta-retrieval system, meta-search engine, or mediator. We propose a fusion technique which is based solely on the actual results returned by each system for each query. The final (fused) ordering of documents is derived by aggregating the orderings of each system in a democratic manner. In addition, the fused ordering is accompanied by a level of democracy (alternatively construed as the level of confidence).
Two primary goals of landscape ecologists are to (1) evaluate changes in ecological pattern and process on natural landscapes through time and (2) determine the ecological consequences of transforming natural landscapes to cultural ones. Paleoecological techniques can be used to reconstruct past landscapes and their changes through time; use of paleoecological methods of investigation in combination with geomorphic and paleoethnobiological data, historical records, and shorter-term ecological data sets makes it possible to integrate long-term ecological pattern and process on a nested series of temporal and spatial scales. `Natural experiments &apos; of the past can be used to test alternative hypotheses about the relative influences of environmental change, biological interactions, and human activities in structuring biotic communities within landscape mosaics. On the absolute time scale of the Quaternary Period, spanning the past 1.8 million years, current distributional ranges of the biota have taken shape and modern biotic communities have assembled. Quaternary environmental changes have influenced the development of natural landscapes over time scales of centuries to hundreds of thousands of years; human cultural evolution has resulted in the transformation of much of the biosphere from natural to cultural landscapes over the past 5,000 years. The Quaternary extends to and includes the present and the immediate future. Knowledge of landscape changes on a Quaternary time scale is essential to landscape ecologists who wish to have a context for predicting future trends on local, regional, and global scales. 
Superconducting Magnetic Energy Storage (SMES) has branched out from its application origins of load leveling, in the early 1970s, to include power quality for utility, industrial, commercial and military applications. It has also shown promise as a power supply for pulsed loads such as electric guns and electromagnetic aircraft launchers (EMAL) as well as for vital loads when power distribution systems are temporarily down. These new applications demand more efficient and compact high performance power electronics. A 250 kW Power Conditioning System (PCS), consisting of a voltage source converter (VSC) and bi-directional two-quadrant DC/DC converter (chopper), was developed at the Center for Power Electronics Systems (CPES) under an ONR funded program. The project was to develop advanced power electronic techniques for SMES Naval applications. This thesis focuses on system analysis and development of a  demonstration test plan to illustrate the SMES systems&apos; ability to be multitasked for ii implementation on naval ships. The demonstration focuses on three applications; power quality, pulsed power and vital loads. An integrated system controller, based on an Altera programmable logic device, was developed to coordinate charge/discharge transitions. The system controller integrated the chopper and VSC controller, configured applicable loads, and dictated sequencing of events during mode transitions. Initial tests with a SMES coil resulted in problems during mode transitions. These problems caused uncontrollable transients and caused protection to trigger and processors to shut down. Accurate models of both the Chopper and VSC were developed and an analysis of these mode transition transients was conducted. Solutions were proposed, simulated and implemented in hardware. S...
Introduction  s, briefly, remind how the DNA ruI0 the synthesis of proteins, whichconstituP the most abutfl0 t organicsunicfl100 in living matter systems. The DNA macromolecu1 is made of two linear chains of nu01OSfl4--H wrapped in adou1P helixstruI1MPfl Each nu cleotide is characterized by one of the fou elementary bases: adenine (A) and gu--O--P (G) deriving frompumflOH and cytosine (C) and thymine (T) coming from pyrimidine. The DNA is localized in the nu cleu of the cell and the transmission of the genetic information in the cytoplasm is achieved , schematically speaking, by the ribonu cleic acid or RNA. This operation is called the transcription, the A, G, C, T bases in the DNA being respectively associated in RNA to the U, C, G, A bases, U denoting the ueflPM1 base. The correspondence law between triples of nuM--1Pfl4IMS called codons, in the desoxyribonu cleic acid (DNA)sequI1H and the amino-acids is called the genetic code. As a codon is an orderedsequdfl1 of three bases (e.g. 
This article surveys and analyzes these developments by establishing  criteria for evaluating trace-driven methods, and then applies these criteria to  describe, categorize, and compare over 50 trace-driven simulation tools. We discuss  the strengths and weaknesses of different approaches and show that no single  method is best when all criteria, including accuracy, speed, memory, flexibility,  portability, expense, and ease of use are considered. In a concluding section, we  examine fundamental limitations to trace-driven simulation, and survey some  recent developments in memory simulation that may overcome these bottlenecks
The intellectual capital of a nation (or a region of nations) requires the articulation of a system of variables that helps to uncover and manage the invisible wealth of a country. Most importantly, an emphasis on human capital allows for a better understanding of the hidden values, individuals, enterprises, institutions, and communities that are both current and potential future sources of intellectual wealth. This paper endeavours to address the Wve research questions. The main outcomes of this paper are the development of a national intellectual capital measurement methodology and index. The NICI is also used within a structural equation model to test several hypotheses related to national intellectual capital development.
Fotonor AS was one of the first aerial photography company to integrate an Applanix POS/AV system with a RC30 camera from LH Systems. This paper describes how this integration is done and what considerations we have taken into account to improve the accuracy of the POS solutions. 1. 
Traditional statistical classifiers rely exclusively on spectral characteristics of multi-dimensional remote sensing imagery, but thematic classes are often spectrally overlapping. The spectral response distributions of classes are dependent on many factors including terrain, slope, aspect, soil type, and atmospheric conditions. The limitation of traditional classifiers in ecient incorporation of these diverse and rich geo-spatial databases has led us to the development of a hybrid classification system. In this paper we propose an efficient two-step hybrid classification system based on a fusion of statistical and knowledge based classifiers for mining remote sensing images. In the first stage we have applied a traditional unsupervised classification technique based on the C-Means clustering algorithm to extract spectral clusters. In the second stage the spectral clusters from the first stage are classified into information classes using a decision tree constructed from ancillary geo-spatial databases. This approach has led to a robust classification framework for mining remote sensing images using ancillary geo-spatial databases. Initial experimentation results show that this hybrid system is both more ecient and accurate than commonly used maximum likelihood classifier (MLC) and decision trees.
This paper provides guidelines and examples for visualising lexical relations using Formal Concept Analysis. Relations in lexical databases often form trees, imperfect trees or poly-hierarchies which can be embedded into concept lattices. Manyto -many relations can be represented as concept lattices where the values from one domain are used as the formal objects and the values of the other domain as formal attributes. This paper further discusses algorithms for selecting meaningful subsets of lexical databases, the representation of complex relational structures in lexical databases and the use of lattices as basemaps for other lexical relations.
The need for formal methods for certifying the good behaviour of computer  software is dramatically increasing with the growing complexity of  the latter. Moreover, in the global computing framework one must face the  additional issues of concurrency and mobility. In the recent years many  new process algebras have been introduced in order to reason formally  about these problems; the common pattern is to specify a type system  which allows one to discriminate between &quot;good&quot; and &quot;bad&quot; processes. In  this paper we focus on an incremental type system for a variation of the  Ambient Calculus called M   , i.e., Mobility types for Mobile processes in  Mobile ambients and we formally prove its soundness in the proof assistant  Coq.
This paper describes theoretical and practical issues that arose during the development of the control and guidance systems for SIRENE, an autonomous underwater shuttle for the transport and accurate positioning of benthic laboratories on the seabed down to a depth of 4000 meters. The nonlinear hydrodynamic model of the vehicle is described, and a class of sliding mode control laws for vehicle stabilization and steering in the vertical and horizontal planes is derived. The control algorithms are combined with a classical line-of-sight guidance law, and the performance of the resulting system is evaluated in a computer based simulation of a realistic mission scenario. The paper concludes with a description of experimental results obtained during a series of sea tests carried out by the French Agency IFREMER and the Instituto Superior Tecnico (IST) off the coast of Toulon, France.
We describe an efficient implementation (MRDTL-2) of the Multirelational  decision tree learning (MRDTL) algorithm [23] which in turn was  based on a proposal by Knobbe et al. [19] We describe some simple techniques  for speeding up the calculation of sufficient statistics for decision trees and related  hypothesis classes from multi-relational data. Because missing values are  fairly common in many real-world applications of data mining, our implementation  also includes some simple techniques for dealing with missing values. We  describe results of experiments with several real-world data sets from the KDD  Cup 2001 data mining competition and PKDD 2001 discovery challenge. Results  of our experiments indicate that MRDTL is competitive with the state-of-the-art  algorithms for learning classifiers from relational databases.
UMTS networks will be based on the Internet Protocol (IP) to provide an efficient support for applications with bursty traffic characteristics, e.g., WWW browsers. Such IP-based networks must include Quality of Service (QoS) mechanisms to enable the usage of real-time applications, such as mobile telephony. Different network services are necessary to satisfy the different needs of applications in wireless mobile networks. To overcome this problem,...
This paper presents a framework for finding point correspondences in monocular image sequences over multiple  frames. The general problem of multi-frame point correspondence is NP Hard for three or more frames. A  polynomial time algorithm for a restriction of this problem is presented and is used as the basis of the proposed  greedy algorithm for the general problem. The greedy nature of the proposed algorithm allows it to be used in  real time systems for tracking and surveillance etc. In addition, the proposed algorithm deals with the problems of  occlusion, missed detections and false positives by using a single non-iterative greedy optimization scheme, and  hence reduces the complexity of the overall algorithm as compared to most existing approaches where multiple  heuristics are used for the same purpose. While most greedy algorithms for point tracking do not allow for entry  and exit of the points from the scene, this is not a limitation for the proposed algorithm. Experiments with real  and synthetic data over a wide range of scenarios and system parameters are presented to validate the claims about  the performance of the proposed algorithm.
We discuss how references and citations within a document to particular sources can be verified and guaranteed. When a document refers through a quotation to another document, the reader should be able to verify that the reference is correct and that any quotation correctly represents the original text. The mechanism we describe enables the authentication of such quotations. It consists of:  .
The segmentation of MRI scans of patients with  white matter lesions (WML) is difficult because the  MRI characteristics of white matter lesions are similar  to those of grey matter. Intensity based statistical  classification techniques misclassify some WML as  grey matter and some grey matter as WML.
A propositional logic of distributed protocols is introduced which includes both the  logic of knowledge and temporal logic. Phenomena in distributed computing systems  such as asynchronous time, incomplete knowledge by the computing agents in the  system, and game-like behavior among the computing agents are all modeled in the  logic. Two versions of the logic, the linear logic of protocols (LLP) and the tree logic  of protocols (TLP) are investigated. The main result is that the set of valid formulas  in LLP is undecidable.
The designs of most systems-on-a-chip (SoC) architectures rely on simulation as a means for performance estimation. Such designs usually start with a parameterizable template architecture, and the design space exploration is restricted to identifying the suitable parameters for all the architectural  components.Howevne  in the case of heterogeneous SoC architectures such as network processors the design space exploration alsoinvTDD; a combinatorial aspect----which architectural components are to be chosen, how should they be interconnected, task mapping decisions----thereby increasing the design space.MoreovW1 in the case of network processor architectures there is also an associated uncertainty in terms of the application scenario and the traffc it will be required to process. As a result, simulation is no longer a feasible option for  evW12D2Tff  such architectures in any automated or semi-automated design space exploration process due to the high simulation timesinvsTW92 To address this problem, in this paper we hypothesize that the design space exploration for network processors should be separated into multiple stages, eachhavTD a  differentlevr  of abstraction. Further, it would be appropriate to use analytical evlytical frameworks during the initial stages and resort to simulation techniques only when  arelativ[N  small set of potential architectures is identified. None of the known performanceevformanc  methods for network  processorshav  been positioned from  thisperspectivW  We show that there are already suitable analytical models for network processor  performanceevformanc  which may be used to support our hypothesis. To this end, we choose a referencesystem-lev1 model of a network processor architecture and compare its  performanceevformanc  results derivs usi...
Existing techniques for the transfer of information in and out of model-based repositories, and in particular the XMI format, are designed for expedient machine processing, and have significant drawbacks for human users. This report describes a system that automatically generates a producer and consumer for a human-usable textual notation corresponding to a given information model. This HUTN system is based on the Meta-Object Facility, an OMG standard for the definition of information models and the subsequent mapping of these models to CORBA interfaces.
Numerous studies on regional variations of French sentence intonation have been conducted for some time, even before any phonological description of intonation was available. These studies are therefore characterized by a strong phonetic bias, leading researchers to gather large sets of experimental data, which once statistically organized, would give an empirical account of intonational differences existing between varieties of French.
The deployment of a geographic location service for Internet hosts enables a whole new class of location-aware applications. We focus on a technique that infers host locations using delay measurements to geographically distributed landmarks, which are hosts with a known geographic location. The problem we deal with is where to place such landmarks and the probe machines that perform the delay measurements. We propose a demographic placement approach to improve the representativeness of each landmark with respect to the hosts to be located. Results show that a relatively small number of landmarks are sufficient to cover the most part of hosts to be located. For a fixed number of landmarks, the demographic approach reduces the distances from most hosts to the nearest landmark. Considering the probe machines, we show that they have to be sparsely placed to avoid gathering redundant data.
One of the limitations in traditional instant messaging platforms is that they predominantly rely on text messages as the primary form of expression. This paper presents FAIM, an instant messaging application that analyzes a person&apos;s facial affect in real time and augments the dialogue with an emotive character representing them. Throughout the paper, we identify a number of design challenges that arise from integrating facial affect into instant messaging, and discuss how each of these issues is addressed in the design of FAIM. We also present a use case scenario of how FAIM works.
Group signature schemes allow a group member to anonymously  sign on group&apos;s behalf. Moreover, in case of anonymity misuse,  a group authority can recover the issuer of a signature. This paper analyzes  the security of two group signature schemes recently proposed by  Tseng and Jan. We show that both schemes are universally forgeable,  that is, anyone (not necessarily a group member) is able to produce a  valid group signature on an arbitrary message, which cannot be traced  by the group authority.
A fixed-pattern padding consists in concatenating to the message m a fixed pattern P . The RSA signature is then obtained by computing (P    mod N where d is the private exponent and N the modulus. In Eurocrypt &apos;97, Girault and Misarsky showed that the size of P must be at least half the size of N (in other words the parameter configurations    are insecure) but the security of RSA fixedpattern padding remained unknown for    &gt;  |N |/2.
The highly automated configuration of large scale computing fabrics increasingly exploits  declarative representations of the fabric&apos;s intended state. We examine three languages for expressing  these representations: LCFG, Pan and SmartFrog. We give examples of their use and  examine which language properties are of particular relevance to the configuration of grid computing  fabrics.
Contemporary file systems implement a set of abstractions and semantics that are suboptimal for many (if not most) purposes. The philosophy of using the simple mechanisms of the file system as the basis for a vast array of higher-level mechanisms leads to inefficient and incorrect implementations. We propose several extensions to the canonical file system model, including explicit support for lock files, indexed files, and resource forks, and the benefit of session semantics for write updates. We also discuss the desirability of application-level file system transactions and file system support for versioning. 1
Computer architects must determine how to most effectively use finite computational resources when running simulations to evaluate new architectural ideas. To facilitate efficient simulations with a range of benchmark programs, we have developed the MinneSPEC input set for the SPEC CPU 2000 benchmark suite. This new workload allows computer architects to obtain simulation results in a reasonable time using existing simulators. While the MinneSPEC workload is derived from the standard SPEC CPU 2000 workload, it is a valid benchmark suite in and of itself for simulation-based research. MinneSPEC also may be used to run large numbers of simulations to find &quot;sweet spots&quot; in the evaluation parameter space. This small number of promising design points subsequently may be investigated in more detail with the full SPEC reference workload. In the process of developing the MinneSPEC datasets, we quantify its differences in terms of function-level execution patterns, instruction mixes, and memory behaviors compared to the SPEC programs when executed with the reference inputs. We find that for some programs, the MinneSPEC profiles match the SPEC reference dataset program behavior very closely. For other programs, however, the MinneSPEC inputs produce significantly different program behavior. The MinneSPEC workload has been recognized by SPEC and is distributed with Version 1.2 and higher of the SPEC CPU 2000 benchmark suite.
Once, sentence processing research set aside prosody in order to focus on syntactic and semantic processing. Experimental sentences were mostly presented visually, often without prosodic markers such as commas. Now that we have made some progress by this `divide and conquer&apos; approach, and now that the technology for working on speech has improved, it may be time to integrate prosody into processing models. I argue here that we have no choice but to do so, because current evidence shows that even in silent reading, prosody is projected onto written sentences, and can influence the course of syntactic processing.
In sequential class- and mixin-based settings, subtyping is essentially a relation on objects: no subtype relation is defined on classes and mixins, otherwise there would be conflicts with the inheritance mechanism, creating type un-safety. Nevertheless, a width-depth subtyping relation on class and mixin types is useful in the realm of mobile and distributed processes, where object-oriented code may be exchanged among the sites of a net. In our proposal, classes and mixins become &quot;first-class citizens&quot; at communication time, and communication is ruled by a type-safe width-depth subtyping relation.
this paper I look at theories of cognition and the central epistemological preoccupation  relating to those theories in mathematics education. First I look at the positivist-empiricist orientation of mathematical epistemology, centred on rational thought, as a means for individual and ultimately social empowerment. I draw attention to the principles of universality on which these theorises are derived, namely, that in the `transfer&apos; of knowledge, the conditions of knowing `hold&apos; for any knower, irrespective of history, identity, interests and circumstances. Secondly, I consider challenges to this central processing cognition model. I look particularly at the theory of situated cognition which subscribes to the view that cognition is produced in practices. Prompting an opposition to  our conventional notion of pure abstraction and maintaining that mathematical truth is attained more by fiat than `fact&apos;, this body of work endeavours to locate mathematical knowing in everyday activity. My aim is to open another &quot;conceptual space&quot; (Grosz, 2000, p. 28) for the discussion on knowers and the known
We specify the dynamic semantics of an object oriented programming  language in an incremental way. We begin with a simple language of arithmetic  and boolean expressions. Then, we add functional abstractions, local  declarations, references and assignments obtaining a functional language with  imperative features. We finally add objects, classes and subclasses to obtain a  prototypical object oriented language with dynamic binding.
Linear Precoding consists in multiplying by a N    K matrix a K- dimensional vector obtained by serial to parallel conversion of a symbol sequence to be transmitted. In this paper, the performance of MMSE receivers for certain large random isometric precoded OFDM systems on fading channels is analyzed. Using new tools, borrowed from Free Probability Theory, it can be shown that the Signal to Interference plus Noise Ratio at the equalizer output converges almost surely to a deterministic value depending on the probability distribution of the channel coefficients when N      and K        1. These asymptotic results are used to answer the trade-off Convolutional Coding versus Linear Precoding issue while preserving a simple MMSE equalization scheme at the receiver. 1. 
The concept of pDSL networking, presented in this work, facilitates the realization of high speed broadband communications over power lines in the indoor environment. A pDSL network consists of a set of virtual high-speed links between a PLC Gateway and multiple pDSL devices. pDSL devices exploit a network response-estimation model and channel training procedures to allocate bandwidth to each virtual link. The paper describes the characteristics of initial  and inbound training processes and presents the criteria for performing optimum bandwidth allocation to all active links on a pDSL network.
We present a context-based scanning algorithm which reorders the input image using a hierarchical representation  of the image. Our algorithm optimally orders (permutes) the leaves corresponding to the pixels, by minimizing the  sum of distances between neighboring pixels. The reordering results in an improved autocorrelation between  nearby pixels which leads to a smoother image. This allows us, for the first time, to improve image compression  rates using context-based scans. The results presented in this paper greatly improve upon previous work in both  compression rate and running time.
In this paper we consider online auction mechanisms for the allocation of M items that are identical to each other except for the fact that the items have different expiration times, and each item must be allocated before it expires. A computational application is the allocation of time slots in a scheduling problem, and an economic application is the allocation of transportation tickets. If we ignore...
... This paper describes Strings, a high performance  distributed shared memory system designed for such SMP clusters. The distinguishing feature of this system is  the use of a fully multi-threaded runtime system, using kernel level threads. Strings allows multiple application  threads to be run on each node in a cluster. Since most modern UNIX systems can multiplex these threads on  kernel level light weight processes, applications written using Strings can exploit multiple processors on a SMP  machine. This paper describes some of the architectural details of the system and illustrates the performance  improvements with benchmark programs from the SPLASH-2 suite, some computational kernels as well as a  full fledged application
In this paper we analyse the applicability of our Context Ontology Language (CoOL), considering a range of use cases. After wrapping up the model in use within this language, we introduce some interesting applications of the language, based on a scenario showing the challenges in context aware service interactions. We focus on two submodels of our model for context aware service interactions, namely Context Bindings and Context Obligations, and demonstrate how to integrate them into existing service architectures.
Security has always been the Achilles&apos; heel of wireless local area networks (WLANs). In this paper the authentication aspect of security is investigated. Since the media used to connect to the network is air, it is more dicult to control the network access than in a regular wired network. Many authentication methods already exists and more are making their way. This paper therefore examines the most interesting ones and the most suitable is chosen for implementation in Skellefte Kraft&apos;s WLAN.
this paper.  The experiments described herein explore unsupervised approaches to NER, with an eye toward using unannotated corpora consisting of a few hundred million words. Recent word sense disambiguation results suggest that some simple techniques can scale well with increased data sizes (Banko and Brill, 2001). This paper presents several experiments in adapting a HMM-based named entity recognizer to a target data set. Our core learning engine is a wordbased HMM, and we show two techniques, informed smoothing and iterative adaptation, for incorporating unsupervised data into the model, which provide overall gains in performance
Introduction  In the pioneering works [e.g. Foerster 1936, Penfield et al. 1937] hand motor somatotopy was described using electrical stimulation of the brain. Today, techniques like MEG, PET, or FMRI allow to show motor somatotopy noninvasively. While somatosensoric mapping usually allows stable conditions, motor mapping is more difficult due to variabilities in movement execution. Therefore, there was a long debate about finger motor somatotopy organization which requires fine scale localization.  However, recent investigations in human subjects achieved with FMRI [Kleinschmidt et al. 1997, Lotze et al. 2000], MEG [Cheyne et al. 1991], and lesion studies [Schieber 1999] indicate that finger motor somatotopy exists, at least concerning the centers of mass of overlapping somatotopic regions. Our group recently described similar findings with MEG [Erdler et al. 1998, 1999].  The goal of the present study was to investigate the quality of dipole solutions for finger motor somatotopy depe
This paper presents the user-centered  interface of a summarization system for  physicians in Bone Marrow Transplantation  (BMT). It serves both retrieval  and summarization, eliciting the query  and presenting multi-document summaries  in a situation-specific organization.
This paper studies optical-layer protection design in a WDM mesh network given  that a dual-homing infrastructure is implemented at the IP layer. The problem is formulated as  an integer linear program and solved using CPLEX.
this paper we offer a descriptive overview of syntactic factors influencing liaison. We provide a detailed analysis in the framework of HPSG, that integrates the morphophonological and syntactic conditions governing this feature of French grammar. Although we do not directly model other factors influencing liaison (such as frequency effects, prosodic considerations, or sociolinguistic variables) our analysis is modular enough to accommodate additional conditions resulting from more complete empirical studies
We introduce intersection schemes (a generalization of uniform oriented matroids  of rank 3) to describe the combinatorial properties of arrangements of pseudocircles  in the plane and on closed orientable surfaces. Similar to the FolkmanLawrence  topological representation theorem for oriented matroids we show that  there is a one-to-one correspondence between intersection schemes and equivalence  classes of arrangements of pseudocircles. Furthermore, we consider arrangements  where the pseudocircles separate the surface into two components. For these strict  arrangements there is a one-to-one correspondence to a quite natural subclass of  consistent intersection schemes.
This report is available by anonymous ftp from ftp.esat.kuleuven.ac.be in the directory pub/sista/markovsky/reports/03-183.ps.gz   K.U.Leuven, Dept. of Electrical Engineering (ESAT), Research group SCD (SISTA), Kasteelpark Arenberg 10, 3001 Leuven-Heverlee, Belgium, Tel. 32/16/32 17 09, Fax 32/16/32 19 70, WWW: http://www.esat.kuleuven.ac.be/sista
Efficient support of Internet-based applications to mobile/nomadic users is a key feature of the thirdgeneration (3G) networks. In light of the shortage and the high cost of the T-UMTS spectrum, the operators are looking into the provision of integrated broadcast/multicast services through hybrid broadcast-UMTS systems. S-UMTS could play an important role in the efficient delivery of some UMTS services to which it is better suited. These services include broadcasting and multicasting applications such as audio/video, e-newspaper and live stock exchange data. The level of IP penetration into 3G networks is a decisive factor for the design of an efficient system, optimized for the delivery of these services. The paper identifies the respective requirements arising for the S-UMTS air interface, in the frame of the architecture scenarios envisaged within the EU IST project SATIN.
Biomedical spectra such as those acquired from magnetic resonance (MR) spectrometers often have the characteristics of high dimensionality and small sample size. These two characteristics make the classification of such spectra difficult. Hierarchical clustering produces robust clustering results, especially when working on small size high-dimensional datasets. The goal of this research is to investigate the effectiveness of hierarchical clustering for the classification of high-dimensional biomedical spectra. The classification results are benchmarked against linear discriminant analysis (LDA).
As the e-government domain is about to become a field of application  for Semantic Web technologies, the actors involved still lack reasoning to  decide on critical issues such as organisational cost/benefit, &quot;user&quot; involvement,  technical integration, and implementation strategy. Firstly, the paper seeks to  identify &quot;semantic problems&quot; in e-government as prerequisite for discussing the  requirements for the application of Semantic Web technologies. Secondly, experiences  from an ongoing project are discussed to identify critical issues from  the systems development perspective. Thirdly, taking into account the problems  identified and the case findings, a research agenda is laid out aiming to guide  and support the application of Semantic Web technologies in e-government.
documents that cannot execute. Certainly, we can review them and draw conclusions about their correctness, but until we have something that runs we cannot know for a fact that they really do exactly what is needed. In addition, in the time it takes to deliver a solution, the market and the technology have moved on, making the delivered system, even if it is correct, irrelevant. Worse, some systems are &quot;wicked,&quot; in that the existence of a solution changes the (perception of the) problem, which makes a complete and detailed specification document somewhat futile.  Agile methods propose to address these problems by delivering small slices of working code as soon as possible. This working functionality is immediately useful to the customer, and it can be interacted with, possibly improving understanding of the system that needs to be built. As these 2  delivery cycles can be short (a week or two), the systems&apos; development process is able to adapt to changing conditions and deliver just wh
The increasing speed gap between processors and memory makes the design of memory hierarchy one of the critical issues in general purpose embedded systems. As memory requirements for embedded applications grow, especially in emerging area of handheld multimedia devices, cache memories become crucial for providing high performance and reducing power. This paper describes a performance evaluation of typical cache design issues such as cache size and organization, block size, and replacement policy. The evaluation is done using simulation tools for architectural exploration based on ARM instruction set and MiBench benchmark suite. Our performance evaluation includes monitoring of dynamic cache behavior, since embedded systems designers are interested not only in the total number of cache misses, but also in the number of cache misses throughout application execution.
Mobile ad hoc networks are characterized by frequently changing  network topologies, and decentralized operation. As the context of a  network component changes frequently, a component needs to be continually  aware of the state of sensors and hosts that are accessible through  the ad hoc network. Middleware to achieve this context-awareness can  be categorized in variants of either publish/subscribe systems or tuplespaces  systems. We analyze the strengths and weaknesses of these  two approaches in terms of coupling between components, given the requirement  that for ad hoc networks coupling should be as low as possible.
Introduction  Combinatorial Optimization Problems (COPs) and Constraint Satisfaction Problems (CSPs) are an important challenge for Arti  cial Intelligence and related elds. Often, problem hardness and size make it impractical to exhaustively explore the search space and thus metaheuristic algorithms are applied. Metaheuristics [1, 3] are approximate algorithms, as they do not guarantee to  nd the optimal solution in bounded time. However, they eectively and eciently exploit search space characteristics to guide the search toward promising search space regions. Furthermore, the usefulness of metaheuristics have been recently proved in successful combination with Constraint Programming [4, 12]. Metaheuristics are general strategies which encompass both constructive methods (which build initial solutions), local search and population-based methods.  In this work we present a new metaheuristic for MAXSAT problems, by outlining the critical decision phases in the design process.  2 Metahe
Appearance based object detection systems utilizing statistical models to capture real world variations in appearance have been shown to exhibit good detection performance. The parameters of these statistical models are typically learned automatically from labeled training images. This process can be difficult in that a large number of labeled training examples may be needed to accurately model appearance variation. In this work we describe a method whereby a training set consisting of a small number of fully labeled training examples augmented with a set of weakly labeled examples can be used to train a detector which exhibits performance better than that which can be obtained with a reduced set of fully labeled training examples alone.
This paper addresses the problem of  actively control acoustic noise in  ducts through the application of  genetic algorithm. Conventional  techniques, like adaptive or classical  control, have convergence problems  due to the acoustic feedback,  propagation delay and secondary  paths. The software was designed to  work in a parallel DSP TMS320C44  architecture, managing real time  interrupts and communication with a  shared memory. An UNIX simulator  was developed with a simplified  model to test the software. The noise  reduction obtained in simulations is  around 40 dB and the experimental  results is 20 dB down.
Ontologies are developed with different tools and languages. Reusing an ontology usually requires transforming it from its original format to a target format. However, many problems usually arise in these transformations, related to the compatibility among tools/languages. We propose an ontology reengineering methodology (with its technological support) as a solution to the ontology translation problem. . . .
Cepstrum coefficients are widely used as features for both speech and music. In this paper, the use of discrete cepstrum coefficients is considered, which are computed from sinusoidal peaks in the short time spectrum. These coefficients are very interesting as features for pattern recognition applications since they allow to represent spectra by points in a multidimensional vector space. A new Mel frequency warping method is proposed that allows to compute the spectral envelope on the Mel scale which, by contrast to current estimation techniques, does not rely on manually set parameters. Furthermore, the robustness and perceptual relevance of the coefficients are studied and improved.
Categorial grammar analyzes linguistic syntax and semantics in terms of type theory and lambda calculus. A major attraction of this approach is its unifying power, as its basic function/argument structures occur across the foundations of mathematics, language and computation. This paper considers, in a light example-based manner, where this elegant logical paradigm stands when confronted with the wear and tear of reality. Starting from a brief history of the Lambek tradition since the 1980s, we discuss three main issues: (a) the fit of the lambda calculus engine to characteristic semantic structures in natural language, (b) the coexistence of the original type-theoretic and more recent modal interpretations of categorial logics, and (c) the place of categorial grammars in the complex total architecture of natural language, which involves - amongst others - mixtures of interpretation and inference.
In response to reports alleging an alarming decline in student knowledge and achievement during the last two decades, America has witnessed a revival-like call for higher standards in public education. Driven by a public desire to measure schools and educators based on student achievement, policymakers have moved quickly to create politically expedient accountability reform laws. Research into early attempts show significant difficulties with regard to their political and legal viability. There is evidence to suggest that policymakers must develop a better understanding of the complex issues that surround both the development and implementation of such policies if those policies are to survive inevitable scrutiny and challenge and bring meaningful reform. This study is a descriptive policy analysis tracing the evolution of Floridas efforts since 1989 to bring accountability to public schools and educators based on measured student performance. A rich description of the political, educational, social, and legal issues that have helped shape efforts to craft such legislation is presented. Based on an analysis of the evolution of these reform efforts, implications for policymakers in Florida and other states, as well as the educational community at-large are reported. iii  DEDICATION To my wife Sherri, whose love, patience, and support helped me make it through. iv  ACKNOWLEDGEMENTS I would be remiss if I did not express my heartfelt thanks to Dr. Jennifer Sughrue for her continual guidance and support through the duration of this study. It is impossible to overstate the value of her assistance in helping me make it through. I would also like to extend my appreciation to Dr. M. David Alexander for his timely advice, helping me steer a course for this research. Also, than...
Recently results have shown that a single server queue Poisson Pareto Burst Process input has a tail which is bounded by hyperbolic functions. We show that the hyperbolic upper and lower bounds for this system can be very misleading, that this hyperbolic tail result is relevant only from a certain threshold onwards, and the magnitude of this threshold may be very large. We also show that any hyperbolic upper and lower bounds for a tail of the stationary waiting time complementary distribution necessarily become further apart as the rate of the process increases.
The cosmos simulator provides fast and accurate switch-level modeling of mos digital circuits. It attains high performance by preprocessing the transistor network into a functionally equivalent Boolean representation. This description, produced by the symbolic analyzer anamos, captures all aspects of switch-level networks including bidirectional transistors, stored charge, different signal strengths, and indeterminate (X) logic values. The lgcc program translates the Boolean representation into a set of machine language evaluation procedures and initialized data structures. These procedures and data structures are compiled along with code implementing the simulation kernel and user interface to produce the simulation program. The simulation program runs an order of magnitude faster than our previous simulator  mossim ii.
Suppose p &lt; q are odd and relatively prime. In this paper we complete the proof  that K n;n has a factorisation into factors F whose components are copies of K p;q if  and only if n is a multiple of pq(p+q). The  nal step is to solve the \c-value problem&quot;  of Martin. This is accomplished by proving the following fact and some variants:  For any 0  k  n, there exists a sequence ( 1 ;  2 ; : : : ;  2k+1 ) of (not necessarily  distinct) permutations of f1; 2; : : : ; ng such that each value in fk; 1 k; : : : ; kg  occurs exactly n times as  j (i) i for 1  j  2k 1 and 1  i  n.
We propose a new method for evaluating the security of block ciphers against differential cryptanalysis and propose new structures for block ciphers. To this end, we define the word-wise Markov (Feistel) cipher and random output-differential (Feistel) cipher and clarify the relations among the differential, the truncated differential and the impossible differential cryptanalyses of the random output-differential (Feistel) cipher. This random output-differential (Feistel) cipher model uses a not too strong assumption because denying this approximation model is equivalenttodenying truncated differential cryptanalysis. Utilizing these relations, weevaluate the truncated differential probability and the maximum average of differential probability of the word-wise Markov (Feistel) ciphers like Rijndael, E2 and the modified version of block cipher E2. This evaluation indicates that all three are provably secure against differential cryptanalysis, and that Rijndael and a modified version of b...
Reinforcement learning aims to determine an (infinite time horizon) optimal control policy from interaction with a system. It can be solved by approximating the so-called Q-function from a sample of four-tuples (x t, u t , r t, x t+1) where x t denotes the system state at time t, u t the control action taken, r t the instantaneous reward obtained and x t+1 the successor state of the system, and by determining the optimal control from the Q-function. Classical reinforcement learning algorithms use an ad hoc version of stochastic approximation which iterates over the Q-function approximations on a four-tuple by four-tuple basis. In this paper, we reformulate this problem as a sequence of {\em batch mode} supervised learning problems which in the limit converges to (an approximation of) the Q-function. Each step of this algorithm uses the full sample of four-tuples gathered from interaction with the system and extends by one step the horizon of the optimality criterion. An advantage of this approach is to allow the use of standard batch mode supervised learning algorithms, instead of the incremental versions used up to now. In addition to a theoretical justification the paper provides empirical tests in the context of the ``Car on the Hill&apos;&apos; control problem based on the use of ensembles of regression trees. The resulting algorithm is in principle able to handle efficiently large scale reinforcement learning problems.
this paper, we tackle the problem of performability evaluation for a scheme of coordinated software and hardware fault tolerance we developed earlier [1]. The scheme involves 1) a time-based (TB) checkpointing protocol developed by Neves and Fuchs for tolerating hardware faults, and 2) our message-driven confidence-driven (MDCD) protocol for software error containment and recovery. The two protocols coordinate through their checkpointing activities, guaranteeing that when recovering from a software or hardware error in a distributed computing environment, the system will always reach a global state that is not only consistent but also valid (i.e., it does not reflect the receipt of notyet -validated messages from low-confidence processes)
this article, we further motivate our proposal of using interactive computer games for AI research, review previous research on AI and games, and present the different game genres and the roles that human-level AI could play within these genres. We then describe the research issues and AI techniques that are relevant to each of these roles. Our conclusion is that interactive computer games provide a rich environment for incremental research on human-level AI
In the process network model, the network evolves by reconfiguration. Reconfiguration changes the representation of the network. With a multi-threaded implementation of the process network system, it is necessary to coordinate concurrent accesses to the representational structure. We compare two approaches to the problem of ensuring consistency of representation during reconfiguration. One is a &quot;localized&quot; view, taken from the viewpoint of the process undergoing reconfiguration. The other requires a &quot;global&quot; view of the entire process network structure. We show how reconfiguration takes place in each case, and compare advantages and disadvantages of each.
Our earlier measurements of global name server performance concentrated on response time measurements. In this paper we examine the shape of response time distributions. These distributions often show clear evidence of multipathing behaviour. We also report on improvements to NeTraMet &apos;s method of collecting data distributions.
In heterogeneous and dynamic environments, efficient execution of parallel computations can require mappings of tasks to processors whose performance is both irregular (because of heterogeneity) and time-varying (because of dynamicity). While adaptive domain decomposition techniques have been used to address heterogeneous resource capabilities, temporal variations in those capabilities have seldom been considered. We propose a conservative scheduling policy that uses information about expected future variance in resource capabilities to produce more efficient data mapping decisions. We first present techniques, based on time series predictors that we developed in previous work, for predicting CPU load at some future time point, average CPU load for some future time interval, and variation of CPU load over some future time interval. We then present a family of stochastic scheduling algorithms that exploit such predictions of future availability and variability when making data mapping decisions. Finally, we describe experiments in which we apply our techniques to an astrophysics application. The results of these experiments demonstrate that conservative scheduling can produce execution times that are both significantly faster and less variable than other techniques.
We have worked on two radio interfaces for IMT-2000 testbed systems, one of which is based on inter-cell  synchronous code division multiple access (CDMA) and the other is based on inter-cell asynchronous CDMA.
A key element of the CoolAgent Personal Assistant vision is the active managementand use of personal, team and organizational information. The finding, filtering, composing, routing and information-triggered notification to a (mobile) user, adapted to the location, schedule, available appliances, tasks and other personal and team context is a key capability within the vision of an agent-based system for personal, professional and team activities. In this paper we report on the current status of a key element, the personal email assistant (PEA), which provides a customizable, machine learning-based environment to support the activities of a major time sink of our daily lives -- the processing of email. The system has been designed to be usable either with or without an agent-based infrastructure, and to be useful with a variety of email systems. In its current form, it leverages and augments the cabilities provided by Exchange and Outlook. It provides capabilities of: smart vacation responder, junk mail filter, efficient email indexing and searching, deleting, forwarding, re-filing, and prioritizing of email. A key contribution of our work has been to leverage high-quality open source components for information retrieval, machine learning, agents and rules to provide a powerful, flexible and robust capability.
The Orion Nebula is a good case study in how science is done by all too  human scientists. This article describes our current understanding of this famous  nebula and demonstrates how intuitions and biases have played an important role  in developing the modern picture of the object.
The present state of the art in computing the error statistics in 3D reconstruction from video  concentrates on estimating the error covariance. A different source of error which has not received  much attention is the fact that the reconstruction estimates are often significantly statistically biased.
This paper presents the implementation of hash functions for IPSEC chip. There is an increasing interest in high-speed cryptographic accelerators for IPSec applications such as VPNs (virtual private networks). Because diverse algorithms are used in Internet, various hash algorithms are required for IPSec chip. Therefore, we implemented SHA-1, HAS-160 and MD5 in one chip. These hash algorithms are designed to reduce the number of gates. SHA-1 module is combined with HAS-160 module. As the result, the required logic elements are reduced by 27%. These hash algorithms have been implemented using Altera&apos;s EP20K1000EBC652-3 with PCI bus interface.
Building asset managers have the formidable task of fulfilling client and tenant accommodation needs as well as striving to retain building integrity while meeting specific financial goals. This undertaking must not be at the expense of various environmental and policy driven requirements. This paper addresses the data, information and knowledge requirements for effective management of a portfolio of buildings as well as discussing available techniques and tools to meet these needs. It provides an overview of tools that are currently available.
INTRODUCTION Liquid crystal displays need reliable anchoring (such as planar, or homeotropic) . To test a range of treatments applied under various conditions, or in an industrial environment, we need a quick and reliable method to measure the zenithal anchoring energy. The method described uses a simple experimental set-up that can be easily automated. Furthermore, because of the simple model used the data analysis is also straightforward, unlike other methods previously published. The experimental set-up is shown in Figure 1. The 2 set-up measures the retardation as a function of applied voltage (an AC voltage is applied and the RMS voltage is recorded). A typical set of experimental data, for the planar anchoring case, is shown in Figure 2. We first model how the director profile varies across the cell. This model is used to calculate the cell&apos;s retardation. By varying the model parameters to get a good fit to the experimental data we can obtain the anchoring strength. The experimen
Worldwide, composting is increasingly becoming a popular option as an environmentally sustainable method of recycling agricultural and municipal wastes. Yet, in Ireland, this technology has not been applied to large volume wastes (pig slurry, spent mushroom compost, cocoa husks, sawdust, shredded paper and poultry litter) for their profitable conversion to economically viable &apos;green&apos; fertilizers. Our preliminary work describes the technology to separate pig slurry solids and the subsequent incomplete (90-day) co-composting with locally available agricultural wastes as absorbent materials. An estimated yield of 2.6 tonnes fresh solids per 100 m of slurry separated was obtained assuming the separator had a throughput of 20 m h    using `perforated rotary drum separator&apos; equipment. During composting carbonaceous material is mineralized as evidenced by the declining C/N ratios in all the 4 batches over time. Concurrently, trends of increasing ash, Ec and bulk density further indicate composting is proceeding albeit slowly in batches 6, 7 and 9. The marked decline in pH from 8.88 to 6.69 and ammonia 0.37% to 0.005% in batch 5 indicate a more advanced decomposition during the same 90-day period. The preliminary results may further suggest that by facilitating better control of environmental parameters the `in vessel&apos; process will result in more efficient composting ultimately producing a more uniform substrate. Pathogen screening of undifferentiated pig slurry indicated that 58% of samples contained Salmonella species and 21% contained Campylobacter species. The presence of oocysts of Cryptosporidium was also detected but the absence of Shigella, E.coli 0157 and Yersinia species were confirmed. Temperature maxima of 61 -- 68    C were achieved during the initial 90-day co-com...
The action system formalism [4] is a state-based approach to distributed computing. In this paper, it is shown how the action system formalism may be used to describe systems that communicate with their environment through synchronised value-passing. Definitions and rules are presented for refining and decomposing such action systems into distributed implementations in which internal communication is also based on synchronised value-passing. An important feature of the composition rule is that parallel components of a distributed system may be refined independently of the rest of the system. Specification and refinement is similar to the refinement calculus approach [2, 22, 24]. The theoretical basis for communication and distribution is Hoare&apos;s CSP [11]. Use of the refinement and decomposition rules is illustrated by the design of an unordered buffer, and then of a distributed message-passing system. 1 Introduction  The action system formalism, introduced by Back &amp; Kurki-Suonio [4], i...
In this paper, we consider a network in which sessions with long-tailed session lengths arrive as Poisson processes. In particular, we assume that a session of type n transmits r n cells per unit time and lasts for a random time ff n with a generalized Pareto distribution given by IP(ff n &gt;x) ff n x -(1+ffn for large x, whereff n ,ff n &gt; 0. The network is assumed to be loop-free with respect to source-destination routes. We characterize the order asymptotics of the complementary buffer occupancy distribution at each node in terms of the input characteristics of the sessions. In particular, we show that the distributions obey a power law whose exponent can be calculated via solving a fixed point and deterministic knapsack problem. The paper concludes with some canonical examples.
Peimbert (2001).  Fig. 2. Upper panel: Hff brightness profile of the best-fit model (solid line), compared to the observed profile by Castaneda et al. (1992) (dot-dashed line). Lower panel: I(ff 6717)/I(ff 6731) ratio profile of the best-fit model, superposed to the observational data by Castaneda et al. (1992) (asterisks).  REFERENCES  
Nonlinear dendritic processing appears to be a feature of biological neurons and  would also be of use in many applications of artificial neural networks. This paper  presents a model of an initially standard linear node which uses unsupervised learning  to find clusters of inputs within which inactivity at one synapse can occlude the activity  at the other synapses.
Usually, the success of systems using automatic course sequencing depends strongly on careful authoring and foreseeing of all curriculum alternatives before any learning session even starts. We believe that tutors, starting from a simple generic curriculum, and assuming that they have the proper tools, can much easier create curriculum alternatives as immediate response to the current learning situation. In this paper we present a tool that provides a flexible environment for tutors allowing them to customize, and develop the curriculum on-the-fly. However, since individual tutoring is quite expensive we shortly discuss possibilities for enabling automatic adjustment of course curriculum to learners&apos; needs by combining on-the-fly curriculum alternatives created by tutors with well-known automatic course sequencing techniques.
This paper proposes the use of particle filtering techniques and Monte  Carlo methods to tackle the in-line and blind equalization of a satellite communication  channel. The main diffculties encountered are the nonlinear distorsions  caused by the amplifier stage in the satellite. Several processing  methods manage to take into account these nonlinearities but they require  the knowledge of a training input sequence for updating the equalizer parameters.
Developing robots with social skills and understanding is a critical step towards enabling them to cooperate with people as capable partners, to communicate with people intuitively, and to learn quickly and effectively from natural human instruction. These abilities would enable many new and exciting applications for robots that require them to play a long-term, supportive, and helpful role in people&apos;s daily lives. This paper describes our work towards building sociable autonomous robots that can work in collaboration with people. Our approach puts an emphasis on task dialog and social communication under the theoretical framework of joint intention theory.
Handover processes in micro-cellular systems are more involved than in cellular systems. In particular, micro-cellular systems suffer from the so-called corner effect where lineof -sight between the mobile station and the base station is suddenly lost when the mobile rounds a corner. As a result, the received signal drops rapidly below threshold level and the call can be lost. It has been shown that, if an accurate estimation of the velocity of the mobile unit is available, the call can be rescued by applying short temporal window averaging on the received signal. Current methods for estimating the velocity of mobile units in micro-cellular systems are based on the level crossing rate of the envelope of the received signals. This paper presents a new velocity estimator based on the instantaneous frequency of the received signal. The performance of the proposed estimator is shown to be superior to that of the level crossing rate method. The average relative error of the proposed estimator is down to below 8% whereas that of the LCR method reaches 14%.
When both TCP and UDP sessions co-exist in the current Internet environment, the performance of TCP sessions easily deteriorate because of congestion incurred by UDP sessions of real-time multimedia applications.
Most of today&apos;s organisations implement intranets to support, amongst other things, their knowledge  management processes. Although intranets appear to be ideal arenas for organisational members to meet and  share knowledge quickly and efficiently, actual usage in organisational practice seems limited. Intranets are  typically marginalised to one-way communication channels for corporate information. The ambition of this  research has been to produce knowledge on how intranets can be designed to better support knowledge  management in organisations. We argue a prerequisite for the intranet to function as a knowledge management  platform is a critical mass of usage anda content relevant for everyday work. Building on the evaluation results  of two implemented interest-based prototype systems, we posit that an intranet must be equipped with  applications that actively afford user participation. In this paperwe contribute knowledge onhow such intranet  applications for knowledge management can be designed to take advantage of users&apos; everyday actions. Such  design-specific knowledge is important for organisations trying to transform their intranets from static  electronic bulletin boards to useful knowledge management platforms.
In simulations of mobile ad hoc networks, the probability distribution governing the movement  of the nodes typically varies over time, and converges to a &quot;steady-state&quot; distribution,  known in the probability literature as the stationary distribution. Some published simulation  results ignore this initialization discrepancy. For those results that attempt to account for this  discrepancy, the practice is to discard an initial sequence of observations from a simulation  in the hope that the remaining values will closely represent the stationary distribution. This  approach is inefficient and not always reliable. However, if the initial locations and speeds of  the nodes are chosen from the stationary distribution, convergence is immediate and no data  need be discarded. We derive the stationary distributions for location, speed, and pause time  for the random waypoint mobility model. We then show how to implement the random waypoint  mobility model in order to construct more efficient and reliable simulations for mobile  ad hoc networks. Simulation results, which verify the correctness of our method, are included.
Experimental data from different sources may suffer from discordant calibrations and possibly covers different regions of the independent variables. A model function spanning the complete range has to account for all available data. In our approach one of the data sets is taken as correct on the absolute scale, while in the other data sets we allow for an unknown scale factor. Bayesian probability theory is employed to evaluate the unknown scale factors and the model parameters.
this paper is the novel drive train developed for the vehicle
We consider (subclasses of) polynomial bottom-up and top-down tree series transducers  over a partially ordered semiring A = (A; ;  ; 0; 1; ), and we compare the classes  of tree-to-tree-series and o-tree-to-tree-series transformations computed by such transducers.
The development of powerful visualisation tools is a major challenge in bioinformatics. Phylogenetics, a field with a growing impact on a variety of life science areas, is experiencing an increasing but poorly met requirement for software supporting the advanced visualisation of phylogenetic trees. Visualisation problems within the domain are commonly experienced by its researchers, but are poorly documented. Furthermore, the applications in the domain have not been reviewed from an information visualisation perspective. In this paper, the problems are defined and the methods employed by phylogenetic applications are reviewed with respect to related research within information visualisation. The results of a survey of the visualisation needs of phylogenetics researchers are also presented.
D. Fensel    and C. Bussler    Vrije Universiteit Amsterdam (VU)  Faculty of Sciences, Division of Mathematics and Computer Science  De Boelelaan 1081a, 1081 HV Amsterdam, the Netherlands  Fax: +31-84-872 27 22, phone: +31-6-51850619  E-mail: dieter@cs.vu.nl    Oracle Corporation  500 Oracle Parkway, Redwood Shores, 94065, CA, U. S. A.
In object-oriented programming languages the class concept is heavily overworked. To simplify things, there is a tendency to unbundle the different roles they play by trying to create more modular inheritance operators [Bracha&amp;Lindstrom92]. This offers the advantage that classes can be composed in a modular way. Prototype-based languages on the other hand, also provide a simpler view on object-oriented programming, but there modular composition is totally neglected.
The thermal generator maintenance scheduling problem has been tackled by a variety of traditional optimisation techniques over the years. While these methods can give an optimal solution to small scale problems, they are often inefficient and impractical when applied to larger problems.  In this paper, we employ a multi-stage approach where the problem is decomposed into smaller sub-problems, each of which can be solved much more efficiently by existing algorithms. After each part of the problem has been solved, the results are then recombined to form the solution to the whole problem.  Both tabu search and a memetic algorithm have been observed to produce very good results but they take a significant amount of time to run. In this paper we utilise both techniques to form the basis of a multi-stage approach to solve the thermal generator maintenance scheduling problem.  The results will demonstrate that the multistage methodology is just as effective for this problem while achieving a significant reduction in run-time.  
In this paper we introduce a complete framework to automatically generate &quot;cinematographic view&quot; of dynamic scenes in real-time. The main goal of such a system is to provide a succession of shots and sequences (of virtual dynamic scenes) that can be related to pure cinema. Our system is based on the use of an image-based control of the camera that allows different levels of visual tasks and a multi-agent system that controls those cameras and selects the type of shot that has to be performed in order to fulfill the constraints of a given cinematographic rule (idiom). This level of adaptation constitutes the major novelty of our system. Moreover, it stands for a convenient tool to describe cinematographic idioms for real-time narrative virtual environments.
In this paper, we describe a new interface for musical performance, using the interaction with a graphical user interface in a powerful manner: theuserdirectly touches a screen where graphical objects are displayed and can use several fingers simultaneously to interact with the objects. The concept of this interface is based on the superposition of the gesture spatial place and the visual feedback spatial place; i t givesthe impression that thegraphical objectsare real. This concept enables a huge freedom in designing interfaces. The gesturedevice we have createdgives the position of four fingertips using 3D sensors and the data is performed in the Max/MSPenvironment. We have realized twopractical examples of musical use of such a device, using Photosonic Synthesis and Scanned Synthesis.
ents, Causality, and Space-Time Diagrams  To model a distributed system, one typically considers processes which communicate by messages and which execute sequences of events (i.e., elementary or atomic actions). These events occur at specific instants in time. They are usually classified into send events, receive events, and internal events. An execution of a distributed system on such an abstract level can be depicted with the help of a space-time diagram (see the first figure) where time moves from left to right. Messages are drawn as arrows, and events are depicted by dots.  Events are related to each other: Events occurring at a particular process are linearly ordered by their local sequence of occurrence, and each receive event has a corresponding send event that happens earlier. Formally, one defines the causality relation `&lt;&apos; as the smallest tr
In reinforcement learning the interaction between the agent  and the environment generally takes place on a  xed time scale, which  means that the control interval is set to a  xed time step. In order to  determine a suitable  xed time scale one has to trade o accuracy in control  against learning complexity. In this paper we present an alternative  approach that enables the agent to learn a control policy by using multiple  time scales simultaneously. Instead of preselecting a  xed time scale,  there are several time scales available during learning and the agent can  select the appropriate time scale depending on the system state. The  dierent time scales are multiples of a  nest time scale which is denoted  as the primitive time scale. Actions on a coarser time scale consist of  several identical actions on the primitive time scale and are called multistep  actions (MSAs). The special structure of these actions is eciently  exploited in our recent MSA-Q-learning algorithm. We use the MSAs  to learn a control policy for a thermostat control problem. Our algorithm  yields a fast and highly accurate control policy; in contrast, the  standard Q-learning algorithms without MSAs fails to learn any useful  control policy for this problem.
In this thesis we propose a complete formal framework for the analysis of timed systems, with the emphasis given on the practicality of the approach. We describe timed systems in the formal model of timed automata, finite-discrete-state automata equipped with clocks in a dense-time domain. Properties of such systems are expressed in the linear-time formalism of timed Büchi automata (timed automata with acceptance conditions), or in one of the branching-time logics CTL, TCTL or etctl. These formalisms cover a large spectrum of properties on the order of events and the timing constraints on the delays between events. We also examine other interesting properties such as deadlock and timelock freedom or reachability. We consider two types of analysis. Verification : given a system and a property, check whether the system satisfies the property. Controller synthesis : given a system and a property, find a restriction of the system which satisfies the property. These problems have been proven decidable in previous works, however, with a high (exponential) complexity, basically due to the fact that the state space is extremely large (state explosion) and has to be entirely generated and explored. To respond to the challenge of making the approach tractable, we propose methods which are efficient in practice, despite of the high worst-case theoretical complexity. Our approach is based on two key elements. First, on abstractions which reduce the concrete state space to a much smaller abstract state space, while preserving all properties of interest. Second, on efficient techniques to compute and explore the abstract state space. We define two sets of abstractions and study the properties they preserve. Time-abstracting bisimulations are equivalences which hide the quantitative aspect of time : we know that some time passes, but not how much. The stronger of these bisimulations preserves all properties of interest. Time-abstracting simulations are abstractions derived by a forward reachability analysis on the system. These abstractions preserve only linear properties. The analysis methods differ depending on the underlying abstraction(s) used. In the case of bisimulations, the approach consists in two steps : first, generate the time-abstracting quotient of the state space, then apply classical (untimed) analysis techniques to the quotient to prove properties of the concrete system. In the case of simulations, the generation of the abstract state space and the analysis are performed at the same time. This technique is called on-the-fly and can often provide fast answers without having to generate the entire (abstract) state space. We develop on-the-fly verification techniques for TBA and ETCTL.
Distributed Model Checking avoids the state explosion problem by using the computational resources of parallel environments LTL model checking mainly entails detecting accepting cycles in a state transition graph. The nested depth-first search algorithm used for this purpose is difficult to parallelize since it is based on the depth-first search traversal order which is inherently sequential. Proposed solutions make use of data structures and synchronization mechanisms in order to preserve the depth-first order. We propose a simple distributed algorithm that assumes cycles to be localized by the partition function. Cycles can then be checked without requiring particular synchronization mechanisms. Methods for constructing such kind of partition functions are also proposed.
The investigation of genetic and evolutionary algorithms on Ising model problems  gives much insight how these algorithms work as adaptation schemes. The  Ising model on the ring has been considered as a typical example with a clear building  block structure suited well for two-point crossover. It has been claimed that  GAs based on recombination and appropriate diversity-preserving methods outperform  by far EAs based only on mutation. Here, a rigorous analysis of the expected  optimization time proves that mutation-based EAs are surprisingly effective. The  (1 + ) EA with an appropriate -value is almost as effcient as usual GAs. Moreover,  it is proved that specialized GAs do even better and this holds for two-point  crossover as well as for one-point crossover.
The rising threat of cyber attacks, especially distributed denial-of-service (DDoS), makes the IP Traceback problem very relevant to today&apos;s Internet security. IP Traceback is one of the security problems associated with identifying the source of the attack packets. This work presents a novel approach to IP Traceback - Deterministic Packet Marking (DPM). The proposed approach is scalable, simple to implement, and introduces no bandwidth and practically no processing overhead on the network equipment. It is capable of tracing thousands of simultaneous attackers during DDoS attack. All of the processing is done at the victim. The traceback process can be performed post-mortem, which allows for tracing the attacks that may not have been noticed initially. The involvement of the Internet service providers (ISP) is very limited, and changes to the infrastructure and operation required to deploy DPM are minimal. DPM performs the traceback without revealing the internal topology of the provider&apos;s network, which is a desirable quality of a traceback scheme.
Montgomery&apos;s algorithm [8], hereafter denoted Mn (,  ),  is a  process for computing Mn (A, B) = ABN mod n where N is a constant  factor depending only on n. Usually,
This paper was written by the author in his private capacity. No official support or endorsement by the National Library of Medicine is intended or should be inferred
We consider a graph theoretic approach for  automatic construction of options in a dynamic  environment. A map of the environment  is generated on-line by the learning  agent, representing the topological structure  of the state transitions. A clustering algorithm  is then used to partition the state  space to different regions. Policies for reaching  the different parts of the space are separately  learned and added to the model in  a form of options (macro-actions). The options  are used for accelerating the Q-Learning  algorithm. We extend the basic algorithm  and consider building a map that includes  preliminary indication of the location of &quot;interesting  &quot; regions of the state space, where  the value gradient is significant and additional  exploration might be beneficial. Experiments  indicate significant speedups, especially  in the initial learning phase.
This paper presents a pattern language covering common  techniques in this area, as well as technology projections and known uses. The target  audience of this paper are developers who want to develop new solution or modify an existing  solution in one of the named software engineering areas, as well as developers who  want to gain a deeper understanding of the internal workings of the software engineering  tools and frameworks they use
In many different fields such as hydrology, telecommunications, physics of condensed matter and finance, the gaussian model results unsatisfactory and reveals difficulties in fitting data with skewness, heavy tails and multimodality. The use of stable distributions allows for modelling skewness and heavy tails but gives rise to inferential problems related to the estimation of the stable distribution&apos;s parameters. The aim of this work is to generalise the stable distribution framework by introducing a model that accounts also for multimodality. In particular we introduce a stable mixture model and a suitable reparameterisation of the mixture, which allow us to make inference on the mixture parameters. We use a full Bayesian approach and MCMC simulation techniques for the estimation of the posterior distribution.
this paper is on supporting multidisciplinary communities of scientists and engineers. We discuss requirements for Grid computing environments (GCEs) in this context, and describe several core support technologies developed to meet these requirements. Our work extends the notion of a programming environment beyond the compile--schedule--execute paradigm, to include functionality such as collaborative application composition, information services, and data and simulation management. Systems designed for five different applications communities are described. These systems illustrate common needs and characteristics arising in multidisciplinary communities and motivate a highlevel design framework for building GCEs that meet those needs. Copyright ff 2002 John Wiley &amp; Sons, Ltd
High quality, physically accurate rendering at interactive rates has widespread application, but is a daunting task. We attempt to bridge the gap between high-quality offline and interactive rendering by using existing environment mapping hardware in combination with a novel Image Based Rendering (IBR) algorithm. The primary contribution lies in performing IBR in reflection space. This method can be applied to ordinary environment maps, but for more physically accurate rendering, we apply reflection space IBR to radiance environment maps. A radiance environment map pre-integrates a Bidirectional Reflection Distribution Function (BRDF) with a lighting environment. Using the reflection-space IBR algorithm on radiance environment maps allows interactive rendering of arbitrary objects with a large class of complex BRDFs in arbitrary lighting environments. The ultimate simplicity of the final algorithm suggests that it will be widely and immediately valuable given the ready availability of hardware assisted environment mapping.
The aim of Hospitalitas (Healthcare Online Shared Platform for Increasing Tessin And  Lombardy Immigrant Treatment and ASsistance) is to create an online platform for the continuing  education on the assistance of immigrant patients coming form non-western cultures .
The goal of this paper is to offer a framework for classification of images and video according to their &quot;type&quot;, or &quot;style&quot;----a problem which is hard to define, but easy to illustrate; for example, identifying an artist by the style of his/ her painting, or determining the activity in a video sequence. The paper offers a simple classification paradigm based on local properties of spatial or spatio-temporal blocks. The learning and classification are based on the naive Bayes classifier. A few experimental results are presented.
Introduction  The TSIMMIS system [1] integrates data from multiple heterogeneous sources and provides users with seamless integrated views of the data. It translates a user query on the integrated views into a set of source queries and postprocessing steps that compute the answer to the user query from the results of the source queries. TSIMMIS uses a  mediation architecture [11] to accomplish this (Figure 1). User Source 1 Source 2 Source N  Mediator Figure 1: The TSIMMIS Architecture Many other data integration systems like Garlic [2, 9] and Information Manifold [4] employ a similar architecture. One of the distinguishing features of TSIMMIS is its use of a semi-structured data model (called the Object Exchange Model or OEM [7]) for dealing with the heterogeneity of the data sources. In particular, it employs source wrappers [3] that provide a uniform OEM interface to the mediator. In SIGMO
This document describes the designs of a generic distributed certification authority and of a trusted party for optimistic fair exchange that are based on fault-tolerant middleware for service replication. It also discusses other uses of the replication middleware for implementing trusted services. It may serve as a blueprint for building generic trusted third-party services that use the state-machine replication approach.
The server-based computing (SBC) model is becoming an increasingly popular approach for delivering computational services with reduced administrative costs and better resource utilization. In this paper, we examine how effectively SBC architectures support multimedia applications. We focus on the effectiveness of the remote display protocol used in three popular SBC platforms for supporting video applications, Citrix Metaframe, Windows Terminal Server, and AT&amp;T VNC. Our results show that SBC can be a viable approach for delivering VCR-quality video in LAN environments, but that existing solutions are inadequate at network access bandwidths found in broadband environments. Our results also show that SBC can deliver video with comparable network efficiency as streaming media solutions. We show that there is wide variation in the performance of the remote display technologies and discuss the factors that influence their performance.  1 Introduction  Anticipating the growing demand for mul...
Programming languages often provide for various sorts of static type checking as a way of detecting invalid programs. However, statically checked type systems often require unnecessarily specific type information, which leads to frustratingly inflexible languages. Polymorphic type systems restore flexibility by allowing entities to take on more than one type. This thesis discusses polymorphism in statically typed programming languages. It provides precise definitions of the term &quot;polymorphism&quot; and for its varieties, &quot;ad-hoc polymorphism&quot;, &quot;universal polymorphism&quot;, &quot;inclusion polymorphism&quot;, and &quot;parametric polymorphism&quot;, and surveys and compares many existing mechanisms for providing polymorphism in programming languages. Finally, it introduces a new polymorphism mechanism, contextual polymorphism. Contextual polymorphism is a variant of parametric polymorphism that is based on contexts, which are abstractions of collections of declarations, and assertions, which link polymorphic routin...
We present a new definition of an implicit surface over a noisy point cloud, based on the weighted least squares approach. It can be evaluated very fast, but artifacts are significantly reduced. We propose to use a different...
With the rapid proliferation of a wide range of input devices, there are many choices in designing or selecting a 6DOF input device. User perception of the devices is an important aspect of design. We complement existing literature on the influence of grip of dominant hand on performance times with our experiments on the influence of grip of non-dominant hand on perceived ease-of-use, control and fatigue. Our results show that for the nondominant hand, the finger grip is perceived as being easy to use, less fatiguing and more controllable.
Nested state diagrams are a commonly accepted design notation for modelling  complex software systems at different levels of detail. Besides nesting, other layering  mechanisms are needed. However, incremental development of layered state diagrams raises the  problem of change management. Changes made at high abstraction layers can have a serious  impact on more concrete layers, as conflicts can be introduced that are propagated through all  layers. To resolve these conflicts, the methodology of reuse contracts is applied. Carefully  choosing a set of incremental modification operators and investigating their interactions allows  us to categorise the conflicts. These conflicts can be detected semi-automatically, facilitating  incremental design.
We present a method to recognize three-dimensional objects from phase-shift digital holograms. The holograms are used to reconstruct various views of the objects. These views are combined to create nonlinear composite filters in order to achieve distortion invariance. We present experiments to illustrate the recognition of a 3D object in the presence of outof -plane rotation and longitudinal shift along the z-axis.
Although the possibility of attacking smart-cards by analyzing  their electromagnetic power radiation repeatedly appears in research  papers, all accessible references evade the essence of reporting conclusive  experiments where actual cryptographic algorithms such as des or rsa  were successfully attacked.
Various network monitoring and performance evaluation schemes generate considerable amount of traffic, which affects network performance. In this paper we describe a method for minimizing network monitoring overhead based on Shortest Path Tree (SPT) protocol. We describe two different variations of the problem: the A-Problem and the E-Problem, and show that there is a significant difference between them. We prove that finding optimal solutions is NP -hard for both variations, and propose a theoretically best possible heuristic for the A-Problem and three different heuristics for the E-Problem, one of them being also theoretically best possible. We show that one can compute in polynomial time an O(ln|V  |)-approximate  solution for each of these problems. Then, we analyze the performance of our heuristics on large graphs generated using Waxman and Power-Law models as well as on real ISP topology maps. Experiment results show more than 80% improvement when using our heuristics on real topologies over the naive approaches.
This paper presents CAMEL, a spatially-distributed conceptual model for simulating reactive transport of phosphorus from diffuse sources at the catchment scale. A catchment is represented in the model using a network of grid cells and each grid cell is comprised of various conceptual storages of water, sediment and phosphorus. To allow for reactive transport processes of phosphorus between grid cells, two cascade routing schemes are used for groundwater and channel water flows, respectively. The model has a modular, object-oriented structure so that it can be easily modified or extended and, furthermore, it can even provide a library of hydrological and hydrochemical processes from which the user can select a sub-set of processes suitable for a particular application. A verification study of the model has been carried out for a hypothetical catchment with satisfactory results.
We survey the evaluation methodology adopted in Information Extraction (IE), as defined in the MUC conferences and in later independent efforts applying machine learning to IE. We point out a number of problematic issues that may hamper the comparison between results obtained by different researchers. Some of them are common to other NLP tasks: e.g., the difficulty of exactly identifying the effects on performance of the data (sample selection and sample size), of the domain theory (features selected), and of algorithm parameter settings. Issues specific to IE evaluation include: how leniently to assess inexact identification of filler boundaries, the possibility of multiple fillers for a slot, and how the counting is performed. We argue that, when specifying an information extraction task, a number of characteristics should be clearly defined. However, in the papers only a few of them are usually explicitly specified. Our aim is to elaborate a clear and detailed experimental methodology and propose it to the IE community. The goal is to reach a widespread agreement on such proposal so that future IE evaluations will adopt the proposed methodology, making comparisons between algorithms fair and reliable. In order to achieve this goal, we will develop and make available to the community a set of tools and resources that incorporate a standardized IE methodology.
this paper we describe the software architecture of the PSET evaluation package, whichwas used to evaluate the segmentation algorithms. The description of the architecture will allow researchers to understand the software better, replicate our results, evaluate new algorithms, experiment with new metrics and datasets, etc. The software is written using the C language on the SUNffUNIX platform and is being made available to researchers at no cost
In this paper we propose a method for personalizing the catalogs  of Web Portals. We propose SCSL, a declarative language  for defining personal semantic channels over Web Portal  catalogs. A semantic channel is actually a view of one or  more Portal catalogs. SCSL offers powerful primitives for  filtering and restructuring available thematic topics and classified  resources. A user can connect to a Portal infomediary  in order to register, browse, or query his/her semantic channel.
Violations of a number of common safety properties of multithreaded programs--such as atomicity, mutual exclusion and absence of dataraces--cannot be observed by looking at the linear execution trace. We characterize a class of such properties, called robust properties, and define a simple but expressive epistemic logic to specify them. We then develop an efficient algorithm to automatically monitor and predict violations of robust safety properties. Our algorithm is based on capturing the causal structure of a computation through a mechanism similar to vector clock updates. The algorithm automatically synthesizes decentralized monitors to evaluate the information at each thread and to detect and predict safety violations. Based on this approach, a tool named DAME has been developed and evaluated on some simple examples.
In order to obtain high confidence in the software embedded into a  smart card, we evaluated different techniques like model checking and  theorem proving. Nevertheless due to the low cost of smart cards and  mechanical constraints, the amount of memory available on chips is small.
Current systems for managing workload on clusters of workstations, particularly those available for Linux-based (Beowulf) clusters, are typically based on traditional process-based, coarse-grained parallel and distributed programming. The DESPOT project is building a sophisticated thread-level resource-monitoring system for computational, storage and network resources [2]. The original implementation of DESPOT was based on SGI&apos;s Performance Co-Pilot (PCP) to facilitate the collection of performance monitoring data and to provide an API for the scheduling algorithm to retrieve the data. Unfortunately, the overhead of PCP and the infrastructure required to use PCP slowed down the performance of the DESPOT scheduling algorithms. In this paper we present an alternative to PCP which we call Distop. Distop was developed specically to satisfy the needs of the DESPOT project for low-overhead, ne-grained resource-monitoring tools for per-process network and other resource usage. We also present experimental results indicating the overhead of our system is minimal while providing accurate resource utilization data.
This paper provides an elementary introduction to the probabilistic automaton (PA) model, which  has been developed by Segala. We describe how distributed systems with discrete probabilities can  be modeled and analyzed by means of PAs. We explain how the basic concepts for the analysis of  nonprobabilistic automata can be extended to probabilistic systems. In particular, we treat the parallel  composition operator on PAs, the semantics of a PA as a set of trace distributions, an extension of the  PA model with time and simulation relations for PAs. Finally, we give an overview of various other state  based models that are used for the analysis of probabilistic systems.
We measure and analyze the single-hop packet delay through operational routers in a backbone IP network. First we present our delay measurements through a single router. Then we identify stepby -step the factors contributing to single-hop delay. In addition to packet processing, transmission, and queueing delays, we identify the presence of very large delays due to non-work-conserving router behavior. We use a simple output queue model to separate those delay components. Our step-by-step methodology used to obtain the pure queueing delay is easily applicable to any single-hop delay measurements.
Smartcard chips vendors have always done their best to embed  more memory inside cards. These efforts are driven to allow their customers -  smartcard manufacturers - to mask more software inside cards (in ROM) but,  above all, to help them to provide cards with more memory dedicated to the  application (EEPROM). Even if the geometry is getting smaller and smaller,  some applications do not match with the current memory limitations due to  smartcard constraints making impossible for the chips to be more than just a  few millimeter square. The goal of the Extended Memory Card project is to  suggest an architecture in which smartcards can securely &quot;contain&quot; more data  than their own memory allows it. The card acts as a key to access information  stored outside of it.
Named entity tagging comprises the sub-tasks of identifying a text span and classifying it, but this view ignores the relationship between the entities and the world. Spatial and temporal entities ground events in space-time, and this relationship is vital for applications such as question answering and event tracking. There is much recent work regarding the temporal dimension [13, 10], but no extensive study of the spatial dimension.
this paper we focus on the routing of bandwidth guaranteed  flows in a dynamic scenario where new connection requests arrive at the network edge nodes
A wide variety of computer architectures have been proposed that attempt to exploit parallelism at different granularities. For example, pipelined processors and multiple instruction issue processors exploit the fine-grained parallelism available at the machine instruction level, while shared memory multiprocessors exploit the coarse-grained parallelism available at the loop level. Using a registertransfer level simulation methodology, this paper examines the performance of a multiprocessor architecture that combines both coarse-grained and fine-grained parallelism strategies to minimize the execution time of a single application program. These simulations indicate that the best system performance is obtained by using a mix of fine-grained and coarse-grained parallelism in which any number of processors can be used, but each processor should be pipelined to a degree of 2 to 4, or each should be capable of issuing from 2 to 4 instructions per cycle. These results suggest that current high-performance microprocessors, which typically can have 2 to 4 instructions simultaneously executing, may provide excellent components with which to construct a multiprocessor system.
this article: John Maxwell, who proposed a similar architecture in conversations with the second author, Ron Kaplan, Miriam Butt, Fr&apos;ed&apos;erique Segond and Veronika Knuppel. In particular we thank Joan Bresnan for extensive comments and suggestions. The issues she raised could not be discussed in sufficient detail in this short contribution. Needless to say that the commentators do not necessarily share the perspective we are taking here. Special thanks go also to Marc Dymetman for judgements on French data. We alone are responsible for remaining errors.
In this paper, a new fuzzy-adaptive mould level control to improve the steel quality in the continuous casting process will be described. The concept does not required additional hardware. The presented commissioning results show a better control behaviour e.g. in case of worn stoppers.
In this paper, we have analyzed the convolutional coded error performance of a 2D-RAKE receiver, in combination with transmit diversity on the downlink of a WCDMA system. The analyses assume correlated fading between receive antenna array elements, and an arbitrary number of independent but nonidentical resolvable multipaths combined by the RAKE receiver in the general Nakagami-m fading channel framework. The closed form expression of pairwise error probability is given in the simple form of a single finite limit integral, with the integrand being an elementary function of array configuration parameters, spatial correlation and operating environment factors. It is shown that the combination of coding and spatial-path combining lead to dramatic performance improvement in various fading environments.
this paper we study the disclosure risk associated to a confidential business data file (target data). We distinguish between two types of data attack. On the one hand Database cross match and on the other hand Match for a single individual (see Elliot/Dale, 1999). To disclose a data set the &quot;data intruder&quot; needs additional information -- e.g. an outside database -- containing a certain number of identical variables (key or matching variables) with the target data
With the pattern-driven process design approach we provide an instrument  for the design of business processes which is based on objective performance  criteria. Since enterprises are more and more confronted with an extreme dynamic  environment a technique for measuring the flexibility of business processes  is incorporated which enables process designers to chose the most flexible process  for their purpose. The pattern-driven process design has been applied within the  communication center domain. Two types of domain specific process patterns  have been identified regarding the qualification-mixture and communicationmixture  within the organization. These real life patterns have been evaluated on  the base of empirical data from four different lines of business: car rental, bank,  book trade and energy industry.
Data-Oriented Parsing (DOP) models of natural language propose that human language  processing works with representations of concrete past language experiences rather than  with abstract linguistic rules. These models operate by decomposing the given  representations into fragments and recomposing those pieces to analyze new utterances.  A probability model is used to select from all possible analyses of an utterance the most  likely one. Previous DOP models were based on simple tree representations that neglect  grammatical functions and syntactic features (Tree-DOP). In this paper, we present a new  DOP model based on the more articulated representations of Lexical-Functional  Grammar theory (LFG-DOP). LFG-DOP triggers a new, corpus-based notion of  grammaticality, and an interestingly different class of probability models. An empirical  evaluation of the model shows that larger as well as richer fragments improve  performance. Finally, we go into some of the conceptual implications of our approach.  1 
The ultimate in color reproduction is a display that can produce arbitrary spectral content over a 300-800 nm range with 1 arc-minute resolution in a full spherical hologram. Although such displays will not be available until next year, we already have the means to calculate this information using physically-based rendering. We would therefore like to know: how may we represent the results of our calculation in a device-independent way, and how do we map this information onto the displays we currently own? In this paper, we give an example of how to calculate full spectral radiance at a point and convert it to a reasonably correct display color. We contrast this with the way computer graphics is usually done, and show where reproduction errors creep in. We then go on to explain reasonable short-cuts that save time and storage space without sacrificing accuracy, such as illuminant discounting and human gamut color encodings. Finally, we demonstrate a simple and efficient tone-mapping technique that matches display visibility to the original scene.
The widespread availability of platforms for producing hyper-systems (i.e. hypertext and hypermedia) shows great potential for producing computer based learning environments. However, we do not yet have a great deal of experience in understanding how to make best use of the structural and media possibilities opened up by such systems. This paper explores the issues by analysing the techniques used by designers within a variety of different hyper-systems and begins to develop a space of potential design solutions and their likely impact on learning.
Packet loss, delay and jitter degrade the quality of services like VoIP (Voice over IP) or Video Streaming over IP networks. In real networks, an experimental measure of these parameters is fundamental in the planning process of new services over novel network infrastructures. Furthermore, currently networks are heterogeneous in terms of access network technologies, end-users&apos; devices, Operating Systems and finally end-users&apos; application. This heterogeneity exacerbates even more the need of a real assessment of Quality of Service metrics. In this work we provide an empirical performance study of a real heterogeneous network with respect to delay, jitter, throughput and packet loss, in UDP and TCP environments, by using an innovative tool for network performance evaluation that we called D-ITG (Distributed Internet Traffc Generator). We also introduce the concept of &quot;Service Condition &quot; as a mechanism to cope with issue related to dynamically changing network service scenarios. A comparative analysis between our practical results and an analytical model recently presented in literature is presented. Results presented in this paper can be used as performance references for development of wireless communication applications over multiservice heterogeneous networks.
The goal of this research is to study the conditions in which evolution may lead to the sustained emergence of novel behaviours, and how this may be applied to the automatic design of complex entities. We argue that with regard to artificial evolution, this field of study has been rather overlooked, with a preference given to mathematical or experimental results based on abstract selection models. We argue further that when practicioners of artificial evolution have tried to harness the dynamics of more natural selective processes, this relative lack of connection with traditional evolutionary theory has often led to implicit assumptions and &quot;commonsense &quot; intuitions, which may have prevented a better understanding (and more reasonable expectations) about these systems.
This paper discusses the evolution of optical systems starting from WDM links to Optical Packet Switching Networks via some stages as Switched Optical Networks, Multilayer Networks with either traffic or wavelength grooming, Optical VPNs and Optical Burst Switching. We discuss the advantages and drawbacks of certain approaches since the future developments of optical networking technology might influence this evolution. We also draw our near-term and long-term visions of optical networking.
: A new unfolding approach to LTL model checking is presented, in which the model checking problem can be solved by direct inspection of a certain finite prefix. The techniques presented so far required to run an elaborate algorithm on the prefix.  KEYWORDS: Net unfoldings, model checking, tableau systems, LTL, Petri nets  Contents  1 Introduction 1 2 Petri nets 2 3 Automata theoretic approach to model checking LTL 3 3.1 From emptiness checking to illegal !-traces and illegal livelocks 6 3.2 Setting and running example . . . . . . . . . . . . . . . . . . 6 4 Basic definitions on unfoldings 7 5 A tableau system for the illegal !-trace problem 9 5.1 Adequate orders . . . . . . . . . . . . . . . . . . . . . . . . . 10 5.2 The tableau system . . . . . . . . . . . . . . . . . . . . . . . 10 6 A tableau system for the illegal livelock problem 13 6.1 Computing the set of checkpoints. . . . . . . . . . . . . . . . 13 6.2 The tableau system . . . . . . . . . . . . . . . . . . . . . . . 13 6.3 A...
this paper, we are going to discuss the challenges we have to face in designing a search service for locating software components on the Grid. Indeed, the specifications of our search engine rely heavily on the concept of Ecosystem of Components. Basically it consists in an extension of the marketplace concept to the Grid. The ecosystem, in fact, provides applications a virtual environment where they can live (i.e. are executed), and find other components to which they can refer (i.e. cooperate) to build larger (and more complex) systems
Many applications make use of hybrid programming models intermixing task and data parallelism in order to exploit modern architectures more efficiently. However, unbalanced computational load or idle times due to tasks that are blocked either in I/O or waiting on results from other tasks can cause significant performance problems. Fortunately, such idle times can be overlapped with useful computation in many cases. In this paper we propose a simple, yet powerful methodology for specifying intra-object parallelism and synchronization in the context of the coordination language Opus. Our design combines both, static and dynamic synchronization on a high level. We motivate our design with some examples and discuss implementation strategies for compilation as well as runtime support. 1 Introduction  With the advent of teraflops supercomputers and the usage of the Internet as a huge &quot;metacomputer &quot;, the complexity of simulations being tackled by scientists and engineers is increasing expone...
Structured peer-to-peer overlay networks have recently emerged as good candidate infrastructure for building novel large-scale and robust Internet applications in which participating peers share computing resources as equals. In the past three year, various structured peer-to-peer overlay networks have been proposed, and probably more are to come. We present a framework for understanding, analyzing and designing structured peer-to-peer overlay networks. The main objective of the paper is to provide practical guidelines for the design of structured overlay networks by identifying a fundamental element in the construction of overlay networks: the embedding of k-ary trees. Then, a number of effective techniques for maintaining these overlay networks are discussed. The proposed framework has been effective in the development of the DKS system.
In [TASC03], we proposed an algebra with four algebraic  operators, whose composition can be used to generate valid compound  terms in a given faceted taxonomy in an effcient and flexible manner.
In this paper we present an empirical comparison among conservative synchronization protocols for distributed discrete-event simulation which are suitable for the bulk-synchronous parallel (BSP) model of computing. A number of optimizations were introduced which produced improved protocols. We also establish comparisons with an optimistic synchronization protocol called BSP Time Warp. We assessed the performance of the protocols by using architecture independent performance metrics, and quanti  ed empirically these metrics by executing a demanding work-load under dierent situations.
XPath and XQuery (which includes XPath as a  sublanguage) are the major query languages for  XML. An important issue arising in effcient evaluation  of queries expressed in these languages is  satisfiability, i.e., whether there exists a database,  consistent with the schema if one is available, on  which the query has a non-empty answer. Our experience  shows satisfiability check can effect substantial  savings in query evaluation.
In this paper, we describe the Rich Representation Language (RRL) which is used in the NECA system. The NECA system generates interactions between two or more animated characters. The RRL is a formal framework for representing the information that is exchanged at the interfaces between the various NECA system modules.
Human&apos;s intention plays a key role in humanmachine interaction as in the case of a robot serving for a handicapped person. The quality of a service robot will be much enhanced if the robot can infer the human&apos;s intension during the interaction process. In this paper, we propose a soft computing-based technique to read a user&apos;s intention using some multisensors-based approach. We have tested the technique by a scenario of `serving a drink to the user&apos;. With such force/torque or vision sensor, the robot can effectively infer the user&apos;s intention  to drink the beverage or not to drink. As an application, this intention technique is employed for building a rehabilitation robot, called KARES II, to perform various human-friendly  human-robot interaction.
Emerging Web standards promise a network of heterogeneous  yet interoperable Web Services. Web  Services would greatly simplify the development  of many kinds of data integration and knowledge  management applications. Unfortunately, this vision  requires that services provide large amounts  of semantic metadata &quot;glue&quot;. As a first step to automatically  generating such metadata, we describe  how machine learning and clustering techniques  can be used to attach attach semantic metadata to  Web forms and services.
This paper investigates the power of genetic algorithms at solving the MAX-CLIQUE problem. We measure the performance of a standard genetic algorithm on an elementary set of problem instances consisting of embedded cliques in random graphs. We indicate the need for improvement, and introduce a new genetic algorithm, the multi-phase annealed GA, which exhibits superior performance on the same problem set. As we scale up the problem size and test on &quot;hard&quot; benchmark instances, we notice a degraded performance in the algorithm caused by premature convergence to local minima. To alleviate this problem, a sequence of modifications are implemented ranging from changes in input representation to systematic local search. The most recent version, called union GA, incorporates the features of union cross-over, greedy replacement, and diversity enhancement. It shows a marked speed-up in the number of iterations required to find a given solution, as well as some improvement in the clique size fou...
... This paper investigates implicit knowledge  in two ontologies of anatomy: the Foundational Model of Anatomy and GALEN. The methods  consist of extracting the knowledge explicitly represented, acquiring the implicit knowledge  through augmentation and inference techniques, and identifying the origin of each semantic  relation. The number of relations (12 million in FMA and 4.6 million in GALEN),  broken down by source, is presented. Major findings include: each technique provides specific  relations; and many relations can be generated by more than one technique. The application  of these findings to ontology auditing, validation, and maintenance is discussed, as well  as the application to ontology integration.
In an experiment, subjects learned about new categories for  which they had prior beliefs, and made probability  judgments at various points during the course of learning.  The responses were analyzed in terms of bias due to prior  beliefs and in terms of sensitivity to the content of the new  categories. These results were compared to the predictions  of four models of belief revision or categorization: (1) a  Bayesian estimation procedure (Raiffa &amp; Schlaifer, 1961);  (2) the integration model (Heit, 1993, 1994), a categorization model that is a generalization of the Bayesian  model; (3) a linear operator model that performs serial  averaging (Bush &amp; Mosteller, 1955); and (4) a simple  adaptive network model of categorization (Gluck &amp; Bower,  1988) that is a generalization of the linear operator model.  Subjects were conservative in terms of sensitivity to new  information, compared to the predictions of the Bayesian  model and the linear operator model. The network model  was able to...
in the last decade in the form of APIs for existing languages or as brand new languages. Although such mobile agents have many benefits, they have not yet conquered the internet. This is partly because programming such mobile agents is not a straightforward task. There is a need for languages that better structure such mobile agent applications. In this position paper we want to advocate the use of prototype based paradigm as a basis for programming such mobile agents.
Water quality Risk Analysis Tool (WaterRAT) is recently developed software for supporting surface water quality management. The software contains a library of river and lake quality models, aiming to give flexibility over specification of model scope, complexity and scale. Various sources of uncertainty can be included in the analysis, including uncertainty in boundary conditions, initial conditions, parameters, model structure and management objectives. Water quality can then be modelled allowing for these sources of uncertainty. Important data uncertainties can be indicated, and so data collection programmes can be suitably refined. In this paper, the motivation for the WaterRAT tool and the methods it employs are presented, its features are described, and its utility for uncertainty evaluation and sensitivity analysis is demonstrated using a river water quality management problem. Emerging challenges for modellers, which cannot yet be addressed using WaterRAT, are discussed.
The paper develops a distributed systems architecture for dependable Internet based online auctions, meeting the requirements of data integrity, responsiveness, fairness and scalability. Current auction services essentially rely on a centralised auction server. Such an approach is fundamentally restrictive with respect to scalability. It is well-known that a tree-based, recursive design approach caters well for scalability requirements. With this observation in mind, the paper develops an approach that permits an auction service to be mapped on to globally distributed auction servers. The paper selects a suitable auction model that treats sellers and buyers symmetrically. This symmetry enables a computational node to play at one level of the tree the role of a seller by dealing with a group of potential buyers as well as to play the role of a potential buyer at the next higher level. Such a symmetric auction (also known as a double auction) is used for supporting a standard auction to be carried out in a hierarchic manner. An architecture is developed and basic algorithms and protocols are presented, together with correctness reasoning.
In recent years, search engine research has grown rapidly in areas such as algorithms, strategies and architecture, increasing both effectiveness and quality of results. However, a very important aspect that is often neglected is the user interface. In this work we analyzed the interfaces of several popular search tools from the user&apos;s point of view, and collected individual feedback in order to determine whether it is possible to improve interface design  Categories &amp; Subject Descriptors: H.5.2  [Information Interfaces and Presentation]: User Interfaces --  User-centered design  General Terms: Human Factors  Keywords: Search engine, user interface, accessibility,  usability.
We discuss an approach to the automatic expansion of domainspecific  lexicons by means of term categorization,anovel task employing techniques from information retrieval (IR) and machine learning (ML). Specifically,we view the expansion of such lexicons as a process of learning previously unknown associations between terms and domains. The process generates, for each c i in a set C = fc1ff::: ffc mg of domains, a lexicon L    1 , bootstrapping from an initial lexicon   0 and a set of documents ` given as input. The method is inspired by text categorization (TC), the discipline concerned with labelling natural language texts with labels from a predefined set of domains, or categories. However, while TC deals with documents represented as vectors in a space of terms, we formulate the task of term categorization as one in which terms are (dually) represented as vectors in a space of documents, and in which terms (instead of documents) are labelled with domains.
this paper we argue about the need to develop an educational-evaluation  model and a methodology that include usability testing as standard procedure  capable to determine the impact of adaptation on learners&apos; behavior in an  educational environment. To this end, heuristic evaluation is modified and  criteria that diagnose potential usability problems related to adaptation are  introduced and then integrated into the layered evaluation framework
We consider the model checking problem for probabilistic pushdown automata (pPDA) and properties expressible in various probabilistic logics. We start with properties that can be formulated as instances of a generalized random walk problem. We prove that both qualitative and quantitative model checking for this class of properties and pPDA is decidable. Then we show that model checking for the qualitative fragment of the logic PCTL and pPDA is also decidable. Moreover, we develop an error-tolerant model checking algorithm for general PCTL and the subclass of stateless pPDA. Finally, we consider the class of properties definable by deterministic B uchi automata, and show that both qualitative and quantitative model checking for pPDA is decidable. 1. 
In the second part of the paper, we work out a constructive  method for (s; `)-codes [8] based on concatenated codes  and MDS-codes [2, 3]. The method is a generalization of the constructive  method for (s; 1)-codes [1, 6]. In addition, we discuss  the constructions of the list-decoding superimposed codes [4, 7],  identi  ed by a family of  nite sets in which no union of L sets is  covered by the union of s others.
Classification schemes are abundant in the literature of failure and its investigation. They  serve a number of purposes, some of them more successfully than others. We examine several wellknown  and less-well-known classification schemes constructed for various purposes relating to  failure and its investigation, and discuss their values and limits. The analysis results in a continuum  of goals and uses for classification schemes, that suggests that the value of certain properties of  these schemes is dependent on the goals the classification is designed to forward. The contrast in  the value of different properties for different uses highlights a particular shortcoming in achieving  certain purposes: we argue that while humans are good at developing one kind of scheme, dynamic,  flexible classifications used for exploratory purposes, we are not so good at developing another,  static, rigid classifications used to trap and organize data for specific analytic goals. Our lack of  strong foundation in developing valid instantiations of the latter impedes progress toward a number  of investigative goals. This shortcoming and its consequences pose a challenge to researchers in the  study of failure: to develop new methods for constructing and validating domain-specific static  classification schemes of demonstrable value in promoting the goals of investigations. We note current  productive activity in this area, and outline foundations for more.
 We address the problem of advice-taking in a given domain, in particular for building a game-playing program. Our approach to solving it strives for the application of machine learning techniques throughout, i.e., for avoiding knowledge elicitation by any other means as much as possible. In particular, we build upon existing work on the operationalization of advice by machine and assume that advice is already available in operational form. The relative importance of this advice is, however, not yet known and can therefore not be utilized well by a program. This paper presents an approach to determine the relative importance for a given situation through reinforcement learning. We implemented this approach for the game of Hearts and gathered some empirical evidence on its usefulness through experiments. The results show that the programs built according to our approach learned to make good use of the given operational advice.   
In this paper, we determine the analytical form of the interaction matrix related to any moments that can be computed from binary or segmented images. We then apply this theoretical result to image-based visual servoing by selecting six combinations of moments able to control the six dof of the system. The experimental results we present show that a correct behavior of the system is obtained if we consider either a simple symetrical object, either a planar object with complex and unknown shape.
The amygdala has repeatedly been implicated  in emotional reactions and in learning  of new emotionally significant stimuli.
A2/1-7	+=35	-2%@+&apos;&apos;+ B+&apos;)	-%$C	D&apos;  /	=FG(-&apos;H/	/?IJK	$$2H-4/2).9+&quot;L)3=MONQPNQPR(S8T	PU VXWY PZ[S8T1UG\&apos;T!S8]1Z,N^T	P`_a\ Y%bffc NQZ5deT	aZO] b af.feT	_Z@Z5d_ Y%b dT	\ W hg=22-! IJi/2)/?)0j) &lt; +X&quot;ff!	22i!	+&apos;+&apos;$k-lM(]!m	NQM(]	\nfe]	_Z,N^SN fe]	U Z,N^T	PoNQP WY m	qpC;2)Ir-1-O &lt; %s C)et$-O)	*u+&apos;)!1-&apos;)k2+&apos; )22/E)$O-)jq)	*u+&apos;)!1-&apos;)h/1-2-2kI4-hB 8T	PU 29030-4 /	s-&apos;&apos;/1-&apos;)B&apos;$% &lt; 	+&apos;	[v)$IJO2)Iw-	--O&quot;;1* &quot;;	+e/	s-&apos;%&apos;/1-&apos;)($%xH)e*,&quot;ff))	-)&apos;	$x-y-:)e* &lt; -&apos;)	+Xz/2).2{*u+&apos;&apos;i/2 ,&quot;ff))	-)&apos;	$x-y- jpC &apos;$.-&apos;37 }	B&apos;.-2s-&apos; 2 ,&quot;ff))	-)&apos;	$x-y-:)e* $%k	$k$ &lt; +&apos;)/k%ff-x	+&apos; ~&apos;e* 29000-39840  ff))	-)&apos; +&apos;)	-).q2+&apos;KzB%-2 &lt; [/E23Q).2&quot;ff	[s-$e q)!IKK-1- )O&quot;(%-)$hO9?)	-h%- &lt; 0	$h%(&apos;.-x35)O+7	2 - 15940-368 $1-89	2!  Keywords  /1-7	+$1-8x&quot;ff&apos; 2 - 15940-36890  0-37870  &apos;	$x-  1. 
This paper is a presentation of ONTO-DLP, an extension  of Disjunctive Logic Programming (DLP) for complex knowledge modelling.
Ubiquitous computing (UbiComp) applications operate within an extremely dynamic and heterogeneous environment. Thus context definition, representation, management and use become important factors that affect their operation. UbiComp applications have to dynamically adapt to changes in their environment as a result of users&apos; or other actors&apos; activities. To ease the development of such applications it is necessary to decouple application composition from context acquisition and representation, and at the same time provide universal models and mechanisms to manage context. This paper presents experiences with using an ontology to represent context of operation together with decision making for UbiComp applications that result from the composition of functionally independent components. These components were embedded in everyday objects, hence (a) their services were affected by their physical properties, (b) their context of operation was defined by the existence / availability of the objects, and (c) their collective functionality was emerging from a set of interactions among them.
This paper presents an action selection framework based on an assemblage of self-organizing neural networks called Cooperative Extended Kohonen Maps. This framework encapsulates two features that significantly enhance a robot&apos;s action selection capability: self-organization in the continuous state and action spaces to provide smooth, effcient and fine motion control; action selection via the cooperation and competition of Extended Kohonen Maps to achieve more complex motion tasks. Qualitative tests demonstrate the capability of our action selection method for both singleand multi-robot motion tasks.
P3P [23, 24] is a set of standards that allow corporations  to declare their privacy policies. Hippocratic  Databases [6] have been proposed to implement  such policies within a corporation&apos;s datastore. From an end-user
Two classes of nonlinear delayed controlled systems are considered: nonlinear hyperbolic equations (such as the Burgers equation without diffusion) and models of mixing processes with non negligible pipes holdups. Both can be seen as systems with delays depending on the control. As for flat systems, the trajectories of such systems can be explicitly parameterized. This is achieved by enlarging the set of allowed manipulations (classical algebraic computations and time derivations) by using compositions and inversions of functions. This provides an easy motion planning algorithm.
We present a new method purity analysis for Java programs. A method is pure if it does not mutate any location that exists in the program state right before method invocation. Our analysis is built on top of a combined pointer and escape analysis for Java programs and is capable of determining that methods are pure even when the methods do heap mutation, provided that the mutation affects only objects created after the beginning of the method. Because our analysis extracts...
Anomalies are unusual and significant changes in a network&apos;s traffic levels, which can often involve  multiple links. Diagnosing anomalies is critical for both network operators and end users. It is  a difficult problem because one must extract and interpret anomalous patterns from large amounts of  high-dimensional, noisy data.
this paper,  the operators of the Differential Evolution algorithm are employed to generate the starting  points of a global optimization method with dynamic search trajectories. Results for various  well-known and widely used test functions are reported, supporting the claim that the proposed  approach improves drastically the performance of the algorithm, in terms of the total  number of function evaluations required to reach a global minimizer
Behavioral reflection is well-known approach enabling exhaustive querying of program state (introspection) as well as controlling its execution (intercession). It is hence an adequate foundation for runtime inspection. Partial behavioral reflection aims at making behavioral reflection more applicable by providing high levels of selectivity and configurability. We first outline the main features of partial behavioral reflection and of Reflex --- our portable Java implementation. We then sketch how we plan to apply such an approach to provide an interactive environment for runtime inspection, which, in particular, could be used to assist in reflective and aspect-oriented programming.
An improved technique for 3D head tracking under varying illumination conditions is proposed. The head is modeled as a texture mapped cylinder. Tracking is formulated as an image registration problem in the cylinder&apos;s texture map image. To solve the registration problem in the presence of lighting variation and head motion, the residual error of registration is modeled as a linear combination of texture warping templates and orthogonal illumination templates. Fast and stable on-line tracking is then achieved via regularized, weighted least squares minimization of the registration error. The regularization term tends to limit potential ambiguities that arise in the warping and illumination templates. Tracking does not require a precise initial fit of the model; the system is initialized automatically using a simple 2D face detector. The only assumption is that the target is facing the camera in the first frame of the sequence. Experiments in tracking are reported.  1 Introduction  Three...
Packet radio (PR) is a technology that applies the packet switching technique to the broadcast radio environment. In a PR network, a single high-speed wideband channel is shared by all PR stations. When a time-division multi-access protocol is used, the access to the channel by stations&apos; transmissions must be properly scheduled in both time and space domains in order to avoid collisions or interferences. It is proven in this paper that such a scheduling problem is NP-complete. Therefore, an efficient polynomial algorithm rarely exists, and a mean field annealing-based algorithm is proposed to schedule the stations&apos; transmissions in a frame consisting of certain number of time slots. Numerical examples and comparisons with some existing scheduling algorithms have shown that the proposed scheme can find near-optimal solutions with reasonable computational complexity. Both time delay and channel utilization are calculated based on the found schedules.
Setuid programs are often exploited by malicious attackers to obtain unauthorized access to local systems. Setuid programs, especially owned by the root user, are granted root privileges, allowing attackers to gain root privileges by exploiting vulnerabilities in the setuid-root programs. The vulnerabilities usually lie in code that does not require root privileges. Nevertheless, the entire code of setuid-root programs is granted root privileges. This paper presents a scheme called privileged code minimization that reduces the risk to setuid programs. In this scheme, setuid-root programs are divided into privileged code and non-privileged code. Privileged code is granted root privileges, while non-privileged code is not. This scheme reduces the size of trusted computing base (TCB) because it reduces the code running with root privileges, reducing the chances of attackers gaining root privileges by subverting setuid programs. Protection between privileged code and nonprivileged code are enforced by fine-grained protection domains: a novel protection mechainsm of the operating system proposed by the authors.
An environment for content-based pictorial retrieval algorithms, called PICSearch, is introduced. In this context, the retrieval is based on color distribution, texture or edges of a query image, or a sketch; i.e. the boundary information of a shape. PICSearch is designed to serve as a platform for any kind of pictorial matching algorithm. The system is designed for researchers developing such algorithms. PICSearch provides an easy-to-use graphical interface and a platform, where the researcher can easily embed his algorithm without the need to create a whole system from scratch. PICSearch is very independent on the underlying operating system and window manager.  PICSearch is released to public use (under the GNU General Public License) and, to our knowledge, it is the first open platform to image retrieval systems freely available.  Keywords: content-based image retrieval, visual information management, open platforms, image databases Computing Reviews (1991) Categories:  H.3.3 [Inf....
Comparative sequence analysis has been used to study specic questions about the structure and function of proteins for many years. Here we propose a knowledge-based framework in which the maximum likelihood rate of evolution is used to quantify the level of constraint on the identity of a site. We demonstrate that site-rate mapping on 3D structures using datasets of rhodopsin-like G-protein receptors and ff- and ff-tubulins provides an excellent tool for pinpointing the functional features shared between orthologous and paralogous proteins. In addition, functional divergence within protein families can be inferred by examining the differences in the site rates, the differences in the chemical properties of the side chains or amino acid usage between aligned sites. Two novel analytical methods are introduced to characterize rateindependent functional divergence. These are tested using a dataset of two classes of HMG-CoA reductases for which only one class can perform both the forward and reverse reaction. We show that functionally divergent sites occur in a cluster of sites interacting with the catalytic residues and that this information should facilitate the design of experimental strategies to directly test functional properties of residues.
Explicitly stated program invariants can help programmers by characterizing certain aspects of program execution and identifying program properties that must be preserved when modifying code. Unfortunately, these invariants are usually absent from code. Previous work showed how to dynamically detect invariants from program traces by looking for patterns in and relationships among variable values. A prototype implementation, Daikon, accurately recovered invariants from formally-specified programs, and the invariants it detected in other programs assisted programmers in a software evolution task. However, Daikon suffered from reporting too many invariants, many of which were not useful, and also failed to report some desired invariants.  This paper presents, and gives experimental evidence of the efficacy of, four approaches for increasing the relevance of invariants reported by a dynamic invariant detector. One of them --- exploiting unused polymorphism --- adds desired invariants to th...
A multi-component communication analysis of the CORBA specification is conducted and an analytic model describing communication costs is proposed. The analysis indicates: a) potential expensive communication when integrating parallel and distributed computing into a CORBA middleware framework, and b) a relationship between communication costs and code layout in CORBA&apos;s Interface Definition Language (IDL).
In this paper, we study how to select the transport layer protocol parameters with considerations  of wireless link characteristics and application QoS requirements. In our approach, the inter-layer  interactions only exchange protocol parameters between layers. We demonstrate that by appropriately  selecting the AIMD protocol parameters, AIMD-controlled flows can efficiently utilize wireless resources  and provide satisfactory QoS to time-sensitive multimedia applications. Extensive simulations  are performed to validate the analytical results, evaluate the performance, and show that AIMD protocols  can outperform the non-responsive UDP protocol when they are used to support multimedia applications  over hybrid networks. Since AIMD and TCP protocols share the same congestion control mechanism,  AIMD protocols are compatible with the legacy and scalable to be deployed incrementally. In addition,  with satisfactory QoS provisioning, end-systems have more incentives to voluntarily regulate multimedia  traffic with an AIMD-based congestion controller, which is vital for network stability and integrity.
Changes in the natural environment affect our quality of life. Thus, government, industry, and the public call for integrated environmental management systems capable of supplying all parties with validated, accurate and timely information. The `near real-time&apos; constraint reveals two critical problems in delivering such tasks: the low quality or absence of data, and the changing conditions over a long period. These problems are common in environmental monitoring networks and although harmless for off-line studies, they may be serious for near real-time systems.
The combination of evidence for Information Retrieval has been studied extensively in order to increase effectiveness. In this paper, we study the selective application of different retrieval approaches on a per-query basis for Web Information Retrieval. Our methodology is based on the assumption that not all queries benefit from a uniform retrieval approach. In order to select the most appropriate retrieval approaches for each query, we use evidence from the hyperlink structure of the retrieved documents and also from the distribution of aggregates, that is the groups of documents from the same domain. Our experimental results show that it is possible to obtain important improvements in retrieval effectiveness by using simple statistical decision mechanisms on a per-query basis.
Roles have been used both as an intuitive concept in order to analyze multi-agent systems and as a formal structure in order to implement coherent and robust teams. The extensive use of roles in implemented systems evidences their importance in multiagent systems design and implementation. In this paper we emphasize the importance of roles for multiagent systems to act in complex domains, identify some of their properties and we review work done concerning specification and exploitation of roles in agent-oriented system engineering methodologies and in multi-agent systems that are deployed in dynamic and unpredictable domains.
A novel motion activity descriptor and its extraction from a  compressed MPEG (MPEG-1/2) video stream are presented. The  descriptor consists of two parts, a temporal descriptor and a spatial  descriptor. To get the temporal descriptor, the &quot;motion intensity&quot; is first  computed based on P frame macroblock information. Then the motion  intensity histogram is generated for a given video unit as the temporal  descriptor. To get the spatial descriptor, the average magnitude of the  motion vector in a P frame is used to threshold the macro-blocks into  &quot;zero&quot; and &quot;non-zero&quot; types. The average magnitude of the motion  vectors and three types of runs of zeros in the frame are then taken as  the spatial descriptor. Experimental results show that the proposed  descriptor is fast, and that the combination of the temporal and spatial  attributes is effective. Key elements of the intensity parameter, spatial  parameters and the temporal histogram of the descriptor have been  adopted by the draft MPEG-7 standard [10].
Approximate linear programming (ALP) offers a promising framework for solving large factored Markov decision processes (MDPs) with both discrete and continuous states. Successful application of the approach depends on the choice of an appropriate set of feature functions defining the value function, and efficient methods for generating constraints that determine the convex space of the solution. The application of the ALP in continuous state-space settings poses an additional challenge - the number of constraints defining the problem is infinite. The objective of this work is to explore various heuristics for selecting a finite subset of constraints defining a good solution policy and for searching the space of such constraints more efficiently. The heuristics that we developed rely upon: (1) the structure of the factored model and (2) stochastic state simulations to generate an appropriate set of constraints. The improvements resulting from such heuristics are illustrated on three large factored MDP problems with continuous states.
In this paper we present a second generation URL monitoring tool which enables the collaborative evaluation of URL content changes. In our implementation, a document monitoring agent works alongside a recommender system. Using information provided by the monitoring agent, the collaborative system alerts users when documents they are monitoring have changed. The monitoring agent provides automatic evaluation of the nature of the change. Users, however, add subjective evaluations; one user&apos;s effort informs all others monitoring the same URL. Based on these subjective evaluations, the collaborative system can filter the changed URLs, providing customized notifications for each user based on individual preferences. In this paper, we describe the implemented system and usage results.
This paper present s oureffort in designing pedagogicalt ools for tr/ hing message passing using channels. Theset ols include a class librarytbr supp channels, a visualizatvi systu tt helps stB00 t s see t/ execut---/ behavior  oft hreads and message passing, and at opology edit/ tdi provides an environment forst/B7 t st o design net work tk ologies. Moreover, since we have made suret he uniformit  y of t/ channel definitB/ acrosstr t hread, parallel and dist8H8B/I environment s, port6--- atB868&apos;/ programt a parallel/dist/tH7H environment is easy.
A major obstacle for bridging the gap between textbook  mathematics and formalising it on a computer is the problem how to  adequately capture the intuition inherent in the mathematical notation  when formalising mathematical concepts. While logic is an excellent tool  to represent certain mathematical concepts it often fails to retain all the  information implicitly given in the representation of some mathematical  objects. In this paper we concern ourselves with matrices, whose representation  can be particularly rich in implicit information. We analyse  dierent types of matrices and present a mechanism that can represent  them very close to their textbook style appearance and captures the information  contained in this representation but that nevertheless allows  for their compilation into a formal logical framework. This  rstly allows  for a more human-oriented interface and secondly enables ecient reasoning  with matrices.
Many problems are difficult to adequately explore until a  prototype exists in order to elicit user feedback. One such  problem is a system that automatically categorizes and manages  email. Due to a myriad of user interface issues, a prototype is  necessary to determine what techniques and technologies are  effective in the email domain. This paper describes the  implementation of an add-in for Microsoft Outlook 2000  TM  that  intends to address two problems with email: 1) help manage the  inbox by automatically classifying email based on user folders, and 2) to aid in search and retrieval by providing a list of email relevant to the selected item. This add-in represents a first step in an  experimental system for the study of other issues related to  information management. The system has been set up to allow  experimentation with other classification algorithms and the source code is available online in an effort to promote further  experimentation.
The paper presents &quot;Any Input XML Output&quot; (AIXO), a general and flexible software architecture for wrappers.
This paper presents a cell method to perform event driven simulations of hard-particle systems on boundless space. An infinite cell division of the space is supposed where only the cells that contain particles are maintained in the computer memory. In order to detect efficiently these non-empty cells a standard hash method is used. This solution enables a more efficient O(1)  simulation of these N-particle systems since the alternative approach is use no cells at all, which leads to O(N ) simulations per each processed event. 1 Introduction Event-driven molecular dynamics is a well established tool for the study of hard-particle fluids [1]. Particles move free of interparticle forces during finite intervals and at discrete instants (events) one or two particles suffer an impulsive force (the respective momenta change discontinuously). The evolution of the system during these intervals is trivial and the simulation proceeds jumping analytically from one event to the next. Applications o...
It is very common in constraint programming that the values a set of variables can take are indistinguishable: for example in graph colouring the names of colours can be interchanged freely. Breaking such symmetry is important for efficient search. In this paper I present a constraint to break this kind of symmetry. I present two variants, the second of which is remarkably elegant but less generally useful. I prove that both constraints have the intended theoretical property of uniquely forcing assignments and thereby breaking all symmetry. These are the first published constraints I am aware of for this task. I outline some theoretical and practical questions that are raised.
In this paper, a new algorithm for performing quantifier elimination from first order formulas over real closed fields is given. This algorithm improves the complexity of the asymptotically fastest algorithm for this problem, known to this date. A new feature of this algorithm is that the role of the algebraic part (the dependence on the degrees of the input polynomials) and the combinatorial part (the dependence on the number of polynomials) are separated. Another new feature is that the degrees of the polynomials in the equivalent quantifier-free formula that is output, are independent of the number of input polynomials. As special cases of this algorithm, new and improved algorithms for deciding a sentence in the first order theory over real closed fields, and also for solving the  existential problem in the first order theory over real closed fields, are obtained.
The SE-Coach is a tutoring system that supports students in applying  the learning strategy known as self-explanation - the process of clarifying  to oneself the solution of an example. In this paper, we describe the student  model used by the SE-Coach to assess the students&apos; self-explanations and to  provide hints to improve them. The assessment is based on the student&apos;s prior  physics knowledge and on the student&apos;s studying actions. We describe a version  of the user model based on a Bayesian network, and a simplified version  that is more efficient but handles only examples with no inferential gaps in  the solution.
In this paper, we present a macroscopic analytical model of collaboration in a group of reactive robots. The model consists of a series of coupled differential equations that describe the dynamics of group behavior. After presenting the general model, we analyze in detail a case study of collaboration, the stick pulling experiment, studied experimentally and in simulation by Ijspeert et al. [14]. The robots&apos; task is to pull sticks out of their holes, and it can be successfully achieved only through collaboration between two robots. There is no explicit communication or coordination between the robots. Unlike microscopic simulations (sensor-based or using a probabilistic numerical model), whose computational time scales with the robot group size, the macroscopic model is computationally efficient, because its solutions are independent of robot group size. Analysis reproduces several qualitative conclusions of Ijspeert et al.: namely, the different dynamical regimes for different values of the ratio of robots to sticks, the existence of optimal control parameters that maximize system performance as a function of group size, and the transition from super-linear to sub-linear performance as the number of robots is increased.
In this paper, we introduce a Bayesian approach, inspired by Probabilistic Principal Component Analysis (PPCA) [1], to detect objects in complex scenes using appearance based models. The originality of the proposed framework is to explicitly take into account general forms of the underlying distributions, both for the in-eigenspace distribution and for the observation model. The approach combines linear data reduction techniques (to preserve computational effciency), nonlinear constraints on the in-eigenspace distribution (to model complex variabilities) and non-linear (robust) observation models (to cope with clutter, outliers and occlusions) . The resulting statistical representation generalizes most existing PCA-based models [1--3] and leads to the definition of a new family of non-linear probabilistic detectors. The performance of the approach is assessed using ROC (Receiver Operating Characteristic) analysis on several representative data-bases, showing a major improvement in detection performances with respect to the standard methods that have been the references up to now.
This article deals with the intrinsic complexity of tracing and reachability questions in the context of elementary geometric constructions. We consider constructions from...
Mechanism design (MD) provides a useful method to implement outcomes with desirable properties in systems with self-interested computational agents. One drawback, however, is that computation is implicitly centralized in MD theory, with a central planner taking all decisions. We consider distributed implementations, in which the outcome is determined by the self-interested agents themselves. Clearly this introduces new opportunities for manipulation. We propose a number of principles to guide the distribution of computation, focusing in particular on Vickrey-Clarke-Groves mechanisms for implementing outcomes that maximize total value across agents. Our solutions bring the complete implementation into an ex post Nash equilibrium.
The first phase of the SET protocol, namely Cardholder Registration, has been modelled inductively. This phase is presented in outline and its formal model is described. A number of basic lemmas have been proved about the protocol using Isabelle/HOL, along with a theorem stating that a certification authority will certify a given key at most once. Many ambiguities, contradictions and omissions were noted while formalizing the protocol.
. There are two ways to learn mathematics: to discover (research) and to rediscover. Inspiring exercises are used to guide students at all levels to rediscover the essential meaning of various individual pieces of mathematics. Their role has become all the more important at a time when Bourbakisation is the dominating fashion. While mathematics is decorated with a thick layer of cosmetics, such as utmost generalities, excessive formalism and rigour, subtle proofs, etc, its natural beauty (intuitive ways of thinking, simplicity of ideas, etc) is obscured. From a pedagogical point of view, to say the least, it would be most desirable if a theorem, or an idea, can be fully explained by, hence rediscovered from, one or two simple examples. These kinds of illustrative examples actually exist everywhere, and at all levels. We can use them as inspiring exercises. In this paper, we give five sets of examples, beginning with a simple one on Abel&apos;s Identity, followed by examples on Hensel&apos;s Lemm...
While bringing considerable exibility and extending the horizons of mobile computing, mobile code raises major security issues. Hence, mobile code, such as Jave applets...
this paper, we propose non-uniform resource allocation policies for the case of the PGPS and RC-EDF scheduling disciplines and connections requiring a hard (deterministic) bound on end-to-end delay. The paper addresses the following problems:  -- How to map the end-to-end delay requirement of a connection into a local resource requirement to be reserved at each switch along the connection&apos;s path?  -- How to divide the resource requirement among the switches on the connection&apos;s path? We explore whether using policies that take the capacities of the switches and/or their loading into account (instead of using uniform allocation) would provide performance gains
Current distributed sensor network platforms lack comprehensive lowpower routing techniques and efficient public key cryptography mechanisms. Reducing power for individual radio transmissions has not been explored sufficiently. Popular sensor node platforms do not include a mechanism for distributing and redistributing shared cryptographic keys among nodes. This paper discusses a technique to tailor node transmit power to the lowest practical level while maintaining reliable network links and presents the first known implementation of elliptic curve cryptography for sensor networks. Results demonstrate that dynamic radio output power scaling is effective in reducing node power consumption by orders of magnitude in certain scenarios. Analysis suggests that secret-key cryptography is already viable on the UC Berkeley MICA2 mote and public-key infrastructure may also be tractable despite the device&apos;s limited memory.
users to interact with embedded systems located in their proximity using Smart Phones. We have identified four models of interaction between a Smart Phone and the surrounding environment: universal remote control, dual connectivity, gateway connectivity, and peer-to-peer. Although each of these models has different characteristics, our architecture provides a unique framework for all of the models. Central to our architecture are the hybrid communication capabilities incorporated in the Smart Phones. These phones have the unique feature of incorporating shortrange wireless connectivity (e.g., Bluetooth) and Internet connectivity (e.g., GPRS) in the same personal mobile device. This feature together with significant processing power and memory can turn a Smart Phone into the only mobile device that people will carry wherever they go.
This paper describes a network-based anomaly detection method for detecting Denial of Service and Network Probe attacks
The notion of quantum secure direct communication (QSDC) has been introduced recently in quantum cryptography as a replacement for quantum key distribution, in which two communication entities exchange secure classical messages without establishing any shared keys previously. In this paper, a quantum secure direct communication scheme using quantum Calderbank-Shor-Steane (CCS) error correction codes is proposed. In the scheme, a secure message is first transformed into a binary error vector and then encrypted(decrypted) via quantum coding (decoding) procedures. An adversary Eve, who has controlled the communication channel, can&apos;t recover the secrete messages because she doesn&apos;t know the deciphering keys. Security of this scheme is based on the assumption that decoding general linear codes is intractable even on quantum computers.
In this paper we combine the goal directed search of  A* with the ability of BDDs to traverse an exponential  number of states in polynomial time. We introduce  a new algorithm, SetA*, that generalizes A* to expand  sets of states in each iteration. SetA* has substantial advantages  over BDDA*, the only previous BDD-based  A* implementation we are aware of. Our experimental  evaluation proves SetA* to be a powerful search  paradigm. For some of the studied problems it outperforms  BDDA*, A*, and BDD-based breadth-first search  by several orders of magnitude. We believe exploring  sets of states to be essential when the heuristic function  is weak. For problems with strong heuristics, SetA*  efficiently specializes to single-state search and consequently  challenges single-state heuristic search in general.
This paper focuses on a framework for developing knowledge management (KM) applications that integrates peer-to-peer (P2P) and multi-agent systems (MAS) technologies. The objective of this framework is to support a particular KM paradigm that emphasizes aspects such as autonomy and distributedness of knowledge sources. In particular, we present a characterization of peer-to-peer in terms of a general architectural pattern and a set of guidelines for designing peer-to-peer applications according to the proposed framework. We adopt an agent-oriented approach that extends Tropos, a software engineering methodology introduced in earlier papers.
We describe the use of the mobile agent paradigm to design an improved infrastructure for data integration in Distributed Sensor Network (DSN). We use the acronym MADSN to denote the proposed Mobile-Agent-based DSN. Instead of moving data to processing elements for data integration, as is typical of a client/server paradigm, MADSN moves the processing code to the data locations. This saves network bandwidth and provides an effective means for overcoming network latency, since large data transfers are avoided. We study two important problems related to MADSN design --- the distributed integration problem, and the optimum performance problem. Compared to DSNs, a mobile-agent implementation of multi-resolution data integration saves up to 90% of the data transfer time. For a given set of network parameters, we analyze the conditions under which MADSN performs better than DSN and determine the condition under which MADSN reaches its optimum performance level.
We present work in progress that uses Latent Semantic  Indexing (LSI) in conjunction with background  knowledge and unlabeled examples to improve  text classification accuracy. The singular  value decomposition (SVD) that is performed by  LSI is done on an expanded term by document matrix  that includes the labeled training examples as  well as the unlabeled examples. We report classification  accuracy on different data sets both with and  withoutthe inclusion of background knowledge and  compare it to other known work.
A parallel packet switch (PPS) is a switch in which the memories run slower than the line rate. Arriving packets are spread (or load-balanced) packet-by-packet over multiple slower-speed packet switches. It is already known that with a speedup of , a PPS can theoretically mimic a FCFS output-queued (OQ) switch. However, the theory relies on a centralized packet scheduling algorithm that is essentially impractical because of high communication complexity. In this paper, we attempt to make a high performance PPS practical by introducing two results. First, we show that small co-ordination buffers can eliminate the need for a centralized packet scheduling algorithm, allowing a full distributed implementation with low computational and communication complexity. Second, we show that without speedup, the resulting PPS can mimic an FCFS OQ switch within a delay bound.
Distributed knowledge management systems (DKMS) have  been suggested to meet the requirements of today&apos;s knowledge  management. Peer-to-peer systems offer technical foundations  for such distributed systems. To estimate the value  of P2P-based knowledge management evaluation criteria that  measure the performance of such DKMS are required. We  suggest a concise framework for evaluation of such systems  within different usage scenarios. Our approach is based on  standard measures from the information retrieval and the  databases community. These measures serve as input to a  general evaluation function which is used to measure the efficiency  of P2P-based KM systems. We describe test scenarios  as well as the simulation software and data sets one can use  therefor.
We show how a large class of sufficient conditions for the existence of bound states, in non-positive central potentials, can be constructed. These sufficient conditions yield upper limits on the critical value, g    c , of the coupling constant (strength), g, and of the potential, V(r)  =-gv(r),  for which a first ff-wave bound state appears. These upper limits are significantly more stringent than hitherto known results.
this paper, we present a new method of extracting a global disparity  measure for vergence control, which does not require a priori segmentation  of the object of interest. Our method uses images acquired by retina-like sensors  and, therefore, the computation is performed in the log-polar plane. The technique  we present here is: (i) global, in the sense that it is an integral measure over the  whole image, (ii) computationally inexpensive, considering that the goal was to use  it in the robot control loop rather than to accurately measure some 3D world features
In connection with his counter-example to the fourteenth problem of Hilbert, Nagata formulated a conjecture concerning the postulation of r fat points of the same multiplicity in P   and proved it when r is a square. Iarrobino formulated a similar conjecture in P   . We prove Iarrobino&apos;s conjecture when r is a d-th  power. 1 
Although pattern languages have already been proposed for security modelling, such languages mostly employ concepts and notations related to object-oriented systems, and have mainly neglected the agent-oriented paradigm. In this position paper we argue about the need to define a security pattern language applicable to agent-based systems that employs concepts based in the agent-oriented paradigm. In addition, we motivate the need to integrate such a language within the development stages of an agent-oriented methodology, and we briefly discuss what such a language should contain,  Keywords: documenting pattern languages, linking requirements to patterns, patterns for secure agent-based systems, integrating patterns with a development methodology, agentoriented software engineering methodologies  1. 
Current trend in operational text categorization is the designing  of fast classification tools. Several studies on improving accuracy  of fast but less accurate classifiers have been recently carried out. In particular,  enhanced versions of the Rocchio text classifier, characterized by  high performance, have been proposed. However, even in these extended  formulations the problem of tuning its parameters is still neglected.
This paper describes how tracking and target selection are used in two behavior systems of the XT-1 vision architecture for mobile robots. The first system is concerned with active tracking of moving targets and the second is used for visually controlled spatial navigation. We overview the XT-1 architecture and describe the role of expectation-based template matching for both target tracking and navigation.  The subsystems for low-level processing, attentional processing, single feature processing, spatial relations, and place/object-recognition are described and we present a number of behaviors that can make use of the different visual processing stages.  The architecture, which is inspired by biology, has been successfully implemented in a number of robots which are also briefly described.  1. 
Qualitative simulations can be seen as knowledge models  that capture insights about system behaviour that should be  acquired by learners. A problem that learners encounter  when interacting with qualitative simulations is the  overwhelming amount of knowledge detail represented in  such models. As a result, the discovery space grows too  large, which hampers the knowledge construction process  of the learner. In this paper we present an approach to  restructure the output of a qualitative reasoning engine in  order to make it better suited for use in interactive learning  environments. The approach combines techniques for  simplifying state-graphs with techniques for aggregating  causal models within states. The result is an approach that  automatically highlights the main behavioural facts in  terms of simulation events and simplified causal accounts,  while leaving the option for the learner to explore the  aggregated constructs in more detail.
Requirements specifications are often inconsistent. Inconsistencies may arise because multiple conflicting requirements are embodied in these specifications, or because the specifications themselves are in a transient stage of evolutionary development. In this paper we argue that such inconsistencies, rather than being undesirable, are actually useful drivers for changing the requirements specifications in which they arise. We present a formal technique to reason about inconsistency handling changes. Our technique is an adaptation of logical abduction -- adapted to generate changes that address some specification inconsistencies, while leaving others. We represent our specifications in quasi-classical (QC) logic -- an adaptation of classical logic that allows continued reasoning in the presence of inconsistency. The paper develops a sound algorithm for automating our abductive reasoning technique and presents illustrative examples drawn from a library system case study.  1 Introduction...
This paper proposes a solution based on forward error recovery, oriented towards providing dependability of composite Web services. While exploiting their possible support for fault tolerance (e.g., transactional support at the level of each service), the proposed solution has no impact on the autonomy of the individual Web services, Our solution lies in system structuring in terms of co-operative atomic actions that have a well-defined behaviour, both in the absence and in the presence of service failures. More specifically, we define the notion of Web Service Composition Action (WSCA), based on the Coordinated Atomic Action concept, which allows structuring composite Web services in terms of dependable actions. Fault tolerance can then be obtained as an emergent property of the aggregation of several potentially non-dependable services. We further introduce a framework enabling the development of composite Web services based on WSCAs, consisting of an XML-based language for the specification of WSCAs.
We present an extension of the model checker Uppaal, capable of synthesizing linear parameter constraints for the correctness of parametric timed automata. A symbolic representation of the (parametric) state space in terms of parametric difference bound matrices is shown to be correct. A second contribution of this paper is the identification of a subclass of parametric timed automata (L/U automata), for which the emptiness problem is decidable, contrary to the full class where it is known to be undecidable. Also, we present a number of results that reduce the verification effort for L/U automata in certain cases. We illustrate our approach by deriving linear parameter constraints for a number of well-known case studies from the literature (exhibiting a flaw in a published paper).
We propose a new method to program robots based on Bayesian inference and learning. The capacities of this programming method are demonstrated through a succession of increasingly complex experiments. Starting from the learning of simple reactive behaviors, we present instances of behavior combinations, sensor fusion, hierarchical behavior composition, situation recognition and temporal sequencing. This series of experiments comprises the steps in the incremental development of a complex robot program. The advantages and drawbacks of this approach are discussed along with these different experiments and summed up as a conclusion. These different robotics programs may be seen as an illustration of probabilistic programming applicable whenever one must deal with problems based on uncertain or incomplete knowledge. The scope of possible applications is obviously much broader than robotics.
Many different demands can be... This paper aims to demonstrate that, for a reasonable set of assumptions, the false alarm rate is the limiting factor for the performance of an intrusion detection system. This is due to the baserate fallacy phenomenon, that in order to achieve substantial values of the Bayesian detection rate, P(Intrusion|Alarm), we have to achieve -- a perhaps in some cases unattainably low -- false alarm rate. A selection of reports...
)  Enrico Giunchiglia, G. Neelakantan Kartha and Vladimir Lifschitz  Department of Computer Sciences  University of Texas at Austin  Austin, Texas 78712  fenrico,kartha,vlg@cs.utexas.edu  1 Introduction  Our goal is to extend and improve the action language AR 0 proposed in [ Kartha and Lifschitz, 1994 ] and to simplify the method for representing actions with indirect effects in circumscriptive theories described in that paper. The new action language AR differs from AR 0 in two ways.  First, AR allows us to represent fluents whose values are non-Boolean, such as Location(x) or Color(x). In the situation calculus, such fluents can be always eliminated in favor of propositional fluents, just as function symbols in first-order logic can be eliminated in favor of predicate symbols. For instance, instead of  Brighter(Color(x; s1); Color(x; s2))  we can write  8c 1 c 2 [HasColor(x; c1; s1)  HasColor(x; c2; s2)  oe Brighter(c 1 ; c 2 )]:  But this may be unnatural and inconvenient. Surprisi...
. Practical parallelizations of multi-phased low-level imageprocessing  algorithms may require working in batch mode. The features  of a common processing model, employing a pipeline of processor farms,  are described. A simple exemplar, the Karhunen-Lo`eve transform, is prototyped  on a network of processors running a real-time operating system.  The design trade-offs for this and similar algorithms are indicated,  when a general solution is sought. Eventual implementation on large- and  fine- grained hardware is considered. The chosen exemplar is shown to  have some features, such as strict sequencing and unbalanced processing  phases, which militate against a comfortable parallelization.  1 Introduction  Many low-level image-processing (IP) algorithms, such as spatial filters, are completely localized in their data references. If adjacent image data are overlapped at boundaries then at a small additional cost a data-farming programming paradigm can be employed, in which the only com...
In this paper, a primal-dual algorithm for TV-type image restoration is analyzed and tested. Analytically it turns out that employing a global L^s -regularization, with s &gt; 1, in the dual problem results in a local smoothing of the TV-regularization term in the primal problem. The local smoothing can alternatively be obtained as the infimal convolution of the l_r-norm, with r^-1 + s^-1 = 1, and a smooth function. In the case r = s = 2, this results in Gauss-TV-type image restoration. The globalized primal-dual algorithm introduced in this paper works with generalized derivatives, converges locally at a superlinear rate and is stable with respect to noise in the data. In addition, it utilizes a projection technique which reduces the size of the linear system that has to be solved per iteration. A comprehensive numerical study ends the paper.
this article is that Web content management technology, if the fit is right, may be a particularly useful lever of innovation for libraries in a federated institutional environment
We incorporate fading and reset mechanisms in an enhanced fine granularity scalability algorithm to reduce the drifting error at low bit rate while still maintaining 1.5dB PSNR gain at high bit rate over the current MPEG-4 fine granularity scalability. Many of previous works use enhancement layers to predict enhancement layers so as to increase the compression efficiency. Drifting error occurs because enhancement layer, the predictor, is not received as expected. Our fading mechanism linearly combines the current reconstructed base layer and previously reconstructed enhancement layer with fading factors between 0 and 1. Our reset mechanism sets the reference frame for prediction to be the base layer periodically. Our theoretic formulation and experiment results show that drifting error can be distributed more uniformly and maximum accumulated mismatch error is significantly reduced while our mechanisms are turned on. Around 1dB can be improved at low bit rate comparing to the one without any drifting reduction mechanism.
This paper reports on measurement results for the use of IEEE 802.11 networks in drive-thru scenarios:wehave  measured transmission characteristics for sending and receiving high data volumes using UDP and TCP in vehicles moving at different speeds that pass one or more IEEE 802.11 access points at the roadside. We discuss possibilities and limitations for the use of scattered WLAN cells by devices in fast moving vehicles and provide an analysis of the performance that can be expected for the communication in such scenarios. Based on these observations, we discuss implications for higher-layer protocols and applications.
In the future webs of unmanned vehicles will act together to robustly achieve elaborate missions within uncertain environments. This web may be a distributed satellite system forming an interferometer, or may be a heterogenous set of rovers and blimps exploring Mars. We coordinate these systems by introducing a reactive model-based programming language (RMPL) that combines within a single unified representation the flexibility of embedded programming and reactive execution languages, and the deliberative reasoning power of temporal planners. To support fast mission planning as graph search, the KIRK planner compiles an RMPL program into a temporal plan network (TPN), similar to those used by temporal planners, but extended for symbolic constraints and decisions. To robustly coordinate air vehicle or rover maneuvers we combine the Kirk planning algorithm with randomized algorithms for kinodynamic path planning. Finally, we describe our Mars exploration testbed, including four RWI ATRV vehicles.
Dealing with spatiotemporal applications at the logical phase of database design reveals a set of data peculiarities which the already existing models can not serve satisfactorily. In this paper, the ontologies of a spatiotemporal environment -namely the basic concepts of objects, attributes and relationships with spatial and temporal extent, as well as operations on them- are defined. Based on these, specifications of modeling tools for the design of spatiotemporal information at the logical phase are given. A formal, yet practical, SpatioTemporal Relational data Model (STRM) is presented as part of a full automatable application design methodology; it provides a small set of representational constructs (relations, layers, virtual layers, object classes, and constraints-all with spatial and temporal extent) on top of well-established models. The power of modeling and ease-of-use of the discussed approach is demonstrated by an example taken from a real application.  1 Introduction  The...
We consider some particular instances of the segmentation problem.  We derive minimum message length (MML) expressions for stating the  region boundaries for some one and two dimensional examples. It is the  found the message length cost of stating region boundaries is dependent  on the noise of the data in the separated regions and also the `degree of  separation&apos; of the two regions.  The framework given here can be extended to different shaped cuts and  also non-constant fits for the regions. Possible applications for the work  presented here include its use in tree (i.e. CART) regression and in image  segmentation.  0  1 Introduction  We consider some instances of the segmentation problem. The segmentation problem arises whereever it is desired to partition data into distinct homogenous regions. The regions have distinct boundaries. This distinguishes the segmentation problem from mixture modelling, where regions are represented by overlapping probability distributions, with no dist...
We describe a first proposal for a strategy language for Maude, to control the rewriting process and to be used at the object level instead of at the metalevel. We also describe a prototype implementation built over Full Maude using the metalevel and the metalanguage facilities provided by Maude. Finally, we include a series of examples that illustrate the main features of the proposed language.
We present a methodology for the on-line estimation of the cell-loss probability of an ATM link. It is particularly suitable for estimating small probabilities of the order 10  \Gamma6  --10  \Gamma9  , with variance orders of magnitude smaller than traditional estimators. The method is justified by the theory of large deviations and the information it requires is based on the actual traffic flows rather than the analysis of some specific traffic models. The method is effective when there is a large degree of statistical multiplexing; in other words, when the number of input traffic sources is large. The statistical properties we require for the traffic are very general and are met by most real-time traffic source models. Implementation issues of this methodology, which demonstrate its simplicity, are also discussed. 1 Introduction  An ATM network provides support for a wide range of services which have differing bandwidth requirements, traffic pattern statistics and Quality of Service...
Detecting salinity in early stages using electromagnetic survey and multivariate geostatistical techniques.
In this paper, we address the task of extracting, from a natural image or a video sequence, parameters of an appearance model describing expressive human faces. The parameters contain the information needed to reproduce a natural looking synthetic expressive face, possibly with different expressions (for visual communication), as well as for facial expression interpretation (for man-to-machine interaction). Results illustrate how a given natural expressive face can be resynthesized and tracked in a video. In a second step, results reveal how to control and force new expressions on a natural face. Then, we show how a facial appearance model may be used for facial expression recognition.
We develop new methods to statically bound the resources  needed for the execution of systems of concurrent, interactive threads.
 We show that problems at the uncolorability phase transition are well out of reach of intelligent algorithms. Since there are not small and easily checkable subgraphs which can be used to confirm uncolorability quickly, we cannot hope to build more intelligent algorithms to avoid hard problems at the phase transition. Also, our results suggest that a conjectured double phase transition in graph coloring occurs only in small graphs. Similar results are likely in other NP-complete problems where instances from phase transitions are hard for all known algorithms, and will help to explain the phenomenon. Furthermore, our results help to elucidate the distinction between polynomial and non-polynomial search behavior.
A vision based technique for nonrigid control is presented that can be used for animation and video game applications. The user grasps a soft, squishable object in front of a camera that can be moved and deformed in order to specify motion. Active Blobs, a nonrigid tracking technique is used to recover the position, rotation and nonrigid deformations of the object. The resulting transformations can be applied to a texture mapped mesh, thus allowing the user to control it interactively. Our use of texture mapping hardware in tracking makes the system responsive enough for interactive animation and video game character control.
Supporting heterogeneous receivers in a multicast group is of particular importance in large internetworks as, e.g., the Internet due to the large diversity of end-system and network access capabilities. Furthermore, it is the nature of large-scale internetworks which makes homogeneous Quality of Service (QoS) support unrealistic at least for the middle-term future. Therefore, we investigate in this paper the implications of differing multicast models in heterogeneous QoS networks. In particular, we approach the problem an edge devices faces when mapping a heterogeneous QoS multicast from an overlaid QoS system onto a system providing only a homogeneous QoS multicast. The generic solution technique we propose for this problem is called foresting. The idea of foresting is to support a heterogeneous multicast by a forest of homogeneous multicast trees. We develop different foresting algorithms and compare them by extensive simulations.
Semantic Web is challenged by the URI meaning issues arising from putting ontologies in open and distributed environments. As a try to clarify some of the meaning issues, this paper proposes a new approach to interpreting distributed ontologies, it&apos;s built on the top of local models semantics, and extends it to deal with the URI sharing by harmonizing the local models via agreement on vocabulary provenance. The commitment relationship is presented to allow the URI sharing between ontologies with richer semantics.
This paper discusses some of the typical characteristics of modern Web applications and analyses some of the problems the developers of such systems have to face. One of such types of applications are integrated Web applications, i.e. applications that integrate several independent Web services. The paper focuses on providing software fault tolerance for such systems. The solution we put forward employs the concept of Coordinated Atomic (CA) actions for structuring such applications and for providing fault tolerance using exception handling. The paper discusses important design and implementation decisions we have made while developing a Travel Agency (TA) case study and attempts to generalise them to allow CA actions to be easily applied for building dependable Web applications.
In this article, a contribution is made to information extraction and Bayesian network learning motivated by two practical information extraction tasks.
Automatic term recognition is a natural language processing technology which is gaining increasing prominence in our information-overloaded society. Apart from its use for quick and efficient updating of terminologies and thesauri, it has also been used for machine translation, information retrieval, document indexing and classification as well  as content representation. Until very recently, term identification techniques rested solely  on the mapping of term linguistic properties onto computational procedures. However,  actual terminological practice has shown that context is also important for term identification and interpretation as terms may appear in different forms depending on the situation of use. The aim of this article is to show the importance of contextual information for automatic term recognition by exploiting the relation between verbal semantic content and term occurrence in three subcorpora drawn from the British National Corpus.
To insert restrained edges into TIN is of great necessity in many fields. At present, for the need of mass data processing, it is expected that the time efficiency of the algorithm be higher and higher. As a result, the paper proposes the method of radial spatial division based on ) , ( i y x Qi to realize the restrained edges mosaic in constructed TIN. First of all, it introduces the basic principle of radial spatial division based on ) , ( i y x Qi . After that, on the basis of the principle the algorithm to realize restrained edges mosaic is given in detail. A spatial division tree is proposed as an efficient implementation method in the aspect of reconstruction of triangles and their spatial relationship after the division. And then, the triangle obtained with this method is sure in the sub-zone is proved. The analysis of time complexity shows that the time complexity to execute ) , ( i i y x Qi is lower than that to compute the distance from a point to a line. Finally, the conclusions are presented. It is shown that the radial spatial division algorithm proposed in this paper has more advantages in time efficiency than the spatial division algorithm based on distance.
The progress of excavation work has been regularly recorded by taking images during the excavation seasons of Finnish Jabal Haroun Project. This multitemporal archaeological imagery is collected during 1998-2003. Images have been taken daily from the archaeological excavation site, namely the monastic complex of St. Aaron located near Petra, in Jordan. The images have been taken with non-metric digital camera and with non-metric video camera with varying imaging strategies. Large amount of images are convergent panoramic images taken by rotating the camera on the tripod around the projection center. Also separate images and some stereo pairs are available. The aim in the project has been to develop simple documentation methods for archaeologists and therefore minimum preparations are needed during the fieldwork to minimize the disturbance for excavation work. Use of signalised control points is minimized. During excavation season basic geometry of the site has been measured with a tacheometer. In some cases natural control points have been measured with the tacheometer and marked on the printouts. The 3D tacheometer data can be used for solving the orientation parameters. The possibilities to use the collected imagery are presented. The progress of excavation is visualized in one example area -- the chapel. The images have been organized by date and their rough orientation relating to the chapel is known. The images are taken from different angles relating to the excavation trenches (the chapel has been excavated in several steps). Therefore a simplified 3D model is used as a visual aid to show the orientation relating to the model. The user can study the available imagery and select the appropriate images for further processing. The original image orientation can be...
Chairs of the Supervisory Committee:  Professor Craig Chambers  Professor David Notkin  Computer Science and Engineering  Software architecture describes the high-level structure of a software system, and can be used for design, analysis, and software evolution tasks. However, existing tools decouple architecture from implementation, allowing inconsistencies to accumulate as a software system evolves. Because of the potential for inconsistency, engineers evolving a program cannot fully trust the architecture to accurately describe the properties or structure of the implementation.
In this paper we extend a program logic for verifying JAVA  CARD applications by introducing a \throughout&quot; operator that allows  us to prove \strong&quot; invariants. Strong invariants can be used to ensure  \rip out&quot; properties of JAVACARD programs (properties that are to be  maintained in case of unexpected termination of the program). Along  with introducing the \throughout&quot; operator, we show how to handle the  JAVACARD transaction mechanism (and, thus, conditional assignments)  in our logic. We present sequent calculus rules for the extended logic.
This paper presents a new method for endocardial (inner) and epicardial (outer) contour estimation from sequences of echocardiographic images. The framework herein introduced is fine-tuned for parasternal short axis views at the papillary muscle level. The underlying model is probabilisticff it captures the relevant features of the image generation physical mechanisms and of the heart morphology. Contour sequences are assumed two-dimensional noncausal first order Markov random processesff each variable has a spatial index and a temporal index. The image pixels are modelled as Rayleigh distributed random variables with means depending on their positions (inside endocardium, between endocardium and pericardium, or outside pericardium). The complete probabilistic model is built under the Bayesian framework. As estimation criterion the maximum a posteriori (MAP) is adopted. To solve the optimization problem one is led to (joint estimation of contours and distributions&apos; parameters), we introduce an algorithm herein named iterative multigrid dynamic programming (IMDP). It is a fully data driven scheme with no ad-hoc parameters. The method is implemented on an ordinary workstation, leading to computation times compatible with operational use. Experiments with simulated and real images are presented.
This paper presents results from experiments devised to evaluate the comparative performance  of a number of parallel algorithms we have devised for Suffx Arrays. This is an indexing  strategy for very large text databases which allows complex queries to be effected in an effcient  way. We have parallelized this strongly sequential strategy and obtained performance results  with actual implementations operating upon a cluster of PCs and benchmark work-loads.
In this paper, we study various supervised learning methods for training feed-forward neural networks. In general, such learning can be considered as a nonlinear global optimization problem in which the goal is to minimize a nonlinear error function that spans the space of weights using heuristic strategies that look for global optima (in contrast to local optima). We survey various global optimization methods suitable for neural-network learning, and propose the NOVEL method, a novel global optimization method for nonlinear optimization and neural network learning. By combining global and local searches, we show how NOVEL can be used to find a good local minimum in the error space. Our key idea is to use a user-defined trace that pulls a search out of a local minimum without having to restart it from a new starting point. Using five benchmark problems, we compare NOVEL against some of the best global optimization algorithms and demonstrate its superior improvement in performance. 1 In...
technology in electronic noses would in many ways be able to outperform humans in the detection of odours, it is highly advantageous for such a system to be able to convey its own perceptions on a symbolic level. The main challenge, however, is to correctly represent the sensor data with a set of predefined labels. The labels then serve as a common lexicon between the human and the electronic device. Additionally, the system would need to be able to manipulate the symbols to accord with its sensor perceptions. Therefore, the system maintains the freedom to evaluate the appropriateness of the given symbols with respect to the sensory analysis space, thus possessing the capability to communicate or describe its own perception of odours using a common lexicon.  To cluster the data from the electronic nose, we implement a robust competitive clustering algorithm (RCA) created by Frigui et al. [2]. The technique is a hybrid of hierarchical and partitional clustering. The method uses the hie
This paper is devoted to scheduling a large collection of independent tasks  onto a large distributed heterogeneous platform, which is composed of a set of  servers. Each server is a processor cluster equipped with a file repository. The  tasks to be scheduled depend upon (input)  les which initially reside on the  server repositories. A given file may well be shared by several tasks. For each  task, the problem is to decide on which server to execute it, and to transfer  the required  les (those which the task depends upon) to that server repository. The objective
A general problem in model selection is to obtain the right parameters that make a model fit observed data. For a Multilayer Perceptron (MLP) trained with Backpropagation (BP), this means finding the right hidden layer size, appropriate initial weights and learning parameters. This paper proposes a method (G-Prop-II) that attempts to solve that problem by combining a genetic algorithm (GA) and BP to train MLPs with a single hidden layer. The GA selects the initial weights and the learning rate of the network, and changes the number of neurons in the hidden layer through the application of specific genetic operators. G-Prop-II combines the advantages of the global search performed by the GA over the MLP parameter space and the local search of the BP algorithm.  The application of the G-Prop-II algorithm to several real-world and benchmark problems shows that MLPs evolved using G-Prop-II are smaller and achieve a higher level of generalization than other perceptron training algorithms, s...
This paper presents how it is possible to introduce active motricity into particle-bond systems used in  applications such as image animation. We chose to add into some neural network capabilities over the  classical approach, in order to obtain a system able to model a larger class of behaviour. Therefore a new  type of binary bond enriched with a neural-based command ability is proposed and tested in this paper. This  &quot;active&quot; bond acts like a controlled muscle in order to produce motricity.
We introduce a new type of Self-Organizing Map (SOM) to navigate  in the Semantic Space of large text collections. We propose a &quot;hyperbolic  SOM&quot; (HSOM) based on a regular tesselation of the hyperbolic  plane, which is a non-euclidean space characterized by constant negative  gaussian curvature. The exponentially increasing size of a neighborhood  around a point in hyperbolic space provides more freedom to map the  complex information space arising from language into spatial relations.
In this paper, we show how elaborate support for framework-based software evolution can be provided based on explicit documentation of the hot spots of object-oriented application frameworks. Such support includes high-level transformations that guide a developer when instantiating applications from a framework by propagating the necessary changes, as well as application upgrading facilities based on these transformations. The approach relies on active declarative documentation of the design and evolution of the framework&apos;s hot spots, by means of metapatterns and their associated transformations.
This pas6 as6flOM thepredicta)IWfifi of networktrao by considering two metrics: (1) how fa into the futurea trar   ra6 processca be predicted with bounded error; (2) wha the minimum prediction error is overa specified prediction   timeinterva. The a..Mz.6)I isba.fl on two staMMflfl6) tra models: thea6IIOzW6)IMO.z moving aving6av   Maafi6fi6fi6a poisson process. In thispas6M we do not at to propose the best tra6 (prediction) model, which is   obviouslya hai ai aiously issue.Instea. we focus on theconstra6)I predicta)IfiflM estimaa) withah6zzFflI6   aI discussionascu the modeling adeling6 The specific time sca6 orbaIMI.fi6 utiliza)MF taili ofa predictive  14 network controlatrol6. forms theconstra6)M Wea6MI tha the two models, though bothshort-ra6) dependent,ca  15  ca6afi stafi6fi6fi of (self-similaM tral quiteate6M.flzfi for the limited timesca6I of interests inmeaO.fl6)MxIIM6a  16  traa maafi6fi6fi6 Thisais6.IFz inmaFMzx6)MflM terms, simply reflects the fa6 tha thesummazW6) exponentia  17  (correlafi6a functionsma ations6.MO a hyperbolica one very well.
Dynamic programming provides a methodology to develop planners and controllers for nonlinear systems. However, general dynamic programming is computationally intractable. We have developed procedures that allow more complex planning and control problems to be solved. We use second order local trajectory optimization to generate locally optimal plans and local models of the value function and its derivatives. We maintain global consistency of the local models of the value function, guaranteeing that our locally optimal plans are actually globally optimal, up to the resolution of our search procedures.  Learning to do the right thing at each instant in situations that evolve over time is difficult, as the future cost of actions chosen now may not be obvious immediately, and may only become clear with time. Value functions are a representational tool that makes the consequences of actions explicit. Value functions are difficult to learn directly, but they can be built up from learned mode...
We propose an architecture for large scale, multi-user, distributed multimedia based on cooperating agents communicating by means of an observable communication channel. In addition to the traditional protocols based on point-to-point communication, coordination and cooperation should be supported via social awareness and overhearing. Overhearing also allows the collection of contextual information without interfering with already deployed systems. Our domain of application is interactive museums, which are typical examples of so-called &quot;active environments&quot; or &quot;ambient intelligence&quot;. Purpose of this paper is to present the core concepts, and outline the future lines of research.
: Recently, the idea of semantic portals on the Web or on the intranet has gained popularity.
Symbolic model checking is a very successful formal verification  technique, classically based on Binary Decision Diagrams (BDDs). Recently,
A common technique for constructing reliable distributed applications is to use atomic actions for controlling operations on persistent objects. Atomic actions are used to ensure that inconsistencies in application state do not arise when failures occur or when concurrent activities operate on shared objects. Within such an application, objects provide a convenient unit for distribution and concurrency-control. The properties of atomic actions and objects can be exploited together to configure distributed applications, without affecting the correct functioning of the application. This leads to the possibility of changing the configuration of concurrency and distribution of the distributed application to improve availability and performance. These changes in concurrency and distribution can be achieved by varying the object decomposition within the application. In this paper, we show how some kinds of reconfiguration can be achieved without any modification to client applications. The observations are a result of constructing reliable distributed applications using the Arjuna system, which provides tools and libraries for programming with atomic actions and persistent objects in C++.
In this short paper a new technique, called Fault Injection Simulation (FIS), is discussed that is suited for deriving results for steady-state measures in discrete event dynamic systems which are strongly influenced by rarely occurring events. FIS is based on division of the observations in those that are affected and those that are not affected by these rare events. If methods are available FIS can be used as a (partly) analytical technique, else as a pure fast simulation technique. Under intuitively appealing assumptions FIS gives an unbiased estimator and a variance reduction. In this short paper we discuss FIS from a practical point of view; when can FIS be used and how should FIS be used. Furthermore, we show how to adjust FIS during the simulation to benefit maximally from its variance reduction capabilities.
The increasing interest in planning in nondeterministic domains  by model checking has seen the recent development  of two complementary research lines. In the first, planning  is carried out considering extended goals, expressed in the  CTL temporal logic, but has been developed under the simplifying  hypothesis of full observability. In the second, simple  reachability goals have been tackled under the more general  hypothesis of partial observability. The combination of  extended goals and partial observability for nondeterministic  domains is, to the best of our knowledge, an open problem,  whose solution turns out to be by no means trivial. This paper
This article introduces and discusses different innovative means for visual specification and animation of complex concurrent systems. We introduce the completely visual programming language Pictorial Janus (PJ) and its application in the customer-oriented design process. PJ implements a completely visual programming language with inherent animation facilities. We outline the transformation of purely visual PJ programs into textual imperative programming languages. The second part of this article investigates animated 3D-presentations and introduces our novel approach to an animated 3D programming language for interactive customer-oriented illustrations.
Back-channel feedback, responses such as uh-uh from a listener, is a pervasive feature of conversation.  It has long been thought that the production of back-channel feedback depends to a large  extent on the actions of the other conversation partner, not just on the volition of the one who  produces them. In particular, prosodic cues from the speaker have long been thought to play a  role, but have so far eluded identification. We have earlier suggested that an important prosodic  cue involved, in both English and Japanese, is a region of low pitch late in an utterance (Ward,  1996). This paper presents evidence for this claim, surveys other factors which elicit or inhibit  back-channel responses, discusses issues in the definition of back-channel feedback, and mentions  a few related phenomena and theoretical issues.  *We thank Keikichi Hirose for the pitch tracker, Yuichiro Fukuchi for efforts to disprove our  hypothesis, Daniel Jurafsky, Kikuo Maekawa, Elizabeth Shriberg, Maki Sugi...
We present a new triangle scan conversion algorithm that works entirely in homogeneous coordinates. By using homogeneous coordinates, the algorithm avoids costly clipping tests which make pipelining or hardware implementations of previous scan conversion algorithms difficult. The algorithm handles clipping by the addition of clip edges, without the need to actually split the clipped triangle. Furthermore, the algorithm can render true homogeneous triangles, including external triangles that should pass through infinity with two visible sections. An implementation of the algorithm on Pixel-Planes 5 runs about 33% faster than a similar implementation of the previous algorithm.
A framework for object segmentation using deformable contours is presented. This framework aims to extract multiple objects whose shapes are similar, but whose image qualities are different. Objects with good image qualities are called &quot;strong objects,&quot; and objects with bad image qualities are called &quot;weak objects.&quot; Our framework is based on the deformable contours method where a new kind of energy is introduced to handle sharing of properties between objects, and also to allow contours of strong objects to guide the  deformation of contours of weak objects. Weighting parameters are automatically generated and used as control parameters in the contour guiding scheme. Dynamic programming is used for implementation of the multiple snake optimization framework. Comprehensive testing has been performed on natural images and synthetic images and the results are encouraging.
From a game developers point of view terrain rendering is much more than just the real-time display of a landscape. More accurately, the latter is only one part of what we call the terrain rendering pipeline. The most important stages of this pipeline are the design of the terrain geometry, the real-time display of the terrain, natural texturing, dynamic lighting, dynamic shadowing, and the enrichment of the terrain with organic features. We intend to survey each stage of this terrain rendering pipeline by giving an example from the actual multi-award winning DX8 game AquaNox. In this way we illustrate the data flow through each stage of the pipeline, effectivley giving a report on the current state of the art in terrain rendering.
Distinct notions of message integrity (authenticity) for block-oriented symmetric encryption are defined by integrity goals to be achieved in the face of different types of attacks. These notions are partially ordered by a &quot;dominance&quot; relation. When chosen-plaintext attacks are considered, most integrity goals form a lattice. The lattice is extended when known-plaintext and ciphertext-only attacks are also included. The practical use of the dominance relation and lattice in defining the relative strength of different integrity notions is illustrated with common modes of encryption, such as the &quot;infinite garble extension&quot; modes, and simple, non-cryptographic, manipulation detection code functions, such as bitwise exclusive-or and constant functions. 
In this paper we present a design for the EpipE, a new expressive electronic music controller based on the Irish Uilleann Pipes, a 7-note polyphonic reeded woodwind. The core of this proposed controller design is a continuous electronic tonehole-sensing arrangement, equally applicable to other woodwind interfaces like those of the flute, recorder or Japanese shakuhachi. The controller will initially be used to drive a physically-based synthesis model, with the eventual goal being the development of a mapping layer allowing the EpipE interface to operate as a MIDI-like controller of arbitrary synthesis models.
We define the `frozen development&apos; of coloring random graphs. We identify two nodes in a graph as frozen if they are the same color in all legal colorings. This is analogous to studies of the development of a backbone or spine in SAT (the Satisability problem). We first describe in detail the algorithmic techniques used to study frozen development. We present strong empirical evidence that freezing in 3-coloring is sudden. A single edge typically causes the size of the graph to collapse in size by 28%. We also use the frozen development to calculate unbiased estimates of probability of colorability in random graphs, even where this probability is as low as 10^-300. We investigate the links between frozen development and the solution cost of graph coloring. In SAT, a discontinuity in the order parameter has been correlated with the hardness of SAT instances, and our data for coloring is suggestive of an asymptotic discontinuity. The uncolorability threshold is known to give rise to har...
A new generation of sensor rich, massively distributed  autonomous system is being developed, such as smart  buildings and reconfigurable factories. To achieve high  performance these systems will need to accurately  model themselves and their environment from sensor  information. Accomplishing this on a grand scale requires  automating the art of large-scale modeling. To  this end we have developed decompositional, model-based  learning (DML). DML takes a parameterized  model and sensed variables as input, decomposes it,  and synthesizes a coordinated sequence of &quot;simplest&quot; estimation tasks. The method exploits a rich analogy  between parameter estimation and consistency-based diagnosis. Moriarty, an implementation of DML, has  been applied to thermal modeling of a smart building,  demonstrating a significant improvement in learning rate.
This paper presents a new technique based on anisotropic diffusion for segmenting color images. This operation is accomplished through two independent anisotropic diffusion processes: one involving only the chromatic information, conveniently embedded in a complex function, and the other affecting the lightness information. The results of the two diffusions are separately segmented and their combination allows the color image to be eventually partitioned. We report on some experimental results verifying the effectiveness of such a technique.  1. 
In this paper we present a method for recovering the reflectance properties of all surfaces in a real scene from a sparse set of photographs, taking into account both direct and indirect illumination. The result is a lighting-independent model of the scene&apos;s geometry and reflectance properties, which can be rendered with arbitrary modifications to structure and lighting via traditional rendering methods. Our technique models reflectance with a lowparameter reflectance model, and allows diffuse albedo to vary arbitrarily over surfaces while assuming that non-diffuse characteristics remain constant across particular regions. The method&apos;s input is a geometric model of the scene and a set of calibrated high dynamic range photographs taken with known direct illumination. The algorithm hierarchically partitions the scene into a polygonal mesh, and uses image-based rendering to construct estimates of both the radiance and irradiance of each patch from the photographic data. The algorithm computes the expected location of specular highlights, and then analyzes the highlight areas in the images by running a novel iterative optimization procedure to recover the diffuse and specular reflectance parameters for each region. Lastly, these parameters are used in constructing high-resolution diffuse albedo maps for each surface.
In this paper, we address the problem of designing  a scalable, accurate query processor for peerto  -peer filesharing and similar distributed keyword  search systems. Using a globally-distributed monitoring  infrastructure, we perform an extensive  study of the Gnutella filesharing network, characterizing  its topology, data and query workloads. We observe
this paper is structured as follows: Section 2 brieAEy describes the AXML data and service integration framework. Then, Section 3 illustrates AXML through the auction demonstration scenario. SOAP client  SOAP  service  AXML storage  Evaluator  XQuery  processor  query  results  definitions  AXML service  update  read update  read  AXML peer S1  wrapper  SOAP  service call service result  query  consults  Figure 1: Outline of the AXML data and web services integration architecture
this paper we  focus on the high quality aspect and, more precisely, we present the role that  two specific ontologies play and the advantages that they provide to the system
In this paper it is shown that the dynamics of a conventional type of blended multiple model system are only weakly related to the local models from which it is formed. A novel class of velocity-based blended multiple model systems is proposed for which the dynamics are directly related to the local models. Indeed, the solution to the blended multiple model system, locally to a specific operating point, is approximated by the weighted linear combination of the solutions to the local models. Moreover, in contrast to conventional blended multiple model systems, the velocity-based blended multiple model systems employs linear local models, thereby providing a degree of continuity with established linear methods and, consequently, facilitating analysis and design.
This  paper reviews conventional record linkage, which assumes shared variables between the external and  the protected data sets, and then shows that record linkage---and thus disclosure---is still possible  without shared variables
Audiovisual speech synthesis systems usually are inflexible with respect to the ability to replace the audio and video synthesis and the control algorithms due to the dependencies of the implemented pieces. In order to enable a newly developed system to exchange modules, to evaluate their specific advantages, and to detect their weak points, the author proposes a framework for audiovisual speech synthesis systems which divides the system into several modules and describes their information flow [7]. This paper presents MASSY, the first prototypic implementation of the framework. Besides the embedded audio synthesis, the presented implementation includes a phonetic articulation module, a visual articulation module, and a face module. The visual articulation module implements two alternative models based on a dominance model for co-articulation in terms of Lfqvist&apos;s suggestion [10][3] and a pattern selection algorithm, respectively. The realized face is a 3D model described in VRML 97 [16] with additionally implemented functionality according to the H-Anim 2001 standard. The facial animation is described in a motion parameter model which is capable to realize the most important visible articulation gestures [4][1]. MASSY is developed in the client-server paradigm, where the server is easy to set up and does not need special or high performance hardware. The required bandwidth is low, and the client is an ordinary web browser with standard, non-proprietary plug-ins. The presented system is suitable for the evaluation of measured or predicted articulation models, as well as for the enhancement of human-computer-interfaces in applications like e.g. virtual tutors in e-learning environments, speech training, video conferencing, computer games, audiovisual information systems,...
Directed diffusion is a prominent example of data-centric routing based on application layer data and purely local interactions. In its functioning it relies heavily...
We study the tradeoff between inference and search in the Davis Putnam  algorithm. We show that neighbour resolution, a restricted form of  resolution applied during search, can be simulated by applying binary resolution  before search. We compare experimentally the cost of the two  different methods. Our results demonstrate that binary resolution during  preprocessing is generally effective at reducing both the size of the search  tree and the total search time.
The so called associative thinking, which humans are known to perform on every day basis, is attributed to the fact that human brain memorizes information using the dynamical system made of interconnected neurons. Retrieval of information in such a system is accomplished in associative sense; starting from an arbitrary state, which might be an encoded representation of a visual image, the brain activity converges to another state, which is stable and which is what the brain remembers. In this paper we explore the possibility of using an associative memory for the purpose of enhancing the interactive capability of perceptual vision systems. By following the biological memory principles, we show how vision systems can be designed to recognize faces, facial gestures and orientations, using low-end videocameras and little computational power. In doing that we use the public domain associative memory code.
In this paper, we present a new method for cell placement. The method is based on a new metric for wirelength that ensures no overlap among cells sharing common nets (repeller model). Moreover, new forces working on the cells are added to the new metric to attract the cells to the less dense regions and help spread out the cells within the placement area. Minimizing traditional metrics (linear or quadratic) results in a placement with substantial amount of overlap. The methodology iterates between global optimization and slicing the placement area to diminish cell overlap and attain uniform distribution of the cells within the placement floor. To help cells spread out, hard constraints are added to the problem in each iteration resulting in a further constrained version of the original problem. Unlike these approaches, no hard constraints are required in the new approach. Besides, the new metric is convex and versatile in the sense that it can be applied to placement problems with no fixed cells (i.e, FPGAs).
The work of this paper is inspired by the flocking phenomenon observed in Reynolds (1987). We introduce a class of local control laws for a group of mobile agents that result in: (i) global alignment of their velocity vectors, (ii) convergence of their speeds to a common one, (iii) collision avoidance, and (iv) minimization of the agents artificial potential energy. These are made possible through local control action by exploiting the algebraic graph theoretic properties of the underlying interconnection graph. Algebraic connectivity affects the performance and robustness properties of the overall closed loop system. We show how the stability of the flocking motion of the group is directly associated with the connectivity properties of the interconnection network and is robust to arbitrary switching of the network topology.
We present a highly polymorphic tool for the construction,  synthesis, structuring, manipulation, investigation, and (symbolic) execution  of graphs. The flexibility
BGP-MS is a user modeling shell system that can assist interactive software systems in adapting  to their current users by taking the users&apos; presumed knowledge, beliefs, and goals into account. It  offers applications several methods for communicating observations concerning the user to BGPMS,  and for obtaining information on currently held assumptions about the user from BGP-MS. It provides a 
Students still take class notes using pencil and paper -- although digital documents are more legible, easier to search in and easier to edit -- in part because of the lack of software to support note-taking. Class notes are characterized by free spatial organization, many small chunks of text, and a dense mix of text and graphic elements. These characteristics imply that a note-taking system should use pen, keyboard, and mouse-or-equivalent; allow the swift entry of text at any desired position; and minimize the need to switch between input tools. A system with these properties was built and used by 10 subjects in a controlled study and by 4 users in their classes. Some users preferred our system to pencil and paper, suggesting that taking class-notes with the computer is feasible.
. Using EREW-PRAM algorithms on a tournament based complete binary tree we implement the insert and extract-min operations with p = log N processors at costs O(1) and O(log log N) respectively. Previous solutions [4, 7] under the PRAM model and identical assumptions attain O(log log N) cost for both operations. We also improve on constant factors the asymptotic bound for extract-min since in it we reduce the use of communication demanding primitives. The tournament tree enables the design of parallel algorithms that are noticeably simple. 1 Tournament trees  Our data structure is a complete binary tree (CBT). Every item stored in the tree consists of a priority value and an identifier. We associate every leaf of the CBT with one item, and use the internal nodes to maintain a continuous binary tournament among the items. A match, at internal node n, consists of determining the item with higher priority (lesser numerical value) between the two children of n and writing the identifier of ...
Copyright (1994)  This paper places a new interpretation on the traditional evolutionary  account of money and banking institutions. This allows the  development of a new conceptual framework for a monetary model  called the Loans Standard. The Loans Standard has an institutional  structure that minimizes the transaction costs of using exchange credit  (i.e. money). Developments within the existing system are interpreted  as evolutionary steps in the direction of the Loans Standard model.  This is a revised version of the paper presented to the 21st Conference of Economists, Melbourne, July 1992  1  2 I Introduction  Goldschlager has proposed a new conceptual framework for a monetary and banking system called the Loans Standard (Goldschlager 91). The Loans Standard (LS) model is based on two theoretical insights into the traditional logical evolutionary account of money and banking development. These insights are described in the next section. Briefly, the first insight is that money i...
Iceberg queries are a special case of SQL queries involving GROUP  BY and HAVING clauses, wherein the answer set is small relative to the database  size. We present here a performance framework and a detailed evaluation within  this framework of the efficiency of various iceberg query processing techniques.
Distributed crawling has shown that it can overcome important limitations of the today&apos;s crawling paradigm. However, the optimal benefits of this approach are usually limited to the sites hosting the crawler. In this work, we propose a location-aware method, called IPMicra, that utilizes an IP address hierarchy, and allows crawling of links in a near optimal location aware manner.
The sharing of association rules is often beneficial in industry,  but requires privacy safeguards. One may decide to disclose only part  of the knowledge and conceal strategic patterns which we call restrictive  rules. These restrictive rules must be protected before sharing since they  are paramount for strategic decisions and need to remain private. To address  this challenging problem, we propose a unified framework for protecting  sensitive knowledge before sharing. This framework encompasses: (a)  an algorithm that sanitizes restrictive rules, while blocking some inference  channels. We validate our algorithm against real and synthetic datasets;  (b) a set of metrics to evaluate attacks against sensitive knowledge and  the impact of the sanitization. We also introduce a taxonomy of sanitizing  algorithms and a taxonomy of attacks against sensitive knowledge.
There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.
Careful consideration of the uncertainties and sensitivities associated with model outputs is  essential when critical decisions are made on the basis of such results. Consideration of uncertainty is  particularly important in the context of natural resource management, where models are often used to  tackle complex and conflicting issues across multiple scales, as is the case in evaluating management  options to reduce surface water pollution. This paper describes an analysis of uncertainty in the  catchment-scale integrated hydrologic, economic, stream sediment and nutrient export model known as  CatchMODS. The paper briefly describes the linked components of CatchMODS and its application in  the Ben Chifley Dam catchment, Australia. An initial investigation to investigate some of the most  important sources of output uncertainty is described. First-order sensitivities to selected model  parameters are found analytically by linearising parts of the model and used, together with knowledge  of where non-linearity has most effect, to point to conditions to be investigated further. The extent of  non-linear effects is also checked by comparing the analytical results with the results of parameterperturbation  tests. Results from the analysis are used to prioritise continuing model development and  data-collection activities. The results are also to be incorporated into a decision-analysis framework to  evaluate management options to reduce surface water pollution. The decision-analysis framework and  incorporation of uncertainty analysis into it are outlined.
With the increasing availability of Web-enabled mobile devices, we are facing the problem to effectively adapt Web content for those devices. For adaptation, Web page structures require rearrangement, i.e., transcoding, to better fit on small displays when layout-specific HTML structures like tables and frames are used. In this article, we present a Fuzzy rule-based language, i.e., Fuzzy-RDL/TT. FuzzyRDL /TT (Fuzzy Rule Description Language for Tree Transformation) specifies sets of transcoding functions and defines their selection with respect to user and hardware profiles. The definition of profile selection is defined by the means of fuzzy rule descriptions where transcoding functions are specified by a Java-oriented language which operate on the DOM (Document Object Model) tree representation of an XML-based document.
UtilNets is a decision-support system for rehabilitation planning and optimisation of the maintenance of underground pipe networks of water utilities. The DSS performs reliability-based life predictions of the pipes and determines the consequences of maintenance and neglect over time in order to optimise rehabilitation policy. Keywords: water network maintenance, rehabilitation planning, probabilistic model, decision support system, geographical information system  UTILNETS: A Water Mains Rehabilitation Decision Support System -3-  Introduction  Many cities are now faced with the major task of rehabilitating their water mains. Twenty percent of these mains in large metropolitan areas are below central business districts and if they fail this will result in severe traffic disruptions. Critical in the event of failure are also pipes whose collapse would result in unavailability of potable water to hospitals and other important customers, unavailability of water for fire fighting, contam...
Forgetting automata are nondeterministic linear bounded automata whose rewriting capability is restricted as follows: each cell of the tape can only be &quot;erased&quot; (rewritten by a special symbol) or completely &quot;deleted&quot;. We show that the &quot;erasing&quot; is in some sense more powerfull than the &quot;deleting&quot;.
The standard model of supervised learning  assumes that training and test data are  drawn from the same underlying distribution.
Due to its low attenuation, ffber has become that medium of choice for point-to-point links. Using Wavelength-Division Multiplexing (WDM), manychannels can be created in the same fiber. A network node equipped with a tunable optical transmitter can select any of these channels for sending data. An optical interconnection combines the signal from the various receivers in the network, and makes it available to the optical receivers, whichmay also be tunable. By properly tuning transmitters and/or receivers, point-to-point links can be dynamically created and destroyed. Therefore, in a WDM network, the routing algorithm has an additional degree of freedom compared to traditional networks: it can modify the network topology to create the routes. In this report, we consider the problem of routing audio/video streams in WDM networks. We present a general linear integer programming formulation for the problem. However, since this is a complex solution, we propose simpler heuristic algorithms, both for the unicast case and for the multicast case. The performance of these heuristics is evaluated in a number of scenarios, with a realistic traffic model, and from the evaluation we derive guidelines for usage of the heuristic algorithms.
This paper presents our recent work on developing parallel algorithms and software for solving the global minimization problem for molecular conformation, especially protein folding. Global minimization problems are difficult to solve when the objective functions have many local minimizers, such as the energy functions for protein folding. In our approach, to avoid directly minimizing a &quot;difficult&quot; function, a special integral transformation is introduced to transform the function into a class of gradually deformed, but &quot;smoother&quot; or &quot;easier&quot; functions. An optimization procedure is then applied to the new functions successively, to trace their solutions back to the original function. The method can be applied to a large class of nonlinear partially separable functions including energy functions for molecular conformation and protein folding. Mathematical theory for the method, as a special continuation approach to global optimization, is established. Algorithms with different solutio...
The objectives of global transaction management in multidatabase  systems(MDBS) are to avoid the inconsistent retrievals and  guarantee the global serializability under the existence of indirect conflict  which is unknown to to the global transaction manager(GTM). Many  researches have shown that it is difficult to design the global concurrency  control method because of local autonomy. In these method global  transactions have a few opportunities to be executed concurrently. We  concentrate our attention on 1) investigation into the more accurate indirect  conflict situation and 2) supporting the higher concurrency degree  by using the concept of global integrity constraints. We define the multidatabase  transaction model and then propose the concurrency control  protocols. In our method the more global transaction can be concurrently  executed, since the refined boundary of possibility of indirect conflict is  offered.
J-Orchestra is an automatic partitioning system for Java programs. JOrchestra  takes as input Java applications in bytecode format and transforms  them into distributed applications, running on distinct Java Virtual Machines. To  accomplish such automatic partitioning, J-Orchestra uses bytecode rewriting to  substitute method calls with remote method calls, direct object references with  proxy references, etc. Using J-Orchestra does not require great sophistication in  distributed system methodology---the user only has to specify the network location  of various hardware and software resources and their corresponding application  classes. J-Orchestra has significant generality, flexibility, and degree of  automation advantages compared to previous work on automatic partitioning.
This paper discusses a novel distributed adaptive algorithm and representation used to construct populations of adaptive Web agents. These InfoSpiders browse networked information environments on-line in search of pages relevant to the user, by traversing hyperlinks in an autonomous and intelligent fashion. Each agent adapts to the spatial and temporal regularities of its local context thanks to a combination of machine learning techniques inspired by ecological models: evolutionary adaptation with local selection, reinforcement learning and selective query expansion by internalization of environmental signals, and optional relevance feedback. We evaluate the feasibility and performance of these methods in three domains: a general class of artificial graph environments, a controlled subset of the Web, and (preliminarly) the full Web. Our results suggest that InfoSpiders could take advantage of the starting points provided by search engines, based on global word statistics, and then use linkage topology to guide their search on-line. We show how this approach can complement the current state of the art, especially with respect to the scalability challenge.
Companies that rely on the Internet for their daily business are challenged by uncontrolled massive worm spreading and the lurking threat of large-scale distributed denial of service attacks. We present a new model and methodology, which allows a company to qualitatively and quantitatively estimate possible financial losses due to partial or complete interruption of Internet connectivity. Our systems engineering approach is based on an in-depth analysis of the Internet dependence of different types of enterprises and on interviews with Swiss telcos, backbone and Internet service providers. A discussion of sample scenarios illustrates the flexibility and applicability of our model.
Despite the overwhelming amounts of multimedia data recently generated and the significance of such data, very few people have systematically investigated multimedia data mining. With our previous studies on content-based retrieval of visual artifacts, we study in this paper the methods for mining content-based associations with recurrent items and with spatial relationships from large visual data repositories. A progressive resolution refinement approach is proposed in which frequent item-sets at rough resolution levels are mined, and progressively, finer resolutions are mined only on the candidate frequent item-sets derived from mining rough resolution levels. Such a multi-resolution mining strategy substantially reduces the overall data mining cost without loss of the quality and completeness of the results.
this article, we discuss the properties of this new measure, namely, its robustness with respect to both crystallographic uncertainties and naturally occurring variations in atomic coordinates, and the remarkable fact that it is essentially independent of the choice of the parameters used in calculating it. We also show how visible volume can be used to align protein structures, to identify structurally equivalent positions that are conserved in a family of proteins, and to single out positions in a protein that are likely to be of biological interest. These properties qualify visible volume as a powerful tool in a variety of applications, from the detailed analysis of protein structure to homology modeling, protein structural alignment, and the definition of better scoring functions for threading purposes.
The core idea of the Semantic Web is to make information accessible to human and software agents on a semantic basis. Hence, Web sites may feed directly from the Semantic Web exploiting the underlying structures for human and machine access. We have developed a domain-independent approach for developing semantic portals, o/oov&quot;ffff SEAL (SEmantic portAL), that exploits semantics for providing and accessing information at a portal as well as constructing and maintaining the portal. In this paper we focus on semanticsbased means that make semantic Web sites accessible from the outside, i.e. semantics-based browsing, semantic querying, querying with semantic similarity, and machine access to semantic information. In particular, we focus on methods for acquiring and structuring community information as well as methods for sharing information.
This paper presents a requirements engineering framework based on  the notions of Actor, Goal, and Intentional Dependency, and applies it to a case  study in the field of Information Systems for e-Government. The framework
In this report we describe boundary value problems in ordinary differential equations arising in the computation of self-similar blow-up solutions...
In this paper we outline a parallel implementation of Krylov-accelerated Schwarz domain decomposition in which subdomain problems are solved to low precision. By so doing, computational time is focused on the convergence of the global iteration rather than wasted on ineffective subdomain iterations. We consider the GCR method using classical GramSchmidt and Householder orthogonalization methods. Our goal is to apply this approach to the incompressible Navier-Stokes equations. For the parallel implementation, we assume a distributed memory system with message passing. 
Introduction  We have been developing atmospheric and oceanic general circulation models that are computationally well optimized for the architecture of the Earth Simulator. Our atmospheric general circulation model, AFES, is adopted from the CCSR/NIES AGCM 5.4.02 [1]. Our oceanic general circulation model, OFES, is based on the GFDL MOM3 [2]. Our coupled oceanic and atmospheric model, CFES, consists of AFES and OIFES (OFES with a sea ice submodel). PFES, Princeton University POM-based sigma-coordinate ocean model, also has been developed for simulating regional ocean currents.  With these models that are extremely efficient on the Earth Simulator, we are able to perform ultra-high resolution numerical experiments in which both planetary scale fields and meso-scale phenomena are simulated simultaneously. Our goal is a better understanding of mechanism and predictability of variations that are induced by inter-scale interactions.  2. AFES  We performed several ultra-high resolution nume
The concept of high-order motion compensation is introduced. It is argued that in hybrid video coding, motion-compensated prediction has to be viewed as a source coding problem with a delity criterion. Based on our considerations, various high-order motion compensation approaches are presented that achieve signicantly improved video coding results. The designed motion-compensated predictors achieve gains by exploiting high-order statistical dependencies in the video signal. 
Rank order morphological operators have been designed to better cope with shape variations and noise than the corresponding operators with plain structuring elements. In this paper,starting from a brief historical overview of rank order &quot;lters and erosions/dilations in digital image processing rank order based morphological operators are surveyed. We also highlight the relations between these operators and show that many are similar if not equivalent. An extensive bibliography is provided.
We introduce planar grouping, a technique where planar relationship information is gathered for a set of rigid points. This information is used to accelerate affine transformations and clipping. The planar grouping technique is an optimization problem, implemented in a best-first greedy search. We present two error metrics, one simple and fast, one based on the quadric error metric (QEM) to achieve higher quality planar grouping. We also apply the quadric error metric to linear grouping.
Path profiles record the frequencies of execution paths through a program. Until now, the best global instruction schedulers have relied upon profile-gathered frequencies of conditional branch directions to select sequences of basic blocks that only approximate the frequently-executed program paths. The identified sequences are then enlarged using the profile data to improve the scope of scheduling. Finally, the enlarged regions are compacted so that they complete in a small number of cycles. Path profiles remove the need to approximate the frequently-executed paths that are so important to the success of the compaction phase. In this paper, we describe how one can modify a trace-based instruction scheduler, and in particular a superblock scheduler, to use path profiles in both the selection and enlargement phases of global scheduling. As our experimental results demonstrate, the use of more detailed profile data allows the scheduler to construct superblocks that are more likely to avo...
This paper will discuss how specific quantum properties are useful for making computation systems more efficient. The similarity of these properties to our mind and emotional mechanisms add up to the possibility that our mind is really a quantum supercomputer [1]. These quantum information properties rely on the information nature of quantum mechanics in contrast to the energy nature of classical computing. With this informational infrastructure underlying consciousness, it becomes easier to comprehend how specific states of awareness and emotions can lead to supermind and hyper awareness of superconsciousness. Thoughts, emotions, and quantum mechanics may have a large informational infrastructure in common, which changes our perspective from energy medicine into informational medicine.
Document structure analysis can be regarded as a syntactic analysis problem. The order and containment relations among the physical or logical components of a document page can be described by an ordered tree structure and can be modeled by a tree grammar which describes the page at the component level in terms of regions or blocks. This paper provides a detailed survey of past work on document structure analysis algorithms and summarize the limitations of past approaches. In particular, we survey past work on document physical layout representations and algorithms, document logical structure representations and algorithms, and performance evaluation of document structure analysis algorithms. In the last section, we summarize this work and point out its limitations.
Recently, a large number of equivalences for probabilistic automata  has been proposed in the literature. Except for the probabilistic  bisimulation of Larsen &amp; Skou, none of these equivalences has been characterized  in terms of intuitive testing scenarios. In our view, this is an  undesirable situation: in the end, the behavior of an automaton is what  an external observer perceives. In this paper, we propose and study a simple  and intuitive testing scenario for probabilistic automata. We prove  that the equivalence induced by this scenario coincides with the trace  distribution equivalence proposed by Segala. A technical result that we  need to establish on the way is an Approximation Induction Principle  (AIP) for probabilistic processes.
Interactive TCP applications, such as Telnet and the Web, are particularly sensitive to network congestion. Indeed, congestion-induced queuing and packet loss can be a significant cause of large delays and variability, thereby decreasing user-perceived quality. We consider addressing these effects using service differentiation, by giving priority to interactive applications&apos; traffic in the network. We study different packet marking schemes and handling mechanisms (packet dropping and scheduling) in the network. For marking packets, two approaches are considered. First, we look into application-based marking, and show how the protection of Telnet traffic against loss can eliminate large echo delays caused by retransmit timeouts, and how, by limiting packet loss for Web page downloads, their delays can be significantly reduced, resulting in enhanced interactivity. Second, we consider differentiation based on TCP state, where we present a marking algorithm that prioritizes packets at the source, based on each connection&apos;s window size. In addition, we describe the shaping mechanisms required for conformance to agreements with the network. We show how this marking results in good response times for short transfers, which are characteristic of interactive applications, without significantly affecting longer ones.
Experience gained from constructing a graphical simulator of pipelines  of processor farms has been transferred to prototype simulations of an  ATM buffered-Banyan switch, and a slotted-ring network. Ways to make a  meaningful and appealing display are indicated. The qualitative information  that can be usefully shown is considered. The design and evaluation  of the simulators is included.
this paper, we single out some limitations of DLP for KRR, which cannot naturally express problems where the size of the disjunction is not known &quot;a priori&quot; (like N-Coloring), but it is part of the input. To overcome these limitations, we further enhance the knowledge modelling abilities of DLP, by extending this language by Parametric Connectives (OR and AND). These connectives allow us to represent compactly the disjunction/conjunction of a set of atoms having a given property. We formally define the semantics of the new language, named DLP      and we show the usefulness of the new constructs on relevant knowledge-based problems. We address implementation issues and discuss related works
We present the Robel supervision system which is able to learn from experience robust ways to perform high level tasks (such as &quot;navigate to&quot;). Each possible way to perform the task is modeled as an Hierarchical Tasks Network (HTN), called modality  whose primitives are sensory-motor functions. An HTN planning process synthesizes all the consistent modalities to achieve a task. The relationship between supervision states and the appropriate modality is learned through experience as a Markov Decision Process (MDP) which provides a general policy for the task. This MDP is independent of the environment and characterizes the robot abilities for the task.
A decision support tool for the management of risks from accidents in chemical industrial sites handling hazardous substances (toxic or flammable) is presented. Management of risk refers to potential land use plans and/or emergency response plans that are based on geographical considerations of the extent of risk. A tool has been developed on a GIS (Geographical Information System) computer platform using digital maps to provide visual focus to the relative magnitude of the consequences of the operation of such chemical sites to the human health and environment.
We describeaprototype system capable of extracting machine print addresses from fax images of English language business letters and fax cover sheets. The system automatically orients incoming page images, locates and parses machine printed addresses, and classifies each address as one of fsender, recipient,  otherg.Wepresent results of preliminary performance tests, and discuss potential improvements. 1 
The MASC (Multiple Associative Computing) model is a generalized associative-style computational model that naturally supports massive data-parallelism and also control-parallelism. A wide range of applications has been developed on this model. Recent research has compared its power to the power of other popular parallel models such as the PRAM and MMB models using simulations. However, the simulation of MMB has identified some important issues regarding the cost of certain basic MASC operations required for associative computing such as broadcasts, reductions, and associative searches. This paper investigates these issues and gives background information and an analysis of timings for these operations, based on implementation techniques and comparison fairness with respect to other models. It aims to provide justification and clarify arguments on the timings for these constant-time or nearly constant-time basic MASC operations.
Modern verification systems such as PVS are now reaching the stage of development where the formal verification of critical algorithms is feasible with reasonable effort. This paper describes one such verification in the field of fault tolerance. The distribution of single-source data to replicated computing channels (Interactive Consistency or Byzantine Agreement) is a central problem in this field. The classic Oral Messages (OM) algorithm solves this problem under the assumption that all channels are either nonfaulty or arbitrarily (Byzantine) faulty. Thambidurai and Park have introduced a &quot;hybrid&quot; fault model that distinguishes additional fault modes, along with a modified version of OM. They gave an informal proof that their algorithm withstands the same number of arbitrary faults, but more &quot;nonmalicious&quot; faults than OM. We detected a flaw in this algorithm while undertaking its formal verification using PVS. The discipline of mechanically-checked formal verification helped us to d...
: - In this paper we propose a framework for developing globally convergent batch training algorithms with adaptive learning rate. The proposed framework provides conditions under which global convergence is guaranteed for any training algorithm with adaptive learning rate. To this end, the learning rate is appropriately tuned along the given descent direction. Providing conditions regarding the search direction and the corresponding stepsize length this framework can also guarantee global convergence for any training algorithm with a different learning rate for eachweight. In cases where the direction-related condition is not fulfflled the search direction is properly corrected and the stepsize length along the new search direction is adapted.  Keywords and phrases: Global convergence, learning rate adaptation, batch training algorithms, steepest descent, feedforward neural networks.  1 Introduction  The goal of neural network training is to iteratively update the network weights to min...
Replication is a commonly proposed solution to problems of scale associated with distributed services. However, when a service is replicated, each client must be assigned a server. Prior work has generally assumed that assignment to be static. In contrast, we propose dynamic server selection, and show that it enables application-level congestion avoidance. To make
The introduction of the joint image manifold allows to treat the problem of recovering camera motion and epipolar geometry as the problem of fitting a manifold to the data measured in a stereo pair. The manifold has a singularity and boundary, therefore care must be taken when fitting it.
Recognition of Chinese characters has been an area of major interest for many years, and a large number of research papers and reports have already been published in this area. There are several major problems with Chinese character recognition: Chinese characters are distinct and ideographic, the character size is very large and a lot of structurally similar characters exist in the character set. Thus, classification criteria are difficult to generate. This paper presents a new technique for the recognition of hand-printed Chinese characters using the C4.5 machine learning system. Conventional methods have relied on hand-constructed dictionaries which are tedious to construct and difficult to make tolerant to variation in writing styles. The paper discusses Chinese character recognition using the Hough transform for feature extraction and C4.5 system. The system was tested with 900 characters written by different writers from poor to acceptable quality (each character has 40 samples) and the rate of recognition obtained was 84%.
Non-quadratic regularizers, in particular the ff 1 norm regularizer can yield sparse  solutions that generalize well. In this work we propose the Generalized Subspace  Information Criterion (GSIC) that allows to predict the generalization error for this  useful family of regularizers. We show that under some technical assumptions GSIC  is an asymptotically unbiased estimator of the generalization error. GSIC is demonstrated  to have a good performance in experiments with the ff 1 norm regularizer as  we compare with the Network Information Criterion and cross-validation in relatively  large sample cases. However in the small sample case, GSIC tends to fail to  capture the optimal model due to its large variance. Therefore, also a biased version  of GSIC is introduced, which achieves reliable model selection in the relevant and  challenging scenario of high dimensional data and few samples.
In the present paper we discuss aspects and opportunities of educational content management, focusing on structuring and interactivity schemes from semantic notions of components. A transition from standard educational annotations to semantic statements of hyperlinks is discussed. We introduce the Hypermedia Learning Object System hylOs, an online learning system. hylOs is based on a cellular information model, encapsulating content with meta data conforming to the Learning Object Meta data (LOM) standard. Content management is provisioned on this semantic meta data level and allows for variable, dynamically adaptable access structures. Context aware multifunctional links, based on the semantic linking scheme, permit a systematic navigation depending on the learners and didactic needs, thereby exploring the capabilities of the semantic web. hylOs is built upon the more general Multimedia Information Repository (MIR) and the MIR adaptive context linking environment (MIRaCLE), its linking extension. MIR is an open system supporting the standards XML, Corba and JNDI as well as highlevel authoring tools for the creation of meta data and WYSIWYG like content editing.
Optimistic parallel discrete event simulation has been demonstrated to be an efficient alternative for the simulation of various classes of large scale systems. In particular, it is well-known that organizing the event processing task as a sequence of iterations delimited by the barrier synchronization of the processors is a convenient strategy to follow since it is possible to impose certain global order to an inherently prone-to-unstability form of simulation. Thus an inmediate concern is to ensure a proper control of the amount of work allowed to be done by the processors at any given period of the simulation in a way which is not costly in terms of computation and communication. This paper describes an automatic method for performing such task.
Web applications are becoming increasingly popular for mobile wireless PDAs. However, web browsing on these systems can be quite slow. An alternative approach is handheld thin-client computing, in which the web browser and associated application logic run on a server, which then sends simple screen updates to the PDA for display. To assess the viability of this thin-client approach, we compare the web browsing performance of thin clients against fat clients that run the web browser locally on a PDA. Our results show that thin clients can provide better web browsing performance compared to fat clients, both in terms of speed and ability to correctly display web content. Surprisingly, thin clients are faster even when having to send more data over the network. We characterize and analyze different design choices in various thin-client systems and explain why these approaches can yield superior web browsing performance on mobile wireless PDAs.
This paper presents a method for admission control and request scheduling for multiply-tiered e-commerce Web sites, achieving both stable behavior during overload and improved response times. Our method externally observes execution costs of requests online, distinguishing different request types, and performs overload protection and preferential scheduling using relatively simple measurements and a straightforward control mechanism. Unlike previous proposals, which require extensive changes to the server or operating system, our method requires no modifications to the host O.S., Web server, application server or database. Since our method is external, it can be implemented in a proxy. We present such an implementation, called Gatekeeper, using it with standard software components on the Linux operating system. We evaluate the proxy using the industry standard TPC-W workload generator in a typical three-tiered e-commerce environment. We show consistent performance during overload and throughput increases of up to 10 percent. Response time improves by up to a factor of 14, with only a 15 percent penalty to large jobs.
A common problem with long-lived large industrial software  systems such as telecom and industrial automation systems is the increasing  complexity and the lack of formal models enabling effcient analyses  of critical properties. New features are added or changed during the system  life cycle and it becomes harder and harder to predict the impact of  maintenance operations such as adding new features or fixing bugs.
Conventional underwater sensors are not well suited to the task of aiding unmanned underwater vehicles to hover. These sensors suffer from several drawbacks such as low sampling rates, low resolution, complexity of operations, drift and cost. Underwater video cameras, however, can provide local measurements of position with respect to a local object. Underwater vision presents several challenges: it suffers from a limited range and poor visibility conditions. Besides, recovering motion from images requires high computing power, which is a limited resource on-board most underwater vehicles. The main objective of this thesis was therefore to investigate visual control methods to dynamically position a typical underwater vehicle with respect to a &amp;ff64257;xed object. These methods also had to have computing powers needs compatible with off-the-shelf embedded computers that can be operated on real vehicles. A hybrid visual servoing technique, adaptated from the 2 1/2 D visual servoing scheme, was proposed. Its performance was assessed in simulations using a six degrees-of-freedom nonlinear dynamic model of an Remotely Operated Vehicle. The effects of sea current disturbances, the targets orientation, and noise under sparse feature tracking conditions was studied. The proposed method proved stable and took into account the restrictive controllability of the vehicle. A 2-D visual servoing scheme which employed the Shi-Tomasi-Kanade sparse feature tracker on unmarked planar targets in a water tank was then proposed. The scheme controlled a planar Cartesian robot which emulated the dynamic behaviour of the surge and sway degrees-of-freedom of a typical underwater vehicle. The effect of sea current disturbances on the stability and performance of the control scheme was also studied. An underwater experimental evaluation of the Shi-Tomasi-Kanade feature tracker under various conditions of lighting and relative speed between the underwater scene and the camera was also performed.
  Rapid improvements in mobile computing devices and wireless networks promise to provide a foundation for ubiquitous computing. However, distributed software must be able to adapt to dynamic situations related to several cross-cutting concerns, including quality-of-service, fault-tolerance, energy
This paper introduces a new face coding and recognition method, the Enhanced Fisher Classifier (EFC), which employs the enhanced Fisher linear discriminant model (EFM) on integrated shape and texture features. Shape encodes the feature geometry of a face while texture provides a normalized shape-free image. The dimensionalities of the shape and the texture spaces are first reduced using principal component analysis, constrained by the EFM for enhanced generalization. The corresponding reduced shape and texture features are then combined through a normalization procedure to form the integrated features that are processed by the EFM for face recognition. Experimental results, using 600 face images corresponding to 200 subjects of varying illumination and facial expressions, show that (i) the integrated shape and texture features carry the most discriminating information followed in order by textures, masked images, and shape images; (ii) the new coding and face recognition method, EFC, performs the best among the Eigenfaces method using      distance measure, and the Mahalanobis distance classifiers using a common covariance matrix for all classes or a pooled within-class covariance matrix. In particular, EFC achieves 98.5% recognition accuracy using only 25 features.
Wegiveavariable-free relational calculus which defines exactly all first-order definable relations  in a arbitrary structure. We then show that, over an arbitrary class C of finite ordered  structures with signature f!!!ffR 1 ff:::ffR ff g, the unary relations uniformly defined by this calculus  over C are characterized by a another simplified variable-free calculus whichwe call Q. Q is  the least set of formal expressions such that:  f(Q\Phi\Phi\Phix) j Q 2Qffx2 ! [f1gg[f(Q\Psi\Psi\Psix) j Q 2Qffx2 ! [f1gg [  f(Q) j Q 2Qg [ f(Q 1 &quot;&quot;&quot;Q 2 ) j Q 1 ffQ 2 2Qg[f(Q 1 [[[Q 2 ) j Q 1 ffQ 2 2Qg:  where \Phi\Phi\Phi and \Psi\Psi\Psi are &quot;shift&quot; operators defined in Section 3.
The Self-Organizing Map (SOM) is one of the most popular neural  network methods. It is a powerful tool in visualization and analysis  of high-dimensional data in various engineering applications. The  SOM maps the data on a two-dimensional grid which may be used  as a base for various kinds of visual approaches for clustering, correlation  and novelty detection. In this chapter, we present novel methods  that enhance the SOM based visualization in correlation hunting  and novelty detection. These methods are applied to two industrial  case studies: analysis of hot rolling of steel and continuous pulp process.
Language integration is an important issue in the area of software maintenance and reengineering. We describe a novel solution in this area: automatically applied and composed split objects. Split objects provide a language integration that goes beyond simple wrappers by integrating object identity, state, methods, and class hierarchies of entities in two languages to one logical entity. The split object concept can be applied as an aspect-oriented solution, in which an aspect of a system is implemented in another language. After describing these concepts and two split object frameworks that we have implemented, we discuss how split objects can be applied for other maintenance and reengineering tasks than language integration. These application fields include software component testing, dynamic feature analysis, and variation and configuration management.
This paper describes an initial design of an associative processor for implementation using field-programmable logic devices (FPLDs). The processor is based loosely on earlier work on the STARAN computer, but updated to reflect modern design practices. We also draw on a large body of research at Kent State on the ASC and MASC models of associative processing, and take advantage of an existing compiler for the ASC model. The resulting design consists of an associative array of 8-bit RISC Processing Elements (PEs), operating in byte-serial fashion under the control of an Instruction Stream (IS) Control Unit that can execute assembly language code produced by a machine-specific back-end compiler.
In rotation invariant template matching one wants to find  nding from the given input image the locations that are similar to the given pattern template, so that the pattern may have any orientation. Template matching is an old but important problem in computer vision and pattern recognition, and thus there have also been many attempts to solve it. Most of the most successful solutions so far come from the signal processing community, based on fast computation of cross correlation or correlation coefficient. The existing combinatorial approaches have ignored the template rotations. This thesis fills in this gap by presenting the  rst rotation invariant combinatorial template matching algorithms. The thesis
Universities and other institutions related to education are investing time and resources in E-learning initiatives. This leads to an increasing number of course offers in E-learning format. There are environments, called Learning Management Systems (LMS), designed to help teachers in the management of their courses. Basically a LMS provides functionalities to manage student records, to facilitate communication between students and between students and teacher, to control accesses and produce statistics, schedules, evaluation and an open platform to help teachers make lecture content available online. However they do not dictate what kind of technology or format should be used to prepare those contents. Although this issue can be seen as an advantage in certain contexts it leads to a format anarchy and makes support for content production impossible. Here is where ADRIAN comes into the scene providing support for content production.
We present a modularized storage and indexing framework that cleanly separates the functional components of a P2P system, enabling us to tailor the P2P infrastructure to the specific needs of various Internet applications.
Defuzzification is one of the fundamental steps in the development of fuzzy knowledge based systems. Given a fuzzy set  over the reference set X, defuzzification applied to  returns an element of X. While a large number of methods exists for the case of X being a numerical scale, only few methods are applicable when X corresponds to a categorical scale.
My PhD thesis [7] claims that the principles behind object-oriented software evolution are independent of a particular domain or phase in the software lifecycle. To validate this claim, a formalism based on graphs and graph rewriting was developed and applied to a particular aspect of software evolution, namely the problem of software upgrading and software merging. When the same piece of software is modified in parallel by different software developers, unexpected inconsistencies can arise. Formal support can be provided to detect and resolve these inconsistencies in a general way.
The Etablissement franais des Greffes (EfG) is a public health agency in charge of organs,  tissues and cells transplantation in France. Among EfG&apos;s missions is the evaluation of the organ  retrieval and transplantation activities, which relies on a national information system (IS). In order to  facilitate data recording, to improve information quality and homogeneity, to allow data interchange  and semantic interoperability with hospital information systems and other registries, a specific work  as been initiated dealing with ontological foundations of medical terminology in the domain of organ  failure and transplantation. The aim of this paper is to describe how the syllepse appeared to us as a  key figure that accounts for medical knowledge acquisition.
This paper presents an algorithm which extends the relatively new  notion of speculative concurrency control by delaying the commitment  of transactions, thus allowing other conflicting transactions to  continue execution and commit rather than restart. This algorithm  propagates uncommitted data to other outstanding transactions thus  allowing more speculativeschedules to be considered. The algorithm  is shown always to find a serializable schedule, and to avoid cascading  aborts. Like speculative concurrency control, it considers strictly  more schedules than traditional concurrency control algorithms. Further  work is needed to determine which of these speculative methods  performs better on actual transaction loads.
This paper proposes a new nonparametric approach to the estimation of the mean Doppler velocity (first spectral moment) and the spectral width (square root of the second spectral centered moment) of a zero-mean stationary complex Gaussian process immersed in independent additive white Gaussian noise. By assuming that the power spectral density of the underlying process is band  li mi  ted, the exact maxity li923:y  od estimates of its spectral moments are derived. An estimate based on the sample covariances is also studied. Both methods are robust in the sense that they do not rely on any assumption concerning the power spectral density (besides being bandlimited). Under weak conditions, the estimates based on sample covariances are best asmptoti all  normal.
Many inference systems used for concept description logics are constraint systems that employ tableaux methods. These have the disadvantage that for reasoning with qualified number restrictions n new constant symbols are generated for each concept of the form ( n R C). In this paper we present an alternative method that avoids the generation of constants and uses a restricted form of symbolic arithmetic considerably different from the tableaux method. The method we use is introduced in Ohlbach, Schmidt and Hustadt 1995 for reasoning with graded modalities. We exploit the exact correspondence between the concept description language ALCN and the multi-modal version of the graded modal logic  K and show how the method can be applied to  ALCN as well. This paper is a condensed version of Ohlbach et al. 1995. We omit proofs and much of the technical details, but we include some examples. 
The understanding of Semantic Web documents is built upon ontologies that define concepts and relationships of data. Hence, the correctness of ontologies is vital. Ontology reasoners such as RACER and FaCT have been developed to reason ontologies with a high degree of automation. However, complex ontology-related properties may not be expressible within the current web ontology languages, consequently they may not be checkable by RACER and FaCT. We propose to use the software engineering techniques and tools, i.e., Z/EVES and Alloy Analyzer, to complement the ontology tools for checking Semantic Web documents. In this approach, Z/EVES is first applied to remove trivial syntax and type errors of the ontologies. Next, RACER is used to identify any ontological inconsistencies, whose origins can be traced by Alloy Analyzer. Finally Z/EVES is used again to express complex ontology-related properties and reveal errors beyond the modeling capabilities of the current web ontology languages. We have successfully applied this approach to checking a set of military plan ontologies.
This paper investigates three different rejection strategies for offline handwritten sentence recognition. The rejection strategies are implemented as a postprocessing step of a Hidden Markov Model based text recognition system and are based on confidence measures derived from a list of candidate sentences produced by the recognizer. The better performing confidence measures make use of the fact that the recognizer integrates a word bigram language model. Experimental results on extracted sentences from the IAM database validate the effectiveness of the proposed rejection strategies.
The Machine Independent Simulation System for PVM 3 (MISS-PVM) makes it possible to develop software for parallel computers which are not available in reality. MISS-PVM can also be used as a tool to produce timing measurements of existing programs which are independent of actual load characteristics. MISSPVM also makes the debugging of parallel programs easier. To exploit these features, it is not necessary to rewrite existing code or create additional code (for either C or Fortran). Programs utilizing PVM can be used without modification. In this report, several new features added to MISS-PVM are described. The use of virtual messages simulates the effect of long messages while sending only small ones, thus increasing the simulation speed. Also, network contention is now included, simulating the concurrent access to shared networks. The new conservative parallel discrete event simulation protocol will also ensure the correct ordering of messages. Finally, the add-on package Workstation ...
Research results [ROSE91] demonstrate that a log-structured file system (LFS) offers the potential for dramatically improved write performance, faster recovery time, and faster file creation and deletion than traditional UNIX file systems. This paper presents a redesign and implementation of the Sprite [ROSE91] log-structured file system that is more robust and integrated into the vnode interface [KLEI86]. Measurements show its performance to be superior to the 4BSD Fast File System (FFS) in a variety of benchmarks and not significantly less than FFS in any test. Unfortunately, an enhanced version of FFS (with read and write clustering) [MCVO91] provides comparable and sometimes superior performance to our LFS. However, LFS can be extended to provide additional functionality such as embedded transactions and versioning, not easily implemented in traditional file systems.  1. Introduction  Early UNIX file systems used a small, fixed block size and made no attempt to optimize block place...
Finding optimal solutions for job shop scheduling problems requires high computational effort, especially under consideration of uncertainty and frequent replanning. In contrast to computational solutions, domain experts are often able to derive good local dispatching heuristics by looking at typical problem instances. They can be efficiently applied by looking at few relevant features. However, these rules are usually not optimal, especially in complex decision situations. Here we describe an approach that tries to combine both worlds. A neural network based agent autonomously optimizes its local dispatching policy with respect to a global optimization goal, defined for the overall plant. On two benchmark scheduling problems, we show both learning and generalization abilities of the proposed approach.  to appear in: Proceedings of the Int. Joint Conference on Artificial Intelligence &apos;99  IJCAI&apos;99, Stockholm, Sweden, August 1999  1 Introduction  Production scheduling is the allocation ...
Finding condensed representations for pattern collections has been an active research topic in data mining recently and several representations have been proposed. In this paper we introduce chain partitions of partially ordered pattern collections as high-level condensed representations that can be applied to a wide variety of pattern collections including most known condensed representations and databases. We analyze the goodness of the approach, study the computational challenges and algorithms for finding the optimal chain partitions, and show empirically that this approach can simplify the pattern collections significantly.
In this work, an application of &quot;browsing by coherence&quot; is presented. An interferometric pair of single-look complex (SLC) ERS-1/2 Tandem images is multi-look processed and the resulting real-valued images are further compressed through a hierarchical method to yield quick-looks that may be browsed at different spatial resolutions, i.e. number of looks. Pairs of quick-look icons are utilized to estimate coherence by means of a new method based on measurements of temporal correlation of speckle. The outcome maps are compared with spatially degraded versions of the coherence map calculated from the original SLC data. Experiments are aimed at showing that the method yields steady results varying with the number of looks of icons that are browsed.
The objective of this study is to evaluate the contribution to semantic integration of the semantic relations extracted from concept names, representing augmented knowledge. Three augmentation methods -- based on linguistic phenomena -- are investigated (reification, nominal modification, and prepositional attachment). The number of concepts aligned in two ontologies of anatomy before and after augmentation serves as the evaluation criterion. Among the 2353 concepts exhibiting lexical resemblance across systems, the number of concepts supported by structural evidence (i.e., shared hierarchical relations) increased from 71% before augmentation to 87% after augmentation. The relative contribution of each augmentation method to the alignment is presented. The limitations of this study and the generalization of augmentation methods are discussed.
The complexity of electronic devices increases continually; that is why diagnosing actual  circuits is more and more difficult. Different diagnosing methods exist. The aim of this paper  is to present a new model-based diagnosis method that uses the Bayesian network to  describe the system and to compute the probability of the most likely diagnoses. This method  determines one of the most probable sets of components which explain the abnormal  behavior of a faulty system. When the list of diagnoses is too long, it is difficult to determine  the set of components that must be repaired. In those cases additional observations must be  used in order to refine diagnosis. Our method has the advantage of calculating diagnoses  fast and thus allows a rapid decision about the necessity of supplementary observations. We  will present its steps and some diagnosis examples and the different tests performed.
Qualitative computer simulations have great potential for teaching people to understand  and interact with their physical environment. Prerequisite for using that potential, is  that these simulations can be explained to humans in ways that they comprehend.
We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find a good storage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage mapping that is valid for all legal schedules. We consider storage mappings that collapse one dimension of a multi-dimensional array, and programs that are in a single assignment form with a one-dimensional schedule. Our technique combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set of linear inequalities, and apply numerical programming techniques to efficiently solve for the shortest occupancy vector. We consider our method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.
The use of high resolution commercial satellite and airborne images for the survey of landmine suspected areas has been suggested recently in the context of Mine Action to (i) map the hazardous area (suspected minefield), and (ii) possibly reduce its extend. Minefields may be identified using methods that directly detect and confirm the location of landmines. Next to this approach, indirect indicators, closely related to the occurrence of the minefields themselves can be used. Such indicators correspond either to direct military activities, e.g. trenches, embankments, protection walls, bunkers, foxholes, fences, etc, or changes in the landscape, e.g. abandoned arable land, unused roads, foot paths and tracks through fields, etc. The present work investigates modelbased approaches for the (semi-) automatic extraction of some of the indirect minefield indicators from high resolution airborne images.
Abstract  This paper introduces conceptual problems that will This paper introduces conceptual problems that will arise in the next 10-20 years as electronic circuits arise in the next 10-20 years as electronic circuits reach nanometer scale, i.e. the size of molecules. reach nanometer scale, i.e. the size of molecules. Such circuits will be impossible to make perfectly, Such circuits will be impossible to make perfectly, due to the inevitable fabrication faults in chips with due to the inevitable fabrication faults in chips with an Avogadro number of components. Hence they will an Avogadro number of components. Hence they will need to be constructed so that they are robust to need to be constructed so that they are robust to faults. They will also need to be (as far as possible) faults. They will also need to be (as far as possible) reversible circuits, to avoid the heat dissipation reversible circuits, to avoid the heat dissipation problem if bits of information are routinely wiped out problem if bits of information are routinely wiped out during the computational process. They will also during the computational process. They will also have to be local if the switching times reach femto- have to be local if the switching times reach femtoseconds, which is possible now with quantum optics. seconds, which is possible now with quantum optics. This paper discusses some of the conceptual issues This paper discusses some of the conceptual issues involved in trying to build circuits that satisfy all involved in trying to build circuits that satisfy all these criteria, i.e. that they are these criteria, i.e. that they are robust robust, , reversible reversible and and local local. We propose an evolutionary engineering based . We propose an evolutionary engineering based mode...
Although research in virtual reality has been done for over 10 years, only a few years ago the non-academic world...
Because there is currently no formal way to specify user interfaces, nor a clean way to decouple a user interface from its application code, we propose in this position paper the use of Declarative Meta Programming (DMP) to solve these problems. DMP uses facts and rules to write down a user interface in a declarative way, and will provide a more formal way to specify user interfaces. Furthermore DMP cleanly separates user interface from application code. The Declarative Meta Programming language SOUL that we intend to use, combines the declarative paradigm (for the user interface specification) and the objectoriented paradigm (for the application code). This position paper thus describes a case in multiparadigm programming.
In the coming decades, world agricultural systems will face serious transitions. Population growth, income and lifestyle changes will lead to considerable increases in food demand. Moreover, a rising demand for renewable energy and biodiversity protection may restrict the area available for food production. On the other hand, global climate change will affect production conditions, for better or worse depending on regional conditions. In order to simulate these combined effects consistently and in a spatially explicit way, we have linked the Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ) with a &quot;Management model of Agricultural Production and its Impact on the Environment&quot; (MAgPIE). LPJ represents the global biosphere with a spatial resolution of 0.5 degree. MAgPIE covers the most important agricultural crop and livestock production types. A prototype has been developed for one sample region. In the next stage this will be expanded to several economically relevant regions on a global scale, including international trade. The two models are coupled through a layer of productivity zones. In the paper we present the modelling approach, develop first joint scenarios and discuss selected results from the coupled modelling system.
In wireless access networks bandwidth is a limited resource. Therefore, Mobile IP users will demand to receive QoS support. However, in a highly dynamic environment --- as mobile environments --- QoS provisioning is a difficult task. In this paper we propose a signaling protocol for mobile users to contact a bandwidth broker for negotiation QoS for a flow. This protocol can also be used for negotiating QoS for traffic aggregates between two bandwidth brokers. Two scenarios are designed to evaluate the protocol.
Distributed Knowledge Management is an approach to  knowledge management based on the principle that the multiplicity  (and heterogeneity) of perspectives within complex  organizations is not be viewed as an obstacle to knowledge  exploitation, but rather as an opportunity that can foster innovation  and creativity. Despite a wide agreement on this  principle, most current KM systems are based on the idea  that all perspectival aspects of knowledge should be eliminated  in favor of an objective and general representation of  knowledge. In this paper we propose a peer-to-peer architecture  (called KEx), which embodies the principle above in  a quite straightforward way: (i) each peer (called a K-peer)  provides all the services needed to create and organize &quot;local&quot; knowledge from an individual&apos;s or a group&apos;s perspective,  and (ii) social structures and protocols of meaning negotiation  are introduced to achieve semantic coordination among autonomous  peers (e.g., when searching documents from other  K-peers). A first version of the system, called KEx, is implemented  as a knowledge exchange level on top of JXTA.
The presented work is strongly motivated by the need of modelling functional style  (FS) as well as categorising unrestricted texts in terms of FS in order to attain a  satisfying outcome in style processing. Towards this end, it is given a three-level  description of FS that comprises: (a) the basic categories of FS, (b) the main features  that characterise each one of the above categories, and (c) the linguistic identifiers  that act as style markers in texts for the identification of the above features. Special  emphasis is put on the problems that faces a computational implementation of the  aforementioned findings as well as the selection of the most appropriate stylometrics  (i.e. stylistic scores) to achieve better results on text categorisation. This approach is  language independent, statistically and empirically-driven, and can be used in  various applications including text categorisation, natural language generation, style  verification in real-world texts, and recognition of style shift between adjacent  portions of text.
The behaviour of asynchronous circuits is often described by Signal Transition Graphs (STGs), which are Petri nets whose transitions are interpreted as rising and falling edges of signals. One of the crucial problems in the synthesis of such circuits is deriving equations for logic gates implementing each output signal of the circuit. This is usually done using reachability graphs.
When interpolating incomplete data, one can choose a parametric model, or opt for a more general approach and use a non-parametric model which allows a very large class of interpolants. A popular non-parametric model for interpolating various types of data is based on regularization, which looks for an interpolant that is both close to the data and also &quot;smooth&quot; in some sense. Formally, this interpolant is obtained by minimizing an error functional which is the weighted sum of a &quot;fidelity term&quot; and a &quot;smoothness term&quot;.
The ability to use linguistic concepts to describe perceptions of measured data is an emerging feature for artificial sensing systems. In this work, we address the problem of symbolically representing odour categories using the data from an electronic nose. One objective is to facilitate human and computer interaction and therefore, the names given to the odours are coorelated with a human user. Perceptual differences that arise between the human perception of odours and the electronic one, represent a challenge to the system. Therefore, to cope with these differences, the system maintains the freedom to evaluate how appropriately the linguistic concepts represent the sensory perceptions. Finally, some experimental results are shown where the odour categories are formed and new odours are described using these categories.
Many constraint satisfaction problems (such as  scheduling, assignment, and configuration) can be  modelled as constraint programs based on matrices  of decision variables. In such matrix models, symmetry  is an important feature. We study and generalise  symmetry-breaking techniques, such as lexicographic  ordering, and propose a labelling technique  achieving the same effect.  1 
The decision problem is studied for the nonmodal or multiplicative-additive fragment of first order linear logic. This fragment is shown to be nexptime-  hard. The hardness proof combines Shapiro&apos;s logic programming simulation of nondeterministic Turing machines with the standard proof of the pspace-  hardness of quantified boolean formula validity, utilizing some of the surprisingly powerful and expressive machinery of linear logic. 1 Introduction  Linear logic, introduced by Girard, is a resource-sensitive refinement of classical logic [10, 29]. Linear logic gains its expressive power by restricting the &quot;structural&quot; proof rules of contraction (copying) and weakening (erasing). The contraction rule makes it possible to reuse any stated assumption as often as desired. The weakening rule makes it possible to use dummy assumptions,  i.e., it allows a deduction to be carried out without using all of the hypotheses. Because contraction and weakening together make it possible to use an assu...
Due to cost, time, and flexibility constraints, simulators are often used to explore the design space when developing new processor architectures, as well as when evaluating the performance of new processor enhancements. However, despite this dependence on simulators, statistically rigorous simulation methodologies are not typically used in computer architecture research. A formal methodology can provide a sound basis for drawing conclusions gathered from simulation results by adding statistical rigor, and consequently, can increase confidence in the simulation results. This paper demonstrates the application of a rigorous statistical technique to the setup and analysis phases of the simulation process. Specifically, we apply a Plackett and Burman design to: 1) identify key processor parameters, 2) classify benchmarks based on how they affect the processor, and 3) analyze the effect of processor performance enhancements. Our technique expands on previous work by applying a statistical method to improve the simulation methodology instead of applying a statistical model to estimate the performance of the processor.
Fixed priority scheduling (FPS) has been widely studied and used in a number of applications, mostly due to its flexibility, simple run-time mechanism and small overhead. However, preemption related overhead in FPS may cause undesired high processor utilization, high energy consumption, or, in some cases, even infeasibility.
This position paper proposes argumentation structures for automated reasoning in context-aware systems, for design of context-aware behaviour, for generating explanations of system actions to users, and for more expressive rules for userprogramming of context-aware systems.
Designing an agent to participate in natural conversation requires more than just adapting a standard agent model to perceive and produce language. In particular, the model must be augmented with social attitudes (including mutual belief, shared plans, and obligations) and a notion of discourse context. The dialogue manager of the TRAINS-93 NL conversation system embodies such an augmented theory of agency. This paper focuses on the representation of mental state and discourse context and the deliberation strategies used in the agent model of the dialogue manager. 1 INTRODUCTION  A dialogue manager is that part of a dialogue system that connects the I/O devices and translators (whether they be spoken or typed language, a command language, menu selection, graphical presentation, etc.) to the parts that do the domain task reasoning and performance. In a simple language front-end system (e.g., for querying a database), dialogue management can be little more than a transducer from the I/O ...
Active Appearance Models (AAMs) are generative parametric models that have been successfully used in the past to track faces in video. A variety of video applications are possible, including dynamic pose estimation for realtime user interfaces, lip-reading, and expression recognition. To construct an AAM, a number of training images of faces with a mesh of canonical feature points (usually handmarked) are needed. All feature points have to be visible in all training images. However, in many scenarios parts of the face may be occluded. Perhaps the most common cause of occlusion is 3D pose variation, which can cause self-occlusion of the face. Furthermore, tracking using standard AAM fitting algorithms often fails in the presence of even small occlusions. In this paper we propose algorithms to construct AAMs from occluded training images and to efficiently track faces in videos containing occlusion. We evaluate our algorithms both quantitatively and qualitatively and show successful real-time face tracking on a number of image sequences containing varying degrees of occlusions. 1 
In our previous work we defined an approach based on symbolic transition system and data type to specify and verify mixed systems. We applied this to components and architectures with full data types, and synchronous communications. However to fit distributed systems, it seems more realistic to consider asynchronous communications. It provides a more primitive communication protocol and maximize the concurrency. To take into account asynchronous communications we distinguish message receipt from message execution and we add mailboxes in our symbolic systems. When we tried to experiment proofs in such a system a difficulty was the presence of buffers and the fact that the receipt instant is distinct from the execution instant. This complicates the specifications and also the proofs. The main problem is that the logic instant to receive is not simply linked with the execution instant. We propose to use an algorithm which decides if the system has bounded mailboxes and computes the reachable mailbox contents of the system. This algorithm gives constraints which are used to specialised the dynamic behaviour of the components according to the current system configuration. Then we are able to generate a PVS specification coping with dynamic behaviour and data type which is simpler since it removes the need for some mailboxes. The component model, the algorithms and the proofs are illustrated on a simple flight system reservation.
The paper addresses the joint estimation of backscatter and extinction coefficients from range/time noisy data under a nonlinear stochastic filtering setup. This problem is representativeofmany remote sensing applications suchasweather radar and elastic-backscatter lidar. A Bayesian perspective is adopted. Thus, in addition to the observation mechanism, relating in a probabilistic sense the observed data with the parameters to be estimated, a prior probability density function has to be specified. We adopt as prior a causal first order auto-regressive (AR) Gauss-Markov random field (GMRF). By using a reduced order state-space representation of the prior, we derivea  nonlinear stochastic filter that recursively computes the backscatter and extinction coefficients at each site. A set of experiments based on simulated data illustrates the potential of the proposed approach.
The effectiveness of existing XML query languages has been studied by many who  focused on the comparison of linguistic features, implicitly reflecting the fact that most XML tools  exist only on paper. In this paper, with a focus on efficiency and concreteness, we propose a  pragmatic first step toward the systematic benchmarking of XML query processing platforms. We  begin by identifying the necessary functionalities an XML data management system should  support. We review existing approaches for managing XML data and the query processing  capabilities of these approaches. We then compare three XML query benchmarks XMach-1,  XMark and XOO7 and discuss the applicability, strengths and limitations of these benchmarks.
This paper reports the results of two studies carried out on memory problems at work. The first study used a diary method to collect a corpus of memory problems. A taxonomy of memory problems was developed and implications for technological support for memory at work are discussed. The second study used a &quot;Memory Lapse Questionnaire&quot; to assess the relative frequencies and severities of various memory problems at work. The results of this questionnaire are reported and implications for the AIR project and its current programme of research are discussed. 
ca reahemic implementsan  controls cellula functionsan the geneticregula9MG agula9MG we must developa new set of theories,aries,TE8G aT methodologies tha combine the twofundaM8TNLOE different waT ofchaODTNDEE8Ta such systems. Wea9MED8T modelingbiologica systemsa stochaMIOG networked hybrid systems tha consist of discreteas continuous components with complexintera9OO9&apos;T  9--11  Even inmaE continuousbiologica systemschamsTE&apos;L9GT by differentia equar-TM&apos; a hybrid model offersa computaM &apos;L88TN traaM&apos;L aaaM&apos; to modeling, 2 COMPUTING IN SCIENCE &amp; ENGINEERING  MODELING AND ANALYZING  BIOMOLECULAR NETWORKS  The authors argue for the need to model and analyze biological networks at molecular and cellular levels. They propose a computational toolbox for biologists. Central to their approach is the paradigm of hybrid models in which discrete events are combin
This paper proposes two new public-key cryptosystems semantically secure against adaptive chosen-ciphertext attacks. Inspired from a recently discovered trapdoor technique based on composite-degree residues, our converted encryption schemes are proven, in the random oracle model, secure against active adversaries (IND-CCA2) under the assumptions that the Decision Composite Residuosity and Decision Partial Discrete Logarithms problems are intractable. We make use of specific techniques that differ from Bellare-Rogaway or Fujisaki-Okamoto conversion methods. Our second scheme is specifically designed to be efficient for decryption and could provide an elegant alternative to OAEP.
Microarrays allow the monitoring of thousands of genes simultaneously. Before a measure of gene activity...
Topological considerations are of  paramount importance in the design of a  P2P lookup service. We present TOPLUS,  a lookup service for structured peer-to-peer  networks that is based on the hierarchical  grouping of peers according to network IP  prefixes. TOPLUS is fully distributed and  symmetric, in the sense that all nodes have  the same role. Packets are routed to their destination  along a path that mimics the routerlevel  shortest-path, thereby providing a small  &quot;stretch&quot;. Experimental evaluation confirms  that a lookup in TOPLUS takes time comparable  to that of IP routing.
This paper represents my own work in accordance with University regulations. CONTENTS iii Contents  1 
Our work is centered on the use of implicit surfaces in interactive applications (at least 10 frames per sec) running on high-end consumer architecture (modeling, simulation, deformable body animation, games). We focus on the Marching Cubes algorithm that we tried to implement in an optimized way. We restrict our work to blended iso-surfaces generated by skeletons, since this kind of implicit surfaces is the most handy to use for animations. Our implementation optimizations deal with the following features: simplifying the field function, accelerating its evaluation for each point (voxel-based technique), generating automatically the triangles for any case of the Marching Cubes. Another point we have considered concerns tesselation ambiguities often resulting in holes appearing in the surface. We have coded a library which is very easy to use and can be downloaded freely. All these optimizations allow us to sample implicit surfaces composed of 200 points in 45 ms on a 450 MHz Pentium II Xeon.
A fundamental tension of between the effciency and stability of social networks is now a well recognized fact in the literature. In general, networks are not simultaneously effcient and stable. Here we consider strongly pairwise stable networks and identify that the presence of middlemen is of particular importance for such a network to be effcient as well. We find that for the component wise egalitarian rule there is no conflict between the effcient and stable networks when these middlemen have no incentive to break up the network.
: The past few years have seen a rise in the popularity of the use of mentalistic attitudes such as beliefs, desires and intentions to describe intelligent agents. Many of the models which formalise such attitudes do not admit degrees of belief, desire and intention. We see this as an understandable simplification, but as a simplification which means that the resulting systems cannot take account of much of the useful information which helps to guide human reasoning about the world. This paper starts to develop a more sophisticated system based upon an existing formal model of these mental attributes.  1 Introduction  In the past few years there has been a lot of attention given to building formal models of autonomous software agents; pieces of software which operate to some extent independently of human intervention and which therefore may be considered to have their own goals and the ability to determine how to achieve those goals. Many of these formal models are based on the use of ...
Shadow Hybrid Monte Carlo (SHMC) is a new method for sampling the phase space of large molecules, particularly biological molecules. It improves sampling of Hybrid Monte Carlo (HMC) by allowing larger time steps and system sizes in the molecular dynamics (MD) step. The acceptance rate of HMC decreases exponentially with increasing system size N or time step &amp;delta;t. This is due to discretization errors introduced by the numerical integrator. SHMC achieves an asymptotic O(N^1/4) speedup over HMC by sampling from all of phase space using high order approximations to a shadow or modified Hamiltonian exactly integrated by a symplectic MD integrator. SHMC satisfies microscopic reversibility and is a rigorous sampling method. SHMC requires extra storage, modest computational overhead, and a reweighting step to obtain averages from the canonical ensemble. This is validated by numerical experiments that compute observables for different molecules, ranging from a small n-alkane butane with 4 united atoms to a larger solvated protein with 14,281 atoms. In these experiments, SHMC achieves an order magnitude speedup in sampling efficiency for medium sized proteins. Sampling efficiency is measured by monitoring the rate at which different conformations of the molecules&apos; dihedral angles are visited, and by computing ergodic measures of some observables.
Contents  1.1 INTRODUCTION ......................................................................................................................... 2 1.2 SCENARIO ONE ......................................................................................................................... 3 1.3 SCENARIO TWO......................................................................................................................... 3 1.4 SCENARIO THREE...................................................................................................................... 4 1.5 REFERENCES ............................................................................................................................. 5 1.1 Introduction  At present various electronic market places, auctions and negotiation systems exist, in the near future full electronic supply chains will be possible and indeed desirable to improve efficiency [1]. This situation, however, presents a problem. While humans are good at
The past few years have seen a steadily increasing understanding of the need for  customisation and tailorability in a range of computational systems. This has  resulted both from a greater understanding of the role of individual and organisational  work practices in human-computer interaction, and from the application of  systems to more complex domains of activity. The increasing requirement that  systems provide flexible and customisable behaviour, and that this customisation  reach deeper into the system, forces a reconsideration of the techniques by which  customisable systems are constructed.
In public auction, all bid values are published, but each bidder participates in auction protocol in anonymous way. Recently, Omote and Miyaji [OM01] proposed a new model of public auction in which any bidder can participate in plural rounds of auction with one-time registration. They have introduced two managers, registration manager (RM) and auction manager (AM), and have used efficient tools such as bulletin board and signature of knowledge [CS97]. In this scheme, even if a bidder is identified as a winner in a round, he can participate in next rounds of auction maintaining anonymity for RM, AM, and any bidder. But a problem of this protocol is that the identity of winner cannot be published. In the winner announcement stage, RM informs the vendor of winner&apos;s identity secretly. Therefore RM&apos;s nal role cannot be verified, and AM and any participating bidder can not be sure of the validity of auction. In this paper, we propose a new public auction scheme which can solve this problem. In the proposed scheme, both RM and AM execute randomization operation in round setup process which makes the publication of winner&apos;s identity be possible while keeping anonymity of winner in next rounds of auction. Moreover, AM provides ticket identifier based on Diffie-Hellman key agreement which is recognized only by the bidder. Our scheme provides real anonymity in plural rounds of auction with one-time registration in a verifiable way.
The aim of this paper is to show the impact of currently available  ubiquitous computing technology on business processes. It shortly describes  and investigates technologies, its drivers, and gives some examples of their  application. This includes technologies like automatic identification,  localization, and sensor technology that seem to have a strong impact on  business processes. Building on these key technologies in ubiquitous  computing, their impact on business processes is investigated. Firstly, business  problems are stated, where the utilization of ubiquitous computing technology  might provide a solution. Then, three application cases are presented to show  the needs that drive the adoption of these technologies and to show how  business processes are affected. A ubiquitous computing process model is  proposed that is derived from the collection of cases. This model helps to  identify application potentials for ubicomp computing technologies in a  business environment. The paper closes with some future perspectives on  ubiquitous computing applications.
Many supervised and unsupervised learning  algorithms are very sensitive to the choice of  an appropriate distance metric. While classification  tasks can make use of class label information  for metric learning, such information  is generally unavailable in conventional  clustering tasks. Some recent research sought  to address a variant of the conventional clustering  problem called semi-supervised clustering,  which performs clustering in the presence  of some background knowledge or supervisory  information expressed as pairwise similarity  or dissimilarity constraints. However,  existing metric learning methods for semisupervised  clustering mostly perform global  metric learning through a linear transformation.
Direct orientation of laser systems has been widely used for airborne laser sensors but not with the terrestrial laser systems. The usual way to operate a terrestrial laser is by scanning a scene while the sensor remains static. In order to cover the whole scene, different scans can be combined by matching several common points, finally the orientation of the scene is performed by identifying and providing coordinates for a minimum of 3 points. This procedure is very time consuming leading to a very low productivity. In order to increase productivity the Institut Cartografic de Catalunya (ICC) has integrated a terrestrial laser scanner in a mobile vehicle with the aim to operating the laser while the vehicle is moving.
  Sampling the configuration space of complex biological molecules is an important and formidable problem. One major difficulty is the high dimensionality of this space, roughly 3N , with the number of atoms N typically in the thousands. This thesis introduces shadow hybrid Monte Carlo (SHMC), a propagator through phase space that enhances the scaling of sampling with space dimensionality. SHMC is a biased variation on the hybrid Monte Carlo algorithm (HMC) that uses an approximation to the modified Hamiltonian to sample more efficiently through phase space. The overhead introduced is modest in terms of time, involving only dot products of the history of positions and momenta generated by the integrator. We present the derivation of SHMC, along with: proof that it preserves microscopic reversibility; analysis of the asymptotic speedup of SHMC over HMC, which is shown to be O(N    ) when using Verlet integrators; and results evaluating correctness and efficiency.
Manipulator control is one of the main research areas in robotics,  requiring in the first instance the manipulator model. Using the Denavit-Hartenberg  methodology this paper develops both forward and inverse kinematics models for a  5-dof Pioneer 2 robot arm. This paper also includes background information about  robot arms, especially the Pioneer 2 robot arm, and discusses some implementation  issues.
Our research presented in this paper concerns the problem of fusing the results returned by the underlying systems to a mediating retrieval system, also called meta-retrieval system, meta-search engine, or mediator. We propose a fusion technique which is based solely on the actual results returned by each system for each query. The final (fused) ordering of documents is derived by aggregating the orderings of each system in a democratic manner. In addition, the fused ordering is accompanied by a level of democracy (alternatively construed as the level of confidence).
Six rhodopsin mutants containing disulfide cross-links between different cytoplasmic regions  were prepared: disulfide bond 1, between Cys65 (interhelical loop I-II) and Cys316 (end of helix VII);  disulfide bond 2, between Cys246 (end of helix VI) and Cys312 (end of helix VII); disulfide bond 3,  between Cys139 (end of helix III) and Cys248 (end of helix VI); disulfide bond 4, between Cys139 (end  of helix III) and Cys250 (end of helix VI); disulfide bond 5, between Cys135 (end of helix III) and  Cys250 (end of helix VI); and disulfide bond 6, between Cys245 (end of helix VI) and Cys338 (Cterminus)  . The effects of local restrictions caused by the cross-links on transducin (GT ) activation and  phosphorylation by rhodopsin kinase (RK) following illumination were studied. Disulfide bond 1 showed  little effect on either GT activation or phosphorylation by RK, suggesting that the relative motion between  interhelical loop I-II and helix VII is not crucial for recognition by GT or by RK. In contrast, disulfide  bonds 2-5 abolished both GT activation and phosphorylation by RK. Disulfide bond 6 resulted in enhanced  GT activation but abolished phosphorylation by RK, suggesting the structure recognized by GT was stabilized  in this mutant by cross-linking of the C-terminus to the cytoplasmic end of helix VI. Thus, the consequences  of the disulfide cross-links depended on the location of the restriction. In particular, relative motions of  helix VI, with respect to both helices III and VII upon light activation, are required for recognition of  rhodopsin by both GT and RK. Further, the conformational changes in the cytoplasmic face that are  necessary for protein-protein interactions need not be cooperative, and may be segmental.
Neural networks are usually trained using local, gradient-based procedures. Such methods frequently  find suboptimal solutions being trapped in local minima. Optimization of neural structures and global  minimization methods applied to network cost functions have strong influence on all aspects of network  performance. Recently genetic algorithms are frequently combined with neural methods to select best architectures  and avoid drawbacks of local minimization methods. Many other global minimization methods  are suitable for that purpose, although they are used rather rarely in this context. This paper provides a  survey of such global methods, including some aspects of genetic algorithms.  
The current best-effort infrastructure in the Internet lacks key characteristics in terms of delay, jitter, and loss, which are required for multimedia applications (voice, video, and data). Recently, significant progress has been made toward specifying the service differentiation to be provided in the Internet for supporting multimedia applications. In this paper, we identify the main traffic types, discuss their characteristics and requirements, and give recommendations on the treatment of the different types in network queues. Simulation and measurement results are used to assess the benefits of service differentiation on the performance of applications.
In recent years, we have come to understand that the design of effective interactive systems is not simply about implementation models and techniques, but also about aspects of the system in use, many of which have come to us from psychology and social science. The issues of work practice, adaptation and evolution which surround interactive systems have become an extremely important area of research.
Data Mining is often concerned with large and complex datasets. However  the data space may be sparse and contain regions that are poorly covered. In such
The use of XML as the de facto data exchange standard has allowed integration of heterogeneous web based software systems regardless of implementation platforms and programming languages. On the other hand, the rich tree-structured data representation, and the expressive XML query languages (such as XPath) make formal specification and verification of software systems that manipulate XML data a challenge. In this paper, we present our initial efforts in automated verification of XML data manipulation operations using the SPIN model checker. We present algorithms for translating (bounded) XML data and XPath expressions to Promela, the input language of SPIN. The techniques presented in this paper constitute the basis of our Web Service Analysis Tool (WSAT) which verifies LTL properties of composite web services.
Scanning techniques combining laser line projection with motion are simple and efficient. But there are number of cases in which laser triangulation fails. Some have well known solutions. Other, like adverse illumination by intense white light or presence of textures make laser projection hard to distinguish, and have no specific solution. In this article, a method is presented to improve retrieving laser projection for those cases. It is build upon two main ideas. First, using auxiliary lines to create local high frequencies. Second, transform a high speed camera in an intensity modulation receiver. The principle is to send a periodic message in the lines intensity and try to track traces of a spatial-temporal deforming pattern in the video sequences produced by the camera. It permits two main improvements. First, when adverse illumination produce other lines, they can be discriminate by the fact they don&apos;t send the message. Second, when adverse illumination produce a highly luminous image or when a texture diffuse a part of the laser energy, it is sufficient to track the noise of the message. By choosing message, it is possible to create every type of noise in order to distinguish it between the rest of image noises.
We consider the problem of redistributing data on homogeneous and heterogeneous ring of processors. The problem arises in several applications, each time after that a load-balancing mechanism is invoked (but we do not discuss the load-balancing mechanism itself). We provide algorithms that aim at optimizing the data redistribution, both for unidirectional and bi-directional rings, and we give complete proofs of correctness. One major contribution of the paper is that we are able to prove the optimality of the proposed algorithms in all cases except that of a bi-directional heterogeneous ring, for which the problem remains open.
The locomotion device is an input interface, which can sense the walking pace and direction of the user, for  the virtual reality system. This paper presents a new type of locomotion device called Omni-direction Ballbearing  Disc Platform(OBDP), which allows the user to walk naturally inside the virtual environment. Instead  of using the 3D tracker, arrays of ball-bearing sensors on a disc are used to detect the pace and an orbiting  frame to identify the walking direction. No other sensor, except the head tracker to detect the user&apos;s head  rotation, is required on the user&apos;s body. In addition, the ball-bearing on the sensor slips the user&apos;s foot back to  the center position of the disc. A prototype of the overhead crane training simulator that fully explores the  advantage of the OBDP is also designed and introduced in this paper.
The process of understanding spoken language requires the efficient processing of ambiguities that arise by the nature of speech. This paper presents an approach that allows the efficient incremental integration of speech recognition and language understanding using Tomita&apos;s generalized LR-parsing algorithm. For this purpose the GLRlattice -parsing-algorithm [11] is revised so that an agenda mechanism can be used to control the flow of computation of the parsing process. Subsequently the HMMevaluations of the word models are combined with a stochastical language model to do a beam search similar to [2, 1, 12], where chartparsers are used to do the job. 1. INTRODUCTION  In [10] M. Tomita proposes a parsing algorithm (Generalized LR-Parsing, GLRP) and extends it in [11] to an algorithm that can parse whole word lattices. This algorithm often works more efficiently with grammars for natural languages than others (see [10, 7]). Nevertheless the lattice-GLRP is not very flexible and require...
In this paper, we describe an algorithm for generating three dimensional models of human faces from uncalibrated images. Input images are taken by a camera generally with a small rotation around a single axis which may cause degenerate solutions during auto-calibration. We describe a solution to this problem by a priori assumptions on the camera. To generate a specific person &apos;s head, a generic human head model is deformed according to the 3D coordinates of points obtained by reconstructing the scene using images calibrated with our algorithm. The deformation process is based on a physical based massless spring model and it requires local re-triangulation in the areas with high curvatures. This is achieved by locally applying Delaunay triangulation method. However, there may occur degeneracies in Delaunay triangulation such as encroaching of edges. We describe an algorithm for removing the degeneracies during triangulation by modifying the definition of the Delaunay cavity. This algorithm has also the effect of preserving the curvature in the face area. We have compared the models generated with our algorithm with the models obtained using cyberscanners. The RMS geometric error in these comparisons are less than 1.8x10E-2.
Guaranteeing that traditional concepts of stability like uniform global exponential or asymptotic stability ffUGES or UGASff are veriffed when using design tools based on new concepts of stability may be of signiff- cant importance. It is especially so when attempting to bridge the gap between theory and practice. This paper addresses the question of the applicabilityofcontraction theory to the design of UGES observers for ocean vehicles. A relation between the concept of exponential convergence of a contracting system and uniform global exponential stability ffUGESff is ffrst given. Then two contraction-based GES observers, respectively for unmanned underwater vehicles ffUUVff and a class of ships, are constructed, and simulation results are provided. 1 
The idea of Information Visualization is to get insights into great amounts of abstract data. Especially document sets found by searching the World Wide Web are a special challenge. The paper gives a short overview on the variety of possible visualizations for this application area. Crucial factors for the success of visualizations are discussed. A combined approach is presented to use alternative simple visualizations, grouped around the traditional result-list, for usage with a local meta web search engine.
We propose a color-based tracking framework that infers alternately an object&apos;s configuration and good color features via particle filtering. The tracker adaptively selects discriminative color features that well distinguish foregrounds from backgrounds. The effectiveness of a feature is weighted by the Kullback-Leibler observation model, which measures dissimilarities between the color histograms of foregrounds and backgrounds. Experimental results show that the probabilistic tracker with adaptive feature selection is resilient to lighting changes and background distractions. 1 
this paper we prove (Theorem 2.2) that for each `-group G, Spec(G) satisfies a topological condition that we call (Id!). Moreover, we show that each nondenumerable set admits a structure of a completely normal spectral space not satisfying property (Id!). In other words, we show that from each nondenumerable cardinal we can obtain an example of a completely normal spectral space that is not homeomorphic to Spec(G) for any `-group G. In this way we simplify a previous construction of Delzell and Madden [7]
this paper, introducing classical Control Theory used for dynamic systems&apos; control, we explain the approach of the Feedback Control recently applied in computing science. We illustrate some related works and a model of middleware for QoS-control, that represents a meaningful example in the direction of the design of an adaptation-based architecture
Virtual LANs (VLANs) allow to interconnect users over campus or wide area networks and gives the users the impression as they would be connected to the same local area network (LAN). The implementation of VLANs is based on ATM Forum&apos;s LAN Emulation and LAN/ATM switches providing interconnection of emulated LANs over ATM and the LAN ports to which the user&apos;s end systems are attached to. The paper discusses possible implementation architectures and describes advanced features such as ATM short-cuts, QoS, and redundancy concepts.  Keywords: virtual LANs, LAN Emulation, Switching, ATM  1. INTRODUCTION  Local Area Networks (LANs) such as Ethernet or Token Ring are widely used to interconnect computers in companies, universities or other institutions. LANs are the basis for different protocols such as TCP/IP, SNA, Netbios, Appletalk etc. LANs have initially been implemented as shared media networks, i.e. a relatively high number of end systems are connected to a common media and have to shar...
In this work we explore on-line training of neural networks for interpreting colonoscopy images through tracking the changing location of an approximate solution of a pattern-based, and, thus, dynamically changing, error function. We have developed a memory-based adaptation of the learning rate for the on-line Backpropagation (BP) and we investigate the use of this scheme in an on-line evolution process that applies an on-line BP-seeded Differential Evolution Strategy to (re-)adapt the neural network to modified environmental conditions. We compare this hybrid strategy to other standard training methods that have traditionally been used for training neural networks off-line. Preliminary results in interpreting colonoscopy images and frames of video sequences suggest that networks trained with this strategy detect malignant regions of interest with high accuracy. Extensive testing in interpreting more complex regions is necessary to fully investigate the properties, the effect of the heuristic parameters and the performance of the hybrid learning strategy in this context.
t + 2D) wavelet decomposition is becoming increasingly popular, as it provides coding performance competitive with state-of-the-art codecs while accommodating varying network bandwidth and different receiver capabilities (e.g., frame-rate, display size, CPU, memory size). However, these temporal multiresolution schemes may introduce a non-negligible delay preventing their use by applications which require low latency or lightweight memory usage. In this paper, we provide a flexible approach to reduce the delay in motion-compensated temporal filtering schemes and illustrate the trade-offs between compression performance and low coding delay in this framework.
this article is not concerned with tightly-coupled multiprocessors or machines employing a global address space. Another approach to coping with the data quantities endemic to IP is resolution reduction, for which one might consider pyramidical machines, for instance in [ 15 ] .
Particle filtering is a very popular technique for sequential state estimation problem. However its convergence greatly depends on the balance between the number of particles /hypotheses and the fitness of the dynamic model. In particular, in cases where the dynamics are complex or poorly modeled, thousands of particles are usually required for real applications. This paper presents a hybrid sampling solution that combines the sampling in the image feature space and in the state space via RANSAC and particle filtering, respectively. We show that the number of particles can be reduced to dozens for a full 3D tracking problem which contains considerable noise of different types. For non-semantic motions, a specific set of dynamics may not exist, but it is avoided in our algorithm. The theoretical convergence proof [1, 3] for particle filtering when integrating RANSAC is difficult, but we address this problem by analyzing the likelihood distribution of particles from a real tracking example. The sampling efficiency (on the more likely areas) is much higher by the impact of RANSAC. We also discuss the tracking quality measurement in the sense of entropy or statistical testing. The algorithm has been applied to the problem of 3D face pose tracking with changing moderate or intense expressions. We demonstrate the validity of our approach with several video sequences acquired in an unstructured environment.
We address the calculation of dc operating points of nonlinear circuits by using parameter embedding methods, and we show that the usefulness of these methods depends on the type of a circuit&apos;s descriptive equations. We discuss various approaches to embedding a parameter into nonlinear equations that describe bipolar and MOS transistor circuits. Embedding algorithms were implemented in an industrial circuit simulator. We demonstrated that homotopy methods can be used as an alternative to the NewtonRaphson -type solvers and that they can be successfully applied to solving nonlinear circuit equations as well as to calculating dc operating points and transfer curves of nonlinear transistor circuits.  I. Introduction  In this paper  1  we address the calculation of dc operating points of nonlinear circuits by means of parameter embedding methods (also known as continuation and homotopy methods). One of the first implementations of continuation methods in an industrial circuit simulator was...
Protecting personal privacy is going to be a prime concern for the deployment  of ubiquitous computing systems in the real world. With daunting Orwellian  visions looming, it is easy to conclude that tamper-proof technical protection  mechanisms such as strong anonymization and encryption are the only  solutions to such privacy threats. However, we argue that such perfect protection  for personal information will hardly be achievable, and propose instead to build  systems that help others respect our personal privacy, enable us to be aware of  our own privacy, and to rely on social and legal norms to protect us from the  few wrongdoers. We introduce a privacy awareness system targeted at ubiquitous  computing environments that allows data collectors to both announce and  implement data usage policies, as well as providing data subjects with technical  means to keep track of their personal information as it is stored, used, and  possibly removed from the system. Even though such a system cannot guarantee  our privacy, we believe that it can create a sense of accountability in a world of  invisible services that we will be comfortable living in and interacting with.
Introduction  It is widely recognized that internal tides have strong influence on the global thermohaline circulation, because it contribute significantly to deep ocean mixing, the essential process for the maintenance of the thermohaline circulation [Munk and Wunsch, 1998]. Internal tides generated by strong tide-topography interactions occasionally break causing intense turbulent mixing [Lien and Gregg, 2001]. Turbulent mixing may also be induced far from wave generation sites, because propagating internal tides can nonlinearly interact with the background internal waves and cascade part of their energy down to small scales where breaking can occur.  The East China Sea and adjacent seas are one of the most important generation regions of internal tides, and hence the associated turbulent mixing. Indeed, using a two-dimensional analytical model, Baines [1982] predicted that the continental shelf slope in the East China Sea is the second largest generator of the M 2 internal tide amon
We devise a scheme which can provide reliable transport services in sensor networks and give an algorithm which minimizes the energy use of our scheme. We use a distributed sink where information arrives at the sink via multiple proxy nodes, called &quot;prongs&quot; in this paper. The sender node uses Forward Error Correction (FEC) erasure coding to encode each packet into multiple fragments and transmits the fragments to each of the prongs over a path which is disjoint from the paths to the other prongs. The erasure coding allows the sink to reconstruct the original packet even if some of the fragments are lost.
Continued advances in mobile networks and positioning  technologies have created a strong market  push for location-based services (LBSs). Examples  include location-aware emergency services, location  based service advertisement, and location sensitive  billing. One of the big challenges in wide deployment  of LBS systems is the privacy-preserving management  of location-based data. Without safeguards,  extensive deployment of location based services endangers  location privacy of mobile users and exhibits  significant vulnerabilities for abuse.
this paper, we will challenge this view. Although we agree that in principle all facts and relations expressed in a diagram can be put into words and formal expressions, we will emphasize that diagrams and other images have a potential of being used in a way different from textual structures. This potential is not fully preserved in the translation of diagrams to text (cf. Myers and Konolige, 1995). The paper will elaborate on the issue of comparing images with textually expressed knowledge
. In a probabilistic cellular automaton in which all local transitions have positive probability, the problem of keeping a bit of information indefinitely is nontrivial, even in an infinite automaton. Still, there is a solution in 2 dimensions, and this solution can be used to construct a simple 3-dimensional discrete-time universal fault-tolerant cellular automaton. This technique does not help much to solve the following problems: remembering a bit of information in 1 dimension; computing in dimensions lower than 3; computing in any dimension with non-synchronized transitions. Our more complex technique organizes the cells in blocks that perform a reliable simulation of a second (generalized) cellular automaton. The cells of the latter automaton are also organized in blocks, simulating even more reliably a third automaton, etc. Since all this (a possibly infinite hierarchy) is organized in &quot;software&quot;, it must be under repair all the time from damage caused by errors. A large part of ...
We study four solutions to the reachability problem for 1-safe Petri nets, all of them based on the unfolding technique. We define the problem as follows: given a set of places of the net, determine if some reachable marking puts a token in all of them. Three of the solutions to the problem are taken from the literature [McM92, Mel98, Hel99], while the fourth one is first introduced here. The new solution shows that the problem can be solved in time O(n  k  ), where  n is the size of the prefix of the unfolding containing all reachable states, and k is the number of places which should hold a token. We compare all four solutions on a set of examples, and extract a recommendation on which algorithms should be used and which ones not. 
A polytypic function is a function that can be instantiated on many data types to obtain data type specific functionality. Examples of polytypic functions are the functions that can be derived in Haskell, such as show , read , and ` &apos;. More advanced examples are functions for digital searching, pattern matching, unification, rewriting, and structure editing. For each of these problems, we not only have to define polytypic functionality, but also a type-indexed data type: a data type that is constructed in a generic way from an argument data type. For example, in the case of digital searching we have to define a search tree type by induction on the structure of the type of search keys. This paper shows how to define type-indexed data types, discusses several examples of type-indexed data types, and shows how to specialize type-indexed data types. The approach has been implemented in Generic Haskell, a generic programming extension of the functional language Haskell.
INTRODUCTION  Query optimization is a computationally intensive process, especially for the complex queries that are typical in current data warehousing and mining applications. The inherent overheads of query optimization are compounded by the fact that a new query is typically optimized afresh, providing no opportunity to amortize these overheads over prior optimizations. While current commercial query optimizers do provide facilities for reusing execution plans generated for earlier queries (e.g. &quot;stored outlines&quot; in Oracle 9i), the query matching is extremely restrictive -- only if the incoming query has a close textual resemblance with one of the stored queries is the associated plan re-used to execute the new query.  Recently, in [1], we proposed a tool called PLASTIC (Plan Selection Through Incremental Clustering) to significantly increase the scope of plan reuse. The tool is based on the observation that even queries which differ in projection, selection and join predicates, as
xvii  Chapter  1 
The problematic in the frontier between agriculture and tropical forest in Province Carrasco-Bolivia, needs to be clarify in terms of their causes and the socio-economics events that are taking place in the region. The aim of this study is determine land cover change, Closed Forest fragmentation and establish a General Linear Model (Logistic model) to find the causes implicated on the process of Closed Forest conversion. An analysis of location and quantification of land cover change was made using as tools Remote Sensing and GIS. Five categories were discriminated (Closed Forest, Open Forest, Grassland, Crops, and Bare ground). Land cover change was determined in a multi-temporal analysis over 16 years (1986-1990, 1990-1996, 1996-2002). Forest fragmentation was measured based on 7 indexes and compared between the three periods of time. For logistic model construction, five independent variables were analyzed; distance to roads (m), distance to settlements (m), land tenure, soil texture, and topography. The multi-temporal analysis reveals that all the bigger changes have been from closed forest to other categories loosing 52.3% of Closed Forest since 1986. The first period characterized for the beginning of colonization shows and increment of Open Forest and Crops and the number of small patches increase at the same time that the biggest forest patch is reduced. A differentiation, in the second period of time, grassland become predominant, the number of small forest patches reduces and the distance between them increase. This could be related to expansion of ranching cattle. The third period characterized for strong investment in roads and expansion of agriculture, increment Open Forest and Crops at the same time that the forest fragments become smaller again. The predi...
We present an approach to the design of personalized recommender systems that integrates content-based methods, collaborative filtering techniques and case-based reasoning while adopting a user-centered perspective. These techniques are employed to support information search and choice processes. In this framework, we developed and tested a system prototype (NutKing) that helps the user to construct a travel plan by recommending attractive travel products or by proposing complete itineraries. In the information search phase, the system aids the user in specifying a successful query that winnows out unwanted products in electronic catalogues and reduces the information overload. This is accomplished through two kinds of query rewriting operators (relaxation and tightening) in a mixed initiative approach. In the choice phase, the search results are sorted according to a case-base similarity metric, which takes into account the similarity between the users&apos; travel preferences. The aim of this adaptive sorting is to highlight products that are potentially interesting, because they are similar to those selected by other users in an analogous context. The prototype has been empirically evaluated in a pilot study. The results of the pilot evaluation are discussed, with special reference to aspects concerning the usersystem  interaction aspects.
Although switched LANs are usually over-provisioned, their characteristics (short RTT, link speed mismatches) lead to increased burstiness, and thus to the occurrence of transient congestion. In order to fully utilize the potential of large switched LANs, a link layer back-pressure mechanism may be used to complement end-to-end flow control by handling the short term congestion. A simple such mechanism, as the one specified in IEEE 802.3x, is shown to improve network performance in some situations, but to lead to poor performance in others. We propose selective backpressure schemes based on destination MAC address and traffc class information, which overcome the limitations of the simpler scheme. These are shown to provide superior performance for a wide range of situations. The results obtained suggest the need to incorporate these enhancements in the 802.3x standard
Although a lot of theoretical work has been done on purely functional data structures, few of them have actually been implemented to general usefulness, let alone as part of a data structure library providing a uniform framework.

In 1998, Chris Okasaki started to change this by implementing Edison, a library of efficient data structures for Haskell.

Unfortunately, he abandoned his work after creating a framework and writing some data structure implementations for parts of it.

This document first gives an overview of the current state of Edison and describes what efficiency in a lazy language means and how to measure it in a way that trades off complexity and precision to produce meaningful results.

These techniques are then applied to give an analysis of the sequence implementations present in Edison. Okasaki only briefly mentions the main characteristics of the data structures he has implemented, but to allow the user to choose the most efficient one for a given task, a more complete analysis seems needed.

To round off Edison&apos;s sequence part, four new implementations based on previously known theoretical work are presented and analysed: two deques based on the pair-of-lists approach, and two data structures that allow constant time appending, while preserving constant time for tail and, for one of them, even init.

To achieve a certain confidence in the correctness of the new implementations, we also present QuickCheck properties that not only check the operations behave as desired by the abstraction, but also allow data structure specific invariants to be tested, while being polymorphic.
Computer animated agents and robots bring a social dimension to human  computer interaction and force us to think in new ways about how  computers could be used in daily life. Face to face communication is  a real-time process operating at a time scale of less than a second. In  this paper we present progress on a perceptual primitive to automatically  detect frontal faces in the video stream and code them with respect to 7  dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise.
Due to fast time-to-market and IP reuse requirements, an increasing amount of the functionality of embedded HW/SW systems is implemented in software. As a consequence, software programming languages like C play an important role in system specification, design, and validation. Besides many other advantages, the C language offers executable specifications, with clear semantics and high simulation speed. However, virtually any tool operating on C specifications has to convert C sources into some intermediate representation (IR), during which the executability is normally lost. In order to overcome this problem, this paper describes a novel IR format, called IR-C, for the use in C based design tools, which combines the simplicity of three address code with the executability of C. Besides the IR-C format and its generation from ANSI C, we also describe its applications in the areas of validation, retargetable compilation, and sourcelevel code optimization.
: The process industries exhibit an increasing need for effcient management  of all the factors that can reduce their operating costs, leading to the necessity for  a global multi-objective optimization methodology that will enable the generation  of optimum strategies, fulfilling the required restrictions. In this paper a genetic  algorithm is developed and applied for the optimal assignment of all the production  sections in a particular mill in the kraft pulp and paper industry, in order to optimize  energy the costs and production rate changes. This system is intended to implement all  programmed or forced maintenance shutdowns, as well as all the reductions imposed  in production rates.  Keywords: Production control, scheduling algorithms, genetic algorithms, global  optimization, pulp industry.  1. INTRODUCTION  Plants in the continuous production industries can be described as groups of departments, each responsible for some specific operations and separated by intermediate buffer...
We present a method to utilize the Shadow Volume Algorithm by Crow and Williams without using a  stencil buffer. We show that the shadow mask can be generated in the alpha channel or even in the screen  buffer, if a hardware-accelerated stencil buffer is not available. In comparison to the original stencil buffer  method, a small speed up can be achieved, if the shadow mask is computed in the alpha buffer. The method  using the screen buffer requires the scene to be rendered a second time after the shadow mask has been  computed. Both methods are less restrictive with respect to hardware requirements, since we use only  standard color blending and depth testing. In general, rasterization bandwidth is the main bottle neck when  generating the shadow mask at high screen resolutions. In order to overcome this bottle neck we propose  a way to compute the shadow mask at a resolution that is lower than the resolution of the screen buffer.
This paper proposes a new method for recovering nonrigid motion and structure of clouds under affine constraints using time-varying cloud images obtained from meteorological satellites. This problem is challenging not only due to the correspondence problem but also due to the lack of depth cues in the 2D cloud images (scaled orthographic projection). In this paper, affine motion is chosen as a suitable model for small local cloud motion. However, the cloud motion model can also be extended and not necessarily be affine. Analysis of intensity images is first used to find the candidates of correspondence for each point. Then affine motion model is utilized to find the structure (cloud-top height), motion parameters, and point correspondences. Extensive analysis has been done on defining appropriate constraints on depth that are necessary for achieving converge. Finally, experimental results on time-varying images of hurricane Luis, captured by GOES-9 satellite in geosynchronous orbit are...
The White-Point Preserving Least Squares (WPPLS) algorithm is a method for colour correction that constrains the white point to be exactly mapped into its correct XYZ equivalent. For printers, however, the mapping is from device coordinates to colorimetric densities: the device white is thus mapped into a zero vector and the WPPLS method cannot go forward. Here we use a polynomial regression model and specify that both white (the zero vector) and an average grey be exactly mapped. Moreover we extend the method to accurately but approximately map a subspace of the entire achromatic curve, thus reproducing the neutral tones with far greater accuracy.
this paper abstracts some fundamental aspects of open systems. Actors provide a natural generalization for objects -- encapsulating both data and procedures. However, actors differ from sequential objects in that they are also units of concurrency: each actor executes asynchronously and its operation may overlap with other actors. This unification of data abstraction and concurrency is in contrast to language models, such as Java, where an explicit and independent notion of thread is used to provide concurrency. By integrating objects and concurrency, actors free the programmer from having to write explicit synchronization code to prevent harmful concurrent access to data within an object
To facilitate effective search on the World Wide Web, several `meta search engines&apos; have been developed which do not search the Web themselves, but use available search engines to find the required information. By means of wrappers, meta search engines retrieve relevant information from the HTML pages returned by search engines. In this paper we present an algorithm to create such wrappers automatically, based on an adaptation of the string edit distance. Our algorithm performs well; it is quick, it can be used for several types of result pages and it requires a minimal amount of interaction with the user.
Topic detection and tracking approaches monitor  broadcast news in order to spot new, previously  unreported events and to track the development  of the previously spotted ones. The  dynamical nature of the events makes the use of  state-of-the-art methods difficult. We present a  new topic definition that has potential to model  evolving events. We also discuss incorporating  ontologies into the similarity measures of  the topics, and illustrate a dynamic hierarchy  that decreases the exhaustive computation performed  in the TDT process. This is mainly  work-in-progress.
this paper mightnothave been written. We express our thanks to them all, as well as to M.A. Taitslin for his valuable comments
The variable length code design of a complete quadtree-based video codec is addressed, in which we jointly optimize the entropy-coding for the parameters of motion-compensated prediction together with the residual coding. The quadtree coding scheme selected for this optimization allows easy access to the rate-distortion costs, thus making it possible to perform rate-distortion optimized bit allocation without exhaustive computation. Throughout the paper, we view the quadtree coder as a special case of tree-structured entropy-constrained vector quantization and derive a design algorithm which iteratively descents to a (locally) optimal quadtree video codec. Experimental results evaluate the performance of the proposed design algorithm.
This paper addresses the relative usefulness of Independent Component Analysis (ICA) for Face Recognition. Comparative assessments are made regarding (i) ICA sensitivity to the dimension of the space where it is carried out, and (ii) ICA discriminant performance alone or when combined with other discriminant criteria such as Bayesian framework or Fisher&apos;s Linear Discriminant (FLD). Sensitivity analysis suggests that for enhanced performance ICA should be carried out in a compressed and whitened Principal Component Analysis (PCA) space where the small trailing eigenvalues are discarded. The reason for this finding is that during whitening the eigenvalues of the covariance matrix appear in the denominator and that the small trailing eigenvalues mostly encode noise. As a consequence the whitening component, if used in an uncompressed image space, would fit for misleading variations and thus generalize poorly to new data. Discriminant analysis shows that the ICA criterion, when carried out in the properly compressed and whitened space, performs better than the eigenfaces and Fisherfaces methods for face recognition, but its performance deteriorates when augmented by additional criteria such as the Maximum A Posteriori (MAP) rule of the Bayes classifier or the FLD. The reason for the last finding is that the Mahalanobis distance embedded in the MAP classifier duplicates to some extent the whitening component, while using FLD is counter to the independence criterion intrinsic to ICA.
Nowadays, multimedia applications are more and more used, and take a larger place in the workloads of modern computing systems. It appears that the classical 1D spatial locality arrangement in the cache is not adapted to the management of this data type, because its structures exhibit an intrinsic 2D locality.
Clinical practice guidelines (CPG) are instrumental in standardizing the quality and delivery of  care across different practitioners, departments and even institutions. Health practitioners when  working with CPG like to reflect on the current best evidence either to validate or to supplement  their understanding of the CPG. In this paper we investigate the potential of supplementing  computerized CPG with current and relevant best-evidence sourced from reliable medical  literature repositories. We present a web-enabled Best-evidence Retrieval and Delivery (BiRD)  system that provides the functionality to autonomously retrieve pertinent medical literature with  respect to user-specified content from a GEM-encoded CPG. We have developed a multi-level  literature search strategy that both categorizes the search query towards a priori defined clinical  query intentions, and subsequently filters insignificant medical terms from the search query. The  resultant is a highly focused medical literature search query that is objectively derived from CPG  content. The technical architecture comprises existing medical language processing tools and  vocabularies, together with newly developed tools to automatically (a) generate optimum search  queries; (b) retrieve medical articles from MEDLINE; and (c) embed the retrieved medical articles  within XML-based CPG.
Middleware has emerged as an important architectural component in modern distributed systems. However, it is
We describe a novel approach to the determination of the focal length of a moving camera with rectangular pixels. The principal point of the camera is assumed to be known and fixed, whereas the focal length is allowed to vary across the sequence. Given three or more such images and a projective reconstruction, we describe a novel auto-calibration technique to obtain a metric reconstruction. Our technique uses semidefinite programming to recover these focal lengths (and hence the metric reconstruction). Our approach is efficient, well behaved with guaranteed convergence, and can be applied to long sequences of video. We present results for our approach using both simulated and real video sequences. 
This paper describes how a user modeling knowledge base for  personalized TV servers can be generated starting from an analysis of lifestyles  surveys. The aim of the research is the construction of well-designed  stereotypes for generating adaptive electronic program guides (EPGs) which  filter the information about TV events depending on the user&apos;s interests.
On January 8th 2003, Eric Filiol published on eprint.iacr.org a paper  [11] in which he claims that AES can be broken by a very simple and very fast  ciphertext-only attack. If such an attack existed, it would be the biggest discovery in  code-breaking since some 10 or more years.
Component-orientation is an emerging paradigm that  promises components that are usable as prefabricated black-boxes. But components
Privacy considerations often constrain data mining projects. This paper addresses the problem of association rule mining where transactions are distributed across sources. Each site holds some attributes of each transaction, and the sites wish to collaborate to identify globally valid association rules. However, the sites must not reveal individual transaction data. We present a two-party algorithm for efficiently discovering frequent itemsets with minimum support levels, without either site revealing individual transaction values.
In this paper, we present a novel approach based on genetic algorithms for performing camera calibration. Contrary to the classical nonlinear photogrammetric approach [1], the proposed technique can correctly find the near-optimal solution without the need of initial guesses (with only very loose parameter bounds) and with a minimum number of control points (7 points). Results from our extensive study using both synthetic and real image data as well as performance comparison with Tsai&apos;s procedure [2] demonstrate the excellent performance of the proposed technique in terms of convergence, accuracy, and robustness.
An inexpensive computer input device was developed that allows the user to operate within both 2D and 3D environments by simply moving and rotating their fist. Position and rotation around the X, Y and Z-axes are supported, allowing full six degree of freedom input. This is achieved by having the user wear a glove, to which is attached a square marker. Translation and rotation of the hand is tracked by a camera attached to the computer, using the ARToolKit software library. Extraction, calibration, normalisation and mapping of the data converts hand motion into meaningful operations within 2D and 3D environments. Four input scenarios are described, showing that the mapping of the position and rotation data to 2D or 3D operations depends heavily on the desired task.
Appropriate abstractions, mechanisms, and policies for resource allocation is quickly emerging as the fundamental problem facing emerging computation and communication environments such as PlanetLab and the Grid. This paper explores the utility of one simple abstraction for global resource allocation with a number of appealing properties: a centralized auction that collects user descriptions of resource configurations and the values placed on these configurations. The task of the clearinghouse is to determine a set of winning bids and to assign appropriate subsets of global resources to individual users. One challenge with this model is the computational complexity associated with determining winners. To make the problem tractable, we propose appropriate bidding languages that constrain the type of bids that users can make, while maintaining required expressiveness. Computing optimal solutions to such auctions for scales of current interest (e.g., 1000 nodes) is intractable on current hardware, even given aggressive optimizations. Thus, we introduce a number of heuristics that appear to perform well in practice. Another challenge with auctions is the lag in clearing the auction and the uncertainty in whether resources will actually be acquired. We introduce a formulation for &quot;Buy it Now&quot; pricing to address some of these limitations.
While being quite successful in providing keyword based access to web pages, commercial search portals, such as Google, Yahoo, AltaVista, and AOL, still lack the ability to answer questions expressed in a natural language. In this paper, we present a probabilistic approach to automated question answering on the Web. Our approach is based on pattern matching and answer triangulation. By taking advantage of the redundancy inherent in the Web, each answer found by the system is triangulated (confirmed or disconfirmed) against other possible answers. Our approach is entirely self-learning: it does not involve any linguistic resources, nor it does require any manual tuning. Thus, the propose approach can easily be replicated in other information systems with large redundancy.
Advertising plays a key role in service oriented recommendation  over a peer-to-peer network. The advertising problem can be considered  as the problem of finding a common language to denote the peers&apos;  capabilities and needs. Up to now the current approaches to the problem  of advertising revealed that the proposed solutions either affect the autonomy  assumption or do not scale up the size of the network. We explain  how an approach based on language games can be effective in dealing  with the typical issue of advertising: do not require ex-ante agreement  and to be responsive to the evolution of the network as an open system. In the
A coinduction-based technique to generate an optimal monitor  from a Linear Temporal Logic (LTL) formula is presented in this  paper. Such a monitor receives a sequence of states (one at a time) from  a running process, checks them against a requirements specification expressed  as an LTL formula, and determines whether the formula has  been violated or validated. It can also say whether the LTL formula  is not monitorable any longer, i.e., that the formula can in the future  neither be violated nor be validated. A Web interface for the presented  algorithm adapted to extended regular expressions is available.
A real-time and reliable automatic video segmentation is one of the outstanding problems in Computer Vision with large applications to compression, transmission and motion analysis. In this paper, we show a novel approach based on the superposition of distance maps linked to centroids of mobile regions acting as attractors of homogenous regions to which different thresholds are applied. The homogeneity of each region is characterized by colour characteristics. The number of colors and the extremal values allowed for parameters corresponding to the shape of regions can be previously configured or learned through an unsupervised training. Our realtime processing does not depend on the scene complexity and it is compatible with egomotion, i.e. it is not necessary to discriminate beforehand between foreground and background. Compatibility of our segmentation algorithms with egomotion allows the design of on-line tracking and shots identification for automatic segmentation of video sequences by using a low-level topological representation, which is symbolically represented by means of a kinematic mobile graph.
A number of environmental modelling frameworks have been developed recently, and plans for new frameworks are under way. Examples such as TIME, OpenMI, SME and OMS share an approach to environmental modelling based on model components, and offer improved model development and deployment. These approaches have methods for ensuring model component-linking compatibility using manual and machine processes either internal or external to the model component. Examples include matching output to input and checking data type compatibility. Semantic integration is also possible, such as with the OpenMI, where a component requests and receives particular data. However, each framework does model component checking in a different way and interoperability between model components of different frameworks is limited. To improve the use of model components it is necessary to consider the development of multi-framework model components (MFMC). Existing software standards enable communication at a low level, but many problems remain at high levels. This paper discusses development of an MFMC in each of TIME and the OMS, that can be accessed from the other framework. Additionally, the requirements for further framework compatibility, such as the OpenMI, are considered. Six main approaches are described, covering methods relevant to both between- and cross-platform compatibility, which range from reimplementation, through Web Services, to declarative modelling. Web services are suggested as a viable option for the problem considered here, although the other techniques warrant further investigation in particular cases.
This paper deals with the dimensioning of token buckets. Two related problems are formulated and solved. The first is the token bucket allocation problem (TBAP) which is finding the cost-minimal token bucket for the transmission of a given VBR traffic stream, e.g. an MPEG movie, from the user&apos;s point of view. This problem has been treated in literature before but as we will show not completely. We will derive and efficient and exact algorithm for this problem. As a second step, the token bucket reallocation problem (TBRP) is investigated based on the results for the TBAP. The dynamic token bucket dimensioning problem consists of finding the cost-minimal series of token buckets for the transmission of the stream. For this problem an exact algorithm is again presented and furthermore, several heuristics are devised. The best heuristic comes closer than 0.25% to the results of the exact algorithm and is orders of magnitudes faster as several numerical simulations show. We also try a mulitregression analysis for token bucket dimensioning.
this paper, information mining methods are proposed. They first rely on a hierarchical model based information representation. Entropic measurement similarity in a modeled dynamic feature space characterizing the dynamics of objects and of image structures are then researched
Web Search Engines provide a large-scale text document retrieval service by processing huge Inverted File indexes. Inverted File indexes allow fast query resolution and good memory utilization since their d-gaps representation can be effectively and efficiently compressed by using variable length encoding methods. This paper proposes and evaluates some algorithms aimed to find an assignment of the document identifiers which minimizes the average values of d-gaps, thus enhancing the effectiveness of traditional compression methods. We ran several tests over the Google contest collection in order to validate the techniques proposed. The experiments demonstrated the scalability and effectiveness of our algorithms. Using the proposed algorithms, we were able to sensibly improve (up to 20.81%) the compression ratios of several encoding schemes.
This paper evaluates whether names and relationships needed in biomedical informatics are present in the UMLS. Methods: Terms for five broad categories of concepts were extracted from LocusLink and mapped to the UMLS Metathesausus. Relationships between gene products and the other four categories (phenotype, molecular function, biological process, and cellular component) were searched for in the Metathesaurus. All gene products in the Gene Ontology database were also mapped to the UMLS in order to evaluate its global coverage of the domain. Results: The coverage of concepts ranged from 2% (gene product symbols) to 44% (molecular functions). The coverage of relationships ranged from 60% for Gene productBiological process to 83% for Gene productMolecular function. Discussion: Terminology and ontology issues are discussed, as well as the need for integrating additional resources to the UMLS
This paper proposes a new nonparametric method for estimation of spectral moments of a zero-mean Gaussian process immersed in additive white Gaussian noise. Although the technique is valid for any order moment, particular attention is given to the mean Doppler (first moment) and to the spectral width (square root of the second spectral centered moment). By assuming that the power spectral density of the underlying process is bandlimited,themaximum likelihood estimates of its spectral moments are derived. A suboptimal estimate based on the sample covariances is also studied. Both methods are robust in the sense that they do not rely on any assumption concerning the power spectral density (besides being bandlimited). Under weak conditions, the set of estimates based on sample covariances are unbiased and strongly consistent. Compared with the classical pulse pair and the periodogram based estimates, the proposed methods exhibit better statistical properties for asymmetric spectra and/or spectra with large spectral widths, while involving a computational burden of the same order.
Tamborine Mountain is a 25 square kilometre plateau located within the urban-rural fringe of the Gold Coast, Queensland, Australia. Renowned for its superb climate, spectacular views, lush farmland and subtropical rainforest, Tamborine Mountain offers a wide variety of nature-based tourism activities such as bushwalking, rainforest appreciation and wine tasting. This phenomenon requires the harmonious development of tourism and the environment. This paper examines site planning at Tamborine Mountain, with a focus on two of its main tourist attractions, namely the Winery and National Parks. A strategic approach is used to evaluate some of the strengths and weaknesses of their existing site planning against opportunities for tourism development. Documentations of local government laws and regional planning are collected from local authorities and libraries, and communications during field trips are analysed to evaluate site planning in these attractions. Some broad recommendations are made for future strategic site planning and development.
This paper summarizes some of the progress that has been made toward populating such a world with embodied conversational agents
The actual gains achieved by replication are a complex function of the number of replicas, the placement of those replicas, the replication protocol, the nature of the transactions performed on the replicas, and the availability and performance characteristics of the machines and networks composing the system. This paper describes the design and implementation of the Replica Management System, which allows a programmer to specify the quality of service required for replica groups in terms of availability and performance. From the quality of service specification, information about the replication protocol to be used, and data about the characteristics of the underlying distributed system, the RMS computes an initial placement and replication level. As machines and communications systems are detected to have failed or recovered, or performance characteristics change, the RMS can be re-invoked to compute an updated mapping of replicas which preserves the desired quality of service. The result is a flexible, dynamic and dependable replication system.
This paper presents a comparative study of strategies for crawling the Web. In particular we show that a combination of breadth-first ordering with the largest sites first is a practical alternative since it is simple to implement, it is fast and is able to retrieve the best ranked pages at a rate that is closer to the optimal than the other alternatives. Our study was performed on an actual sample (90%) of the Chilean Web which was crawled by using simulators so that all strategies were compared under the same conditions and actual crawls to validate our conclusions. We also explored the effects of large scale parallelism in the page retrieval task and multiple-page requests in a single connection for effective amortization of waiting and latency times.
Most Java-based systems that support portable parallel and distributed computing either require the programmer to deal with intricate low-level details of Java which can be a tedious, timeconsuming and error-prone task, or prevent the programmer from controlling locality of data. In this paper we describe JavaSymphony, a programming paradigm for distributed and parallel computing that provides a software infrastructure for wide classes of heterogeneoussystems ranging from small-scale cluster computing to large scale wide-area meta-computing. The software infrastructure is written entirely in Java and runs on any standard compliant Java virtual machine.  In contrast to most existing systems, JavaSymphony provides the programmerwith the flexibility to control data locality and load balancing by explicit mapping of objects to computing nodes. Virtual architectures are specified to impose a virtual hierarchy on a distributed system of physical computing nodes. Objects can be mapped and dyn...
This paper presents a modified set theoretic framework for estimating the state of a linear dynamic system based on uncertain measurements. The measurement errors are assumed to be unknown but bounded by ellipsoidal sets. ased on this assumption, a recursive state estimator is (re--)derived in a tutorial fashion. It comprises both the prediction step (time update), i.e., propagation of a set of feasible states by means of the system model and the filter step (measurement update), i.e., inclusion of a new measurement into the current estimate. The main contribution is an efficient square--root formulation of this estimator, which is well suited especially for practical applications.
We envision a future in which clouds of microcomputers  can be sprayed in an environment to provide, by  spontaneously networking with each other, an endlessly  range of futuristic applications. However, beside the  vision, spraying may also act as a powerful metaphor  for a range of other scenarios that are already under  formation, from ad-hoc networks of embedded and  mobile devices to worldwide distributed computing.
Th- paper proposes Jarcler,whc h is aspect oriented mid dleware for using replicated objects in Java. It enablesth users to customizeth beh vior of replicated objects per class sothW th beh vior fits requirements of a particular application. Althcat reflectionhfl been a typical tech nique for customizing such beh vior,thr paper sh wsth5 reflection forces programmers to write a program far from thom intuition; aspect oriented programming provided by Jarcler makes it easier to describeth customization.Ths paper illustratesthu issuethue-1 an example of simple network game.
We describe a tracking algorithm to address the interactions among objects, and to track them individually and confidently via a static camera. It is achieved by constructing an invariant bipartite graph to model the dynamics of the tracking process, of which the nodes are classified into objects and profiles. The best match of the graph corresponds to an optimal assignment for resolving the identities of the detected objects. Since objects may enter/exit the scene indefinitely, or when interactions occur/conclude they could form/leave a group, the number of nodes in the graph changes dynamically. Therefore it is critical to maintain an invariant property to assure that the numbers of nodes of both types are kept the same so that the matching problem is manageable. In addition, several important issues are also discussed, including reducing the effect of shadows, extracting objects&apos; shapes, and adapting large abrupt changes in the scene background. Finally, experimental results are provided to illustrate the efficiency of our approach.
The objective of this project is to simulate a Peer-to-Peer type of environment with the JADE multi-agent system platform to investigate the use of social networks to optimize the speed of search and to improve quality of service in the Peer-to-Peer environment. Our project uses the Gnutella protocol as a starting point. The Gnutella protocol broadcasts messages for searching files. This message passing generates much traffic in the network. This degrades the quality of service. We propose a model where each peer has a &quot;friends list&quot;, for each category of interest. Once peers generate their &quot;friends list&quot;, they use these lists for searching files in the network. The model has been implemented and some initial experiments have been performed.
This paper uses classical logic for a simultaneous description of the  syntax and semantics of a fragment of English and it is argued that  such an approach to natural language allows procedural aspects of linguistic  theory to get a purely declarative formulation. In particular,  it will be shown how certain construction rules in Discourse Representation  Theory, such as the rule that inde  nites create new discourse  referents and de  nites pick up an existing referent, can be formulated  declaratively if logic is used as a metalanguage for English. In this case  the declarative aspects of a rule are highlighted when we focus on the  model theory of the description language while a procedural perspective  is obtained when its proof theory is concentrated on. Themes of  interest are Discourse Representation Theory, resolution of anaphora,  resolution of presuppositions, and underspeci  cation.
Olfaction is a challenging new sensing modality for intelligent systems. With the emergence of electronic noses it is now possible to detect and recognise a range of different odours for a variety of applications. An existing application is to use electronic olfaction on mobile robots for the purpose of odour based navigation. In this work, we introduce a new application where electronic olfaction is used in cooperation with other types of sensors on a mobile robot in order to acquire the odour property of objects. The mobility of the robot facilitates the execution of specific perceptual actions, such as moving closer to objects to acquire odour properties. Additional sensing modalities provides the spatial detection of objects and electronic olfaction then acquires the odour property which can be used for discrimination and recognition of the object being considered. We examine the problem of deciding when, how and where the e-nose should be activated by planning for active perception. We investigate the use of symbolic reasoning techniques in this context and consider the problem of integrating the information provided by the e-nose with both prior information and information from other sensors (e.g., vision). Finally, experiments are performed on a mobile robot equipped with an e-nose together with a variety of sensors that can perform decision making tasks in realistic environments.
This paper presents a new parallel algorithm for the dynamic convex hull problem. This algorithm is a parallel adaptation of the Jarvis March Algorithm. The computational model selected for this algorithm is the associative computing model (ASC) which supports massive parallelism through the use of data parallelism and constant time associative search and maximum functions. Also, ASC can be supported on existing SIMD computers.
Normally the SISCI interface provides a Distributed Shared Memory (DSM)  [PTM97] abstraction using the Scalable Coherent Interface (SCI). This paper describes  and discusses the design and the concepts behind a library called VIA2SISCI that we  have developed. The library maps the semantics of the Virtual Interface Architecture  (VIA) to SISCI--semantics and establishes a middleware between SISCI and high--  level communication facilities. We focus on several important concepts of VIA that  had to be mapped to the SISCI services. The presented concepts may be interesting  and useful beyond the scope of this paper.
The optimisation of image processing tools individually in a chain of processes does not yield an optimal chain. If we are to consider different steps of image processing and pattern recognition within a scene  analysis system as different components, the effect of one component on another should not be underestimated. However, most scene analysis systems for natural object recognition are developed without any consideration for component interactions. In this paper we demonstrate how the application of different image segmentation algorithms directly relates to the quality of texture measures extracted from segmented regions and directly impact on the classification ability. The difference between the best and the worst performances is found to be significant. We then develop the methodology for determining the optimal chain for scene analysis and show our experimental results on the publicly available  benchmark &quot;Minerva&quot;.
Creating metadata by annotating documents is one of the major techniques for putting machine understandable data on the Web. Though there exist many tools for annotating web pages, few of them fully support the creation of semantically interlinked metadata, such as necessary for a truely Semantic Web. In this paper, we present an ontology-based annotation environment, OntoAnnotate, which offers comprehensive support for the creation of semantically interlinked metadata by human annotators. Based on this environment, we then investigate the human factor of metadata creation. In some experiments, we explore the base line for inter-annotator agreements  in a rather typical test setting. 
The objective of this paper is to show that for every color space there exists an optimum skin detector scheme such that the performance of all these skin detectors schemes is the same. To that end, a theoretical proof is provided and experiments are presented which show that the separability of the skin and no skin classes is independent of the color space chosen.
MoMi (Mobile Mixins) is a coordination language for mobile processes that  communicate and exchange object-oriented code in a distributed context. MoMi&apos;s key idea  is structuring mobile object-oriented code by using mixin-based inheritance. Mobile code is compiled and typed locally, and can successfully interact with code present on foreign sites only if its type is subtyping-compliant with what is expected by the receiving site. In this  paper, we study a subtyping relation for MoMi that includes both width subtyping and depth subtyping, in order to achieve a significantly more flexible, yet still simple, communication pattern. Technical problems arising from the depth subtyping are solved by defining a static  annotation procedure.
In the recent years, the Web has been rapidly &quot;deepened&quot; with the prevalence of databases online. On this deep Web, many sources are structured by providing structured query interfaces and results. Organizing such structured sources into a domain hierarchy is one of the critical steps toward the integration of heterogeneous Web sources. We observe that, for structured Web sources, query schemas  (i.e., attributes in query interfaces) are discriminative representatives of the sources and thus can be exploited for source characterization. In particular, by viewing query schemas as a type of categorical data, we abstract the problem of source organization into the clustering of categorical data. Our approach hypothesizes that &quot;homogeneous sources&quot; are characterized by the same hidden generative models for their schemas. To find clusters governed by such statistical distributions, we propose a new objective function, model-differentiation, which employs principled hypothesis testing to maximize statistical heterogeneity among clusters. Our evaluation over hundreds of real sources indicates that (1) the schemabased clustering accurately organizes sources by object domains (e.g., Books, Movies), and (2) on clustering Web query schemas, the model-differentiation function outperforms existing ones, such as likelihood, entropy, and context linkages, with the hierarchical agglomerative clustering algorithm.
Most knowledge management (KM) projects aim at creating a knowledge  base system in which all corporate knowledge is organized according to a single, supposedly  shared and objective classification. The underlying assumption is that knowledge  can be made objective refining it of all its subjective, contextual, and social aspects. However,
Developing fast gaits for legged robots is a difficult  task that requires optimizing parameters  in a highly irregular, multidimensional space. In  the past, walk optimization for quadruped robots,  namely the Sony AIBO robot, was done by handtuning  the parameterized gaits. In addition to  requiring a lot of time and human expertise, this  process produced sub-optimal results. Several recent  projects have focused on using machine learning  to automate the parameter search. Algorithms  utilizing Powell&apos;s minimization method and policy  gradient reinforcement learning have shown significant  improvement over previous walk optimization  results. In this paper we present a new algorithm  for walk optimization based on an evolutionary  approach. Unlike previous methods, our algorithm  does not attempt to approximate the gradient of  the multidimensional space. This makes it more  robust to noise in parameter evaluations and avoids  prematurely converging to local optima, a problem  encountered by both of the algorithms mentioned  above. Our evolutionary algorithm matches the  best previous learning method, achieving several  different walks of high quality. Furthermore, the  best learned walks represent an impressive 20%  improvement over our own best hand-tuned walks.
The objectives of the CATI project (Charging and Accounting Technology for the Internet) include the design, implementation, and evaluation of charging and accounting mechanisms for Internet services and Virtual Private Networks (VPN). They include the enabling technology support for open, Internet-based Electronic Commerce platforms in terms of usage-based transport service charging as well as high-quality Internet transport services and its advanced and flexible configurations for VPNs. In addition, security-relevant and trust-related issues in charging, accounting, and billing processes are investigated. Important application scenarios, such as an Internet telephony application as well as an Electronic Commerce shopping network, demonstrate the applicability and efficiency of the developed approaches. This work is complemented by an appropriate cost model for Internet communication services, including investigations of suitable usage-sensitive pricing models.  1 Introduction and Mot...
This paper is devoted to the investigation of that question. We present two variants of local search algorithms where the search time can be set as an input parameter. These two approaches are: a time-predefined variant of simulated annealing and a specially designed local search that we have called the &quot;degraded ceiling&quot; method. We present a comprehensive series of experiments, which show that these approaches significantly outperform the previous best results (in terms of solution quality) on the most popular  benchmark exam timetabling problems. Of course there is a price to pay for such better results: increased execution time. We discuss the impact of this trade-off between quality and execution time. In particular we  discuss issues involving the proper estimation of the algorithm&apos;s execution time and assessing its importance
In speech understanding systems, the interface between acoustic and linguistic modules is often represented by the N best sequences that match the input signal. They compose a set that will be linguistically analyzed in order to find the interpretation of the input. An appropriate representation of the N-Best could make linguistic processing more efficient. Here a representation based on a context-free model is proposed that is obtained by an algorithm inherited by the data compression field. This algorithm is based on the subword tree of the concatenation of the N best sequences. The proposed representation seems particularly appropriate when coupled with a bidirectional parser and some experiments demonstrate that the approach is worth pursuing. Such experiments focus on the comparison between the proposed representation and a sequential processing of the N hypotheses given by the acoustic module. The comparison takes into consideration the efficiency attained in the two cases, in te...
Recent studies have suggested that the soft-error rate in microprocessor logic will become a reliability concern by 2010. This paper proposes an effcient error detection technique, called fingerprinting, that detects differences in execution across a dual modular redundant (DMR) processor pair. Fingerprinting summarizes a processor&apos;s execution history in a hash-based signature; differences between two mirrored processors are exposed by comparing their fingerprints. Fingerprinting tightly bounds detection latency and greatly reduces the interprocessor communication bandwidth required for checking. This paper presents a study that evaluates fingerprinting against a range of current approaches to error detection. The result of this study shows that fingerprinting is the only error detection mechanism that simultaneously allows high-error coverage, low error detection bandwidth, and high I/O performance.
Both the Scalable Coherent Interface (SCI) and the Virtual Interface Architecture (VIA) aim at providing effective cluster communication. While the former is a standardized subnet technology the latter is a generic architecture which can be applied to a variety of physical medias. Both approaches provide user level communication, but they achieve it on different ways and thus, have different characteristics that are independent of the actual implementation. In this paper we report and compare the raw network speed of an SCI and a VIA implementation as delivered by MPI and show how it affects application performance by means of the NAS Parallel Benchmark Suite.  Keywords---NetPIPE, NAS Parallel Benchmarks, SCI, VIA, MPI  I. INTRODUCTION  High performing interconnect systems are of key importance for effective cluster computing. Several solutions have been introduced, one of which is the Scalable Coherent Interface (SCI), a technology to build distributed shared memory as well as message...
this paper, we will consider in depth laryngeal processes in Icelandic and Sanskrit which have long puzzled linguists. On Sanskrit throwback we have treatments by the Indian grammarian Pan . ini and Whitney&apos;s influential 1889 grammar. Thr&apos;ainsson made a pioneering analysis of Icelandic preaspiration in 1978 using the then-novel autosegmental theory. We will examine the role of syllable structure and consider the various ways Optimality Theory has been brought to bear on the issues involved. For Icelandic, we will propose a new OT treatment that solves several outstanding issues with published analyses
Stochastic games are a general model of interaction between multiple agents.
The goal of this paper is to provide a formalization of monotonic belief and belief about belief in a multiagent environment. We distinguish between ideal beliefs, i.e., those beliefs which satisfy certain &quot;idealized&quot; properties which are unlikely to be possessed by real agents, and real beliefs. Our formalization is based on a set-theoretic specification of beliefs and, then, on the definition of the appropriate constructors which present the sets identified. This allows us to provide a uniform and taxonomic characterization of the possible ways in which ideal and real beliefs can arise. We provide intuitions about the conceptual importance of the cases analyzed by proving and discussing some equivalence results with some important modal systems modeling various forms of (non) logical omniscience.  
This article explores the effciency of motion-compensated three-dimensional transform coding, a compression scheme that employs a motion-compensated transform for a group of pictures. We investigate this coding scheme experimentally and theoretically. The practical coding scheme employs in temporal direction a wavelet decomposition with motion-compensated lifting steps. Further, we compare the experimental results to that of a predictive video codec with single-hypothesis motion compensation and comparable computational complexity. The experiments show that the 5/3 wavelet kernel outperforms both the Haar kernel and, in many cases, the reference scheme utilizing single-hypothesis motion-compensated predictive coding. The theoretical investigation models this motion-compensated subband coding scheme for a group of K pictures with a signal model for K motion-compensated pictures that are decorrelated by a linear transform. We utilize the Karhunen-Loeve Transform to obtain theoretical performance bounds at high bit-rates and compare to both optimum intra-frame coding of individual motion-compensated pictures and single-hypothesis motion-compensated predictive coding. The investigation shows that motion-compensated three-dimensional transform coding can outperform predictive coding with single-hypothesis motion compensation by up to 0.5 bits/sample. Preprint submitted to Elsevier Science 30 April 2004 Key words: Video Coding, Motion Compensation, Adaptive Wavelets, Lifting,  Three-Dimensional Subband Coding of Video  1 
To implement high-performance global interconnect without impacting the placement and performance of existing blocks, the use of buffer blocks is becoming increasingly popular in structured-custom and block-based ASIC methodologies. Recent works by Cong, Kong and Pan [5] and Tang and Wong [18] give algorithms to solve the buffer block planning problem. In this paper, we address the problem of how to perform buffering of global multiterminal nets given an existing buffer block plan. We give a provably good algorithm based on a recent approach of Garg and K onemann [8] and Fleischer [7] (see also Albrecht [1] and Dragan et al. [6]). Our method routes connections using available buffer blocks, such that required upper and lower bounds on buffer intervals--- as well as wirelength upper bounds per connection---are satisfied. In addition, our algorithm allows more than one buffer to be inserted into any given connection and observes buffer parity constraints. Most importantly, and unlike previous works on the problem [5, 18, 6], we take into account multiterminal nets. Our algorithm outperforms existing algorithms for the problem [5, 6], which are based on 2-pin decompositions of the nets. The algorithm has been validated on top-level layouts extracted from a recent high-end microprocessor design.
A hybrid Multi-Objective Evolutionary Algorithm is used  to tackle the uncapacitated exam proximity problem. In this hybridization,  local search operators are used instead of the traditional genetic  recombination operators. One of the search operators is designed to repair  unfeasible timetables produced by the initialization procedure and  the mutation operator. The other search operator implements a simplified  Variable Neighborhood Descent meta-heuristic and its role is to  improve the proximity cost. The resulting non dominated timetables are  compared with thouse produced by other optimization methods using  15 public domain datasets. Without special fine-tuning, the hybrid algorithm  was able to produce timetables ranking first and second in 9 of  the 15 datasets.
In this paper we describe a new binarisation method designed specifically for OCR of low quality camera images: Background Surface Thresholding or BST. This method is robust to lighting variations and produces images with very little noise and consistent stroke width. BST computes a &quot;surface&quot; of background intensities at every point in the image and performs adaptive thresholding based on this result. The surface is estimated by identifying regions of lowresolution text and interpolating neighbouring background intensities into these regions. The final threshold is a combination of this surface and a global offset. According to our evaluation BST produces considerably fewer OCR errors than Niblack&apos;s local average method while also being more runtime efficient.
This paper introduces in details this problem, and lower bounds, usefull to evaluate the minimum duration of a project
viii CHAPTER 1 
this paper is available in [3]
In this paper, we explore the use of automatic  syntactic simplification for improving content  selection in multi-document summarization. In  particular, we show how simplifying parentheticals  by removing relative clauses and appositives  results in improved sentence clustering, by  forcing clustering based on central rather than  background information. We argue that the inclusion  of parenthetical information in a summary  is a reference-generation task rather than a  content-selection one, and implement a baseline  reference rewriting module. We perform our  evaluations on the test sets from the 2003 and  2004 Document Understanding Conference and  report that simplifying parentheticals results in  significant improvement on the automated evaluation  metric Rouge.
Model checking based on the causal partial order semantics  of Petri nets is an approach widely applied to cope with the state  space explosion problem. One of the ways to exploit such a semantics  is to consider (finite prefixes of) net unfoldings, which contain enough  information to reason about the reachable markings of the original Petri  nets. In this paper, we propose several improvements to the existing  algorithms for generating finite complete prefixes of net unfoldings. Experimental  results demonstrate that one can achieve significant speedups  when transition presets of a net being unfolded have overlapping parts.
We present a system that tries to automatically collect and monitor Japanese blog collections that include not only ones made with blog softwares but also ones written as normal web pages. Our approach is based on extraction of date expressions and analysis of HTML documents. Our system also extracts and mines useful information from the collected blog pages.
We examine various methods for data clustering and data classification that are based on the minimization of the so-called cluster function and its modifications. These functions are nonsmooth and nonconvex. We use Discrete Gradient methods for their local minimization. We consider also a...
StarT-jr is an experimental parallel system composed of a network of personal  computers (PCs). The system leverages the momentum of the microprocessor  and PC industries to achieve excellent single node performance at a low cost.
Scene analysis is an important area of research with the aim of identifying objects and their relationships in natural scenes. MINERVA benchmark has been recently introduced in this area for testing different image processing and classification schemes. In this paper we present results on the classification of eight natural objects in the complete set of 448 natural images using neural networks. An exhaustive set of experiments with this benchmark has been conducted using four different segmentation methods and five texture-based feature extraction methods. The results in this paper show the performance of a neural network classifier on a ten fold cross-validation task. On the basis of the results produced, we are able to rank how well different image segmentation algorithms are suited to the task of region of interest identification in these images, and we also see how well texture extraction algorithms rank on the basis of classification results.
This paper deals with an application of image sequence analysis. In particular, it addresses the problem of determining the number of people who get into and out of a train carriage when it&apos;s crowded and background and/or illumination might change. The proposed system analyses image sequences and processes them using an algorithm based on the use of several morphological tools and optical flow motion estimation.
Knowledge intensive organizations have vast array of information contained in large document repositories. With the advent of E-commerce and corporate intranets/extranets, these repositories are expected to grow at a fast pace. This explosive growth has led to huge, fragmented, and unstructured document collections. Although it has become easier to collect and store information in document collections, it has become increasingly difficult to retrieve relevant information from these large document collections. This paper addresses the issue of improving retrieval performance (in terms of precision and recall) for retrieval from document collections.  There are three important paradigms of research in the area of information retrieval (IR): Probabilistic IR, Knowledge-based IR, and, Artificial Intelligence based techniques like neural networks and symbolic learning. Very few researchers have tried to use evolutionary algorithms like genetic algorithms (GA&apos;s). Previous attempts at using GA&apos;s have concentrated on modifying document representations or modifying query representations. This work looks at the possibility of applying GA&apos;s to adapt various matching functions. It is hoped that such an adaptation of the matching functions will lead to a better retrieval performance than that obtained by using a single matching function. An overall matching function is treated as a weighted combination of scores produced by individual matching functions. This overall score is used to rank and retrieve documents. Weights associated with individual functions are searched using Genetic Algorithm.  The idea is tested on a real document collection called the Cranfield collection. The results look very encouraging.  Keywords: Information Storage and Retrieval Systems (HA09), Information R...
Nowadays, there exists a great interest in wireless and mobile devices. However, the development of graphical user interfaces (GUIs) for applications in these environments must consider new problems: 1) Different device capabilities and 2) Automatic monitoring of user interfaces. In this paper, we present an architecture that solves previous problems. We advocate the use of specifications of GUIs and the dynamic generation of the adequate visualization for a specific device without reimplementing each GUI for different devices.
This paper studies the comparative tracking performance of the recursive least squares (RLS) and least mean square (LMS) algorithms for time-varying inputs, specifically for linearly chirped narrowband input signals in additive white Gaussian noise. It is shown that the structural differences in the implementation of the LMS and RLS weight updates produce regions where the LMS performance exceeds that of the RLS and other regions where the converse occurs. These regions are shown to be a function of the signal bandwidth and signal-to-noise ratio (SNR). LMS is shown to place a notch in the signal band of the mean lag filter, thus reducing the lag error and improving the tracking performance. For the chirped signal, it is shown that this produces smaller tracking error for small SNR. For high SNR, there is a region of signal bandwidth for which RLS will provide lower error than LMS, but even for these high SNR inputs, LMS always provides superior performance for very narrowband signals.
Herbicide transport through the vadose zone was studied in the Upper Rhne River Valley (SouthWest Switzerland). The herbicides atrazine and isoproturon were applied to instrumented field plots and the concentrations reaching the groundwater were measured. The solute transport is closely linked to precipitation. Following the first heavy rainfall after the application, the chemicals are quickly transported through the vadose zone and reach the groundwater in a short time. The transport experiments were simulated with the mechanistic deterministic model HYDRUS-1D. The mobile-immobile water concept was used to account for the rapid transport. In the study area, the shallow groundwater influences considerably the water conditions in the unsaturated zone; in such cases the use of a one-dimensional model to simulate the water flow and the chemical  transport in the vadose zone is difficult because of problems in defining the lower boundary condition. Groundwater flow is typically three-dimensional and therefore, a saturated - unsaturated 3-D model or the coupling of an unsaturated 1-D model to a 3-D saturated model would be more appropriate. Nevertheless, HYDRUS-1D allowed to describe qualitatively some observations and to confirm the assumption that accelerated flow occurs on the experimental plots.
Xanthi is a medium populated town that lies near the north borders of Greece. The old town of Xanthi is stated in the centre of the urban area and is one of the best-preserved traditional towns of Greece. Most of the old houses belong to:  . the so-called &quot;traditional type&quot;,  . neoclassical influenced houses and  . buildings strongly influenced of 19th--20th century European architecture.
We consider the situation where several XML sources have to be integrated which are  assumed to contain complementary, overlapping contents. These overlappings have to  be detected, and then appropriate operations have to be applied to the internal database  to generate a result view. The approach uses the XPathLog language for formulating  queries and updates of an XML database.
In this paper, we describe three key problems for trust management in federated systems and present a layered architecture for addressing them. The three problems we address include how to express and verify trust in a flexible and scalable manner, how to monitor the use of trust relationships over time, and how to manage and reevaluate trust relationships based on historical traces of past behavior. While previous work provides the basis for expressing and verifying trust, it does not address the concurrent problems of how to continuously monitor and manage trust relationships over time. These problems close the loop on trust management and are especially relevant in the context of federated systems where remote resources can be acquired across multiple administrative domains and used in potentially undesirable ways (e.g., to launch denial-ofservice attacks).
The services provided by critical infrastructure systems are essential to the operation of modern society. These systems include the financial payments system, transportation systems, military command and control systems, the electric power grid, and telecommunications systems including the Internet. Widespread failure of any of these system might result in severe financial loss or perhaps human injury. Critical infrastructure systems rely heavily on distributed information systems for operation. These information systems must therefore be dependable; that is, they must &quot;deliver service that can justifiably be trusted.&quot;  Traditional dependability alone does not provide a rich enough model to deal with the faults in large, critical distributed systems operating in hostile environments. These systems require not simply dependability but instead require survivability. Informally, survivability is when a system has &quot;the ability to continue to provide service (possibly degraded or different) in a given environment  when various events cause major damage to the system or its operating environment.&quot;. One means of
This paper presents a peer-to-peer based authentication and authorization infrastructure to minimize authentication delays when mobile users roam across different wireless networks. The basic idea is to avoid exchanging security information between networks visited by a roaming user and the user&apos;s home authentication, authorization, and accounting (AAA) server that is typically located in the home network possibly far away from the visited network. Instead, authentication and authorization of a roaming user shall be supported by an AAA server in the visited network. We propose that the AAA server that is responsible for authentication and authorization in a newly visited network locates the AAA server in the previously visited network and retrieves the required security information from that AAA server. The AAA servers can be organized in a peerto -peer manner and peer-to-peer mechanisms can be applied for searching and transferring security information between them. We propose several mechanisms for quickly locating the previously responsible AAA server in order to decrease authentication delays. The performance of these mechanisms is evaluated by simulations. Real performance measurements show the rather low performance overhead of application level forwarding used in peer-to-peer networks.
The RT Role-based Trust-management framework provides policy language, semantics, deduction engine, and pragmatic features such as application domain specification documents that help distributed users maintain consistent use of policy terms. This paper provides a general overview of the framework, combining some aspects described in previous publications with recent improvements and explanation of motivating applications.
This paper addresses the problem of determining an object&apos;s 3D location from a stream of camera images recorded by mobile robot. The approach presented here allows people to &quot;train&quot; robots to recognize specific objects, by presenting it examples of the object to be recognized. A decision tree method is used to learn significant features of the target object from individual camera images. Individual estimates are integrated over time using Bayes rule, into a probabilistic 3D model of the robot&apos;s environment. Experimental results illustrate that the method enables a mobile robot to robustly estimate the 3D location of objects from multiple camera images.  
Intelff has created an Evaluator Toolkit (ET) as a way to easily integrate information   management technologies into a myriad of applications. In the initial report (Mock,   1998) a vector-based classifier and a rule induction algorithm were described. While the   rule induction algorithm was effective, it suffers from a slow induction process that   requires tens of minutes to generate rules. This report describes a modification to the rule  induction algorithm that is intended to support the incremental learning of rules and the   generation of more meaningful relevance rankings for matching rules. The modified   system is called the Hybrid Rule Induction classifier because it incorporates a shortened   vector of words into an inducted profile of rules. To support incremental learning, all   negative training examples are remembered by each profile. Performance results on the   Reuters-21578 data set indicate that the incremental training is almost as effective as   block training, and more effective than if no incremental training is done at all. However,   induction time is often too slow for real-time usage on current processors but may be   acceptable for background usage.     1. Rule Induction Background  One technique that is used to classify documents is through a list of decision rules. Decision rules take up very little memory, usually a few kilobytes, and are in a format easier for people to understand and modify than TF-IDF frequencies used in vector-based classifiers. We will pose the rules to be unordered (we could apply the rule in any order, not an if-then-else order as is generated by decision trees) in disjunctive normal form (DNF). For example, the rules could look like the following:  1. if AGENT then Class=Agents  2. if INTELLIGENT and MOBILE then ...
In this paper we study algorithms for pricing of financial contracts using a lattice process of interest rates. If the cash-flows generated by the contract depend on the history of the interest rates (path dependent contracts), then the pricing algorithms are typically of exponential complexity. We demonstrate that for some models, there exist polynomial algorithms for path dependent contracts. These models include product form cash-flows, additive cash-flows, delayed cash-flows and limited path dependent cash-flows. 1 
This paper discusses a generalization of the function transformation scheme used in [4, 5, 19, 20] for global energy minimization applied to the molecular conformation problem. A mathematical theory for the method as a special continuation approach to global optimization is established. We show that the method can transform a nonlinear objective function into a class of gradually deformed, but &quot;smoother&quot; or &quot;easier&quot; functions. An optimization procedure can then be applied to the new functions successively, to trace their solutions back to the original function. Two types of transformation are defined: isotropic and anisotropic. We show that both transformations can be applied to a large class of nonlinear partially separable functions including energy functions for molecular conformation. Methods to compute the transformation for these functions are given.
With the invention of high throughput methods, researchers are capable of producing large amounts of biological data. During the analysis of such data the need of a functional grouping of genes arises. In this paper, we propose a new clustering algorithm for the partition of genes or gene products according to their known biological function based on Gene Ontology terms. Ontologies offer a mechanism to capture knowledge in a shareable form that is also processable by computers. Our functional cluster algorithm promises to automatize, speed up and therefore improve biological data analysis.
We present the results of applying lossless and lossy data compression to a three-dimensional object reconstruction and recognition technique based on phase-shift digital holography. We find that the best lossless (Lempel-Ziv, Lempel-Ziv-Welch, Huffman, Burrows-Wheeler) compression rates can be expected when the digital hologram is stored in an intermediate coding of separate data streams for real and imaginary components. The lossy techniques are based on subsampling, quantization, and discrete Fourier transformation. For various degrees of speckle reduction, we quantify the number of Fourier coefficients that can be removed from the hologram domain, and the lowest level of quantization achievable, without incurring significant loss in correlation performance or significant error in the reconstructed object domain.
The first generation of peer-to-peer file sharing systems followed the traditional client-server paradigm. However, legality and scalability issues have driven the development of decentralized file sharing protocols; the most popular of these being Gnutella. To date, such systems have been unable to match the Quality of Service (Qos) offered by centralized architectures. AGnuS improves QoS on Gnutella by increasing file availability, improving network friendliness and increasing file quality. This is achieved by layering caching, load balancing, content-based routing and filtering services on top of the core Gnutella protocol.
The capacity for shared attention is a cornerstone of  human social intelligence. We propose that the development of shared attention depends on a proper interaction of motivational biases and contingency learning mechanisms operating in an appropriately structured environment. Atypical contingency learning leads to deficits in shared attention as seen in children with autism. To test this theory, we describe a unique research effort that combines theoretically rigorous modeling techniques using both simulated and robotic learning systems with novel empirical investigations of social learning and development in infants and toddlers with and without developmental disabilities. We believe that studying embodied learning models, whose input data (from a real or virtual caregiver) is modeled after real infant-caregiver interactions, will lead to a better understanding of the development and dysfunction of shared attention.
We present an anisotropic diffusion equation designed to restore interferometric images. It has two main purposes. the first is to preserve the structures and discontinuities formed  by the fringes. The second is to incorporate the noise modeling which is specific to this kind of images. Besides we show that our model formalizes previous related work in interferometry filtering.
This thesis has for theme the contribution for building methodologies and tools for designing multitechnological systems. These research and development works take place in a European Project concerning based simulation on specifications and performance indicators versus thermal and electric effects. The objective is to create a modelling platform showing the certified models and to reply to the set up criteria in focussing on the function, the behaviour &amp; structure, and the physics of the component. This platform implements procedures (language VHDL-AMS, methodologies,) and resources (CAO tools, libraries, ). Such a platform is based on conceptual bases gathering design methods (top/down and bottom/up approaches) and modelling methods (at functional, behavioural and structural, and physical levels).
The Differentiated Services (DiffServ) architecture for the Internet implements a scalable mechanism for qualityof -service (QoS) provisioning. Bandwidth brokers represent the instances of the architecture, that automate the provisioning of a DiffServ service between network domains. Although several bandwidth broker implementations (e.g. [1]) have been proposed, the alternatives and trade-offs of the different viable approaches of inter-broker communication were not studied up to now.  This paper presents the broker signaling trade-offs considered in the context of a DiffServ scenario used by the Swiss National Science Foundation project CATI [8], and it presents results gathered by simulations.  1 Introduction  The DiffServ architecture (RFC 2475) uses automated bandwidth brokers [7] to negotiate service level agreements (SLA) between different autonomous systems. These agreements describe the volume of DiffServ traffic that can be exchanged between two domains and the price for that...
We consider the task of learning hidden Markov models (HMMs) when only partially (sparsely) labeled observation sequences are available for training. This setting is motivated by the information extraction problem, where only few tokens in the training documents are given a semantic tag while most tokens are unlabeled. We first describe the partially hidden Markov model together with an algorithm for learning HMMs from partially labeled data. We then present an active learning algorithm that selects &quot;difficult&quot; unlabeled tokens and asks the user to label them. We study empirically by how much active learning reduces the required data labeling effort, or increases the quality of the learned model achievable with a given amount of user effort.
In this article we propose a calculus of qualitative geographic coordinates  which allows reasoning about cardinal directions on grid-based reference  systems in maps. Grids in maps can be considered as absolute reference  systems. The analysis reveals that the basic information coded in these  reference systems is ordering information. Therefore, no metric information is  required. We show that it is unnecessary to assume a coordinate system based  on numbers in order to extract information like a point P is further north than a  point Q. We investigate several grids in maps resulting from different types of  projections. In addition, a detailed examination of the north arrow is given since  it supplies a grid with ordering information. On this basis, we provide a general  account on grids, their formalization and the inferences about cardinal  directions drawn using qualitative geographic coordinates.
Time-series prediction is important in physical and financial domains. Pattern recognition techniques for time-series prediction are based on structural matching of the current state of the time-series with previously occurring states in historical data for making predictions. This paper describes a Pattern Modelling and Recognition System (PMRS) which is used for forecasting benchmark series and the US S&amp;P financial index. The main aim of this paper is to evaluate the performance of such a system on noise free and Gaussian additive noise injected time-series. The results show that the addition of Gaussian noise leads to better forecasts. The results also show that the Gaussian noise standard deviation has an important effect on the PMRS performance. PMRS results are compared with the popular Exponential Smoothing method.  Keywords  Univariate time-series Pattern Recognition Noise injection  Computational Intelligence Forecasting System performance  3 I. INTRODUCTION  Time-series predi...
We explore a number of pragmatic principles of communication in a series of computer simulations. These principles characterize both the environment and the behavior of the interacting agents. We investigate how a common language can emerge and when it will be useful to communicate, rather than try the task without communication. When we include the cost of communicating, it becomes favorable to communicate only when expectations are not met.
The main drawback of all dynamic AOP technologies available today is the rather high performance overhead in comparison to static weaving approaches. In this paper, we propose an approach to improve the performance of both the interception mechanism and the aspect interpreter of a dynamic AOP system. The interception of the base application is optimized by employing the Java HotSwap technology in such a way that only those joinpoints where aspects are applied upon are trapped. When new aspects are added, all corresponding joinpoints are hotswapped for a trapped version. Likewise, when aspects are removed, the corresponding traps are removed, if no other aspect is applicable at the given trap. In order to improve the aspect interpreter, we propose the Jutta system that allows generating and caching a highly optimized code fragment for each joinpoint. This code fragment contains the combined aspectual behavior for the joinpoint at hand. We integrate HotSwap and Jutta in the JAsCo dynamic AOP system and perform extensive benchmarks to evaluate the performance gain of this approach. In addition, the enhanced JAsCo performance is compared to a selection of current state-of-the-art dynamic AOP approaches. These benchmarks indicate that JAsCo, enhanced with HotSwap and Jutta, is able to improve on the current state-of-the-art performance-wise.
Embedded system programs tend to spend much time in small loops. Introducing a very small loop cache into the instruction memory hierarchy has thus been shown to substantially reduce instruction fetch energy. However, loop caches come in many sizes and variations -- using the configuration best on the average may actually result in worsened energy for a specific program. We therefore introduce a loop cache exploration tool that analyzes a particular program&apos;s profile, rapidly explores the possible configurations, and generates the configuration with the greatest power savings. We introduce a simulation-based approach and show the good energy savings that a customized loop cache yields. We also introduce a fast estimation-based approach that obtains nearly the same results in seconds rather than tens of minutes or hours.
We report on the fabrication and characterization of thick UV-patternable hybrid sol-gel films by spin coating using a modified sol-gel process. Thick coating of 25 microns with crack-free surface can be produced by one spin coating step. Thick layers of more than 100 m can be obtained with a multi-coating process. Characterization using different techniques including optical microscope and atomic force microscope shows both a smooth surface of optical quality and precise patterns. Parameters for hybrid sol-gel coating preparation, photolithography processing including prebaking, exposure, development and postbaking were studied in detail.
As the Internet evolves toward the global multiservice network of the future, a key consideration is support for services with guaranteed quality of service. The proposed differentiated services framework is seen as the key technology to achieve this. DiffServ currently concentrates on control/data plane mechanisms to support QoS, but also recognizes the need for management plane aspects through the bandwidth broker. In this article we propose a model and architectural framework for supporting DiffServ-based end-toend QoS in the Internet, assuming underlying MPLS-based explicit routed paths. The proposed integrated management and control architecture will allow providers to offer both quantitative and qualitative services while optimizing the use of underlying network resources.
The performance loss due to separation of detection and decoding on the binary-input Gaussian CDMA channel is calculated in the large system limit. It is shown that a previous result in [1] found for Gaussian input alphabet holds also for binary input alphabet.
We present a new finger search tree with O(1) worst-case  update time and O(log log d) expected search time with high probability  in the Random Access Machine (RAM) model of computation for a large  class of input distributions. The parameter d represents the number of  elements (distance) between the search element and an element pointed  to by a finger, in a finger search tree that stores n elements. For the need  of the analysis we model the updates by a &quot;balls and bins&quot; combinatorial  game that is interesting in its own right as it involves insertions and  deletions of balls according to an unknown distribution.
The Transmissio Co tro Pros co (TCP) has been the proH co o f chofl--- fo many Internet applicatioU requiring reliablecoablek[%[z The designo TCP has been challenged by the extensio o cosioU]]fl o ver wireless links. In this paper, we investigate a Bayesianappro9 h to infer at the so[fl] ho[ thereaso o a packetlok9 whether cother tio o wireless transmissio errom Ourappro] h is&quot;mo09flk end-toU][ since it requires  long-term average quantity (namely, lo%flH7kU average pac etlo] pro9[z]kU yo ver the wireless segment) that may be bestok[---z---z with help fro the netwo r (e.g. wireless access agent). Specifically, we use Maximum Lieliho o dRatio tests to evaluate TCP as a classifiero the typeo f pac etlo[9 We study the effectivenesso f short-termclassificatio  pac eterro% (co%]0kU97 vs. wireless), givenstatiokU9 prio erro pro0---%kU97[H and distributiok o f pac et delays co[]z%kU9fl o the typeo f pac etlo9 (measuredo ver alo7]0 time scale). Usingo ur Bayesian-based appro9 h and extensive simulatio]k wedemozkU990 that an effciento nline erro classifier can be built aslo9 as co00[]kU9[7[Hko loo[ andlokzz dueto wireless transmissio errom pro duce suffciently different statistics. We intro duce a simple queueingmo delto underline theco[797kU9H delaydistributio% arising fro different indso pac etlo[9] o ver aheteroU[%H99 wired/wireless path. To infer cok]%]%9kU delay distributio---k wecofl[]0k Hidden Maro vMo del (HMM) which explicitly coykzzflH discretized delay values ouesk ed by TCP as parto its statedefinitio7 in additio to an HMM whichdo es no as in [9]. Wedemo]kU[fl] ho w estimatio accuracy is influenced by differentpro oo0flz o f co[0[]HkU versus wirelessloele and penaltieso n inco%]kU classificatiok  1 
This position paper argues for the concurrent, iterative development of requirements and architectures during the development of software systems. It presents the &quot;Twin Peaks&quot; model -- a partial and simplified version of the spiral model -- that illustrates the distinct, yet intertwined activities of requirements engineering and architectural design. The paper suggests that the use of various kinds of patterns -- of requirements, architectures, and designs -- may provide a way to increase software development productivity and stakeholder satisfaction in this setting.
Feature subset selection is important not only for the insight gained from determining relevant modeling variables but also for the improved understandability, scalability, and possibly, accuracy of the resulting models. Feature selection has traditionally been studied in supervised learning situations, with some estimate of accuracy used to evaluate candidate subsets. However, we often cannot apply supervised learning for lack of a training signal. For these cases, we propose a new feature selection approach based on clustering. A number of heuristic criteria can be used to estimate the quality of clusters built from a given feature subset. Rather than combining such criteria, we use ELSA, an evolutionary local selection algorithm that maintains a diverse population of solutions that approximate the Pareto front in a multi-dimensional objective space. Each evolved solution represents a feature subset and a number of clusters; two representative clustering algorithms, K-means and EM, are applied to form the given number of clusters based on the selected features. Experimental results on both real and synthetic data show that the method can consistently find approximate Pareto-optimal solutions through which we can identify the significant features and an appropriate number of clusters. This results in models with better and clearer semantic relevance. 1. 
We consider the single server queue with service in random order. For a large class  of heavy-tailed service time distributions, we determine the asymptotic behavior of the  waiting time distribution. For the special case of Poisson arrivals and regularly varying  service time distribution with index    it is shown that the waiting time distribution is  also regularly varying, with index 1    ff, and the pre-factor is determined explicitly. Another
Biological evidence strongly suggests that insects utilize visual cues for their navigation tasks. This paper discusses the evolution of a simple controller for visual homing by means of evolutionary algorithms. The application is representative for a class of (real world) problems, for which the choice of the fitness function is non-trivial, since the data are not known in advance. For this class of problems, recombination has a much higher influence on the convergence than previously assumed. We show how convergence rates comparable to those of neural network learning algorithms can be achieved.
Electrical stimulation has been known since Galvani&apos;s work with frogs in the 1780&apos;s and is the most widely used method of stimulation in modern clinical medicine. Magnetic stimulation does however have certain advantages over electrical stimulation including its ability to stimulate nerves painlessly. It appears a natural progression to try a method of combined magnetic and electric stimulation. This could potentially combine the advantages of the two methods and minimise or negate their disadvantages. In particular it may improve the focality of magnetic stimulation to that when used alone. 1. 
It is commonly assumed that if a programmer is willing to invest the potentially significant effort required to port an application program to run on a multiprocessor system using a low-level parallel language or library, they will be able to take advantage of a larger degree of parallelism to achieve higher performance than when using a higher-level language or an automatically parallelizing compiler. However, there has been little work examining the relationship between programming complexity (or ease-of-use) and performance. As a first step towards quantifying this relationship, we use the cyclomatic program complexity metric, borrowed from the software engineering field, and the number of program source statements as measures of the relative complexity of a parallel implementation of a application program compared to it&apos;s equivalent sequential implementation. We compare several different programming paradigms across a common set of application programs executed on two workstation clusters, a shared-memory multiprocessor, an IBM SP2 and a Cray T3D. We find that message-passing languages tend to be both the most widely supported and the most complex. Other programming paradigms such as shared-memory and High-Performance Fortran produce more compact and easily understood programs than message-passing languages but are not yet well supported.
Three of the important, generic, implementation issues encountered when developing controllers for pitch-regulated constant-speed wind turbines are considered, namely, (1) accommodation of the strongly nonlinear rotor aerodynamics; (2) automatic controller start-up/shut-down; and (3) accommodation of velocity and acceleration constraints within the actuator. Both direct linearisation and feedback linearisation methods for accommodating the nonlinear aerodynamics are investigated and compared. A widely employed technique for accommodating the nonlinear aerodynamics, originally developed on the basis of physical insight, is rigorously derived and extended to cater for all wind turbine configurations. A rigorous stability analysis of controller start-up is presented for the first time and novel design guidelines are proposed which can significantly reduce the power transients at controller start-up. The relation to anti-wind-up is noted and several aspects of an existing wind-turbine controller start-up strategy are observed to be novel in the anti-wind-up context. Restrictive position, velocity and acceleration constraints may all be present in wind turbines and the dynamic behaviour of the actuator cannot be neglected. A novel, and quite general, anti-wind-up method, based on the startup strategy, is proposed which caters for all these circumstances. The separate strategies for resolving the implementation issues are combined to achieve an elegant controller realisation which accommodates all the implementation issues in an integrated manner. The importance of adopting an appropriate controller realisation is considerable and is illustrated for a 300 kW wind turbine. The implementation issues encountered in this paper are, of course, not confined to wind turbines but are...
This paper deals with the problem of how to interrelate theory-specific treebanks and how to transform one treebank format to another. Currently, two approaches to achieve these goals can be differentiated. The first creates a mapping algorithm between treebank formats [18]. Categories of a source format are transformed into a target format via a given set of general or language-specific mapping rules. The second relates treebanks via a transformation to a general model of linguistic categories, for example based on the EAGLES recommendations for syntactic annotations of corpora [5], or relying on the HPSG framework [12]. For both approaches, the following desiderata are discussed [3]:  
Coverage analysis measures how much of the target code is run during a test and is a useful mechanism for evaluating the effectiveness of a system test or benchmark. In order to improve the quality of the Linux kernel, we are utilizing GCOV, a test coverage program which is part of GNU CC, to show how much of the kernel code is being exercised by test suites such as the Linux Test Project. This paper will discuss the issues that make code coverage analysis a complicated task and how those issues are being addressed. We will describe tools that have been developed to facilitate analysis and how these tools can be utilized for kernel, as well as application code coverage analysis.
In this paper we propose a family of algorithms combining tree-clustering with conditioning that trade space for time. Such algorithms are useful for reasoning in probabilistic and deterministic networks as well as for accomplishing optimization tasks. By analyzing the problem structure it will be possible to select from a spectrum the algorithm that best meets a given time-space specification.   
We introduce a method for segmenting a shape from an image and simultaneously determining its symmetry axis. The symmetry is used to help the segmentation and in turn the segmentation determines the symmetry. The problem is formulated as one of minimizing a goodness of fitness function and Dijkstra&apos;s algorithm is used to find the global minimum of the cost function. The results are illustrated on real images.
This paper presents the design and evaluation of a text categorization method based on the Hierarchical Mixture of Experts model. This model uses a divide and conquer principle to define smaller categorization problems based on a predefined hierarchical structure. The final classifier is a hierarchical array of neural networks. The method is evaluated using the UMLS Metathesaurus as the underlying hierarchical structure, and the OHSUMED test set of MEDLINE records. Comparisons with an optimized version of the traditional Rocchio&apos;s algorithm adapted for text categorization, as well as at neural network classifiers are provided. The results show that the use of the hierarchical structure improves text categorization performance with respect to an equivalent at model. The optimized Rocchio algorithm achieves a performance comparable with that of the hierarchical neural networks.
We explore the use of distributed on-line motion planning algorithms for multiple mobile agents, in Air Traffic Management Systems (ATMS). The work is motivated by current trends in ATMS to move towards decentralized air traffic management, in which the aircraft operate in &quot;free flight&quot; mode instead of following prespecified &quot;sky freeways&quot;. Conflict resolution strategies are an integral part of the free flight setting. In [TPS96], a set of predefined coordination maneuvers has been proposed. The purpose of this paper is to extend this set of maneuvers to cover all possible conflict scenarios involving multiple agents. A distributed motion planning algorithm based on potential and vortex fields is used. While the algorithm is not always guaranteed to generate flyable trajectories, the obtained trajectories can serve as qualitative prototypes for coordination maneuvers between multiple aircraft. The actual maneuvers are generated by approximating these prototypes with trajectories made u...
Multi-view algorithms, such as co-training  and co-EM, utilize unlabeled data when the  available attributes can be split into independent  and compatible subsets. Co-EM  outperforms co-training for many problems,  but it requires the underlying learner to estimate  class probabilities, and to learn from  probabilistically labeled data. Therefore, coEM  has so far only been studied with naive  Bayesian learners. We cast linear classifiers  into a probabilistic framework and develop  a co-EM version of the Support Vector Machine.
We present herein a fully distributed algorithm  for group or coalition formation among autonomous  agents. The algorithm is based on  a distributed computation of maximal cliques  (of up to pre-specified size) in the underlying  graph that captures the interconnection topology  of the agents. Hence, given the current  configuration of the agents, the groups that are  formed are characterized by a high degree of  connectivity, and therefore high fault tolerance  with respect to node and link failures. We also  briefly discuss how our basic algorithm can be  adapted in various ways so that the formed  groups satisfy the requirements (&quot;goodness&quot; criteria)  other than mere strong inter-group communication  connectivity. We envision various  variants of our basic algorithm to prove themselves  useful subroutines in many multi-agent  system and ad hoc network applications where  the agents may repeatedly need to form temporary  groups or coalitions in an effcient, fully  distributed and online manner.
A conventional local model network (LMN) consists of a set of affined local models. It has poor interpretability on the process dynamic characters. Recent research on velocity-based multiple models show that the velocity-based approach is ideally suited to the development of local controller (LC) networks. As the applications of digital computer are popular in control, both the discrete-time conventional LM network and the discrete-time velocity-based multiple model approaches were developed theoretically. Simulation results on a complex nonlinear process continuous stirred tank reactor (CSTR) prove the effectiveness of the proposed approach. Copyright ff Controlo 2000.   Keywords: local model network, velocity-based multiple model network, nonlinear, discrete time  domain.  1. 
Problem Frames are a systematic approach to the decomposition of problems that allows us to relate requirements, domain properties, and machine specifications. Having decomposed a problem, one approach to solving it is through a process of composing solutions to sub-problems. In this paper, we contribute to supporting such a process by  providing a way to compose multiple Problem Frames. We develop a systematic approach to composing inconsistent requirements. We introduce  Composition Frames, a requirements construct that  models relevant aspects of composition and thus deals  with unwanted effects, such as interference of  overlapping reactions to events. Throughout the paper we use a simple case study to illustrate and validate our ideas.
Novel algorithmic features of multimedia applications and advances in VLSI technologies are driving forces behind the new multimedia signal processors. We propose an architecture platform which could provide high performance and flexibility,andwould require less external I/O and memory access. It is comprised of array processors to be used as the hardware accelerator and RISC cores to be used as the basis of the programmable processor. It is a hierarchical and scalable architecture style which facilitates the hardware-software codesign of multimedia signal processing circuits and systems. While some controlintensive functions can be implemented using programmable CPUs, other computation-intensive functions canrelyonhardware accelerators.
Subject NP inversion in extraction contexts in French raises diffculty  for standard phrase structure approaches. We present an analysis which relies  both on domain union and lexical types for verbs. This analysis extends to French  locative inversion whose properties have never been examined. Having unified the  syntax of LI with that of other extractions, we reexamine the relation between NP  inversion and information packaging in constructions with main clause assertive  force.
The main interest behind the Brainstormers&apos; eort in the robocup soccer domain is to develop and to apply machine learning techniques in complex domains. Especially, we are interested in reinforcement learning methods, where the training signal is only given in terms of success or failure. Our  nal goal is a learning system, where we only plug in &apos;win the match&apos; - and our agents learn to generate the appropriate behaviour. Unfortunately, even from very optimistic complexity estimations it becomes obvious, that in the soccer domain, both conventional solution techniques and also advanced today&apos;s reinforcement learning techniques come to their limit - there are more than (108  50)  23  dierent states and more than (1000)  300  dierent policies per agent per half time. This paper describes a modular approach of the Brainstormers team to tackle this complex decision problem at dierent levels.
Motivated by the statistical evaluation of complex computer models, we deal with the issue of objective prior specification for the parameters of Gaussian processes. In particular, we derive the Jeffreys-rule, independence Jeffreys and reference priors for this situation, and prove that the resulting posterior distributions are proper under a quite general set of conditions. Another prior specification strategy, based on maximum likelihood estimates, is also considered, and all priors are then compared on the grounds of the frequentist properties of the ensuing Bayesian procedures. Computational issues are also addressed in the paper, and we illustrate the proposed solutions by means of an example taken from the field of complex computer model validation.
. We present a novel application of inductive logic programming  (ILP) in the area of quantitative structure-activity relationships  (QSARs). The activity we want to predict is the biodegradability of  chemical compounds in water. In particular, the target variable is the  half-life in water for aerobic aqueous biodegradation. Structural descriptions  of chemicals in terms of atoms and bonds are derived from the  chemicals&apos; SMILES encodings. Definition of substructures are used as  background knowledge. Predicting biodegradability is essentially a regression  problem, but we also consider a discretized version of the target  variable. We thus employ a number of relational classification and regression  methods on the relational representation and compare these to  propositional methods applied to different propositionalisations of the  problem. Some expert comments on the induced theories are also given.  1 Introduction  The persistence of chemicals in the environment (or to environmen...
In this paper, we present the design of a credit-based trading mechanism for peer-to-peer file sharing networks. We divide files into verifiable pieces; every peer interested in a file requests these pieces individually from the peers it is connected to. Our goal is to build a mechanism that supports fair large scale distribution in which downloads are fast, with low startup latency. We build a trading model in which peers use a pairwise currency to reconcile trading differences with each other and examine various trading strategies that peers can adopt. We show through analysis and simulation that peers who contribute to the network and take risks receive the most benefit in return. Our simulations demonstrate that peers who set high upload rates receive high download rates in return, but free-riders download very slowly compared to peers who upload. Finally, we propose a default trading strategy that is good for both the network as a whole and the peer employing it: deviating from that strategy yields little or no advantage for the peer.
Large-scale mobility in the era of embedded and mobile  computers yields significant challenges with respect to  personalization, networking, and security. Currently,  mobile computing devices cannot be seamlessly and flexibly  obtain information about the context in which computing  takes place. This requires dedicated personalization and  localization in combination with advanced autocustomization  techniques in order to adapt them to the  needs of the current context. For natural interaction, for  instance, many devices offer different input and output  modalities, which enable different modes of operation to  perform the same task. Auto-customization of modalities  and devices in range can be obtained, if the system has  sufficient information on the user&apos;s preferences, abilities,  the current situation, and the device that performs a specific  task. To aid the system&apos;s decision, we propose a set of  interrelated and expandable profiles, which are prepared  for automated adaptation, what will play a central part in  making the technology to become invisible.
Display characteristics, network Quality of Service, and the user&apos;s current task all exhibit a wide range of variation when users interact with mobile and ubiquitous devices. It is desirable to enable applications to adapt to these variations. The user&apos;s experience in interacting with the application can be significantly enhanced by adapting the data presented. However, we find that naive degradation of data can quickly result in an unacceptable presentation.
The investigation of single trees in a forest is of ecological and economical interest. One aim is to capture the geometric aspects of a tree: the length and diameter of the trunk and individual branches, the change of the radius along the branch and similar measures. These measures can be determined automatically from terrestrial laser scanner data. The conditions for scanning in the forest, but also the irregular structure and surface of trees aggravate the reconstruction process. The branches of the trees are locally modelled by circular cylinders. With the radius, the axis direction and the axis position the main parameters of interest are captured. We describe a set of algorithms for automatically fitting and tracking cylinders along branches and reconstructing the entire tree. Especially for coniferous trees the computation of an outer hull, giving the extent in different directions and at different heights is an alternative, as the dense foliage coverage renders a distinction between branches and needles impossible. Examples for the different reconstructions of trees are presented.
We propose a simple, direct approach for computing the expected cost of random partial match queries in random quadtrees. The approach gives not only an explicit expression for the leading constant in the asymptotic approximation of the expected cost but also more terms in the asymptotic expansion if desired. 
We present an application of the measure of maximum entropy for credal  sets: as a branching criterion for classification trees based on imprecise probabilities.
Wide-band mobile communication systems are based on the Orthogonal Frequency Division Multiplexing (OFDM) transmission technique for several reasons [1]. The objective of this paper is the design of flexible Data Link Control (DLC) protocols, which combine the information of the time variant radio channel (described by the channel transfer function) and the incoming data streams from different applications with both, constant and variable data rates. The goal is to fulfil the quality of service (QoS) requirements in terms of priority and throughput for all wireless terminals and to improve the average system throughput.
this paper 
This paper examines the linkage between the self-efficacy of public managers to employ information technology (IT) and managerial perceptions of IT effects on the operations of public organizations. A conceptual model posits that computer self-efficacy is influenced by several factors such as organizational support, IT usage within the organization, and experiential knowledge. Furthermore, computer self-efficacy simultaneously affects managerial perceptions of IT impacts on organizational processes. Data from a national study of state program managers is employed to test five hypotheses regarding computer self-efficacy. Two Stage Least Squares is then used to estimate the effect of computer self-efficacy on perceptions of IT impacts. Results indicate that the level of computer self-efficacy is influenced by the availability of IT training and the extent of IT usage within the organization. Computer selfefficacy is also shown to be associated with more positive perceptions of IT effects within public organizations.
The ability of the Material Emission Database and Indoor Air Quality simulation program (&quot;MEDB-IAQ&quot;) to use small chamber test results to model volatile organic compound (VOC) profiles in a newly constructed 2-storey single family home was evaluated. Selected building materials used in the construction of the CCHT (Canadian Centre for Housing Technology) &quot;Reference&quot; house were collected and subjected to chamber testing. VOC emission rates were modeled from the results and entered into MEDB-IAQ. A simulation of the house construction and subsequent operation for an additional nine months was then run with the software package. Air exchange rate data from the house was also used as model input. Predicted VOC results were then compared with air samples taken from the house and analyzed by GC/MS. Prediction accuracy depended on source complexity, source identification and emitting area, and on sink effects. Trends in relative contributions of individual materials could be readily identified.
This paper describes the design of a multilingual speech recognizer using an LVCSR dictation database which has been collected under the project GlobalPhone. This project at the University of Karlsruhe investigates LVCSR systems in 15 languages of the world, namely Arabic, Chinese, Croatian, English, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish. Based on a global phoneme set we built different multilingual speech recognition systems for five of the 15 languages. Context dependent phoneme models are created data-driven by introducing questions about language and language groups to our polyphone clustering procedure. We apply the resulting multilingual models to unseen languages and present several recognition results in language independent and language adaptive setups.  1. Introduction  As the demand for speech recognition systems in multiple languages grows, the development of multilingual systems which combine the phonetic inven...
We obtain the following new coloring results:   A 3-colorable graph on n vertices with maximum degree  can be colored, in polynomial time, using O(( log )    log n) colors. This slightly improves an O((     )  log n) bound given by Karger, Motwani and Sudan. More generally, k-colorable graphs with maximum degree  can be colored, in polynomial time, using O((    1=k  )  log n) colors.
Following conventional wisdom, capital market imperfections are  considered to be an obstacle to long-run growth and prosperity. In  this paper a simple extension to the work of Galor and Zeira (1993)  provides a caveat to this rule of thumb. Admitting lotteries the dynamics  of the model changes fundamentally: the poverty trap found in  the original work vanishes for a wide class of parameters. It turns out  that reducing credit market imperfections does not improve long run  growth perspectives in general. In some cases this creates a poverty  trap and persistent income inequality in the economy.
Abstract  We show in this paper that the algorithm for solving the model checkingproblem  with a nested depth-first search proposed in [1] can interfere with algorithms that  support partial order reduction. We introduce a revised version of the algorithm that  guarantees compatibility. The change also improves the performance of the nested  depth-first search algorithm when partial order reduction is not used.
This paper investigates the determinants of the pattern of banks foreign investment. We extended previous analyses in three directions. First, we use a unique database that includes information on 260 large banks from OECD countries and all their foreign branches and subsidiaries in each one of the other OECD countries. Second, we consider explicitly the role of institutional and regulatory characteristics. Third, we considered within a unified framework a wide set of variables that are likely to influence the pattern of bank internationalization. Consistent with previous research, we find that a high degree of integration between the home and the destination countries has an effect on the location choice of multinational banks. However, we also find that the marginal effect of integration is much lower than that of other explanatory variables. Profit opportunities resulting from a high expected economic growth and the prospect of competing with relatively less efficient banks appear to be a key factor affecting the expansion abroad, especially in the case of subsidiaries. Institutional characteristics of the destination country also play a crucial role. For example, financial centers attract branches of foreign banks, but not subsidiaries, while lower regulatory restrictions on banking activities are associated with a stronger presence of foreign subsidiaries, but not of branches.
Distributed content-based publish-subscribe middleware provides  the necessary decoupling, flexibility, expressiveness, and scalability required by modern distributed applications. Unfortunately, this middleware usually does not provide reliability, especially in the presence of highly reconfigurable scenarios. Indeed, this problem has been thus far largely disregarded by the research community and solutions developed in other contexts are not immediately applicable.
Although the blister model for the Orion Nebula is widely accepted, an alternative model, where the optical appearance of the nebula is primarily determined by self-extinction, continues to be considered by some and has never specifically been disproven. I summarize the characteristics of the two models and show that the evidence falls overwhelmingly against the self-extinction model and in favor of the blister model.
In this paper we study the performance of TCP over mobile ad-hoc networks. We present a comprehensive set of simulation results and identify the key factors that impact TCP&apos;s performance over ad-hoc networks. We use a variety of parameters including link failure detection latency, route computation latency, packet level route unavailability index, and flow level route unavailability index to capture the impact of mobility. We relate the impact of mobility on the different parameters to TCP&apos;s performance by studying the throughput, loss-rate, and retransmission timeout values at the TCP layer. We conclude from our results that existing approaches to improve TCP performance over mobile ad-hoc networks have identified and hence focused only on a subset of the affecting factors. In the process we identify a comprehensive set of factors influencing TCP performance. Finally, using the insights gained through the performance evaluations, we propose a framework called Atra consisting of three simple and easily implementable mechanisms at the MAC and routing layers to improve TCP&apos;s performance over ad-hoc networks. We demonstrate that Atra improves on the throughput performance of a default protocol stack by 50-100%.
We introduce I/O Brush, a new drawing tool aimed at young children, ages four and up, to explore colors, textures, and movements found in everyday materials by &quot;picking up&quot; and drawing with them. I/O Brush looks like a regular physical paintbrush but has a small video camera with lights and touch sensors embedded inside. Outside of the drawing canvas, the brush can pick up color, texture, and movement of a brushed surface. On the canvas, children can draw with the special &quot;ink&quot; they just picked up from their immediate environment. In our preliminary study with kindergarteners, we found that children not only produced complex works of art using I/O Brush, but they also engaged in explicit talk about patterns and features available in their environment. I/O Brush invites children to explore the transformation from concrete and familiar raw material into abstract concepts about patterns of colors, textures and movements.
This paper covers discovery of event, background to understand division operation, SRT algorithm, analyzes the pentium bug and surveys its veri  cations
Quantification of individual magnetic resonance spectroscopy (MRS) signals is possible in the time domain using interactive nonlinear least-squares fitting methods which provide maximum likelihood parameter estimates under certain assumptions or using fully automatic, but statistically suboptimal black-box methods. In kinetic experiments time series of consecutive MRS spectra are measured in which some of the parameters of the signals like e.g. frequencies or dampings are known to remain constant over time. The purpose of this paper is to show how AMARES and HTLS, two representative examples of the previously mentioned methods, can be extended to the simultaneous processing of all spectra in the time series using the common information present in the spectra. We show that this approach yields statistically better results than processing the individual signals separately.
Performing worst-case execution time (WCET) analysis on  the highly portable real-time Java architectures without resulting in the  under utilisation of the overall system has several challenges. Current  WCET approaches are tied to either a particular language or target  architecture. It should also be stressed that most WCET analysis approaches  are usually only considered in relation to procedural programming  languages. In this paper, we propose a comprehensive portable  WCET analysis approach, and demonstrate how Java virtual machine  timing models can be derived effectively on real-time and embedded  Java-based systems.
Space-variant, or foveating, vision architectures are of importance in both machine and biological vision. In this paper we focus on a particular space-variant map, the log-polar map, which approximates the primate visual map and which has been applied in machine vision by a number of investigators during the past two decades. Associated with the log-polar map, we define a new linear integral transform, which we call the exponential chirp transform. This transform provides frequency domain image processing for spacevariant image formats, while preserving the major aspects of the shift-invariant properties of the usual Fourier transform. We then show that a log-polar coordinate transform in frequency (similar to the MellinTransform) provides a fast exponential chirp transform. This provides size and rotation, in addition to shift, invariant properties in the transformed space. Finally, we demonstrate the use of the fast exponential chirp algorithm on a data-base of images in a template matching task, and also demonstrate its uses for spatial filtering. Given the general lack of algorithms in space-variant image processing, we expect that the fast exponential chirp transform will provide a fundamental tool for applications in this area.
This paper evaluates and compares the performance of an existing implementation of these approaches in terms of cost of communication in presence of migration. Based on a Markov chain analysis, we will construct and solve two mathematical models, one for each mechanism and will use them to evaluate the cost of location. For the purpose of validation, we have developed for each mechanism a benchmark that uses ProActive [2], a Java library that provides all the necessary primitives for code mobility. Experiments conducted on a LAN and on a MAN have validated both models and have shown that the location server always performs   Fabrice Huet is also affliated to CNRS, I3S and UNSA
Even though the efficiency of the PCR reaction decreases,  analyses are made in terms of Galton-Watson processes,  or simple deterministic models with constant replication probability  (efficiency). Recently Schnell and Mendoza have suggested  that the form of the efficiency, can be derived from enzyme kinetics. This results in
A sequence u = u 1 u 2 :::u n is said to be nonrepetitive if no two adjacent blocks of u  are exactly the same. For instance, the sequence abcbcba contains a repetition bcbc,  while abcacbabcbac is nonrepetitive. A well known theorem of Thue asserts that  there are arbitrarily long nonrepetitive sequences over the set fa; b; cg. This fact  implies, via König&apos;s Infinity Lemma, the existence of an infinite ternary sequence  without repetitions of any length. In this
The paper presents a simple and effective knowledge-based approach for the task of text classification. The approach uses topic identification algorithm named FIFA to text classification. In this paper the basic process of text classification task and FIFA algorithm are described in detail. At last some results of experiment and evaluations are discussed.
While 2D and 3D vector fields are ubiquitous in computational sciences, their use in graphics is often limited to regular grids, where computations are easily handled through finite-difference methods. In this paper, we propose a set of simple and accurate tools for the analysis of 3D discrete vector fields on arbitrary tetrahedral grids. We introduce a variational, multiscale decomposition of vector fields into three intuitive components: a divergence-free part, a curl-free part, and a harmonic part. We show how our discrete approach matches its well-known smooth analog, called the HelmotzHodge decomposition, and that the resulting computational tools have very intuitive geometric interpretation. We demonstrate the versatility of these tools in a series of applications, ranging from data visualization to fluid and deformable object simulation.
A general statistical method for improving the performance of compiled  code is presented, along with its preliminary application and evaluation to  three sources of non-determinism that arise with natural language processing in  ALE (Carpenter and Penn 1996), a logic programming language based on the  logic of typed feature structures (Carpenter 1992).
In this paper, a study on articulation rate variation within the  prosodic phrase is presented. The dialect under investigation is  south Swedish. South Swedish shares many prosodic  properties with Danish, a language claimed to lack phrasefinal  lengthening. The results of the present study nevertheless  indicate that the articulation rate in south Swedish phrases is  significantly lower in phrase-final words than in preceding  words, and that the reduction in articulation rate between  successive words in the prosodic phrase is not restricted to the  finalpartofthephrase.
Island biogeographers have predicted that in oceanic systems, oblong islands oriented perpendicular to the dispersal paths of organisms should intercept more species and individuals than (1) circular islands of the same size, and (2) oblong islands of equal area oriented parallel to the direction of travel. Landscape ecolo-  gists expect similar relations with habitat patches in a terrestrial matrix. Yet in neither situation is there ade-  quate empirical information topermit conclusions about the prevalence of such effects. Totest the hypothesis that intercept-related patch variables influence community structure on the landscape scale, we studied rela-  tions between the richness and abundance of cavity-nestingbirds and patch shape, size, and orientation rela-  tive to a northerly migration path. The influences of other patch features on nest abundances were removed analytically. Multiple regression indicated that the mean and total number of nesting species, and nest dances for migrants were significantly associated with patch orientation or a patch area x orientation interac-  tion, but not patch shape. Nest abundances for permanent residents were not associated with patch shape or orientation, although area effects, possibly reflecting dispersal interception, were evident. These results are consistent with the hypothesis that stochastic interception of migrating or dispersing organisms influences patch community structure. In addition to richness and abundance effects apparent in this analysis, the sex ratio, age structure, growth rate, social structure, and genetic features of patch populations may also be in-  fluenced. The interception of moving organisms by patches may thus be a key factor influencing population and community persistencein reserves. If so, land...
A common way of localization in robotics is using triangulation on a  system composed of a sensor and some landmarks (which can be artificial or natural). First, 
this paper we focused on a single-user-per-ONU or fiber-to-the-home (FTTH) scenario. If EPON is deployed in a fiber-to-the-curb (FTTC) or fiber-to-the-multi-dwelling-unit (FTTMDU), i.e., if one ONU serves multiple independent users, the requirement to have multiple LLIDs allocated to the ONU becomes even more important. For one reason, the OLT (operator) should have a centralized control over each user access and SLA. Another reason is that usage statistics, such as lost or corrupted frames, throughput, etc. must be maintained per subscriber
This paper may not be considered as the official position of Statistics Sweden. It  expresses the opinion of the author
Image Cube Trajectory (ICT) analysis is a new and robust method to estimate the 3D structure of a scene from a set of 2D images. The motion of points in 3D space is represented by trajectories in an image cube. The advantage of this method is that the motion information of a single 3D point can be represented and analyzed for all available images simultaneously. ICT analysis is based on the definition of an occlusion compatible search strategy in the image cube for known parametrized camera setups. In this paper we derive rules for an optimized sampling of the considered 3D space. They can also be applied to many other 3D reconstruction approaches such as voxel coloring [1]. We will restrict our discussion to a circular moving camera.
In this paper we present a method and an architecture for constructing intelligent simulation systems over the World Wide Web. In order to achieve this, different technologies such as Logic Programming, Object-Oriented Programming and Virtual Reality have been integrated. The proposed approach is illustrated by the presentation of a multi-agent VRML game where agents appear to possess dynamic as well as intelligent behavior. We are currently working on developing a generic tool for providing intelligence to Virtual Reality worlds.
Interlinear text has long been considered a valuable format in the presentation of multilingual data, and a variety of software tools have facilitated the creation and processing of such texts by researchers. Despite the diversity of tools, a common core of editorial functionality is provided. Identifying these core functions has important implications for software engineers who seek to efficiently build tools that support interlinear text editing. While few applications are specifically designed for the creation or manipulation of interlinear text, a number of tools offer varying degrees of incidental support for this modality. In this paper we provide a comprehensive set of critieria upon which the derivation of functional criteria can be based. We describe the basis on which a group of tools was selected for investigation, along with the evaluation criteria. Finally we consolidate our findings into a functional specification for the development of software applications for the editing of interlinear text.
In this paper, we study program transformations called uselesscode  elimination and program slicing in the context of the ff-calculus.
Passive radar is a rapidly emerging technology with many distinct advantages over traditional radar. Its exploitation of &quot;illuminators of opportunity&quot; renders it covert, as well as less expensive. Several passive radar systems, such as Lockheed Martin&apos;s Silent Sentry and John Sahr&apos;s Manastash Ridge Radar at the University of Washington, are already capable of detecting and tracking aircraft. Our goal is to enhance such systems with the addition of automatic target recognition capabilities.
The very low affinity of the T-cell receptor (TCR) for the peptide-major histocompatibility complex (pMHC) has made it very challenging to design assays for testing the functionality of these molecules on small scales, which in turn has severely hampered the progress in developing expression and refolding methodologies for the TCR. We have now developed an ELISA assay for detecting pMHC binding to functional recombinant TCRs. It uses tetramers of biotinylated pMHCs bound to a neutravidin--horseradish peroxidase conjugate and detects the presence of functional TCR, bound in a productive orientation to an immobilized anti-Cb antibody. Specificity can be stringently demonstrated by inhibition with monomeric pMHCs. The assay is very sensitive and specific, and requires only very small amounts of protein. It has allowed us to study the unstable recombinant TCR P14, which we expressed and refolded from Escherichia coli. The TCR P14 is directed against the most abundant epitope of LCMV. We have confirmed the specificity of the interaction by BIAcore, and were able to determine the dissociation constant of the interaction of the P14 TCR and of the gp33-pMHC as 6 mM. This affinity ranks it among the tighter ones of TCR--pMHC interactions, and unusually low affinity thus does not seem to be the cause of the modest protective power of these T-cells, compared to others elicited in the anti-LCMV response. This strategy of multimerizing one partner and immobilizing the other in both a native form and productive orientation should be generally useful for characterizing the weak interactions of cell-surface molecules.  
Formal models for agent design are important for both practical and theoretical reasons. The Constraint-Based Agent (CBA) model includes a set of tools and methods for specifying, designing, simulating, building, verifying, optimizing, learning and debugging controllers for agents embedded in an active environment. The agent and the environment are modelled symmetrically as, possibly hybrid, dynamical systems in Constraint Nets. This paper is an integrated presentation of the development and application of the CBA framework, emphasizing the important special case where the agent is an online constraint-satisfying device. Using formal modeling and specification, it is often possible to verify complex agents as obeying real-time temporal constraint specifications and, sometimes, to synthesize controllers automatically. In this paper, we take an engineering point of view, using requirements specification and system verification as measurement tools for intelligent systems. Since most intelligent systems are real-time dynamic systems, the requirements specification must be able to represent timed properties. We have developed timed 8-automata for this purpose. We present this formal specification, examples of specifying requirements and a general procedure for verification. The CBA model demonstrates the power of viewing constraint programming as the creation of online constraint-solvers for dynamic constraints.
This paper  shows how bilinear models can be used to learn the style-content structure of a  pattern analysis or synthesis problem, which can then be generalized to solve  related tasks using differentstyles andffor content. We focus on three kinds of  tasks: extrapolating the style of data to unseen content classes, classifying data  with known content under a novel style, and translating two sets of data, generated  in differentstyles and with distinct content, into each other&apos;s styles. We  show examples from color constancy, face pose estimation, shape-from-shading,  typography and speech
Evidence is presented which indicates that almost all HCGs are dynamically  associated with generally elongated loose groups of galaxies, and are compact cores  of the latter. Most of those loose groups, the direction of elongation of which lies at  angles close to 45    with the line of sight, were revealed by previous investigations.
This paper defines the fault-tolerant mutual exclusion problem in a  message-passing asynchronous system and determines the weakest failure  detector to solve the problem. This failure detector, which we call the  trusting failure detector, and which we denote by    , is strictly weaker than  the perfect failure detector    but strictly stronger than the eventually  perfect failure detector ffP. The paper shows that a majority of correct  processes is necessary to solve the problem with    .Moreover,T is also the  weakest failure detector to solve the fault-tolerant group mutual exclusion  problem.
Many natural language processing applications make use of hierarchical classifications whilst also having a statistical framework which requires an estimation of the probability distribution over the taxonomy. Data for estimating the probability distributions typically comes from corpora but estimation is complicated by the ambiguity of the data. One application involving such a task is the automatic acquisition of selectional preferences. The method of estimating these class probabilities is crucial to the success of the technique and this paper compares the previous schemes and concludes that correct modelling of class inclusion is essential. Modifications to previous approaches are described which help to curb the effect of ambiguity and avoid overly-general results.
Data cleaning is a vital process that ensures the quality of data stored in real-world databases. Data cleaning problems are frequently encountered in many research areas, such as knowledge discovery in databases, data warehousing, system integration and e-services. The process of identifying the record pairs that represent the same entity (duplicate records), commonly known as record linkage, is one of the essential elements of data cleaning. In this paper, we address the record linkage problem by adopting a machine learning approach. Three models are proposed and are analyzed empirically. Since no existing model, including those proposed in this paper, has been proved to be superior, we have developed an interactive Record Linkage Toolbox named TAILOR. Users of TAI- LOR can build their own record linkage models by tuning   system parameters and by plugging in in-house developed   and public domain tools. The proposed toolbox serves as   a framework for the record linkage process, and is designed in an extensible way to interface with existing and   future record linkage models. We have conducted an extensive experimental study to evaluate our proposed models using not only synthetic but also real data. Results   show that the proposed machine learning record linkage   models outperform the existing ones both in accuracy and   in performance.   1. 
It becomes increasingly important for both network researchers and operators to know the trend of network traffic and to find anomaly in their network traffic. This paper describes an on-going effort within the WIDE project to collect a set of free tools to build a traffic data repository containing detailed information of our backbone traffic. Traffic traces are collected by tcpdump and, after removing privacy information, the traces are made open to the public. We review the issues on user privacy, and then, the tools used to build the WIDE traffic repository. We will report the current status and findings in the early stage of our IPv6 deployment.
In this paper, we review recent developments concerning the application of lowdensity  parity-check (LDPC) codes to the Gilbert-Elliott (GE) channel. Firstly, we  discuss the analysis of LDPC estimation-decoding in these channels using density  evolution. We show that the required conditions of density evolution are satisfied  in the GE channel, and that analysis demonstrates that large potential gains over  the memoryless assumption. We also give results which mitigate the complexity of  characterizing the GE parameter space using DE. Subsequently, we give a design  tool for finding good degree sequences for irregular LDPC codes in the GE channel.
We describe a Markov chain Monte Carlo based particle filter that effectively deals  with interacting targets, i.e., targets that are influenced by the proximity and/or behavior  of other targets. Such interactions cause problems for traditional approaches to  the data association problem. In response, we developed a joint tracker that includes  a more sophisticated motion model to maintain the identity of targets throughout an  interaction, drastically reducing tracker failures. The paper presents two main contributions:  (1) we show how a Markov random field (MRF) motion prior, built on the  fly at each time step, can substantially improve tracking when targets interact, and (2)  we show how this can be done efficiently using Markov chain Monte Carlo (MCMC)  sampling. We prove that incorporating an MRF to model interactions is equivalent to  adding an additional interaction factor to the importance weights in a joint particle  filter. Since a joint particle filter suffers from exponential complexity in the number of  tracked targets, we replace the traditional importance sampling step in the particle filter  with an MCMC sampling step. The resulting filter deals efficiently and effectively  with complicated interactions when targets approach each other. We present both qualitative  and quantitative results to substantiate the claims made in the paper, including  a large scale experiment on a video-sequence of over 10,000 frames in length.
Tonal languages, such as Mandarin, convey information using both phonemes and tones. Using a recently proposed framework for measuring the functional load of a phonological contrast (i.e. how much use a language makes of the contrast), we carry out several computations to estimate how much use Mandarin makes of tones. The most interesting result is that identifying the tone of a syllable is at least as important as identifying the vowels in the syllable. Another computation suggests that the contrast between low and neutral tone carries relatively little information.
This paper reviews the different strategies presented in the literature and discusses various approaches being considered in Canada. One of the suggested options is the use of refuge areas in a building. This option implies that occupants with disabilities do not have to evacuate during a fire; rather they move to an area of refuge where they protect-in-place and will be rescued later. Another option being considered is the provision of safe elevators in highrise buildings. Most of the technical problems required to ensure that such elevators can be operated safely in a fire emergency have been addressed, however, there are still some outstanding human factor issues. A third option is to develop specific evacuation procedures for people with disabilities. The &quot;buddy&quot; system, for example, identifies one or a few persons who have the responsibility of looking after or reporting the presence of a person with limitations in case of an emergency. Another system is to have an available list, for the responding firefighters, of the people who may have problems evacuating. These special evacuation strategies assume that the people with disabilities will be carried out by hand or by using special devices. In assessing the effectiveness of these various life safety strategies for occupants with disabilities, the general opinion is that there is no single life safety option that will solve all of the problems. Most likely, a combination of different options will be used to ensure an acceptable level of life safety for all occupants in a building. The physical layout of a building, the type of occupancy and the characteristics of the occupants are important parameters that should be considered when determining how to provide life safety for all occupants. ii Guylne Proulx, Ph.D., 1...
Recently spectacular improvements in the performance of SAT solvers have been achieved through nogood recording (clause learning). In the CSP literature, on the other hand, nogood recording remains a fairly minor technique for improving backtracking algorithms. In this paper we demonstrate how recent nogood recording techniques from SAT can be generalized to CSPs. The result is a significant enhancement over current nogood recording techniques used in CSPs. We also report on some preliminary empirical results which indicate that generalized nogood recording can have a significant performance benefit.
We present a novel framework for hierarchical collision detection that can be applied to virtually all bounding volume (BV) hierarchies. It allows an application to trade quality for speed. Our algorithm yields an estimation of the quality, so that applications can specify the desired quality. In a time-critical system, applications can specify the maximum time budget instead, and quantitatively assess the quality of the results returned by the collision detection afterwards. Our
We present an expected polynomial time algorithm to generate a labeled  planar graph uniformly at random. To generate the planar graphs, we derive  recurrence formulas that count all such graphs with n vertices and m edges, based  on a decomposition into 1-, 2-, and 3-connected components. For 3-connected  graphs we apply a recent random generation algorithm by Schaeffer and a counting  formula by Mullin and Schellenberg.
Introduction  Finely focused electron beams are used in lithographic processes to expose polymer resist layers. Very complex device patterns with high resolution nanometer size can be created. Such resolution is in general superior to presently available optical lithography techniques because the electron probe size may be much smaller than the corresponding diffraction limited image in optical lithography.  In any lithographic process, the resist image is the important item. Therefore it is very important to understand how that resist image is produced and what factors are influencing the quality of the image.  In order to utilise EBL successfully in nanometer range, the interaction and scattering of electrons within the resist layer and the underlying substrate must be well understood. For example, the effects of beam energy, type of resist, resist thickness, substrate type, and several other variables are critical in producing an optimal pattern in the resist. The resolution obtaina
This paper shows that differences in industry concentration help explain the crosssection  of average stock returns. Firms in concentrated industries earn lower returns,  even after controlling for size, book-to-market and momentum. The premium for industry  concentration exhibits systematic business cycle variation. In addition, the premium on  book-to-market is higher in more concentrated industries. Standard explanations based  on measurement error, deregulation, capital structure, and correlation with known risk  factors do not explain these findings. We hypothesize that this occurs because either (i)  barriers to entry in highly concentrated industries insulate firms from aggregate shocks  that lead to economic distress, or (ii) firms in highly competitive industries are riskier  because they engage in more innovative activities and thus command higher expected returns.
The evolution of database management systems (DBMS) leans towards an increasing structuration  of the data models used. Recent standards like SQL3 and OQL go beyond the all-relational  world integrating more and more features from the object-oriented paradigm. This urges the data  mining community to shift its research focus on the tools and techniques that fit the complex  structure of the data. However, major results on the subject are missing. In this paper we try to  summarize the features of structured data that challenge the existing mining techniques. We also  suggest possible solutions inspired from a recent work on mining complex object datasets. The  key element of our approach is an object metrics that enables clustering of complex objects for  class design purposes. With its definition based on recursive decomposition of objects into simple  values, the metric fits objects of arbitrary complex structure. We believe the approach can be  generalized to other mining techniques and to a broader range of structured data models.
In this paper, an efficient algorithm to simultaneously implement array  alignment and data/computation distribution is introduced and evaluated. We
We study smoothing properties of discretizations of a linear parabolic initial boundary problem with a possibly non-selfadjoint elliptic operator. The solution at time t ? 0 of this problem, as well as its time derivatives, are in L r for initial values in L s even when r ? s.
We model the power fluctuation as cycle-to-cycle power gradient and minimize the mean of the power gradients using ILP. We propose scheduling schemes for three modes of datapath design : single supply voltage and single frequency (SVSF), multiple supply voltages and dynamic frequency clocking (MVDFC), and multiple supply voltages and multicycling (MVMC). Various experiments are conducted on selected highlevel synthesis benchmarks. Experimental results in terms of several parameters, such as mean power gradient, mean cycle power, peak power, and power delay product, are presented.
This paper looks upon the standard genetic algorithm as an artificial self-organizing process. With the purpose to provide concepts that make the algorithm more open for scalability on the one hand, and that fight premature convergence on the other hand, this paper presents two extensions of the standard genetic algorithm without introducing any problem specific knowledge, as done in many problem specific heuristics on the basis of genetic algorithms. In contrast to contributions in the field of genetic algorithms that introduce new coding standards and operators for certain problems, the introduced approach should be considered as a heuristic appliable to multiple problems of combinatorial optimization, using exactly the same coding standards and operators for crossover and mutation, as done when treating a certain problem with a standard genetic algorithm. The additional aspects introduced within the scope of segregative genetic algorithms (SEGA) are inspired from optimization as well as from the views of bionics. In the present paper the new algorithm is discussed for the travelling salesman problem (TSP) as a well documented instance of a multimodal combinatorial optimization problem. In contrast to all other evolutionary heuristics that do not use any additional problem specific knowledge, we obtain solutions close to the best known solution for all considered benchmark problems (symmetric as well as asymmetric benchmarks) which represents a new attainment when applying evolutionary computation to the TSP.
The advancement in the technology has given us the new and most  powerful communication medium, Internet. It has opened up many  new avenues and also has posed dierent problems. These can be categorized into two broad categories, namely problems regarding storage  and problems regarding communication. This can be explained in scienti  c terms as computer-supported build-up of mathematical knowledge  and computer-supported retrieval of mathematical knowledge. In this paper
Through the use of a recently developed taxonomy of information systems implementation, a new research method is developed. The Information Systems Implementation Research Method (ISI-RM) allows the examination of IS implementation cases in a new way that leads to results that are standardized for easier adoption by variance researchers. The paper examines the use of ISI-RM on a single case study. While untested in this paper, the ISI-RM may also be used as a metaanalytic tool, extracting important factors from a multitude of existing case studies.
With the availability of full-scale genome of various organisms, one of the recent bioinformatics challenges
is to accurately assign gene products into their functional classes. Standard bioinformatics tools such as
detecting sequence homology by using PSI-BLAST [1] and FASTA [3] provide initial hints to the experimental
determination of function.
At the abstract level, protein functional class prediction can be regarded as mapping a sequence to its
biological function(s). In the field of machine learning, the annotation of gene products can be viewed as a
standard classification problem. For some multi-class classification problems, the set of positive examples is
very small compared to the set of negative examples; this is the common scenario in the functional annotation
problem where there exist a lot of functional classes but the number of the examples (protein sequences) in each
class is relatively low. This imbalanced proportion of examples in each class contributes to the poor
performance of standard machine learning techniques (e.g. decision trees). These approaches tend to produce a
strong discrimination classifier (high overall predictive accuracy) with very low sensitivity (positive coverage)
when learning on these types of problems.
In this paper we propose a novel machine learning approach POS-SET, that generates a classifier with a
better sensitivity, while not loosing too much positive prediction accuracy. We present the general framework of
POS-SET and describe its application to learn sets of positive rules in classifying protein functional classes of
P.gingivalis.
We enumerate ternary length-ff square-free words, which are words avoiding  squares of all words up to length ff,forff    24. We analyse the singular behaviour  of the corresponding generating functions. This leads to new upper entropy bounds  for ternary square-free words. We then consider ternary square-free words with  fixed letter densities, thereby proving exponential growth for certain ensembles with  various letter densities. We derive consequences for the free energy and entropy of  ternary square-free words.
This paper describes an approach to visualizing Z specifications  based on Formal Concept Analysis (FCA). The approach takes  a source specification written in L    T E X and produces a formal context  representing the static structure of the specification. The corresponding  formal concept lattice can be used to investigate and explore various  properties of the specification. The line diagram does not replace, but  is intended to be used in conjunction with, the original Z specification.
We apply eigenvalue techniques for cut evaluation to produce relations  between the weight and order of induced subgraphs, and apply these results  to bound the stability number.
Structural risk minimisation (SRM) is a general complexity regularization method which automatically selects the model complexity that approximately minimises the misclassification error probability of the empirical risk minimiser. It does so by adding a complexity penalty term ff(m, k) to the empirical risk of the candidate hypotheses and then for any fixed sample size m it minimises the sum with respect to the model complexity variable k.
The compactness of a routing table is a complexity measure of the memory space needed to store the routing table on a network whose nodes have been labelled by a consecutive range of integers. It is defined as the smallest integer k such that, in every node u, every set of labels of destinations having the same output in the table of u can be represented as the union of k intervals of consecutive labels. While many works studied the compactness of deterministic routing tables, few of them tackled the adaptive case when the output of the table, for each entry, must contain a fixed number  of routing directions. We prove that every n-node network supports shortest path routing tables of compactness at most n= for an adaptiveness parameter , whereas we show a lower bound of n=    .
Neural network models of categorical perception (compression of within-category similarity and dilation of between-category differences) are applied to the symbol-grounding problem (of how to connect symbols with meanings) by connecting analog sensorimotor projections to arbitrary symbolic representations via learned category-invariance detectors in a hybrid symbolic/nonsymbolic system. Our nets are trained to categorize and name 50x50 pixel images (e.g., circles, ellipses, squares and rectangles) projected onto the receptive field of a 7x7 retina. They first learn to do prototype matching and then entry-level naming for the four kinds of stimuli, grounding their names directly in the input patterns via hidden-unit representations (&quot;sensorimotor toil&quot;). We show that a higher-level categorization (e.g., &quot;symmetric&quot; vs. &quot;asymmetric&quot;) can learned in two very different ways: either (1) directly from the input, just as with the entry-level categories (i.e., by toil), or (2) indirectly, from boolean combinations of the grounded category names in the form of propositions describing the higher-order category (&quot;symbolic theft&quot;). We analyze the architectures and input conditions that allow grounding (in the form of compression/separation in internal similarity space) to be &quot;transferred&quot; in this second way from directly grounded entry-level category names to higher-order category names. Such hybrid models have implications for the evolution and learning of language.
Ubiquitous computing promotes physical spaces with hundreds of specialized embedded devices that increase our productivity, alleviate some specific everyday tasks and provide new ways of interacting with the computational environment. Because the computational environment is spread across the physical space, personal computers lose the focus of attention. Therefore, the users&apos; view of the computational environment is finally extended beyond the physical limits of the computer. Physical spaces become computer systems, or in other terms, Active Spaces. However, these Active Spaces require novel system software capable of seamlessly coordinating their hidden complexity. Our goal is to extend the model provided by current computer systems to allow interaction with physical spaces and their contained entities (physical and virtual) by means of a single abstraction called Active Space.
Dimensionality reduction of empirical co-occurrence data is a fundamental problem in unsupervised  learning. It is also a well studied problem in statistics known as the analysis of cross-classified  data. One principled approach to this problem is to represent the data in low dimension with minimal  loss of (mutual) information contained in the original data. In this paper we introduce an  information theoretic nonlinear method for finding such a most informative dimension reduction.
Irreversible computation necessarily results in energy dissipation due to information loss. While small in comparison to the power consumption of today&apos;s VLSI circuits, if current trends continue this will be a critical issue in the near future. Reversible circuits offer an alternative that, in principle, allows computation with arbitrarily small energy dissipation. Furthermore, reversible circuits are essential components of quantum logic. We consider the problem of testing these circuits, and in particular, generating efficient test sets. The reversibility property significantly simplifies the problem, which is generally hard for the irreversible case. We discuss conditions for a test set to be complete, give a number of practical constructions, and consider test sets for worst-case circuits. In addition, we formulate the problem of finding minimal test sets into an integer linear program (ILP) with binary variables. While this ILP method is infeasible for large circuits, we show that combining it with a circuit decomposition approach yields a practical alternative.
Simulation is widely used to estimate losses due to default and other credit events in financial portfolios. The challenge in doing this efficiently results from (i) rareevent aspects of large losses and (ii) complex dependence between defaults of multiple obligors. We discuss importance sampling techniques to address this problem in two portfolio credit risk models developed in the financial industry, with particular emphasis on a mixed Poisson model. We give conditions for asymptotic optimality of the estimators as the portfolio size grows.
We have carried out a survey to search for hidden blazars in a sample of z2 radio{loud quasars. The idea is based on our prediction that we should be able to see large C IV line variability not associated with observed continuum variations or most other emission lines in every radio{loud quasar. Here we report the initial results including the discovery of large C IV line variations in two quasars.
Clustering algorithms are exploratory data analysis tools that have proved to be essential for gaining valuable insights on various aspects and relationships of the underlying systems. In this paper we present gCLUTO, a stand-alone clustering software package which serves as an easy-to-use platform that combines clustering algorithms along with a number of analysis, reporting, and visualization tools to aid in interactive exploration and clustering-driven analysis of large datasets. gCLUTO provides a wide-range of algorithms that operate either directly on the original feature-based representation of the objects or on the object-to-object similarity graphs and are capable of analyzing different types of datasets and finding clusters with different characteristics. In addition, gCLUTO implements a project-oriented work-flow that eases the process of data analysis.
The most influential factors in the quality of the solutions found by an evolutionary algorithm are a correct coding of the search space and an appropriate evaluation function of the potential solutions. The coding of the search space for the obtaining of decision rules is approached, i.e., the representation of the individuals of the genetic population. Two new methods for encoding discrete and continuous attributes are presented. Our &quot;natural coding&quot; uses one gene per attribute (continuous or discrete) leading to a reduction in the search space. Genetic operators for this approached natural coding are formally described and the reduction of the size of the search space is analysed for several databases from the UCI machine learning repository.
This article develops some aspects of Anatol Vieru&apos;s compositional technique based on finite difference calculus on periodic sequences taking values in a cyclic group. After recalling some group-theoretical properties, we focus on the decomposition algorithm enabling to represent any periodic sequence taking values in a cyclic group as a sum of a reducible and a reproducible sequence. The implementation of this decomposition theorem in a computer aided music composition language, as OpenMusic [1] , enables to easily verify if a given periodic sequence is reducible or reproducible. In this special case, one of the two factors will be identically zero. This means that every periodic sequence has in itself a certain degree of reducibility and reproducibility. We also suggest how to use this result in order to explain some regularities of distribution of numerical values in the case of the finite addition process and how to generalize the decomposition theorem by means of the Fitting Lemma. This opens the problem of the musical relevance of a generalized module-theoretical approach in Vieru&apos;s theory of periodic sequences.
RE 1. a: An image of a vase moving to the right. b: A sequence of frames may be piled up to form a flip book; time is the third dimension. c: A skeleton view of the (x,y,t) volume helps suggest its structure. d: The space-time volume may be sliced to illustrate the fact that the motion is equivalent to spatio-temporal orientation. e: In the case of continuous motion, the volume is densely filled. The moving vase traces out an extruded shape that is sheared due to the motion.  O  (a)  (b) (c)  (d) (e)  x  t  y  x  t  y  x  t  y  x  To get a better understanding of the structure of the spacetime volume, we can cut a slice through it in an (x,t) plane, as illustrated in Figure 1d. The vase traces out an extruded shape that is sheared due to the motion. In the case of continuous motion, the spatio-temporal volume is densely filled, as shown in Figure 1e. The (x,t) slice is slanted as a result of the rightward motion.  Figure 2a shows an (x,y,t) volume taken from a video sequence showing a 
We derive a Berry-Esseen bound, essentially of the order of the square of the standard deviation, for the number of maxima in random samples from $(0, 1)^d$. The bound is, although not optimal, the first of its kind for the number of maxima in dimensions higher than two. The proof uses Poisson processes and Stein&apos;s method. We also propose a new method for computing the variance and derive an asymptotic expansion. The methods of proof we propose are of some generality and applicable to other regions such as d-dimensional simplex.
... This article presents the design and implementation of a user interface for DISCIPLE, a platform-independent telecollaboration framework. DISCIPLE supports sharing of Java components that are imported into the shared workspace at run-time and can be interconnected into more complex components. As a result, run-time interconnection of various components allows user tailoring of the human-computer interface. Software architecture for customization of both a group-level and application-level interfaces is presented, with interface components that are loadable on demand. The architecture integrates the sensory modalities of speech, sight, andtouch.Insteadofimposingone &quot;right&quot; solutionontousers,theframework lets users tailor the user interface that best suits their needs. Finally, laboratory experience with DISCIPLE tested on a variety of applications with the framework is discussed along with future research directions.
The proliferation of incompatible e-commerce systems applying different  security technologies imposes difficult choices on all the concerned parties.
Human Identification using gait is a challenging computer vision task due to the dynamic motion of gait and the existence of various sources of variations such as viewpoint, walking surface, clothing, etc. In this paper we propose a gait recognition algorithm based on bilinear decomposition of gait data into time-invariant gait-style and timedependent gait-content factors. We developed a generative model by embedding gait sequences into a unit circle and learning nonlinear mapping which facilitates synthesis of temporally-aligned gait sequences. Given such synthesized gait data, bilinear model is used to separate invariant gait style which is used for recognition. We also show that the recognition can be generalized to new situations by adapting the gait-content factor to the new condition and therefore obtain corrected gait-styles for recognition.
This paper discusses the behaviour of lightweight steel framed (LSF) unrestrained floors, protected with gypsum board ceilings, in five standard fire resistance tests. Parameters investigated in this test series were joist spacing, number of gypsum board layers in the ceiling membrane, floor cavity insulation and presence of concrete topping in the sub-floor. The fire resistance of LSF floors appears to be essentially governed by the ability of gypsum board to remain in place under fire exposure; other factors are of secondary importance. Retrospective numerical thermal-structural simulations of these tests show good agreement with measured temperature and deformation histories. The development of floor deflections is governed by the thermal bowing of steel joists except for the last one or two minutes in the tests, when &quot;run-away&quot; deformations develop due to the formation of inelastic hinges near mid-span. Evaluation of bending moment resistance of heated joists using current design provisions for cold formed steel structures, adjusted to account for the deterioration of strength and stiffness of steel at elevated temperatures, leads to conservative and fairly accurate predictions of fire resistance.
A key aspect of the autonomy of living things is their ability to find and use sources of energy in the natural environment. Clearly, any comprehensive attempt at producing artificial life should demonstrate an equivalent capability; equally clearly, so should any truly autonomous robot. To date, both Alife agent simulations and robotic implementations have used environments and energy sources much too simple or structured to allow such equivalence to be claimed. This paper describes recent progress on an attempt to break free of these limitations by developing the world&apos;s first artificial predator -- a robot which lives free on agricultural land, hunting and catching slugs, and fermenting the corpses to produce the biogas which is its sole source of energy.
A system of coordinated projectors and cameras enables the creation of projected light displays that are robust to environmental disturbances. This paper describes approaches for tackling both geometric and photometric aspects of the problem: (1) the projected image remains stable even when the system components (projector, camera or screen) are moved; (2) the display automatically removes shadows caused by users moving between a projector and the screen, while simultaneously suppressing projected light on the user. The former can be accomplished without knowing the positions of the system components. The latter can be achieved without direct observation of the occluder. We demonstrate that the system responds quickly to environmental disturbances and achieves low steady-state errors.
Operational semantics is often presented in a rather syntactic  fashion using relations specified by inference rules or equivalently by  clauses in a suitable logic programming language. As it is well known,  various syntactic details of specifications involving bound variables can  be greatly simplified if that logic programming language has term-level  abstractions (-abstraction) and proof-level abstractions (eigenvariables)  and the specification encodes object-level binders using -terms and universal  quantification. We shall attempt to extend this specification setting  to include the problem of specifying not only relations capturing  operational semantics, such as one-step evaluation, but also properties  and relations about the semantics, such as simulation. Central to our  approach is the encoding of generic object-level judgments (universally  quantified formulas) as suitable atomic meta-level judgments. We shall  encode both the one-step transition semantics and simulation of (finite)  -calculus to illustrate our approach.
Access to cultural exhibits is a central issue in museums and exhibition galleries that is recently approached under a new, technological perspective. Although the cultural industries practices in the cases of museums and cultural exhibits have remained practically unchanged for long, in recent years we are witnessing a gradual adoption of media-technologies in various aspects, such as collections archiving and digital document preservation, media- and Web-presentation, graphical animations, etc. Lately, Internet and Web-based technologies have been employed for providing access, mostly to images of exhibited objects. In few cases, the incorporation of higher-end technology, such as virtual reality, artificial intelligence, or robotics, is explored. In this paper we present such an effort, the TOURBOT project (an acronym for TOUr-guide RoBOT), which emphasizes the development of alternative ways for interactive museum telepresence, essentially through the use of robotic &quot;avatars&quot;, and comment on the experience gained from its use in a museum setting.
used measures of scalp temperature for the early diagnosis of brain lesions, as well as for functional imaging in normal subjects, which are today the two main applications of techniques such as magnetic resonance imaging.    His imaging ideas were inspired from studies at the time that measured skin temperature to infer the localisation and mechanism of arterial lesions of the limbs, and to guide amputation whenever necessary.  In order to apply this approach to the field of neurology, Broca devised a &quot;thermometric crown, allowing the simultaneous application of six thermometers around the head. It is made of a series of small identical cotton pockets, tied together with a circular band of elastic material, with thermometers placed inside the pockets&quot;.    He later improved this apparatus by means of two additional thermometers critically located over the inferior frontal gyrus &quot;which is assigned to language&quot;.  Broca was well aware that &quot;the dura mater, the skull, the scalp constitute
In this paper we investigate the behavior of the  discrete time AR (Auto Regressive) representations over a finite time  interval, in terms of the finite and infinite spectral structure of the  polynomial matrix involved in the AR-equation. A boundary mapping  equation and a closed formula for the determination of the solution, in  terms of the boundary conditions, are also given.
We present a new scheme for digital watermarking of point-sampled geometry based on spectral analysis. By extending existing algorithms designed for polygonal data to unstructured point clouds, our method is particularly suited for scanned models, where the watermark can be directly embedded in the raw data obtained from the 3D acquisition device. To handle large data sets efficiently, we apply a fast hierarchical clustering algorithm that partitions the model into a set of patches. Each patch is mapped into the space of eigenfunctions of an approximate Laplacian operator to obtain a decomposition of the patch surface into discrete frequency bands. The watermark is then embedded into the low frequency components to minimize visual artifacts in the model geometry. During extraction, the target model is resampled at optimal resolution using an MLS projection. After extracting a watermark from this model, the corresponding bit stream is analyzed using statistical methods based on correlation. We have applied our method to a number of point-sampled models of different geometric and topological complexity. These experiments show that our watermarking scheme is robust against numerous attacks, including low-pass filtering, resampling, affine transformations, cropping, additive random noise, and combinations of the above.
Sludge from the wastewater treatment of frozen seafood factory (The Union Frozen Products Co., Ltd.) is used to substitute chemical fertilizer (15-15-15) for vegetable production. Growth responses of water convolvulus (Ipomoea aguatica), green kuang futsoi (Brassica chinensis), Chinese kale (Brassica alboglabra) and lettuce (Lactuca  sativa) to different rates of dry sludge were investigated. Sludge at the rates of 3.75, 7.5, and 11.25 t ha   for each treatment and chemical fertilizer (15-15-15) at the rate of 3.75 t   was incorporated into Kamphaeng Saen Soil Series (Typic Haplustalfs, fsi, mixed). From this study, the dry sludge from this factory demonstrated the main properties as follows: pH 5.25, CEC 112.57 meq/100 mg, organic matter 26.35%, total nitrogen 13.73%, available phosphorus 513.95 mg kg   , and soluble potassium 0.87 mg kg   results of plant production showed that the dry sludge provided higher vegetative growth rate of plants than the chemical fertilizer. Dry sludge at the rate of 11.25 t ha   gave the best yield of water convolvulus and green kuang futsoi i.e., 1.68 and 6.05 g/plant, respectively, whereas, the rate of 7.5 t ha   gave the best yield of chinese kale and the rate of 3.75 t ha   gave the best yield of lettuce i.e., 2.46 and 5.12 g/plant, respectively. Moreover, it can increase macronutrient and also organic matter in soil. From the study results it can be concluded that this sludge can be used as the organic fertilizer for leafy vegetable production.
The automatic distinction of arguments and modifiers is a necessary step for the automatic acquisition of subcategorisation frames and argument structure. In this work, we report on supervised learning experiments to learn this distinction for the difficult case of prepositional phrases attached to the verb. We develop statistical indicators of linguistic diagnostics for argumenthood, and we approximate them with counts extracted from an annotated corpus. We reach an accuracy of 86.5%,over a baseline of 74% , showing that this novel method is promising in solving this difficult problem.
A pattern-based approach to the presentation, codification  and reuse of property specifications for finite-state verification was proposed  by Dwyer and his colleagues in [4, 3]. The patterns enable nonexperts  to read and write formal specifications for realistic systems and  facilitate easy conversion of specifications between formalisms, suchas  LTL, CTL, QRE. In this paper we extend the pattern system with events -- changes of values of variables in the context of LTL.
this paper was partially supported by the Special Research Program SFB F1104 AURORA  of the Austrian Science Fund. 1 Contents 1 
With the recent technological feasibility of electronic commerce over the Internet, much attention has been given to the design of electronic markets for various types of electronically-tradable goods. Such markets, however, will normally need to function in some relationship with markets for other related goods, usually those downstream or upstream in the supply chain. Thus, for example, an electronic market for rubber tires for trucks will likely need to be strongly influenced by the rubber market as well as by the truck market. In this paper we design...
In this paper, the modified S-curve membership function methodology is used in a real life industrial  problem of mix product selection. This problem occurs in production planning management where by a  decision maker plays an important role in making decision in a fuzzy environment. As an analyst, we try to  find a good enough solution for the decision maker to make a final decision. An industrial application of FLP  through the S-curve membership function has been investigated using a set of real life data collected from a  Chocolate Manufacturing Company. The problem of fuzzy product mix selection has been defined. The  objective of this paper is to find an optimal units of products with higher level of satisfaction with vagueness  as a key factor. This problem has been considered because all the coefficient such as technical and resource  variables are uncertain. This is considered as one of sufficiently large problem involving 29 constraints and 8  variables. Since there are several decisions that were to be taken, a table for optimal units of products respect  to vagueness and degree of satisfaction has been defined to identify the solution with higher level of units of  products and with a higher degree of satisfaction. It is to be noted that higher units of products need not lead  to higher degree of satisfaction. Optimal units of products and satisfactory level have been computed using  FLP approach. The fuzzy outcome shows that higher units of products need not lead to higher degree of  satisfaction. The findings of this work indicates that the optimal decision is depend on vagueness factor in  the fuzzy system of mix product selection problem. Further more the high level of units of products obtained  when the vagueness in the system is low.     Keywords: Unc...
this technical convenience. However, the most important result of this book is that the two senses coincide; we will prove that in the section after this one
The principle of fractal image coding presented in this paper is based on the theory of L-IFS (Local Iterated Function Systems). The algorithm exploits the fact that a real-world image  is formed approximately of transformed copies of parts of itself. Thus, the construction of fractal codes is directly made on partitions of the image support. It is based on piece-wise  similarities between blocks of different sizes.
The problem of context has a long tradition in dierent areas of arti  cial intelligence (AI). However, formalizing context has been widely  discussed only since the late 80s, when J. McCarthy argued that formalizing  context was a crucial step toward the solution of the problem  of generality. Since then, two main formalizations have been proposed  in AI: Propositional Logic of Context (PLC) and Local Models Semantics  /MultiContext Systems (LMS/MCS). In this paper, we propose  the  rst in depth comparison between these two formalizations, both  from a technical and a conceptual point of view. The main technical  result of this paper is the formal proof of the following facts: (i) that  PLC can be embedded into a particular class of MCS, called MPLC;  (ii) that MCS/LMS cannot be embedded in PLC using only lifting  axioms to encode bridge rules, and (iii) that, under some important  restrictions (including the hypothesis that each context has  nite and  homogeneous propositional languages), MCS/LMS can be embedded  in PLC with generic axioms. The last part of the paper contains a  comparison of the epistemological adequacy of PLC and MCS/LMS  for the representation of the most important issues about contexts.
In this report we propose a new coding scheme based on the Matching Pursuit algorithm and exploiting some of  the new features introduced by H.264 for motion estimation. Main points of this work are the design of a redundant  dictionary suitable for coding displaced frame differences, the use of fast techniques for atom selection, which work in  the Fourier domain and exploit the spatial localization of the atoms, the adaptive coding scheme aimed at optimizing  the resouce allocation for transmitting the atom parameters and the Rate-Distortion optimization.
A polynomial SDP (semidefinite program) minimizes a polynomial objective function over a feasible region described by a positive semidefinite constraint of a symmetric matrix whose components are multivariate polynomials. Sums of squares relaxations developed for polynomial optimization problems are extended to propose sums of squares relaxations for polynomial SDPs with an additional constraint for the variables to be in the unit ball. It is proved that optimal values of a sequence of sums of squares relaxations of the polynomial SDP, which correspond to duals of Lasserre&apos;s SDP relaxations applied to the polynomial SDP, converge to the optimal value of the polynomial SDP. The proof of the convergence is obtained by fully utilizing a penalty function and a generalized Lagrangian duals that were recently proposed by Kim et al for sparse polynomial optimization problems. Key words.
This document is a revised version, where several mistakes and typos have been corrected. We would like to thank Prof. Raymond Devillers for a careful review of the original document that allowed to highlight these mistakes and typos
this paper we consider a SNOW where parallel jobs are run in an opportunistic fashion as in Condor. In such an environment, workstation owner processes have preemptive priority over batched parallel programs. Owner process workstation reclamations stop execution of the batch job task and hence may significantly impact the parallel job response time
This work addresses symmetrization of traveling waves for systems of conservation laws via an Hamiltonian formalism. We propose a functional whose minimum is obtained at the traveling wave : a condition is that the dispersion is non zero and in some sense negligible with respect to the dissipation. The Hamilton-Jacobi equation is given.
This report gives an overview about the measurement  utilities available for the 1    trial and their use.
We study a type system equipped with universal types and equirecursive types, which we refer to as F . We show that type equality may be decided in time O(n log n), an improvement over the previous known bound of O(n    ). In fact, we show that two more general problems, namely entailment of type equations and type unification, may be decided in time O(n log n), a new result. To achieve this bound, we associate, with every F type, a first-order canonical form, which may be computed in time O(n log n). By exploiting this notion, we reduce all three problems to equality and unification of first-order recursive terms, for which effcient algorithms are known.
This paper provides an introduction of our NSF-funded research project on advancing digital imagery technologies for Asian art and cultural heritages. This international collaborative research project aims at developing technologies related to the preservation, retrieval, and dissemination of digital imagery. Researchers in the US, China, and South Korea will collectively investigate and develop technologies for acquiring, browsing, managing, and searching large collections of high quality art images. One of the main research questions the team of US researchers focuses on is the problem of automatic indexing and retrieval of digital art images. Building on the foundation of a successful image retrieval platform, the SIMPLIcity system with the ALIP algorithm, the team is developing techniques to automatically associate linguistic terms with image features for indexing Asian art images. The testbed databases of art images for this research project in the US will begin by using some of the rich image resources of the Emperor and the Chinese Memory Net projects by Ching-chih Chen. This image knowledge base consist of high quality scans, with extensive metadata information including detailed keyword information, as well as comprehensive textual descriptions. The research work aims at demonstrating that (1) modern machine learning and statistical data mining tools are capable of learning from non-structured or semi-structured input data suchashuman annotations, (2) statistical image modeling techniques can be used in automatic linguistic indexing and concept dictionary building. Finally,we discuss the challenges and the importance for the line of interdisciplinary researchwork.
This paper presents new measurements of the magnetic field component  of naturally-produced electromagnetic radiation in the ELF/VLF  range. The measurements are compared to calculations based on  modal propagation theory. The nocturnal spectrum below 5 kHz often  contains a sharp increase in spectral level at the cutoff frequency of  the first mode (ff1700 Hz), and a distinct variation of the spectral behavior  at the cutoff frequency of the second mode (ff3400 Hz), where  the indicated cutoff frequencies were calculated for perfectly conducting  earth and ionosphere, with the ionosphere 88 km above the earth. These
This report documents our experience with different optical flow estimation methods and our attempt to use optical flow both qualitatively and quantitatively. Special attention is devoted to improving the Lucas-Kanade method to obtain dense flow. We use a simple clustering technique to find looming objects. This method has the potential of supporting obstacle avoidance using optical flow. Experiments using real images demonstrate that this simple clustering is effective for certain scenes. We also point out when this technique will fail. We try to use optical flow quantitatively to recover the structure of a piecewise planar environment. First, we use the widely-known 8-parameter planar flow equations to locate individual planes in the scene. Second, in lieu of full flow, we try to use normal flow to compute both the ego-motion and the structure. Both trials fail ungracefully, mostly due to noisy flow data. We describe the mathematics of both methods and our experimental results.
They identify the full or solid with &quot;what is,&quot; and the void or rare with &quot;what is not&quot; (hence they hold that &quot;what is not&quot; is no less real than &quot;what is&quot;) [Arist. Metaphys. A 4 985b4]  J. Needham joined explicitly with Berthelot in remarking the atomism does not appear anywhere in the alchemical treatises, neither in the East nor in the West. Which was puzzling, especially because western alchemists claimed to follow Democritus. In general it was noticed that other Western philosophical schools can be mirrored in the East, but atomism seems to be absent.
Chloroplast DNA in higher plants exist as closed circular molecules of about 150 kb (30), usually presenting inverted repeat sequences separating two single copy regions [1]. It is available the complete chloroplast genomes of around 13 higher plants species available in the gene bank. Our group has completely sequenced the sugarcane chloroplast DNA which is 141182 nucleotides in size. We have used bioinformatics tools Phre, Phrap &amp; Consed [2,3,4] to perform the quality analysis of the nucleotides as well as to assemble the complete sequence and to identify the ORFs for annotation and to allow both, organizational and evolutionary analyses by comparison with the plastid genomes available [5]. By comparative analyses among sugarcane plastome, it was observed that all maize functional gene groups and also ycfs (conserved putative plastid genes with unknown function) found in other species plastome were identified in sugarcane. Also, comparisons based on structural organization and gene expression regulation analysis by mRNA editing [6,7] showed higher similarity between sugarcane and maize plastomes than between sugarcane and rice ones. The results suggest a common evolution among C 4 grasses and also yield new elements for research on this type of photosynthesis and metabolism, as well as for sugarcane chloroplast transformation technology.
In this paper we proposed a visual speech recognition network based on Support Vector Machines. Each word of the dictionary is modeled by a set of temporal sequences of visemes. Each viseme is described by a support vector machine, and the temporal character of speech is modeled by integrating the support vector machines as nodes into Viterbi decoding lattices. Experiments conducted on a small visual speech recognition task show a word recognition rate on the level of the best rates previously reported, even without training the state transition probabilities in the Viterbi lattices and using very simple features. This proves the suitability of support vector machines for visual speech recognition.
We report initial results of exposing low-Z solid and high-Z liquid targets to 150-ns, 410    proton pulses with spot sizes on the order of 1 to 2 mm. The energy deposition density approached 100 J/g. Diagnostics included fiberoptic strain sensors on the solid target and high-speed photography of the liquid targets. This work is part of the R&amp;D program of the Neutrino Factory and Muon Collider Collaboration.
A restarted symplectic Lanczos method for the Hamiltonian eigenvalue problem is presented. The Lanczos vectors are constructed to form a symplectic basis. Breakdowns and near-breakdowns are overcome by inexpensive implicit restarts. The method is used to compute eigenvalues, eigenvectors and invariant subspaces of large and sparse Hamiltonian matrices and low rank approximations to the solution of continuous-time algebraic Riccati equations with large and sparse coefficient matrices.
A data-driven method of fundamental frequency (F 0 ) contour synthesis was developed for Japanese text-to-speech (TTS) conversion systems. In the method, synthesis is done using the F 0 contour generation process model, and the model parameters for each accent phrase are estimated using statistical methods. Although it was already shown that the synthesized F 0 contours sounded highly natural as those using heuristic rules arranged by experts, occasional low quality happened depending on sentences to be synthesized. In the current paper, information on sentence structure, automatically obtainable through the parsing process, is added to input parameters of the statistical methods to obtain a better estimation. The experimental results showed that the new parameter was effective for improving especially phrase component estimation. Furthermore, data-driven estimation of accent phrase boundaries for input text, a necessary step to realize TTS conversion, was also realized in a similar way. The rate of correct estimation reached 90 %.
We consider the design of a wireless network for mobile users to provide cheap access to delay tolerant data. Since data can tolerate delay, we design a network with intermittent but high speed access. We show that this design can support a higher user density than the conventional wireless networks. To increase the utilization of the high speed bursts we propose to introduce buffers in the network and the mobile. We then propose the architecture and the software protocol stack for such a system. Algorithmic issues like MAC layer resource allocation and application layer buering are then discussed in detail. 
We prove that each plane graph of girth at least five on n &amp;ge; 4 vertices can be colored by at least &amp;lceil;n/2&amp;rceil; + 1 colors in such a way that it does not contain a multicolored face, i.e. the face whose all the vertices have mutually distinct colors.
Many standard computer vision algorithms depend upon the solution of Poisson equations, retinex lightness computation being one such algorithm. Using lightness as an example, we show how it can be computed on a varying resolution sensor so long as the sensor architecture is based on a conformal mapping. We develop the varying resolution lightness algorithm by the well known technique of using a conformal transplant. In the final analysis, very little of the algorithm needs to be changed, but a speed up in running time of roughly 50 times is obtained even though the total data is reduced by only a factor of 28.
Let ffmax (ff) denote the maximum value for the connectivity of any graph  which embeds in the topological surface ff. The connectivity interval for ff is  the set of integers in the interval [1,ff max (ff)]. Given an integer i in [1,ff max (ff)]  it is a trivial problem to demonstrate that there is a graph G i with connectivity  i which also embeds in ff. We will say that one can saturate the connectivity  interval in this case. Note
Racke recently gave a remarkable proof showing that any undirected multicommodity ow problem can be routed in an oblivious fashion with congestion that is within a factor of O(log    n) of the best o-line solution to the problem. He also presented interesting applications of this result to distributed computing. Maggs, Miller, Parekh, Ravi and Wu have shown that such a decomposition also has an application to speeding up iterative solvers of linear systems.
We present several simple representations of universal partially  ordered sets and use them for the proof of universality of the class of  oriented trees ordered by the graph homomorphisms. This (what we  believe surprising result) solves several open problems. It implies for  example universality of cubic planar graphs. This is in sharp contrast  with representing even groups (and monoids) by automorphisms (and  endomorphims) of a bounded degree and planar graph. Thus universal  partial orders (thin categories) are representable by much simpler  structures than categories in general.
Mobile robots in real-life settings would benefit from being able to localize sound sources. Such a capability can nicely complement vision to help localize a person or an interesting event in the environment, and also to provide enhanced processing for other capabilities such as speech recognition. In this paper we present a robust sound source localization method in three-dimensional space using an array of 8 microphones. The method is based on a frequency-domain implementation of a steered beamformer along with a probabilistic post-processor. Results show that a mobile robot can localize in real time multiple moving sources of different types over a range of 5 meters with a response time of 200 ms.
Test case prioritization techniques schedule test cases for execution in an order that attempts to  increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection -- a measure of how quickly faults are detected within the testing process. An  improved rate of fault detection during testing can provide faster feedback on the system under test and  let software engineers begin correcting faults earlier than might otherwise be possible. One application  of prioritization techniques involves regression testing -- the retesting of software following modifications;  in this context, prioritization techniques can take advantage of information gathered about the previous  execution of test cases to obtain test case orderings. In this paper, we describe several techniques for  using test execution information to prioritize test cases for regression testing, including: (1) techniques  that order test cases based on their total coverage of code components; (2) techniques that order test  cases based on their coverage of code components not previously covered; (3) techniques that order test  cases based on their estimated ability to reveal faults in the code components that they cover. We report  the results of several experiments in which we applied these techniques to various test suites for various  programs and measured the rates of fault detection achieved by the prioritized test suites, comparing  those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites. Analysis  of the data shows that each of the prioritization techniques studied improved the rate of fault detection  of test suites, and this improvement occurred even with the least expensive of those techniques. The  data...
this paper. If we were to omit it only few lines in the paper would change. There is another way to model self consciousness, by requiring that the measure m i (t i ) associated with an i-type is in 2(T &amp;i ) rather than 2(T ). We preferred the latter for notational advantages
Reputation systems in mobile ad-hoc networks can be tricked by the spreading of false reputation ratings, be it false accusations or false praise. Simple solutions such as exclusively relying on one&apos;s own direct observations have drawbacks, as they do not make use of all the information available. We propose a fully distributed reputation system that can cope with false disseminated information. In our approach, everyone maintains a reputation rating and a trust rating about everyone else that they care about. From time to time first-hand reputation information is exchanged with others; using a modified Bayesian approach we designed and present in this paper, only second-hand reputation information that is not incompatible with the current reputation rating is accepted. Thus, reputation ratings are slightly modified by accepted information. Trust ratings are updated based on the compatibility of second-hand reputation information with prior reputation ratings. Data is entirely distributed: someone&apos;s reputation and trust is the collection of ratings maintained by others. We enable node redemption and prevent the sudden exploitation of good reputation built over time by introducing re-evaluation and reputation fading. We present the application of our generic reputation system to the context of neighborhood watch in mobile ad-hoc networks, specifically to the CONFIDANT [3] protocol for the detection and isolation of nodes exhibiting routing or forwarding misbehavior. We evaluate the performance by simulation.
this paper, we present LpX, a type theory for agents to specify negotiation objects in an online exchange. An online exchange is a multi--agent system in which agents buy and sell. We have created LpX to study the use of type theory in resolving semantic heterogeneity. By using type theory we can formulate the entire process of resolving semantic heterogeneity in a single framework. Resolving semantic heterogeneity involves merging independently created ontologies into a single ontology and performing translation between schemas. Using an expressive type theory, we can use a collection of types as an explication of an ontology as well as represent schemas. As a result, we can use proof rules of the type system to relate independently created ontologies and use implicit coercion to perform schema translation between related types
System-on-chip (SoC) designs use bus protocols for high performance data transfer among the Intellectual Property (IP) cores. These protocols incorporate advanced features such as pipelining, burst and split transfers. In this paper, we describe a case study in formally verifying a widely used SoC bus protocol: the Advanced Micro-controller Bus Architecture (AMBA) protocol from ARM. In particular,
In this paper, we review basic properties of the Kronecker product, and give an overview of its history and applications. We then move on to introducing the symmetric Kronecker product, and we derive several of its properties. Furthermore, we show its application in finding search directions in semidefinite programming.  
... this paper, and it is the starting point  for proving some novel results about the undecidability of second-order  unification presented in the rest of the paper. We prove that second-order  unification is undecidable in the following three cases: (1) each second-order  variable occurs at most twice and there are only two second-order  variables; (2) there is only one second-order variable and it is unary; (3)  the following conditions (i)ff(iv) hold for some fixed integer n: (i) the  arguments of all second-order variables are ground terms of size &lt;n, (ii)  the arity of all second-order variables is &lt;n, (iii) the number of occurrences  of second-order variables is ff5, (iv) there is either a single second-order  variable or there are two second-order variables and no first-order  variables. 
One well-known approach to a posteriori analysis of finite element solutions of elliptic problems estimates the error in a quantity of interest in terms of residuals and a generalized Green&apos;s function. The generalized Green&apos;s function solves the adjoint problem with data related to a quantity of interest and measures the effects of stability, including any decay of influence characteristic of elliptic problems. We show that consideration of the generalized Green&apos;s function can be used to improve the effciency of the solution process when the goal is to compute multiple quantities of interest and/or to compute quantities of interest that involve globally-supported information such as average values and norms. In the latter case, we introduce a solution decomposition in which we solve a set of problems involving localized information, and then recover the desired information by combining the local solutions. By treating each computation of a quantity of interest independently, the maximum number of elements required to achieve the desired accuracy can be decreased significantly.
This paper describes an effort to identify common metrics for task-oriented human-robot interaction (HRI). We begin by discussing the need for a toolkit of HRI metrics. We then describe the framework of our work and identify important biasing factors that must be taken into consideration. Finally, we present preliminary results, including a summary of task-specific metrics already in use and suggested common metrics for standardization. Preparation of a larger, more detailed HRI metric toolkit is in progress.
We propose a distributed algorithm for allocating bandwidth in stable ad hoc networks. After having discussed the problem of bandwidth allocation in such networks, we define a sequence of feasible solutions to this problem. This sequence has the property to be an increasing sequence in terms of overall used bandwidth. After a theoretical analysis of the sequence, we design a distributed algorithm based on this sequence. We test our algorithm by simulations on different topologies like chains, rings, meshes and geometric random graphs. We compare our solutions with the optimal solution in terms of global bandwidth allocation that presents the smallest standard deviation and with the the fairest solution regarding to max-min fairness. The simulations show that the global used bandwidth is less than 25% from optimality in the worst case and the standard deviation is the smallest of the three tested solutions.
We have undertaken new observations tracing the evolution of shocks along the best resolved examples of protostellar jets by means of high resolution, deep K-band spectroscopy at UKIRT and NTT. In this paper we analyse trends in H 2 excitation within the well collimated jet of HH 212, comprising multiple bows along each flow axis. The aim of our survey is to better understand the origin of the sequence of knots observed along many jets by directly comparing to theoretical models including flow velocity variability and KelvinHelmholtz (KH) instabilities. We also investigate any evolution in shock excitation from C-type near the jet source, where ionisation fractions may be low and B-field strengths high, to J-type further downstream where ambient densities decrease. We establish whether fluorescent excitation becomes significant for knots closer to the source in each system. Using these results we can now go on to directly compare our results with model predictions, e.g., knot luminosity, separation and excitation in pulsating jet models (Smith et al. 1997).
this document, we focus on the C++ shader API. The lowlevel shader API, which the high-level shader API compiles to, is based on the DX9 assembly language but with a function call-based API in the style of ATI&apos;s OpenGL vertex shader extensions. This whitepaper describes a work in progress and the detailed syntax of the final system may differ from what is shown below. Please access our website at  http://www.cgl.uwaterloo.ca/Projects/rendering/  for more up to date information. The syntax described here also differs from that documented for earlier versions of SMASH
We investigate the problem of routing traffic through a congested network in an environment of non-cooperative users. We use the worst-case coordination ratio suggested by Koutsoupias and Papadimitriou to measure the performance degradation due to the lack of a centralized traffic regulating authority. We provide a full characterization of the worst-case coordination ratio in the restricted assignment and unrelated parallel links models. In particular, we quantify the tradeoff between the &quot;negligibility&quot; of the traffic controlled by each user and the coordination ratio. We analyze both pure and mixed strategies systems and identify the range where their performance is similar.
Rapid exploration of the design space with simulation models is essential for quality hardware systems research and development. Despite striking commonalities across hardware systems, designers routinely fail to achieve high levels of reuse across models constructed in existing general-purpose and domain-specific languages. This lack of reuse adversely impacts hardware system design by slowing the rate at which ideas are evaluated. This paper presents an examination of existing languages to reveal their fundamental limitations regarding reuse in hardware modeling. With this understanding, a solution is described in the context of the design and implementation of the Liberty Structural Specification Language (LSS), the input language for a publicly available high-level digital-hardware modeling tool called the Liberty Simulation Environment. LSS is the first language to enable low-overhead reuse by simultaneously supporting static inference based on hardware structure and flexibility via parameterizable structure. Through LSS, this paper also introduces a new type inference algorithm and a new programming language technique, called use-based specialization, which, in a manner analogous to type inference, customizes reusable components by statically inferring structural properties that otherwise would have had to have been specified manually.
It was conjectured by Tutte that every 4-edge-connected graph admits a nowherezero 3-flow. In this paper, we give a complete characterization of graphs whose squares admit nowhere-zero 3-flows and thus confirm Tutte&apos;s 3-flow conjecture for the family of squares of graphs. 1 
We are developing a prototype intelligent intrusion detection system (IIDS) to demonstrate the effectiveness of data mining techniques that utilize fuzzy logic and genetic algorithms. This system combines both anomaly based intrusion detection using fuzzy data mining techniques and misuse detection using traditional rule-based expert system techniques. The anomaly-based components are developed using fuzzy data mining techniques. They look for deviations from stored patterns of normal behavior. Genetic algorithms are used to tune the fuzzy membership functions and to select an appropriate set of features. The misuse detection components look for previously described patterns of behavior that are likely to indicate an intrusion. Both network traffic and system audit data are used as inputs for both components.
We study the impact of heterogeneity of nodes, in terms of their energy, in wireless sensor networks that are hierarchically clustered. In these networks some of the nodes become cluster heads, aggregate the data of their cluster members and transmit it to the sink. We assume that a percentage of the population of sensor nodes is equipped with additional energy resources---this is a source of heterogeneity which may result from the initial setting or as the operation of the network evolves. We also assume that the sensors are randomly (uniformly) distributed and are not mobile, the coordinates of the sink and the dimensions of the sensor field are known. We show that the behavior of such sensor networks becomes very unstable once the first node dies, especially in the presence of node heterogeneity. Classical clustering protocols assume that all the nodes are equipped with the same amount of energy and as a result, they can not take full advantage of the presence of node heterogeneity. We propose SEP, a heterogeneous-aware protocol to prolong the time interval before the death of the first node (we refer to as stability period), which is crucial for many applications where the feedback from the sensor network must be reliable. SEP is based on weighted election probabilities of each node to become cluster head according to the remaining energy in each node. We show by simulation that SEP always prolongs the stability period compared to (and that the average throughput is greater than) the one obtained using current clustering protocols. We conclude by studying the sensitivity of our SEP protocol to heterogeneity parameters capturing energy imbalance in the network. We found that SEP yields longer stability region for higher values of extra energy brought by more powerful nodes.
We present a flexible architecture for trusted computing, called Terra, that allows applications with a wide range of security requirements to run simultaneously on commodity hardware. Applications on Terra enjoy the semantics of running on a separate, dedicated, tamper-resistant hardware platform, while retaining the ability to run side-by-side with normal applications on a generalpurpose computing platform. Terra achieves this synthesis by use of a trusted virtual machine monitor (TVMM) that partitions a tamper-resistant hardware platform into multiple, isolated virtual machines (VM), providing the appearance of multiple boxes on a single, general-purpose platform. To each VM, the TVMM provides the semantics of either an &quot;open box,&quot; i.e. a general-purpose hardware platform like today&apos;s PCs and workstations, or a &quot;closed box,&quot; an opaque special-purpose platform that protects the privacy and integrity of its contents like today&apos;s game consoles and cellular phones. The software stack in each VM can be tailored from the hardware interface up to meet the security requirements of its application (s). The hardware and TVMM can act as a trusted party to allow closed-box VMs to cryptographically identify the software they run, i.e. what is in the box, to remote parties. We explore the strengths and limitations of this architecture by describing our prototype implementation and several applications that we developed for it.
Parallel scientific applications require high-performance I/O support from underlying file systems. A comprehensive understanding of the expected workload is therefore essential for the design of high-performance parallel file systems. We re-examine the workload characteristics in parallel computing environments in the light of recent technology advances and new applications.
Tycho is the next-generation user-interface system we are building for the Ptolemy project. It is a complete [incr Tcl] application structured as an extensible class library. Our goal is to make it easy to extend this basic application with functionality and a user interface for specialized applications such as electronic design and simulation. The Tycho library includes a selection of general-purpose widgets, syntax-sensitive text editors, and graphical editing support. It incorporates architectural features that make it easy for different editors and viewers to share data and screen space. Finally, structured support for incorporating C and Java packages into this framework allows us to use those languages to complement the scripting and user-interface features of Tcl/Tk.
Speedup learning seeks to improve the efficiency of search-based problem solvers. In this paper, we propose a new theoretical model of speedup learning which captures systems that improve problem solving performance by solving a user-given set of problems. We also use this model to motivate the notion of &quot;batch problem solving,&quot; and argue that it is more congenial to learning than sequential problem solving. Our theoretical results are applicable to all serially decomposable domains. We empirically validate our results in the domain of Eight Puzzle.  1 Introduction  Speedup learning seeks to improve the efficiency of search-based problem solvers. While the theory for concept learning is well-established, (for example, see [Valiant 1984; Natarajan 1991]), the theory of speedup learning is rapidly evolving [Cohen 1989; Elkan &amp; Greiner 1991; Etzioni 1990; Greiner 1989; Laird 1990; Natarajan &amp; Tadepalli 1988; Natarajan 1989; Subramanian &amp; Feldman 1990; Tadepalli 1991a, etc.]. In this paper...
Rate models are often used to study the behavior of large networks of spiking neurons. Here we propose a procedure to derive rate models which take into account the fluctuations of the input current and firing rate adaptation, two ubiquitous features in the central nervous system which have been previously overlooked in constructing rate models. The procedure is general and applies to any model of firing unit. As examples, we apply it to the leaky integrate-and-fire (IF) neuron, the leaky IF neuron with reversal potentials, and to the quadratic IF neuron. Two mechanisms of adaptation are considered, one due to an afterhyperpolarization current, the other to an adapting threshold for spike emission. The parameters of these simple models can be tuned to match experimental data obtained from neocortical pyramidal neurons. Finally, we show how the stationary model can be used to predict the time-varying activity of a large population of adapting neurons.
Performance profile trees have recently been proposed as a  theoretical basis for fully normative deliberation control. In  this paper we conduct the first experimental study of their  feasibility and accuracy in making stopping decisions for  anytime algorithms on optimization problems. Using data  and algorithms from two different real-world domains, we  compare performance profile trees to other well-established  deliberation-control techniques. We show that performance  profile trees are feasible in practice and lead to significantly  better deliberation control decisions. We then conduct experiments  using performance profile trees where deliberationcontrol  decisions are made using conditioning on multiple  features of the solution to illustrate that such an approach is  feasible in practice.
The Semantic Web aims at easy integration and usage of content by building on a semi-structured data model where data semantics are explicitly specified through ontologies. The use of ontologies in real-world applications such as community portals has shown that a new level of data independence is required for ontology-based applications. For example, the customization of information towards the needs of specific user communities is often need. This paper extends previous work [22, 21] on this issue and presents a view language for the fundamental data models of the Semantic Web, viz. RDF and RDFS, and how it can be implemented. The basic novelty of the view language is the semantically appropriate classification of views into inheritance taxonomies based on query semantics. Additionally, the underlying distinction between unary predicates (classes) and binary predicates (properties) taken in RDF/S is maintained in the view language. So-called external ontologies allow the integration of multiple source databases, offer control over the publishing of data and enable the generation of views spanning across databases.
This paper evaluates the predictions of two existing models  of graph comprehension: BOZ (Casner, 1990) and  UCIE (Lohse, 1993). Each model was implemented in  Java and was used to make predictions about the relative  efficiency of different graphical presentations of numerical  data for use in different tasks. These predictions were  then compared with the results of human subjects performing  the same tasks using the same presentations. The  results of the human study do not correspond to the predictions  of either model. In particular, while both models  predict that the tabular presentations would have the  worst results, in practice the tables actually proved to be  the best presentation type. A possible explanation for this  result is that the models capture optimal, expert performance,  while the subjects used less efficient techniques.
This paper presents our experiments in applying  Latent Semantic Analysis (LSA) to dialogue  act classification. We employ both LSA  proper and LSA augmented in two ways. We  report results on DIAG, our own corpus of tutoring  dialogues, and on the CallHome Spanish  corpus. Our work has the theoretical goal of assessing  whether LSA, an approach based only  on raw text, can be improved by using additional  features of the text.
We used Hubble Space Telescope WFPC2 images to identify six early-type galaxies  with surface-brightness profiles that decrease inward over a limited range of radii near  their centers. The implied luminosity density profiles of these galaxies have local minima  interior to their core break radii. NGC 3706 harbors a high surface brightness ring of  starlight with radius ff 20 pc. Its central structure may be related to that in the doublenucleus  galaxies M31 and NGC 4486B. NGC 4406 and NGC 6876 have nearly flat cores  that on close inspection are centrally depressed. Colors for both galaxies imply that this  is not due to dust absorption. The surface brightness distributions of both galaxies are  consistent with stellar tori that are more diffuse than the sharply defined system in NGC  3706. The remaining three galaxies are the brightest cluster galaxies in A260, A347, and  A3574. Color information is not available for these objects, but they strongly resemble  NGC 4406 and NGC 6876 in their cores. The thin ring in NGC 3706 may have formed  dissipatively. The five other galaxies resemble the endpoints of some simulations of the  merging of two gas-free stellar systems, each harboring a massive nuclear black hole. In  one version of this scenario, diffuse stellar tori are produced when stars initially bound  to one black hole are tidally stripped away by the second black hole. Alternatively,  some inward-decreasing surface-brightness profiles may reflect the ejection of stars from  a core during the hardening of the binary black hole created during the merger.
One approach to model checking software is based on the abstract-check-refine paradigm: build an abstract model, then check the desired property, and if the check fails, refine the model and start over. We introduce the concept of lazy abstraction to integrate and optimize the three phases of the abstract-check-refine loop. Lazy abstraction continuously builds and refines a single abstract model on demand, driven by the model checker, so that different parts of the model may exhibit different degrees of precision, namely just enough to verify the desired property. We present an algorithm for model checking safety properties using lazy abstraction and describe an implementation of the algorithm applied to C programs. We also provide sufficient conditions for the termination of the method.
The problem of image registration subsumes a number of problems and techniques in multiframe image analysis, including the computation of optic flow (general pixel-based motion), stereo correspondence, structure from motion, and feature tracking. We present a new registration algorithm based on spline representations of the displacement field which can be specialized to solve all of the above mentioned problems. In particular, we show how to compute local flow, global (parametric) flow, rigid flow resulting from camera egomotion, and multiframe versions of the above problems. Using a spline-based description of the flow removes the need for overlapping correlation windows, and produces an explicit measure of the correlation between adjacent flow estimates. We demonstrate our algorithm on multiframe image registration and the recovery of 3D projective scene geometry. We also provide results on a number of standard motion sequences.
Introduction.  Microarray experiments have been used to measure genes&apos; expression levels under different cellular conditions or along certain time course. Initial attempts to interpret these data begin with grouping genes according to similarity in their expression profiles. The widely adopted clustering techniques for gene expression data include hierarchical clustering, self-organizing maps, and K-means clustering. Bayesian networks and neural networks have also been applied to gene clustering. Sharan &amp; Shamir [3] provided a survey on this topic. Clustering techniques typically discover the inherent structure of the genes expression profiles based on some similarity measures. The clustering results largely depend on how the similarity measure corresponds to the biological correlation between genes. Before reliable conclusion about biological functions can be drawn from the data, the gene clusters obtained from microarray analysis must be investigated with respect to known biological r
Software correctness is usually viewed as a relationship between specification and  implementation, but our analysis of colloquial usage shows that a third process, certification, plays  a key role in establishing correctness. Just as poorly implemented systems can be untestable, so too  badly specified systems can be difficult to test. Certification can be viewed as a set of industrial  standards for functional specifications, where failure to meet these standards renders functional  specifications unacceptable from a testability point of view. Moreover, even though certification  addresses issues in testability, it is the responsibility not of the test crew but of the specification  team. We provide examples to motivate our observations on specification technique, and conclude  by demonstrating the existence of uncertifiable systems.
Understanding the fundamental performance limits of wireless sensor networks is critical towards their appropriate deployment strategies. Both the data transmission rate and the lifetime of sensor networks can be constrained, due to interference among the transmissions and the limited energy source of the sensor. In addition to presenting the general results with respect to the maximum sustainable throughput of wireless sensor networks, this chapter focuses on the discussion of the energy-constrained fundamental limits with respect to the network throughput and lifetime. With an adequate definition of operational lifetimes, our asymptotic analysis shows that, with fixed node densities, operational lifetime of sensor networks decreases in the order of 1/n as the number of initially deployed nodes n grows. Even with renewable energy sources on each of the sensors (e.g., solar energy sources), our analysis concludes that the maximum sustainable throughput in energyconstrained sensor networks scales worse than the capacity based on interference among concurrent transmissions as long as the physical network size grows with n in the order greater than log n.In  this case, when the number of nodes is suffciently high, the energy-constrained network capacity dominates.
When streaming semi-structured data is processed by a well-designed query processor, parsing constitutes a significant portion of the running time. Further improvements in performance therefore require some method to overcome the high cost of parsing. We have designed a general-purpose mechanism by which a producer of streaming data may augment the data stream with hints that permit a downstream processor to skip parsing parts of the stream. Inserting such hints requires additional processing by the producer of data; however, the resulting stream is more valuable to consumers (since they have to perform less processing) , making such processing worthwhile. We present a set of hint schemes and describe how they are used by query engines. We demonstrate the benefits of our approach using an experimental study based on a hints-aware XPath query engine. Our results show that XHints can improve the performance of XPath query engines by as much as 100%. 1 
We propose a novel local appearance modeling method for object detection and  recognition in cluttered scenes. The approach is based on the joint distribution of local  feature vectors at multiple salient points and factorization with Independent Component  Analysis (ICA). The resulting densities are simple multiplicative distributions modeled  through adaptative Gaussian mixture models. This leads to computationally tractable  joint probability densities which can model high-order dependencies. Our techinque  has been initially tested under different natural and cluttered scenes with different degrees  of occlusions with promising results. With this present work, we provide a large  statistical test with the MNIST digit database in order to demonstrate the improved  performance obtained by explicit modeling of high-order dependencies.
A wireless ad hoc network is formed by a group of wireless hosts, without the use of any infrastructure. To enable communication, hosts cooperate among themselves to forward packets on behalf of each other. A key challenge in ad hoc networks lies in designing efficient routing strategies. While several routing protocols have been proposed, most of them aim to select one optimal route between the source and destination. The MAC layer at each intermediate node is then required to forward packets to the next downstream node on that route. We argue that choosing a single optimal route at the network layer may not be sufficient. Knowledge of short-term channel conditions at the MAC layer can play an important role in improving end-to-end performance. Instantaneous interference, channel contention, power constraints and other considerations may be taken into account along with the network layer&apos;s long-term view. This paper proposes MAC-layer anycasting -- a forwarding strategy that combines the guidelines from the network layer, with MAC layer knowledge of the local channel. We describe some applications of MAC-layer anycasting, and discuss the performance related tradeoffs.
We introduce Amigo -- an Instant Messaging (IM) client for handheld computers. Amigo allows free-form images as well as handwriting to be sent between people, taking advantage of the touch sensitive display of mobile devices. Amigo differs from other IM clients in that the text written by the user never has to be translated into ASCII data. Twenty students used Amigo for two weeks. Preliminary use results show that Amigo functions well as an IM client for handheld computers, and also introduces new ways for people to interact using IM: mixed text/image sessions, collaborative drawings and instant gaming.
Additive noise removal from a given signal is an important problem in signal processing. Among the most appealing aspects of this field are the ability to refer it to a well-established theory, and the fact that the proposed algorithms in this field are efficient and practical. Adaptive methods based on anisotropic diffusion (AD), weighted least squares (WLS), and robust estimation (RE) were proposed as iterative locally adaptive machines for noise removal. Tomasi and Manduchi (see Proc. 6th Int. Conf. Computer Vision, New Delhi, India, p.839-46, 1998) proposed an alternative noniterative bilateral filter for removing noise from images. This filter was shown to give similar and possibly better results to the ones obtained by iterative approaches. However, the bilateral filter was proposed as an intuitive tool without theoretical connection to the classical approaches. We propose such a bridge, and show that the bilateral filter also emerges from the Bayesian approach, as a single iteration of some well-known iterative algorithm. Based on this observation, we also show how the bilateral filter can be improved and extended to treat more general reconstruction problems.
Although Entity-Relationship (ER) modelling techniques are commonly used for information modelling,  Object-Role Modelling (ORM) techniques are becoming increasingly popular, partly because they  include detailed design procedures providing guidelines for the modeller. As with the ER approach, a  number of different ORM techniques exist. In this paper, we propose an integration of two theoretically  well founded ORM techniques: FORM and PSM. Our main focus is on a common terminological  framework, and on the notion of subtyping. Subtyping has long been an important feature of semantic  approaches to conceptual schema design. It is also the concept in which FORM and PSM differ the  most in their formalization. The subtyping issue is discussed from three different viewpoints covering  syntactical, identification, and population issues. Finally, a wider comparison of approaches to subtyping  is made, which encompasses other ER-based and ORM-based information modelling techniques, and  highlights how formal subtype definitions facilitate a comprehensive specification of subtype constraints.
Eclogue:    The logic programming language Prolog is used to provide a rapid-prototype simulator for the VERILOG Hardware Description Language (HDL). The simulator is based on an operational semantics  of a significant subset of the language. Using this approach allows the exploration of sometimes subtle behaviours of parallel programs and the possibility of rapid changes or additions to the semantics of the language covered. It also acts as a check on the validity of the original operational semantics.
Internet based mobile ad hoc network (IMANET) is an emerging technique that combines a wired network (e.g. Internet) and a mobile ad hoc network (MANET) for developing a ubiquitous communication infrastructure. However, IMANET has several limitations to fulfill users&apos; demands to access various kinds of information such as limited accessibility to the wired Internet, insufficient wireless bandwidth, and longer message latency. In this paper, we address the issues involved in information search and access in IMANET. A broadcast based Simple Search (SS) algorithm and an aggregate caching mechanism are proposed for improving the information accessibility and reducing average communication latency in IMANET.Aspart  cache replacement policy, called Time and Distance Sensitive (TDS) replacement, are developed to reduce the cache miss ratio and improve the information accessibility. We evaluate the impact of caching, cache management, and access points, which are connected to the Internet, through extensive simulation. The simulation results indicate that the proposed aggregate cache can significantly improve an IMANET performance in terms of throughput and average number of hops to access data. In particular, with aggregate caching, more than 200% improvement in throughput is achieved compared to the IMANET with no cache case, when the access pattern follows a Zipf distribution.
This paper proposes a formal framework for development  and exploitation of a corpus, based on the HPSG  linguistic theory. The formal representation of the  annotation scheme facilitates the annotation process  and ensures the quality of the corpus and its usage  in different application scenarios. Also, evaluation  over HPSG annotation scheme is discussed. The advantages  of the approach are presented in comparison  with other related works.
In [9], we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera parameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions.
This is a survey of basic facts about bounded arithmetic and about the relationships between bounded arithmetic and propositional proof complexity. We introduce the theories S     2 of bounded arithmetic and characterize their proof theoretic strength and their provably total functions in terms of the polynomial time hierarchy. We discuss other axiomatizations of bounded arithmetic, such as minimization axioms. It is shown that the bounded arithmetic hierarchy collapses if and only if bounded arithmetic proves that the polynomial hierarchy collapses. We discuss Frege and extended Frege proof length, and the two translations from bounded arithmetic proofs into propositional proofs. We present some theorems on bounding the lengths of propositional interpolants in terms of cut-free proof length and in terms of the lengths of resolution refutations. We then define the RazborovRudich notion of natural proofs of P   NP and discuss Razborov&apos;s theorem that certain fragments of bounded arithmetic cannot prove superpolynomial lower bounds on circuit size, assuming a strong cryptographic conjecture. Finally, a complete presentation of a proof of the theorem of Razborov is given. 1 Review of Computational Complexity  1.1 Feasibility  This article will be concerned with various &quot;feasible&quot; forms of computability and of provability. For something to be feasibly computable, it must be computable in practice in the real world, not merely effectively computable in the sense of being recursively computable.
Linear codes over F q are considered for use in detecting and in correcting the additive errors in some subset E of F  q .(Themost familiar example of such an error set E is the set of all n-tuples of Hamming weight at most t.) In this set-up, the basic averaging arguments for linear codes are reviewed with emphasis on the relation between the combinatorial and the information-theoretic viewpoint. The main theorems are (a correspondingly general version of) the Varshamov-Gilbert bound and a `random-coding&apos; bound on the probability of an ambiguous syndrome. These bounds are shown to result from applying the same elementary averaging argument to two different packing problems, viz., the combinatorial `sphere&apos; packing problem and theprobabilistic `Shannon packing&apos;. Some applications of the general bounds are indicated, e.g., hash functions and Euclidean-space codes, and the connection to Justesen-type constructions of asymptotically good codes is outlined.
By constructing and analyzing a physically  situated brain-based device (i.e. a device  with sensors and actuators whose behavior  is guided by a simulated nervous system),  we show that reentrant connectivity and dynamic  synchronization can provide an effective  mechanism for binding the visual features  of objects.
Efficient two-step algorithms are described for optimizing the stopband response of the prototype filter for cosine-modulated and modified DFT filter banks either in the minimax or in the least-mean-square sense subject to the maximum allowable aliasing and amplitude errors. The first step involves finding a good start-up solution using a simple technique. This solution is improved in the second step by using nonlinear optimization. Several examples are included illustrating the flexibility of the proposed approach for making compromises between the required filter lengths and the aliasing and amplitude errors. These examples show that by allowing very small amplitude and aliasing errors, the stopband performance of the resulting filter bank is significantly improved compared to the corresponding perfect-reconstruction filter bank. Alternatively, the filter orders and, consequently, the overall delay can be significantly reduced to achieve practically the same performance.
In this study we showhow one can use Fault-Tolerant Units (FTU) in an optimal way to make a TDMA network robust to bursty random perturbations. We consider two possible objectives. If one wants to minimize the probability of losing all replicas of a given message, then the optimal policy is to spread the replicas over time. This is proved using convexity properties of the loss probability. On the contrary if one wants to minimize the probability of losing at least one replica, then the optimal solution is to group all replicas together. This is proved by using majorization techniques. Finally we show how these ideas can be adapted for the TTP/C protocol.
In this paper we study the stationary distributions for reflected diffusions with  jumps in the positive orthant. Under the assumption that the stationary  distribution possesses a density in R    + that satisfies certain finiteness conditions,  we characterize the Fokker-Planck equation. We then provide necessary  and suffcient conditions for the existence of a product-form distribution for  diffusions with oblique boundary reflections and jumps. For this we exploit a  recent characterization of the boundary properties of such reflected processes.
Exponential growth of Internet traffic and the proliferation of new user applications warrant the development of new Internet infrastructure. Due to the fundamental satellite system characteristics such as global coverage, broadcast nature, and bandwidth on demand, satellite systems are excellent candidates for providing high data rate Internet access and global connectivity accommodating multimedia applications. However, to meet this goal, provisioning of quality-of-service (QoS) within the advanced satellite network systems is the critical requirement. Congestion remains the main obstacle to Quality of Service (QoS) on the Internet. In today&apos;s TCP networks, ECN is the only explicit mechanism, which delivers congestion signals to the source.
This paper is devoted to an application-level multicast protocol, HBM (Host Based Multicast), which can be used when native multicast routing is not available. Being purely end-to-end, application-level multicast proposals in general, and HBM in particular, are intrinsically more fragile than traditional routing solutions relying on well administered and dedicated routers. Improving their robustness is therefore of high practical importance and we believe it is a key aspect for the acceptance of the technology by end users who won&apos;t tolerate that a multi-participant video-conference session be subject to frequent cuts. In this work we identify two classes of problems that lead to packet losses, and for each class we introduce and compare several schemes meant to improve the HBM robustness. Experiments show that in both cases simple yet efficient solutions to the problem exist.
The support of schema versioning has been considered in the literature on temporal databases only  at a limited extent. In particular, solutions for managing schema versions along transaction-time as  different interfaces on the same temporal data were proposed so far.
Variable Neighborhood Search (VNS) is a recent metaheuristic, or framework for  building heuristics, which exploits systematically the idea of neighborhood change,  both in the descent to local minima and in the escape from the valleys which contain  them. In this tutorial we first present the ingredients of VNS, i.e., Variable Neighborhood  Descent (VND) and Reduced VNS (RVNS) followed by the basic and then the  general scheme of VNS itself which contain both of them. Extensions are presented,  in particular Skewed VNS (SVNS) which enhances exploration of far away valleys and  Variable Neighborhood Decomposition Search (VNDS), a two-level scheme for solution  of large instances of various problems. In each case, we present the scheme, some  illustrative examples and questions to be addressed in order to obtain an efficient implementation.
This paper studies a recently developed an approach to reasoning about mutable data structures, which uses an assertion language with spatial conjunction and implication connectives. We investigate computability and complexity properties of a subset of the language, which allows statements about the shape of pointer structures (such as &quot;there is a link from x to y&quot;) to be made, but not statements about the data held in cells (such as &quot;x is a prime number&quot;). We show that validity, even for this restricted language, is not r.e., but that the quantifierfree sublanguage is decidable. We then consider the complexity of model checking and validity for several fragments. 1 
During summer 2002, we ran a workshop module for a group of 28 eighth-grade girls. Our aim was ambitious: to introduce these students, ages 12 and 13, to computer science by focussing on the deep intellectual topic of self-stabilizing distributed algorithms and by imparting an intuitive appreciation for their use in fault tolerance. At the same time, we hoped to dispel some negative stereotypes of computer science. The module was a success according to evaluations and comments from the participants. This paper describes the sequence of exercises we developed as an elementary-level introduction to the graduate-level topics of fault tolerance and self-stabilization. We report them with the hope that others will try them in college classrooms, as we plan to do. Categories &amp; Subject Descriptors  K.3.2 [Computers and Education] : Computer and Information Science Education -- computer science education.
We characterize the complete set of protocols that may be used to securely encrypt n  quantum bits using secret and random classical bits. In addition to the application of such  quantum encryption protocols to quantum data security, our framework allows for generalizations  of many classical cryptographic protocols to quantum data. We show that the encrypted  state gives no information without the secret classical data, and that 2n random classical bits  are the minimum necessary for informationally secure quantum encryption. Moreover, the  quantum operations are shown to have a surprising structure in a canonical inner product  space. This quantum encryption protocol is a generalization of the classical one time pad  concept. A connection is made between quantum encryption and quantum teleportation[1],  and this allows for a new proof of optimality of teleportation.
We present an algorithm for selecting support vector machine (SVM) meta-parameter values which is based on ideas from design of experiments (DOE) and demonstrate that it is robust and works effectively and efficiently on a variety of problems.
Scheduling in hard real-time systems requires a priori knowledge of worst-case execution times (WCET). Obtaining the WCET of a task is a difficult problem. Static timing analysis techniques approach this problem via path analysis, pipeline simulation and cache simulation to derive safe WCET bounds. But such analysis has traditionally been constrained to only small programs due to the complexity of simulation, most notably the complexity of static cache simulation, which requires inter-procedural analysis. This
This paper describes critical factors for co-ordinated enterprise and IS/IT development and change in complex organizations in Sweden. These factors represent an answer on the crucial question: Why is co-ordinated development of enterprise and IS/IT difficult today? The survey is based upon in-depth interviews and concentrated workshops which present a current and qualitative view upon this area. 20 persons representing different development management roles from 6 large organizations with complex operations have participated. Responses relate to two main questions, which concern the problem generating factors and to what degree these are critical. The factors have been categorised into nine subject areas and certain factor groups within each area. A vast number of statements are included in the material.
From a general uncertainty principle we derive uncertainty principles on spheres in any dimension which extend, for real-valued functions, known uncertainty principles on spheres in two and three dimensions. For the uncertainty principle on the circle, we show that for 0 &lt;  &lt; 1, there is a sequence of trigonometric polynomials of degree  k whose uncertainty diers from the optimal by O(1=k   ) as  k !1, and similar results are given for spheres in higher dimensions. An alternative uncertainty principle on the circle is also considered for which minimum uncertainty is attained, and the minimizing functions are shown to have similarities to the Gaussian (which gives minimum uncertainty in the Heisenberg uncertainty principle). x1. 
Ordering information is a critical task for  natural language generation applications. In this
this paper we study a consistency condition, called belief affirming, which relates beliefs about strategies to payoffs. The essence of this condition is as follows. At each point in time, t, a player&apos;s belief and action determine her expected payoff, E(t), in the next period. At the same time, the player also observes her payoff history. If in the long run, these histories seem to contradict her expected payoff then her confidence in her beliefs will be shaken. If on the contrary, past and future payoffs fit, then the process affirms the player&apos;s beliefs. This belief affirming, in terms of payoffs, is the condition we study here
In this report we deal with a definition of the architecture modelling technique.
Communication complexity has recently been recognized as  a major obstacle in the implementation of combinatorial auctions. In this  paper, we consider a setting in which the auctioneer (elicitor), instead of  passively waiting for the bids presented by the bidders, elicits the bidders&apos;  preferences (or valuations) by asking value queries. It is known that in  the more general case (no restrictions on the bidders&apos; preferences) this  approach requires the exchange of an exponential amount of information.
We analyze spread-spectrum and quantization projection data  hiding methods from a game-theoretic point of view, using the bit error rate (BER) as the payoff, and assuming that the embedder simply follows point-by-point constraints given by a perceptual mask, whereas for the attacker an MSE-like constraint is imposed. The optimal attacking and decoding strategies are obtained by making use of a theorem that in addition states that those strategies constitute an equilibrium of the game. Experimental results supporting our analyses are also shown.
We discuss the theoretical structure and constructive methodology for large-scale graphical models, motivated by their potential in evaluating and aiding the exploration of patterns of association in gene expression data. The theoretical discussion covers basic ideas and connections between Gaussian graphical models, dependency networks and specific classes of directed acyclic graphs we refer to as compositional networks. We describe a constructive approach to generating interesting graphical models for very high-dimensional distributions that builds on the relationships between these various stylized graphical representations. Issues of consistency of models and priors across dimension are key. The resulting methods are of value in evaluating patterns of association in large-scale gene expression data with a view to generating biological insights about genes related to a known molecular pathway or set of specified genes. Some initial examples relate to the estrogen receptor pathway in breast cancer, and the Rb-E2F cell proliferation control pathway.
the present case, a Polymer Matrix Composite (PMC) is being considered as a replacement for carbon steel in flexible risers manufactured by Wellstream Inc., Panama City, Florida. The Materials Response Group (MRG) at Virginia Tech had the primary responsibility to develop the models for long-term behavior, especially remaining strength and life. The MRG is also responsible for the characterization of the material system with a focus on the effects of time, temperature, and environmental exposure. The present work is part of this effort. The motivation to use a composite material in a non-bonded flexible riser for use in the offshore oil industry is put forth. The requirements for such a material are detailed. Strength analysis and modeling methods are presented with experimental data. The effect of matrix crystallinity on composite mechanical properties is shown. A new method for investigating matrix behavior at elevated temperatures developed. A remaining strength life prediction methodology is recalled and applied to the case of combined fatigue and rupture loading. iii  DEDICATION  This work is dedicated to my family David, Raylene, and Erika, who never let me limit my expectations. iv  ACKNOWLEDGMENTS  There are many who contributed to this work; I would like to acknowledge the following:  Dr. Kenneth Reifsnider- Aside from serving as my committee chair, Dr. Reifsnider has provided mentorship and given me opportunities that I likely would not have had if not for his insight and his faith in me. I thank him for giving me the opportunity to teach and &quot;run&quot; my own project, and for the support he provided in my effort to become a graduate student.
This paper considers the problem of path planning for teams of mobile  robots. It investigates two decoupled and prioritized approaches to coordinate the  movements of the mobile robots in their environment. The first approach considered  is the coordination technique. The second approach is an A    -based path  planning technique which computes the paths for the individual robots in the configuration  time-space. Thereby it trades off the distance to both to static objects  as well as to other robots and the length of the path to be traveled. In different  experiments carried out with real robots and in simulations we demonstrate that  the A    -based approach is well suited to control the motions of a team of robots  in various environments and illustrate its advantages over the coordination technique.
Through the study of memory over a long period of time, we have come to organise the human memory into various categories, or &quot;systems&quot; (Tulving, 1995). Two systems, which have been well studies are long-term memory (LTM) and working memory (WM). An essential component of long-term memory system is semantic memory which contains our model of the world. This is the knowledge that has been generalized over various situations that the human being has encountered during his lifespan. The working memory system, on the other hand, represents transitory information -- memory of the situation at hand -- and therefore acts like a limited-capacity, temporary store for the visual and auditory information perceived by an individual (Baddeley, 1995). The goal
A successful experiment in international collaboration in aircraft design education has been conducted by the aeronautical and aerospace engineering departments at Virginia Tech and Loughborough University for several years. This paper discusses some of  the lessons learned in that experiment related to the management of both the collaboration and the international student design teams. An emphasis is placed on the need for good communication among both participating faculty and students as well as on the necessity of both interpersonal &quot;soft&quot; skills and management skills in building the design teams. Suggestions are made for others who may wish to develop their own international collaboration experiments in engineering design education.
We analyze plane strain thermomechanical deformations of a prenotched rectangular plate impacted on one side by a prismatic body of rectangular cross-section and moving parallel to axis of the notch. Both the plate and the projectile are made of the same material. Strain hardening, strain-rate hardening and thermal softening characteristics of the material are modeled by the Johnson-Cook relation. The effect of different material parameters, notch-tip radius, impact speed and the length of the projectile on the maximum tensile principal stress and the initiation and propagation of shear bands at the notch-tip is analyzed. It is found that for high impact speeds or enhanced thermal softening, two shear bands, one at      128 - to it propagate from the notch tip. Otherwise, only one shear band nearly parallel to the notch-ligament initiates at the notch-tip. The notch-tip distortion for high strength materials is quite different from that for low strength materials. The maximum tensile principal stress occurs at a point on the upper surface of the notch-tip and for every set of values of material parameters and impact speeds studied equals about 2.3 times the yield stress of the material in a iii  quasistatic simple tension or compression test. We assume that the brittle fracture occurs when the maximum tensile principal stress equals twice the yield stress of the material in  a quasistatic simple tension test and a shear band initiates when the effective plastic strain at a point equals 0.5. The effect of material and geometric parameters on the time of initiation of each failure mode is computed. It is found that for low impact speeds (&lt; 30 m/s), a material will fail due to the maximum tensile principal stress exceeding its limiting value, and at high impact speeds due ...
The simulation of mobility models such as the random waypoint often cause subtle problems, for example the decay of average speed as the simulation progresses, a difference between the long term distribution of nodes and the initial one, and sometimes the instability of the model. All of this has to do with time averages versus event averages. This is a well understood, but little known topic, called Palm calculus. In this paper we first give a very short, easy to grasp background on Palm calculus. Then we apply it to the random waypoint model and variants (with pause time, random walk). We show how to simply obtain the stationary distribution of nodes and speeds, on a connected (possibly non convex) area. We also show how to perform a perfect (i.e. transient free) simulation without computing complicated integrals. Last, we analyze decay and explain it as either convergence to steady state or lack of convergence.
Software execution environments like operating systems, mobile code platforms and  scriptable applications must protect themselves against potential demages caused by  malicious code. Monitoring the execution history of the latter provides an effective  means for controlling the access pattern of system services. Several authors have recently  proposed increasingly general automata models for characterizing various classes  of security policies enforceable by execution monitoring. An open question raised by  Bauer, Ligatti and Walker is whether one can further classify the space of security  policies by constraining the capabilities of the execution monitor. This paper presents  a novel information-based approach to address the research problem. Specifically, security  policies are characterized by the information consumed by an enforcing execution  monitor.
Although vendors have made multiple-monitor systems for many years, our interfaces have been stuck in a 30-year old windows paradigm focused on displays much smaller than the desktops we use when working with paper. Advances in flat panel displays and graphics cards now enable affordable personal computers with 6-8 monitors and may someday eliminate seams. This paper argues that vendors should be developing wideband visual interfaces that are designed for displays that fill the human visual field. We describe a longitudinal field study of window activity that found that windows almost always filled a typical single monitor display and that subjects occasionally struggled with window thrashing when they needed to work with two or more windows at the same time. Vendors need not wait for affordable seamless wideband displays before addressing these findings. We have implemented several novel user interface techniques for creating seam-aware applications that target wideband displays based on multiple monitors.
This paper describes how speakers adapt their language during error resolution when interacting with the animated agent Pixie. A corpus of spontaneous human-computer interaction was collected at the Telecommunication museum in Stockholm, Sweden. Adult and children speakers were compared with respect to user behavior and strategies during error resolution. In this study, 16 adults and 16 children speakers were randomly selected from a corpus from almost 3.000 speakers. This sub-corpus was then analyzed in greater detail. Results indicate that adults and children use partly different strategies when their interactions with Pixie become problematic. Children tend to repeat the same utterance verbatim, altering certain phonetic features. Adults, on the other hand, often modify other aspects of their utterances such as lexicon and syntax. Results from the present study will be useful for constructing future spoken dialogue systems with improved error handling for adults as well as children.
In this paper we present a statistical learning algorithm for synthesizing  new random instances of a sound texture given an example of such a texture  as input. A large class of natural and artificial sounds such as rain, waterfall,  traffic noises, people babble, machine noises, etc., can be regarded as sound  textures --- sound signals that are approximately stationary at some scale.
In this paper we propose a data intensive approach  for inferring sentence-internal temporal  relations, which relies on a simple probabilistic  model and assumes no manual coding.
In this paper we will examine the problem of learning an effcient fuzzy logic rule set for the control of the inverted pendulum ffwith nonlinear dynamicsff using an evolutionary algorithm. In particular we compare a twolayered rule set with a single fuzzy logic rule set. Furthermore we look at the effect that differentchoices of objective function ffin the evolutionary algorithmff have on the rule sets that are learnt.
Online course offerings have not only gained in popularity among technology-mediated training methods, but they also have produced a prominent change in the landscape of academia. It is therefore imperative to obtain a solid understanding of the important elements that contribute to effective online learning. The major contribution of this paper is to investigate a complex set of interrelated factors in the relatively new sphere of online learning. The intercombination of these particular factors appears to be important from past research, but it has never been explicitly addressed, and never in an experimental setting. Findings of this study have shown that feedback and learner control have a significant interaction effect for declarative knowledge acquisition. For satisfaction, however, feedback is only salient for declarative knowledge learning, and not for procedural knowledge learning. Other factors, such as interest and comfort level produce effects in most situations.
Introduction  In the clinical application of magnetoencephalography ( MEG ) to epilepsy, cases in which epileptic discharges are in the temporal base area or the frontal base area are relatively common. These regions are not spherical and the spherical model as a conductor model may not be suitable rigorously.  Source localization by MEG was once thought to be accurate due to little influence of volume current but importance of volume current began to be emphasized recently [1]. Software using the realistic headshape model as a conductor model is available commercially now.  In order to evaluate commercially distributed software from the clinical point of view, authors conducted a realistic-shaped phantom study and report the efficacy and limitations of the realistic headshape model especially in cease with sources in the temporal base area and the frontal base area.  2 Methods  2.1 Phantom  A model skull for medical education was utilized to make a realistic headshape phantom. Orbital
The feature interaction problem in Intelligent Networks obstructs more and more the rapid introduction of new features. Detecting such feature interactions turns out to be a big problem. The size of the systems and the sheer computational complexity prevents the system developer from checking manually any feature against any other feature. We give an overview on current (verification) approaches and categorize them into property-oriented and automata-theoretic approaches. A comparison turns out that each approach complements the other in a certain sense. We propose to apply both approaches together in order to solve the feature interaction problem. 1 Introduction  Telephone switching systems are a classical example for long-lived and perpetually evolving software in the telecommunications domain. The first software controlled switching exchanges essentially still provided the Plain Old Telephone Service (POTS). Step by step, new features have been added since then which were supposed t...
We present an approach for the integration of Reinforcement Learning methods into Petri net based specifications of robot behaviors. Our work aims at opening an existing design methodology for embedded systems for the design of autonomous mechatronical systems with adaptive behavior. In order to combine Petri nets and learning methods, we modeled Q-Learning --- a variant of Reinforcement Learning --- with high-level Petri nets. The result can be integrated into Petri net models of autonomous mechatronical systems, e.g. behavior-based robots. For an evaluation of our approach, we have implemented a realistic application example, a part of the well-known robot contest &apos;Capture the flag&apos;. The example has been evaluated by simulation as well as on a physical system.
Many industrial real-time systems have evolved over a long period of time and were initially so simple that it was possible to predict consequences of adding new functionality by common sense. However, as the system evolves the possibility to predict the consequences of changes becomes more and more difficult unless models and analysis method can be used. Moreover, traditional real-time models, e.g., fixed priority analysis, may be too simple for accurately capturing a complex system&apos;s characteristics. For instance, assuming worst-case execution time may not be realistic. Hence, analyses based on these models may give an overly pessimistic result.
This paper investigates a fourth approach: novel programming constructs. Specifically, we investigate the intersection of Modular Programming (MP) [1] and Aspect-Oriented Programming (AOP) [2]. We show in the context of an illustrative example how aspectual mechanisms work in AspectJ [3] and Hyper/J [4] and we show how combining modules with aspects provides better aspect-oriented modularity. We offer a concrete language design, Aspectual Collaboration (henceforth AC), both to demonstrate the feasibility and to investigate the details of such a combination
In this paper we present an approach to use fuzzy logic and data mining techniques in order to improve the quality of intelligent agents services in the Internet context. The overall objective is to design and implement a software model for collaborative environments capable of extracting knowledge from newsgroups, chat services ... The construction of the components of this software model is mainly based on fuzzy logic technologies (fuzzy clustering, fuzzy ontology, fuzzy synonymy relationship) and Java Software Development techniques. To represent and manage this knowledge we use fuzzy rules and fuzzy deformable prototypes.
We study the performance of file servers, comparing NFS implementation  in Linux to our experimental lightweight system called ORFA. The  aim is to find out NFS bottlenecks in the case of high bandwidth local  network.
Many heuristics have been developed for adapting on-disk data layouts to expected and observed workload characteristics. This paper describes a two-tiered software architecture for cleanly and extensibly combining such heuristics. In this architecture, each heuristic is implemented independently and an adaptive combiner merges their suggestions based on how well they work in the given environment. The result is a simpler and more robust system for automated tuning of disk layouts, and a useful blueprint for other complex tuning problems such as cache management, scheduling, data migration, and so forth.
Ten years ago, a group of researchers, led by  Francisco Varela, were proposing an alternative  vision of the immune system main behavior and  function. I was part of this group. This new  vision saw the immune system not as behaving  distinctively with self and non-self or according  to any dichotomy imposed a priori and from  outside (the self-recognition vision), but rather as  behaving in a unique way. From this indifferent  behavior, any external impact would  progressively been treated in two different ways,  reactive and tolerant, but now, consequently and  from inside the system (the self-assertion view).
We give an overview of issues surrounding computerverified theorem proving in the standard pure-mathematical context.
Introduction a. Purpose This appendix lays out a theoretical and practical framework for the collection and evaluation of data during rebound testing. As mentioned in Section 9-9, rebound tests (also known as &quot;pulsing tests&quot;) are commonly used to assess the attainment of cleanup in the vadose zone. However, little information is available to guide field practitioners through the data collection and its interpretation (Holbrook et al., 1998). The objective of this appendix is to fill this void by providing a step-by-step approach to planning and performing a rebound test along with straightforward mathematical techniques for evaluating the data to determine adequacy of cleanup. The material presented in this appendix is intended to be an introduction to rebound testing and serves as a practical starting point for further development of this important procedure. b. Background The intent of rebound testing is to assess residual contamination remaining in soils after a period of active rem
Wireplanning is an approach in which the timing of inputoutput paths is planned before modules are specified, synthesized or sized. If these global wires are optimally segmented and buffered, their delay is linear in the path length and independent of the position of the modules along these paths. From timing requirements, the total budget left to modules after allocating the appropriate delay to the wires can be determined. This paper describes how this budget can be optimally divided amongst the modules. A novel, static timing-like, mathematical programming formulation is introduced such that the total module area is minimized. Instead of only the worst delay, all pin-to-pin delays are implicitly taken into account. If area-delay tradeoffs are convex, a reasonable approximation in practice, the program can be solved efficiently. Further, efficiency of different formulations is discussed, and a low-cost method of making the budget relatively immune to downstream uncertainties and surprises is presented. The efficiency of the formulation is clear from benchmarks with over 2000 nodes and 5e19 paths.
abibliophile and text addict, who inhaled words and revelled in the worlds thus created iii iv
A locally decodable code encodes n-bit strings x in m-bit codewords C(x), in such a way  that one can recover any bit x i from a corrupted codeword by querying only a few bits of that  word. We use a quantum argument to prove that LDCs with 2 classical queries need exponential  length: m =  2    . Previously this was known only for linear codes (Goldreich et al. 02). Our  proof shows that a 2-query LDC can be decoded with only 1 quantum query, and then proves  an exponential lower bound for such 1-query locally quantum-decodable codes. We also show  that q quantum queries allow more succinct LDCs than the best known LDCs with q classical  queries. Finally, we give new classical lower bounds and quantum upper bounds for the setting  of private information retrieval. In particular, we exhibit a quantum 2-server PIR scheme with    ) qubits of communication, improving upon the O(n    ) bits of communication of the  best known classical 2-server PIR.
This paper retraces and analyzes the debate around a major infrastructure project in central Stockholm, the construction of a third railroad track over the islet of Riddarholm. Using the analytical framework of the New Rhetoric (Perelman and Olbrechts-Tyteca 1958), it shows that the debate is not only a matter of diverging views about the necessity or the impact of the project but, as well, a matter of epistemology. Whereas both sides tend to refer to similar values and make use of matching rhetorical devices, they differ quite radically as to which knowledge they regard as valid and as to how they have organized their approach to the debate. Demonstration faces argumentation, the New Rhetoric suggests, as its contribution to our understanding of the genesis of urban projects. Keywords: Urban Project, City Management, Infrastructure, Third Track, Railroads; Stockholm, Riddarholmen; Public Debate, Argumentation, New Rhetoric, Cham Perelman, Demonstrative Logic, Argumentative Logic; Narrative; Genius Loci. May 29   , 2000  The author acknowledges the financial support of the Swedish National Bank Tercentenary Fund; he is grateful for the press material that RTK [the Office of Regional Planning and Urban Transportation (Stockholm)] handed out; and he thanks Petra Adolfsson, Christine Blomqvist, Peter Dobers, Leif Holmberg, Peter Parker, and Richard Sotto as well as two anonymous reviewers for their constructive comments on previous drafts. Herv Corvellec  The Story Could Have Been Simple, However Intricate Urban Projects Can Be. Ten Years of Public Debate (1989-1999) Take Us Into The Genesis Of The Project. The New Rhetoric, Opposing Demonstration and Argumentation, Help Us Understand Why It Has Not. The Story Could Have Been Simple We are in Sweden, in the late 1980. Ther...
The main contribution of this thesis is a simplification, a generalization and some modifications of the homomorphic cryptosystem proposed by Paillier in 1999, and several cryptological protocols that follow from these changes. The Paillier
this document, we will present an overview of the framework and its metamodel, followed by specification of the infrastructure services. We will also relate out work to existing standardization and industry initiatives. 1.1 Notational Conventions The keywords &quot;MUST&quot;, &quot;MUST NOT&quot;, &quot;REQUIRED&quot;, &quot;SHALL&quot;, &quot;SHALL NOT&quot;, &quot;SHOULD&quot;, &quot;SHOULD NOT&quot;, &quot;RECOMMENDED&quot;, &quot;MAY&quot;, and &quot;OPTIONAL&quot; in this document are to be interpreted as described in RFC 2119 [RFC 2119]. This document uses namespace prefixes througout; they are listed in table 1.1. Note that the choice of any namespace prefix is arbitrary and not semantically   A working prototype is available at http://maximus.uvt.nl/kees/phd-kees/ 4 Prefix Namespace Notes wsdl http://www.w3.org/2003/06/wsdl WSDL 1.2 specification http http://www.w3.org/2003/06/wsdl/http HTTP bindings for WSDL 1.2 xs http://www.w3.org/2001/XMLSchema XML Schema specification  bpws http://schemas.xmlsoap.org/ws/2003/03/business-process/ BPEL4WS specification Table 1.1: Prefixes and namepsaces used in this document. significant. 5 EFSOC Framework and Metamodel Figure 2.1: The EFSOC framework EFSOC is a multi-tiered framework for service-oriented computing as shown in figure 2.1. The figure uses UML notation to represent that the framework consists of a number of tiers, each of which may contain any number of infrastructure services and any number of elements elements. All infrastructure services are regular services, which allows us to describe and publish them clearly in a standard format, such as WSDL. The core assumption of our approach is that all all services, business processes and actors play a certain role in achieving business goals and that coordination between subjects takes place by exchanging events. This is represented in the metamodel by the relati...
this paper, we only consider using the packet lifetime for discarding stale and duplicate data. When the real-time server is located in the Internet, clock synchronization between the lasthop router and the server would typically be required. The Network Time Protocol (NTP) [34] can provide sufficient accuracy for our purposes. When the realtime server is located close to the last-hop router or the network delay to the server is static and known, clock synchronization is not necessary. In our experience, it is a common practice for network operators to locate servers as close as possible to their intended users
We have extended the VRML standard to allow free-form manipulation of objects while immersed. Our driving goal was immersed training for equipment operations and maintenance, and to this end we developed a sensor that allows 6DOF  manipulation, a cooperating sensor that allows snapping objects into place as part of an assembly, and a two-handed manipulation approach for these sensors.
In this paper, we report a performance gap between a schedule with good makespan on the task scheduling model and the corresponding parallel program on distributed memory parallel machines. The main reason is the software overhead in the interprocessor communication. Therefore, speedup ratios of schedules on the model do not well approximate to those of parallel programs on the machines. The purpose of the paper is to get a task scheduling algorithm that generates schedules with good approximation to the corresponding parallel program.
 There is growing evidence that consumers are influenced by Internet-based opinion forums before making a variety of purchase decisions. Firms whose products are being discussed in such forums are therefore tempted to try to manipulate consumer perceptions by injecting anonymous messages that praise their products or by offering incentives to consumers to do so. This paper offers a theoretical analysis of the impact of such strategic behavior on firm profits and consumer surplus. We examine a setting where two firms simultaneously introduce imperfect substitute experience goods of different qualities and consumers obtain quality information from an online forum. Firms attempt to influence consumer beliefs about their respective quality through costly forum manipulation. The most striking result of our analysis is that strategic manipulation can either decrease or increase the information value of online forums to consumers relative to the case where no manipulation takes place. Specifically, there exist settings where the presence of honest consumer opinions induces firms to reveal their own, more precise, knowledge of product qualities by manipulating at relative intensities that are proportional to their actual qualities. However, even in such cases, if a sufficiently large number of honest consumers post their opinions online, the cost of manipulation to firms outweighs its benefits: the impact of the incremental signal precision on firm revenues is lower than the corresponding manipulation costs. The social cost of online manipulation can be reduced by developing technologies that increase the unit cost of manipulation and by encouraging higher participation of consumers who post “honest” opinions.
In this paper we consider the fully discrete wavelet Galerkin scheme for the fast solution of boundary integral equations in three dimensions. It produces approximate solutions within discretization error accuracy oered by the underlying Galerkin method at a computational expense that stays proportional to the number of unknowns. We focus on implementational details of the scheme, in particular on numerical integration of relevant matrix coecients. We illustrate the proposed algorithms by numerical results.
This paper describes some aspects of the software development approach  which is used in open source software development projects and has  evolved over time to a successful software development model. It identifies and  the core processes and the deployed software infrastructure of this software development  model. This enables the identification of possible improvement opportunities  and possibilities to enhance the open source software development  model and to adopt some elements of this approach in a commercial software  development environment. Since open source software development has already  been proven, that it is able to produce successful software products it  seems to make sense to integrate some successful elements of this approach  into proprietary and distributed software development projects.
Distributed systems are inherently complex, and therefore difficult to design and develop. Experience shows that new technologies---such as components, aspects, and application frameworks---can be effectively used for building distributed applications. However, our experience also shows that most of the applications built in that way are difficult to be re-used, documented, and maintained. Probably, one of the major reasons is the lack of a clear separation between the concepts used at different levels (application domain, application architecture, supporting application platform, programming language, etc.). In this paper we present our experience with a platform we developed for building distributed applications using components and aspects. In particular, we show how many of the (conceptual) problems we hit when trying to document, re-use, and implement it in different contexts can be naturally solved with the adoption of the MDA concepts. In addition, we describe the process we followed for identifying and separating the entities that live in different &quot;models&quot; (in the MDA sense), and the required transformations between them. MDA offers a good framework for specifying different views of our model, and mappings to platform-specific profiles. In this way, we are able to address the particular needs of different stakeholders: from the designer interested in developing new applications following our (component and aspect-based) modeling approach, to the software vendor that wants to implement a proprietary version of our supporting middleware framework in CORBA, EJB or .NET.
As an example of modeling complex dynamic real-life systems, like those represented by the widespread &quot;marketplace pattern&quot;, dynamic market trends (DMT) for DAX options are forecasted.
This paper describes the qualitatively different views of users that Finnish IS  designers have. This view is a basis for the IS-user relationship. A method of  empirical research for investigating human beings&apos; views of the surrounding world,  phenomenography, is presented. The preliminary results of the analysis indicate that  IS designers tend to give meaning to users through the intentions of the situation in  question rather than connecting characteristics typical of a human being to the user. A Human being
Interested in including a skylight in your next building design, but not quite sure if the benefits outweigh the costs? Looking for ways to improve the design of the skylights you currently manufacture? Help is at hand, thanks to SkyVision, an easy-to-use Windows-based computer program. SkyVision predicts skylight performance for any given day and for various types of skylights. The software is a useful tool whether you are a building designer or architect, skylight manufacturer or educator.
The increasing popularity of information services that rely on content delivery in mobile environments motivates the need for a mobile push service---an efficient and flexible content dissemination service that targets mobile users. We analyze the features of a mobile push service by investigating representative usage scenarios and propose an architecture for mobile content delivery systems. The architecture is based on the publish/subscribe (P/S) paradigm which supports many-to-many interaction of loosely-coupled entities. We define the set of services that need to collaborate with the P/S infrastructure to address the dynamics of mobile environments.  
We present a scheme for highlighting the trust issues of merit within pervasive computing, based on an analysis of scenarios from the healthcare domain. The first scenario helps us define an analysis grid, where the human and technical aspects of trust are considered. The analysis is applied to a second scenario to examine its suitability. We then discuss the various categories of the analysis grid in the light of this examination and of the literature on the subject of trust. We believe that this approach could form the basis of a generalised trust analysis framework to support the design, procurement and use of pervasive computing.
The influence NPs can have on the aspectual behaviour of verbal expressions, witness the pair &apos;eat an apple in ten minutes/*for ten minutes&apos; and &apos;eat apples *in ten minutes/for ten minutes, requires an analysis of how static semantic information (NP) interacts with dynamic semantic information (verb). An interpretation of verbs and NPs is presented in which the interaction is analyzed by using an extension of dynamic logic (DL). First, models for DL are extended by adding a domain E of events (together with an event structure E). The intuition behind this addition is that each transition (pair of states) which is an element of the relation denoted by a program in DL is brought about by an event from E. This makes it possible to view a change either as an object (event) or as a transformation of a state. Second, in addition to sequential programs, parallel programs (relations between sets of states) are introduced. At the level of E this corresponds to the distinction between events and sets of events. The dynamic component of a verbal expression denotes an event-type P that corresponds to a program (relation between states) at the level of the transition structure S. This program has particular properties in terms of which aspectual distinctions are defined. The parallel program corresponding to sets of events is partly determined by the cardinality information introduced by the determiner as part of an argument NP. At the level of E this information functions as a boolean condition expressing the result that is brought about by the set of events. Static information therefore interacts with dynamic information by providing a condition that must hold upon termination of events.
One of the most fundamental tasks of an automatic parallelization tool is to find an optimal domain decomposition for a given application. For regular domain problems (such as simple matrix manipulations) this task may seem trivial. However, communication costs in message passing programs often significantly depend on the memory layout of data blocks to be transmitted. As a consequence, straightforward domain decompositions may be non-optimal.
This paper presents a comparative evaluation  among the systems that participated  in the Spanish and English lexical sample  tasks of SENSEVAL-2. The focus is  on pairwise comparisons among systems  to assess the degree to which they agree,  and on measuring the difficulty of the test  instances included in these tasks.
Rate Monotonic Analysis (RMA) is a standard technique for the analysis of task schedulability which has also successfully been applied to the schedulability analysis of CAN messages. The prediction of worst-case end-to-end response times in a distributed CAN application requires the integrated analysis of both tasks and messages - the socalled &quot;Holistic&quot; approach. The work discussed in this paper contributes to the practical application of an holistic approach to analysis by: a) providing a framework for describing distributed periodic systems as graphs of precedence constrained tasks and messages, b) developing a tool to automate the analysis and support systems design and, c) validating the approach by empirical means. We describe the analysis tool, X rma, and show how it supports the design and evaluation of distributed hard real-time systems. The analysis of an example distributed control system composed of multiple control/feedback loops is presented to illustrate how the tool is used to verify that critical end-toend response times can be met by an implementation. To verify the analysis, timing measurements have been conducted on distributed control systems which use the VxWorks real-time kernel for task scheduling and CAN for inter-processor communication. The empirical results confirm that the analytical approach allows reliable bounds to be predicted for distributed responses in systems of practical complexity. However, it is shown that predicting tight bounds requires the elimination of pessimistic properties of distributed scheduling models.
A central problem in analysis of gene expression data is clustering of genes with similar expression  profiles. In this paper, I describe an hierarchical clustering procedure that is based on simple probabilistic  model. This procedure clusters genes with respect to a target classification of conditions, so that genes  that are expressed similarly in each group of conditions are clustered together.
Portals have proven to be useful client-side applications for providing user-oriented services for accessing the grid. Grids are increasingly being used for collaborative work within the scientific community. The job processing time for high performance computations can be reduced by the usage of computational grids, with its wide availability to resources. Grid users would similarly benefit from having access to databases, mainly, those involved in collaborative data analysis of large datasets and those requiring sharing of data. OGSA-DAI provides an extension to the OGSA framework by allowing access to and integration of data held in heterogeneous data resources. In this paper, we describe our experiences in designing and building a portlet to OGSA-DAI and in testing the grid services access to a relational database by means of a synthetic database workload.
In this paper we present results from a detailed measurement study of TCP (Transmission Control Protocol) running  over a wireless link. Our primary goal was on obtaining a breakdown of the computational energy cost of TCP at the  sender and receiver (excluding radio energy costs) as a first step in developing techniques to reduce this cost in actual  systems. We analyzed the energy consumption of TCP in FreeBSD 4.2 and FreeBSD 5 running on a wireless laptop and  Linux 2.4.7 running on a wireless HP iPAQ 3630 PocketPC. Our initial results showed that 60 - 70% of the energy cost  (for transmission or reception) is accounted for by the Kernel -- NIC (Network Interface Card) copy operation. Of the  remainder,  ff15%  is accounted for in the copy operation from user space to kernel space with the remaining 15% being  accounted for by TCP processing costs. We then further analyzed the TCP processing cost and determined the cost of  computing checksums accounts for 20 -- 30% of TCP processing cost. Finally, we determined the processing costs of  two primary TCP functions -- timeouts and triple duplicate ACKs. Putting all these costs together, we present techniques  whereby energy savings of between 20% -- 30% in the computational cost of TCP can be acheived.
In order to capture current educational practices in eLearning courses, more advanced `learning design&apos; capabilities are needed than are provided by the open eLearning specifications hitherto available. Specifically, these fall short in terms of multi-role workflows, collaborative peer-interaction, personalization and support for learning services. We present a new specification that both extends and integrates current specifications to support the portable representation of units of learning (e.g. lessons, learning events) that have advanced learning designs. This is the Learning Design specification. It enables the creation of a complete, abstract and portable description of the pedagogical approach taken in a course, which can then be realized by a conforming system. It can model multi-role teaching-learning processes and supports personalization of learning routes. The underlying generic pedagogical modelling language has been translated into a specification (a standard developed and agreed upon by domain and industry experts) that was developed in the context of IMS, one of the major bodies involved in the development of interoperability specifications in the field of eLearning. The IMS Learning Design specification is discussed in this article in the context of its current status, its limitations and its future development.  
The relationship between graph coloring and the immersion order is considered. Vertex  connectivity, edge connectivity and related issues are explored. These lead to the  conjecture that, if G requires at least t colors, then G must have immersed within it  K t , the complete graph on t vertices. Evidence in support of such a proposition is  presented. For each fixed value of t, there can be only a finite number of minimal  counterexamples. These counterexamples are characterized based on Kempe chains,  connectivity, cutsets and degree bounds. It is proved that minimal counterexamples  must, if any exist, be both 4-vertex-connected and t-edge-connected. The t = 5 case is  examined in additional detail. The historical context and probable difficulty of settling  this conjecture, as well as specific hurdles to its final resolution, are also discussed.
We investigate the orbital structure in a class of three-dimensional (3D) models of barred galaxies. We consider different values of the pattern speed, of the strength of the bar and of the parameters of the central bulge of the galactic model. The morphology of the stable orbits in the bar region is associated with the degree of folding of the x1 characteristic. This folding is larger for lower values of the pattern speed. The elongation of rectangular-like orbits belonging to x1 and to x1-originated families depends mainly on the pattern speed. A detailed investigation of the trees of bifurcating families in the various models shows that major building blocks of 3D bars can be supplied by families initially introduced as unstable in the system, but becoming stable at another energy interval. In some models without radial and vertical 2:1 resonances we find, except for the x1 and x1-originated families, also families related to the z-axis orbits, which support the bar. Bifurcations of the x2 family can build a secondary 3D bar along the minor axis of the main bar. This is favoured in the slowly rotating bar case.
Recent advances in the technology of multi-unit recordings make it possible to test Hebb&apos;s hypothesis that neurons do not function in isolation but are organized in assemblies. This has created the need for statistical approaches to detecting the presence of spatiotemporal patterns of more than two neurons in neuron spike train data. We examine three measures for the presence of higher order patterns of neural activation -- coefficients of log-linear models, connected cumulants, and redundancies -- and present arguments in favor of the coefficients of log-linear models. We present test statistics for detecting the presence of higher order interactions in spike train data. We also present a Bayesian approach for inferring the existence or absence of interactions and estimating their strength. The two methods are shown to be consistent in the sense that highly significant correlations are also highly probable. A heuristic for the analysis of temporal patterns is also proposed. The methods are applied to experimental data, to synthetic data drawn from our statistical models, and to synthetic data produced from biologically plausible simulations of neural activity. Our experimental data are drawn from multi-unit recordings in the somato-sensory cortex of anesthetized rats obtained by Diamond, from multi-unit recordings in the frontal cortex of behaving monkeys obtained by Vaadia and from multi-unit recordings in the visual cortex of behaving monkeys obtained by Freiwald.
This report explains how the Rapidly Deployable Radio Network ffRDRNff establishes the connections between an edge node and its associated remote nodes. The RDRN tries to maximize the minimum signal-to-interference ratio of the remote nodes by optimally deciding the number radio beams to form, the direction to steer each beam, the relativepower of each beam, and the assignment of beams to remote nodes. This report documents how the RDRN makes these decisions. Contents  1 
Independent component analysis (ICA) has proved to be a highly useful tool for modeling brain data and in particular electroencephalographic (EEG) data. In this paper, a new method is presented that may better capture the underlying source dynamics than ICA algorithms hereto employed for brain signal analysis. We suppose that a brief, impulse-like activation of an effective signal source elicits a short sequence of spatio-temporal activations in the measured signals. This leads to a model of convolutive signal superposition, in contrast to the instantaneous mixing model commonly assumed for independent component analysis of brain signals. In the spectral-domain, convolutive mixing is equivalent to multiplicative mixing of complex signal sources within distinct spectral bands. We decompose the recorded mixture of complex signals into independent components by a complex version of the infomax ICA algorithm. Some results from a visual spatial selective attention experiment illustrate the differences between real time-domain ICA and complex spectral-domain ICA, and highlight properties of the obtained complex independent components.
Reflective Relational Machines were introduced by S. Abiteboul, C. Papadimitriou and V. Vianu in 1994, as variations of Turing machines which are suitable for the computation of queries to relational databases. The machines are equipped with a relational store which can be accessed by means of dynamically built first order logic queries. We initiate a study on approximations of computable queries, defining, for every natural k, the k-approximation of an arbitrary computable query q. This, in turn, motivates us to define a new variation of Reflective Relational Machines by considering two different logics to express dynamic queries: one for queries and a possibly different one for updates to the relational store. We prove several results relating k-approximations of queries with the new machines, and also with classes of queries defined in terms of preservation of equality of FO^k theories. Finally, we summarize a few open problems related to our work.
To overcome problems in understanding security protocols, a constructivist approach is  used. Learners are enabled to experiment with a given protocol, either alone or in a team of  co-learners. This means, to use pre-defined and automatically generated PROMELA building  blocks for all communicating parties and to support students in playing some roles in  an interactive way. To explain these building blocks more detailed, the Needham-SchroederPublic  -Key authentication protocol is given as an example.
DSPs with dual memory banks offer high memory bandwidth, which is required for high-performance applications. However, such DSP architectures pose problems for C compilers, which are mostly not capable of partitioning program variables between memory banks. As a consequence, timeconsuming assembly programming is required for an efficient coding of time-critical algorithms. This paper presents a new technique for automatic variable partitioning between memory banks in compilers, which leads to a higher utilization of available memory bandwidth in the generated machine code. We present experimental results obtained by integrating the proposed technique into an existing C compiler for the AMS Gepard, an industrial DSP core.
A Java virtual machine (JVM) must sometimes check whether a value of one type can be can be treated as a value of another type. The overhead for such dynamic type checking can be a significant factor in the running time of some Java programs. This paper presents a variety of techniques for performing these checks, each tailored to a particular restricted case that commonly arises in Java programs. By exploiting compile-time information to select the most applicable technique to implement each dynamic type check, the run-time overhead of dynamic type checking can be significantly reduced.
A flow-directed inlining strategy uses information derived from control-flow analysis to specialize and inline procedures for functional and object-oriented languages. Since it uses control-flow analysis to identify candidate call sites, flowdirected inlining can inline procedures whose relationships to their call sites are not apparent. For instance, procedures defined in other modules, passed as arguments, returned as values, or extracted from data structures can all be inlined. Flow-directed inlining specializes procedures for particular call sites, and can selectively inline a particular procedure at some call sites but not at others. Finally, flow-directed inlining encourages modular implementations: control-flow analysis, inlining, and post-inlining optimizations are all orthogonal components. Results from a prototype implementation indicate that this strategy effectively reduces procedure call overhead and leads to significant reduction in execution time. 1 Introduction Functio...
Preference programming provides a new paradigm  for expressing (default) decisions, preferences between  decisions, and search strategies in a declarative  and unified way and for embedding them in  a constraint and rule language. Business experts  can thus directly specify preferences and search directives  in form of rules without needing to program  search strategies as required by constraint  programming based configuration tools. Preference  programming allows to describe preferences  between individual decisions, as well as groups of  decisions and decision rules. There can be dynamic  (or context-dependent) preferences, inconsistent  preferences, and meta-preferences. Following  [Brewka, 1989; Junker, 1997] , preferences constrain  the order in which decisions are made during  search. It is possible to enumerate all configurations  or to focus search to preferred configurations,  which respect the default choices and preferences  of the user.
This paper  explores the question, How should this curriculum be revised in light of  the Texas Instruments TI-92, a hand-held hybrid of graphing calculator  and computer?  BACKGROUND  The C    PC project, now some 10 years old, is an ongoing curriculum-development and teacher-enhancement effort directed by Franklin Demana and Bert K. Waits. The teacher-enhancement component is based on intensive week-long summer inservice courses that have been attended by thousands  of high school teachers at locations throughout the United States. These courses are now taught by a cadre of high school teachers as a part of the Teachers Teaching with Technology (T  3  ) program, headquartered at the University of Texas at Arlington. The curriculum-development component has led to a series of high school and college textbooks. The first regular edition of the high school text was Demana and Waits (1990). From the beginning, the curriculum has been designed to help students acquire the skills and understandings necessary for the successful study of calculus and science. The text materials focus on functions and graphs because students&apos; lack of understanding of graphing and functions accounts for a major portion of their difficulties in calculus and because focusing students&apos; attention on graphs and functions improves their readiness for calculus
This presentation provides a retrospective overview of asset management in the construction industry and emphasis is placed on assessing the decision-support tools for municipal infrastructure planning. The present study classifies levels of implementation of asset management using the six &quot;Whats&quot; for asset management, a proposed implementation plan for the domain. The study identifies the extent of the asset management market in North America according to these six &quot;Whats&quot;; addresses the need for decision-support tools for municipal-type organizations, and identifies the challenges for maintenance, repair and renewal planning faced by asset owners and managers. Integration with existing systems such as computerized maintenance management systems, geographic information systems and corporate legacy systems is the largest challenge for developing and using decision-support tools In reply to: asset management.
A pregroup is a partially ordered monoid in which every element has a left and a right adjoint. The main result is that for some well-behaved subgroups of the group of diffeomorphisms of the real numbers, the set of all endofunctions of the integers that are asymptotic at   to (the restriction to the integers of) a function in the subgroup is a pregroup. 1. 
We consider the design of mechanisms for pricing multicast transmissions.
In this paper, we analyze the behavior of packet-switched communication networks in which packets arrive dynamically at the nodes and are routed in discrete time steps across the edges. We This work was supported by Army grant DAAH 04-95-1-0607 and ARPA contract N00014-95-1-1246. A preliminary version of this work appeared in Proceedings of the 1996 IEEE Symposium on Foundations of Computer Science. IEEE Computer Society Press, Los Alamitos, Calif.
Equation-based congestion control has been a promising alternative to TCP for real-time multimedia streaming over the Internet. However, its behavior remains unknown in the mobile ad hoc wireless network (MANET) domain. In this paper, we study the behavior of TFRC (TCP Friendly Rate Control [3, 4]) over a wide range of MANET scenarios, in terms of throughput fairness and smoothness. Our result shows that while TFRC is able to maintain throughput smoothness in MANET, it obtains less throughput than the competing TCP flows (i.e., being conservative). We analyze several factors contributing to TFRC&apos;s conservative behavior in MANET, many of which are inherent to the MANET network. We also show that TFRC&apos;s conservative behavior cannot be completely corrected by tuning its loss event interval estimator. Our study shows the limitations of applying TFRC to the MANET domain, and reveals some fundamental difficulties in doing so.
this paper, we first survey a line of research --- detailed in [58, 24, 59, 86, 87, 16] --- aimed at recasting Petri net processes in the light of ideas from process algebras and categorical algebra. In particular, we shall focus on Petri net concatenable processes [24, 86], on strongly concatenable processes [87, 16], and on their representation in terms of symmetric monoidal categories
Test plans are used to guide, organize and document the testing activities during hardware design process. Manual test planning and configuration is known to be labor intensive, time consuming and error prone. It is desirable to develop efficient approaches to model testing and to develop test tools to automate test-planning activities.
The subdifferential formula for the sum of two convex functions defined  on a locally convex space is proved under a general qualification condition. It is  proved that all the similar results which are already known can be derivated from  the formula.
Novelty detection is the ident ification of new or unknown data or signal that a machine learning system is not aware of during training. In this paper we focus on neural network based approaches for novelty detection. Statistical approaches are covered in part-I paper.
So-called sensor nodes combine sensors, processors, wireless communication capabilities, and autonomous power supply in a tiny device. Large-scale networks of these untethered devices can be deployed unobtrusively in the physical environment in order to monitor a wide variety of realworld phenomena with unprecedented quality and scale. A fundamental service in sensor networks is the determination of time and location of events in the real world. This task is complicated by various challenging characteristics of sensor networks, such as their large scale, high network dynamics, restricted resources, and restricted energy. We develop new approaches for determination of time and location under these constraints, and devise design principles based on our experience. We illustrate the practical feasibility of our approaches by a concrete application.
Genuinely valuable educational resources are becoming  widely available via the Internet and our pedagogic  understanding of how to make use of these valuable  resources is increasing rapidly. However locating  information and other resources is often a serious barrier to genuinely effective use of the Internet.
This report documents research undertaken in the summer of 2001 to investigate the nature of visitor experiences at the Explore@Bristol interactive science museum. The experimental work involved an observational study of visitor behaviour at six of the exhibits in Explore, augmented by interviews and discussions. The outcome is a provisional model of visitor experience identifying three dimensions that might make such experiences compelling: Drama/Sensation, Challenge/Self Expression, and Social. Though developed in the context of Explore, we consider the model to have wider application in the consumer space. * Internal Accession Date Only Approved for External Publication   Enquiries concerning this report should be directed to Richard Hull email: rh@hplb.hpl.hp.com   Copyright Hewlett-Packard Company 2002 1 
We address the design of complex monolithic systems, where processing cores generate and consume a varying and large amount of data, thus bringing the communication links to the edge of congestion. Typical applications are in the area of multi-media processing. We consider a meshbased Networks on Chip (NoC) architecture, and we explore the assignment of cores to mesh cross-points so that the traffic on links satisfies bandwidth constraints. A single-path deterministic routing between the cores places high bandwidth demands on the links. The bandwidth requirements can be significantly reduced by splitting the traffic between the cores across multiple paths. In this paper, we present NMAP, a fast algorithm that maps the cores onto a mesh NoC architecture under bandwidth constraints, minimizing the average communication delay. The NMAP algorithm is presented for both single minimum-path routing and splittraffic routing. The algorithm is applied to a benchmark DSP design and the resulting NoC is built and simulated at cycle accurate level in SystemC using macros from the pipes library. Also, experiments with six video processing applications show significant savings in bandwidth and communication cost for NMAP algorithm when compared to existing algorithms.
In this report we explore some interesting combinatorial properties of minimum cuts of a graph such as submodularity of cuts and crossing cuts lemma. Our main goal is to represent all the (at most n(n-1)/2) min-cuts of a graph compactly, and retrieve them in O(k) time, where k is the size of any min-cut. We discuss various properties of the cactus, and the algorithm by Karzanov and Timofeev to construct the cactus of an unweighted, undirected graph in O(kn^2) time. Here we propose a new algorithm, which constructs the cactus of an weighted, undirected graph in O(nm + n^2 log n + tm log n) time, where t is the number of cycles in the resulting cactus.
this paper, we do not focus on the quality of information, but on the quality of web pages. In other words, we present examples of good practise, as well as selected results from research done in the agri- food  sector. We will try to give useful tips on how to prevent common mistakes when creating a web site
We propose a framework which extends Antitonic Logic Programs  [13] to an arbitrary complete bilattice of truth-values, where belief  and doubt are explicitly represented. Inspired by Ginsberg and Fitting  &apos;s bilattice approaches, this framework allows a precise de  nition of  important operators found in logic programming, such as explicit and  default negation. In particular, it leads to a natural semantical integration  of explicit and default negation through the Coherence Principle  [38], according to which explicit negation entails default negation. We  then de  ne Coherent Answer Sets, and the Paraconsistent Well-founded  Model semantics, generalising many paraconsistent semantics for logic  programs. In particular, Paraconsistent Well-Founded Semantics with  eXplicit negation (WFSXp ) [3, 11]. The framework is an extension of Antitonic  Logic Programs for most cases, and is general enough to capture  Probabilistic Deductive Databases, Possibilistic Logic Programming, Hybrid  Probabilistic Logic Programs, and Fuzzy Logic Programming. Thus,  we have a powerful mathematical formalism for dealing simultaneously  with default, paraconsistency, and uncertainty reasoning. Results are  provided about how our semantical framework deals with inconsistent  information and with its propagation by the rules of the program.
Switched Reluctance Machines (SRMs) are receiving significant attention from industries in the last decade. They are extremely inexpensive, reliable and weigh less than other machines of comparable power outputs. Although the design principles of the machine are available as a concatenation of many different sources, the need for a unified, step-by-step design procedure from first principles of electromagnetics is an absolute requirement. This dissertation discusses a procedure that can be applied by engineers with a basic background in electromagnetics. Subsequent to the design of the machine, existing finite element software can do the analysis of the machine. However, this is a laborious process and the need for an analytical method is preferable to verify the design procedure before the final verification by finite elements. The analytical procedure as well as a procedure to calculate iron losses is also developed in this dissertation. A prototype machine has been developed as an example of the design process and an existing prototype is analyzed to verify the analysis procedure. The similarities between the SRM and the Permanent Magnet Brushless DC Machine (PMDBC) beg the consideration of the development of a converter that can be used to drive either machine. One such converter has been developed in this dissertation. The design of the drive for both the machines is seen to be very similar. As a consequence, a universal controller that can be used to operate both machines has been developed and implemented with a DSP. Simulations and experimental correlation for both drives have been presented. iii  Dedication To my parents,  Dr. Prabha Vijayraghavan and Mr. R. Vijayraghavan iv  Acknowledgements I wish to express my gratitude to my advisor, Professor Krishnan Ra...
Polar ordination is used to project all pixels in Landsat TM band space  onto a single axis whose endpoints are the centroids of training areas in two spectrally  distinct natural forest types -- one with and the other without bamboo -- which dominate  the landscape in the southwest Amazon. Assuming the same wood density in both  forests, biometric data collected in the training areas (Oliveira, 2000) indicated that  biomass of forest with high bamboo density was 29% lower than forest without  bamboo. Linear interpolation along the ordination axis models biomass of spectrally  intermediate pixels. Thresholds based on band-space distance from the axis and  ordinated distance beyond the axis endpoints are used to mask pixels unacceptable for  modeling: water, beaches, pastures, roads, urban area, and deep topographic shade.
Metric-Space Denotational  Semantics for Reactive Probabilistic Processes  M.Z. Kwiatkowska and G.J. Norman  School of Computer Science,  University of Birmingham,  Edgbaston, Birmingham B15 2TT, UK  Abstract  We consider the calculus of Communicating Sequential Processes (CSP) [8] extended with action-guarded probabilistic choice and provide it with an operational semantics in terms of a suitable extension of Larsen and Skou&apos;s [14] reactive probabilistic transition systems. We show that a testing equivalence which identi  es two processes if they pass all tests with the same probability is a congruence for a subcalculus of CSP including external and internal choice and the synchronous parallel. Using the methodology of de Bakker and Zucker [3] introduced for classical process calculi, we derive a metric-space semantic model for the calculus and show it is fully abstract.
We present an analysis of images of Saturn&apos;s moon Titan, obtained by the Voyager 1 spacecraft on November 8--12, 1980. Orange filter (590--640 nm) images were photometrically corrected and a longitudinal average removed from them, leaving residual images with up to 5% contrast, and dominated by surface reflectivity. The resultant map shows the same regions observed at 673 nm by the Hubble Space Telescope (HST). Many of the same albedo features are present in both datasets, despite the short wavelength (600 nm) of the Voyager 1 images. A very small apparent longitudinal offset over the 14 year observation interval places tight constraints on Titan&apos;s rotation, which appears essentially synchronous at 15.9458    0.0016 days (orbital period    0.000005 days). The detectability of the surface at such short wavelengths puts constraints on the optical depth, which may be overestimated by some fractal models.
An optimal numerical algorithm for the reconstruction of a surface from its shading  image is presented. The algorithm solves the 3D reconstruction from a single shading  image problem. The shading image is treated as a penalty function and the hight of the  reconstructed surface is a weighted distance. A first order numerical scheme based on  Sethian&apos;s Fast Marching Method [19, 18] is used to compute the reconstructed surface. The surface
How does contrast affect reading rate? What is the role of contrast sensitivity? We measured reading rate as a function of the contrast and character size of text for subjects with normal vision. Reading rates were highest (about 350 words/min) for letters ranging in size from 0.25 to 2. Within this range, reading was very tolerant to contrast reduction-for 1 letters, reading rate decreased by less than a factor of two for a tenfold reduction in contrast. The results were very similar for white-on-black and black-on-white text. Reading rate declined more rapidly for very small (&lt; 0.25) and very large (&gt; 2) letters. People with low vision usually require large characters to read, so high contrast is particularly important for them. Taking 35 words/min to be a threshold for reading, we constructed a contrast-sensitivity function (CSF) for reading. We were able to relate the shape of this CSF to the shape of sine-wave grating CSFs. 
This study is part of research that is investigating the notion that human performance in dynamic and intentional decision making environments, such as ambulance dispatch management, can be improved if information is portrayed in a manner that supports the decision strategies invoked to achieve the goal states of the process being controlled. Hence, in designing interfaces to support real-time dispatch management decisions, it is suggested that it would be necessary to first discover the goal states and the decision strategies invoked during the process, and then portray the required information in a manner that supports such a user group&apos;s decision making goals and strategies. The purpose of this paper is to report on the experiences gleaned from the use of a cognitive task analysis technique called Critical Decision Method as an elicitation technique for determining information portrayal requirements. This paper firstly describes how the technique was used in a study to identify the goal states and decision strategies invoked during the dispatch of ambulances at the Sydney Ambulance Co-ordination Centre. The paper then describes how the interview data was analysed within and between cases in order to reveal the goal states of the ambulance dispatchers. A brief description of the resulting goal states follows, although a more detailed description of the goals states and their resulting display concepts has been reported elsewhere (Wong et al., 1996b). Finally, the paper concludes with a set of observations and lessons learnt from the use of the Critical Decision Method for developing display design concepts in dynamic intentional environments.
Paracatadioptric sensors combine a parabolic shaped mirror and a camera inducing an orthographic projection. Such a configuration provides a wide field of view while keeping a single effective viewpoint. In general the paracatadioptric image of a line is a conic curve. The estimation of line images is an important subject for applications such as reconstruction and visual control of motion. However the estimation of the conic curves where lines are mapped is hard to accomplish. In general only a small arc of the conic is visible in the image and conventional conic fitting techniques are unable to correctly estimate the curve. This paper shows that line images can be accurately estimated by constraining the search space. A conic curve is the paracatadioptric image of a line if, and only if, the image of the circular points lie on the curve and two certain points are conjugate with respect to the conic. Considering the space of all conic curves, the line images lie in a linear subspace which depends on the system calibration. The paracatadioptric projection of a line can estimated by fitting a conic in the subspace to the data points. The proposed approach is computationally efficient since the fitting problem can be solved by an eigensystem 1 
The common software engineering education method of theory presented in lectures along with application of these theories in an associated class project is insufficient, on its own, to effectively communicate the complex, fundamental dynamics underlying real-world software engineering processes. This paper introduces and lays out plans for SimSE, a detailed, graphical, fully interactive educational software engineering simulation environment that teaches the software process in a practical manner without the time and scope constraints of an actual class project. Once completed, this tool will enable students to form a concrete understanding of the software process by allowing its users to explore different approaches to managing the software process and giving them insight into the complex cause and effect relationships underlying the process.
In a modern business process, documents are important carriers of  information between organisations. Document management solutions are increasingly  based on structured document formats, such as XML (Extensible  Markup Language). However, deployment of structured documents is a complicated  task, which requires systematic analysis of requirements of all groups  either producing or using documents. In this paper, we describe a method for  modelling user roles in document analysis. The method is used to support  analysis of requirements of individual and organisational users of documents as  well as analysis of organisational needs related to security and access control.
This paper presents a method for constructing queries that are sufficient to retrieve a target web page. These queries can be thought of as content-based addresses for the target page and can have many potential uses.  KEYWORDS: distributed digital libraries, query, web, search engines, content-based addresses, dead links, QuerySearch  INTRODUCTION  In a global network of digital libraries or indeed any d istr ibuted digital library, address-based documen t identifiers can fail in many ways. Documents can easily be lost if any part of the address changes, i.e., if the document moves or the domain name changes or disappears. One solution to this problem is to introduce Universal Resource Names as persistent, globally unique identifiers just like the ISBN for printed books (1). A complementary solution that does not require any naming authority and that has additional benefits is to create content-based addresses. The World Wide Web (2) is one model of a distributed, global, digital libr...
A new algorithm, called Hamming Clustering (HC), for the solution of classification  problems with binary inputs is proposed. It builds a logical network containing only and,  or and not ports, which, besides satisfying all the input-output pairs included in a given  finite consistent training set, is able to reconstruct the underlying Boolean function. The basic
Modal and modal-like formalisms such as temporal or description  logics go beyond propositional logic by introducing operators  that allow for a guarded form of quantification over states or paths of  transition systems. Thus, they are more expressive than propositional  logic, yet computationally better behaved than first-order logic. We propose  constraint-based methods to model and solve modal satisfiability  problems. We model the satisfiability of basic modal formulas via appropriate  sets of finite constraint satisfaction problems, and then resolve  these via constraint solvers. The domains of the constraint satisfaction  problems contain other values than just the Boolean 0 or 1; for these values,  we create specialised constraints that help us steer the decision procedure  and so keep the modal search tree as small as possible. We show  experimentally that this constraint modelling gives us a better control  over the decision procedure than existing SAT-based models.
This paper describes a Web-services-based system which we have developed to enable organizations to semi-automatically preserve their digital collections by dynamically discovering and invoking the most appropriate preservation service, as it is required. By periodically comparing preservation metadata for digital objects in a collection with a software version registry, potential object obsolescence can be detected and a notification message sent to the relevant agent. By making preservation software modules available as Web services and describing them semantically using a machine-processable ontology (OWL-S), the most appropriate preservation service(s) for each object can then be automatically discovered, composed and invoked by software agents (with optional human input at critical decisionmaking steps). We believe that this approach represents a significant advance towards providing a viable, cost-effective solution to the long term preservation of large-scale collections of digital objects.
It has become widely accepted that in neurons, spike timing is important.
In [D] Dumont showed that certain classes of permutations on n letters are counted by the Genocchi numbers. In particular, Dumont showed that the (n + 1)st Genocchi number is the number of permutations on 2n letters with the following properties: (1) each even integer must be followed by a smaller integer (this rule disallows the sequence from ending with an even integer), (2) each odd integer is either followed by a larger integer or is final in the sequence. We call such permutations by Dumont permutations of the first kind. In this paper we study the number of Dumont permutations of the first kind on n letters avoiding the pattern 132 and avoiding (or containing exactly once) an arbitrary pattern on k letters. In several interesting cases the generating function depends only on k. 
This document provides description of evaluation tests performed on the SCAMPI  architecture. This is the first release of D3.4 deliverable, which includes performance tests of the  SCAMPI architecture software running on top of the Gigabit Ethernet SCAMPI adapter or commodity Intel Gigabit Ethernet adapter with monitoring functions implemented in software and first application  tests. The final release of D3.4 will include also tests with the 10 Gigabit Ethernet SCAMPI adapter,  tests with monitoring functions implemented in firmware and more application tests. These tests are  yet to be performed, due to ongoing software, firmware and hardware development.
In 2001, the United Kingdom Engineering and Physical Sciences Research Council awarded three of the authors of this paper a grant for &quot;A Web-based resource for design theory&quot;. As the project developed, we have had to face a number of problems, ranging from fundamental questions such as &quot;What is a design?&quot;, through research topics such as &quot;How should the concept of partial balance be extended to designs which do not have constant block size?&quot;, to more practical problems concerning what format we should use to store designs. This paper gives a brief description of the project to date, concentrating on theoretical questions about designs and their classification.
In this paper, we present multicast traffic engineering and we compare it to unicast traffic engineering. We study the advantages given by the integration of multicasting and MPLS. We present current proposals for multicast traffic engineering and using MPLS network. We describe our approach, the MPLS multicast tree (MMT) protocol. In order to reduce forwarding states and enhance scalability, MMT utilizes MPLS LSPs between branching routers of the multicast tree. We present a simulator for MMT and finally we discuss some simulation results.
The World-Wide Web was originally developed as a shared, writable,  hypertext medium, a facility that is still widely needed.
In ATM networks, Connection Admission Control (CAC) has been recognized as one of the most important means to provide satisfactory quality of service (QoS) and protect the network from congestion, especially for supporting real-time services such as voice and video. An efficient CAC strategy, which can both guarantee the QoS of admitted connections and achieve good resource utilization, then becomes a crucial issue for ATM network providers. The concept of a user-network traffic contract has been introduced by ATM Forum. Starting from this point, we propose a measurement-based CAC strategy. We first discuss how to obtain an accurate description (traffic parameters) of user traffic by using on-line measurement in conjunction with dynamic renegotiation. The results show that the proposed strategy is reliable and simple to implement. We then move on to examine the possibility and methodology for exploiting the effect of statistical multiplexing in resource allocation to achieve higher network resource utilization.
In current interframe video compression systems, the encoder performs predictive coding to exploit the similarities of successive frames. The Wyner-Ziv Theorem on source coding with side information available only at the decoder suggests that an asymmetric video codec, where individual frames are encoded separately, but decoded conditionally (given temporally adjacent frames) could achieve similar efficiency. We report results on a Wyner-Ziv coding scheme for motion video that uses intraframe encoding, but interframe decoding. In the proposed system, key frames are compressed by a conventional intraframe codec and in-between frames are encoded using a Wyner-Ziv intraframe coder. The decoder uses previously reconstructed frames to generate side information for interframe decoding of the Wyner-Ziv frames.
Accurate real-time speech recognition is not currently possible in the mobile embedded space where the need for natural voice interfaces is clearly important. The continuous nature of speech recognition coupled with an inherently large working set creates significant cache interference with other processes. Hence real-time recognition is problematic even on high-performance general-purpose platforms. This paper provides a detailed analysis of CMU&apos;s latest speech recognizer (Sphinx 3.2), identifies three distinct processing phases, and quantifies the architectural requirements for each phase. Several optimizations are then described which expose parallelism and drastically reduce the bandwidth and power requirements for real-time recognition. A special-purpose accelerator for the dominant Gaussian probability phase is developed for a 0.25 CMOS process which is then analyzed and compared with Sphinx&apos;s measured energy and performance on a 0.13 2.4 GHz Pentium4 system. The results show an improvement in power consumption by a factor of 29 at equivalent processing throughput. However after normalizing for process, the specialpurpose approach has twice the throughput, and consumes 104 times less energy than the general-purpose accelerator. The energy-delay product is a better comparison metric due to the inherent design trade-offs between energy consumption and performance. The energydelay product of the special-purpose approach is 196 times better than the Pentium4. These results provide strong evidence that real-time large vocabulary speech recognition can be done within a power budget commensurate with embedded processing using today&apos;s technology. 1 
 XML is increasingly gaining ground as a standard for data representation and exchange over the web. XML data most often has some embedded structure; therefore, it is interesting to exploit this structure whenever present. The structure of an XML document can be modeled by DTDs, XML Schema, or XQuery types. Both XML Schema and XQuery have a type system that supports a subtyping relation between types. The purpose
The segmentation of bank cheque images is a fundamental phase of its automatic processing. In the segmentation phase, one of the most important steps is the background elimination, that has to respect the physical integrity of the rest of the cheque image information. This paper describes a simple and robust solution for the background elimination problem, using a process that involves two stages: the original image enhancement and a posterior global thresholding process.
We present polynomial upper and lower bounds on the number of iterations performed by Lloyd&apos;s method for k-means clustering. Our upper bounds are polynomial in the number of points, number of clusters, and the spread of the point set. We also present a lower bound, showing that in the worst case the k-means heuristic needs to performff n) iterations, for n points on the real line and two centers. Surprisingly, our construction spread is polynomial. This is the first construction showing that the k-means heuristic requires more than a polylogarithmic number of iterations. Furthermore, we present two alternative algorithms, with guaranteed performances, which are simple variants of Lloyd&apos;s method. Results of our experimental studies on these algorithms are also presented.
Development of dynamic Web sites is often performed by teams  consisting of graphic designers and software developers. Communication  between these different team members has to be supported with a simple  modeling approach that considers their different academical backgrounds.
Extending the classical Legendre&apos;s result, we describe all solutions of the inequality |alpha - a/b| &lt; c/b^2 in terms of convergents of continued fraction expansion of alpha. Namely, we show that a/b =(rp_{m+1} +- sp_{m})/(rq_{m+1} +- sq_{m}) for some nonnegative integers m, r, s such that rs &lt; 2c. As an application of this result, we describe a modification of Verheul and van Tilborg variant of Wiener&apos;s attack on RSA cryptosystem with small secret exponent.
Increasing computational capabilities and decreasing costs of commonly  used devices, and the concomitant development of short range, ad-hoc  networking technologies, will help realize the pervasive computing paradigm. In  this paper, we present the use of semantically rich descriptions for devices to discover,  and cooperate with, others in their vicinity. In particular, we describe our  ongoing projects that have used DAML for service discovery, service composition,  data management, and trust based security in pervasive computing environments.
The expected accuracy of CO 2 retrieval from IASI data is assessed. A particular signal processing is developed to efficiently exploit the CO 2 information of the IASI spectrum, through a Discrete Fourier Transform (DFT) filtering. the DFT filtering allows to process the entire IASI CO 2 information in about 50 data elements. An information content analysis indicates that, providing a climatological prior knowledge on the CO 2 concentration variability and the meteorological forecast of temperature and water vapour profiles, the mean tropospheric CO 2 concentration can be derived from a single IASI spectrum with an accuracy of about 2 parts per million by  volume (ppmv), i.e., better than 1 %. the DFT filtering allows to process the entire IASI CO 2 information in about  50 data elements.
Image-guided diagnosis and therapy are widely used in medicine nowadays.
The terminal Steiner tree problem is a special version of  the Steiner tree problem, where a Steiner minimum tree has to be found  in which all terminals are leaves. We prove that no polynomial time approximation  algorithm for the terminal Steiner tree problem can achieve  an approximation ratio less than (1    o(1)) ln n unless NP has slightly superpolynomial  time algorithms. Moreover, we present a polynomial time  approximation algorithm for the metric version of this problem with a performance  ratio of 2ff, where ff denotes the best known approximation ratio  for the Steiner tree problem. This improves the previously best known  approximation ratio for the metric terminal Steiner tree problem of ff + 2.
M. Josep Blesa and Fatos Xhafa  Departament de LSI  Universitat Politcnica de Catalunya  Campus Nord, Mdul C6  Jordi Girona, 1-3  08034-Barcelona, Spain.
This paper considers the relation between item recognition and cued recall -- two standard measures of episodic memory. Going beyond measures of performance  on each task, we examine the degree to which correlations between  successful recognition and successful recall of a single studied episode reflect  the commonality of memory processes underlying the two tasks. Specifically,  we consider whether four computational memory models (local and  global match versions of both matrix and convolution-correlation models)  can account for the relatively invariant correlation (ff 0.5) between successive  recognition and recall tests. Whereas basic versions of each model  cannot account for the correlation, versions that take into account variability  in goodness-of-encoding and in response criteria, as well as output encoding,  are able to account for the level of dependency between tasks. These  elaborated models also succeeded in fitting data from two new experiments  that manipulated the level of variability in goodness-of-encoding across conditions. This model-
We define a protocol for on-chip communication that supports dynamic interconnect networks with global memory management based on the notion of distributed shared memory with a uniform address space. The protocol is implemented by a memory manager. It is beneficial to separate the steady-state processing (data communication with static interconnect) from changing from one steady state (or user function) to another, which may necessitate reallocation of resources or changes in the network topology.  1. INTRODUCTION  Embedded systems can often be specified as a set of communicating tasks (or processes). Tasks can be as large as MPEG encoders, transcoders, hard disk (interfaces), or be at a finer grain, such as DCT/IDCT, or motion estimation functions. The functionality offered to the user grows day by day and hence not only the number of tasks in a system increases, but task graph transitions evolve from simple mode switching to more refined dynamic behaviour (which includes, for example...
In this paper, a new genetic algorithm for multi-objective optimization problems is introduced. That is called &quot;Neighborhood Cultivation GA (NCGA)&quot;. In the recent studies such as SPEA2 or NSGA-II, it is demonstrated that some mechanisms are important; the mechanisms of placement in an archive of the excellent solutions, sharing without parameters, assign of fitness, selection and reflection the archived solutions to the search population. NCGA includes not only these mechanisms but also the neighborhood crossover. The comparison of NCGA with SPEA2 and NSGA-II by some test functions shows that NCGA is a robust algorithm to find Pareto-optimum solutions. Through the comparison between the case of using neighborhood crossover and the case of using normal crossover in NCGA, the effect of neighborhood  crossover is made clear.
This paper explores the role of communication and collaboration within the GRID. We discuss how a new generation of communications applications are  becoming more resource hungry. We propose a platform  for communication and collaboration applications that could itself be built on the GRID fabric. We discuss briefly the attributes of such a platform.
Web crawler design presents many different challenges: architecture, strategies, performance and more. One of the most important research topics concerns improving the selection of &quot;interesting&quot; web pages (for the user), according to importance metrics. Another relevant point is content freshness, i.e. maintaining freshness and consistency of temporary stored copies. For this, the crawler periodically repeats its activity going over stored contents (re-crawling process). In this paper, we propose a scheme to permit a crawler to acquire information about the global state of a website before the crawling process takes place. This scheme requires web server cooperation in order to collect and publish information on its content, useful for enabling a crawler to tune its visit strategy. If this information is unavailable or not updated the crawler still acts in the usual manner. In this sense the proposed scheme is not invasive and is independent from any crawling strategy and architecture.
This paper analyzes the asymptotic performance of maximum likelihood (ML) channel estimation algorithms in wideband code division multiple access (WCDMA) scenarios. We concentrate on systems with periodic spreading sequences (period larger than or equal to the symbol span) where the transmitted signal contains a code division multiplexed pilot for channel estimation purposes. First, the asymptotic covariances of the training-only, semi-blind conditional maximum likelihood (CML) and semi-blind Gaussian maximum likelihood (GML) channel estimators are derived. Then, these formulas are further simplified assuming randomized spreading and training sequences under the approximation of high spreading factors and high number of codes. The results provide a useful tool to describe the performance of the channel estimators as a function of basic system parameters such as number of codes, spreading factors, or traffic to training power ratio.
ad-hoc  netw  orks (DWANs) pose numerous technical challenges. Among them,  tw  o arewef&apos;7 considered as crucial: autonomous localized operation and minimization ofenergy consumption. We address the fundamental problem  ofhow  to maximize life-time ofthe  netw  ork by using only local  informationwnfo  preserving  netw  ork connectivity. We start by introducing the Care-Free Sleep (CS) Theorem that provides provably optimal necessary and suff- cient conditions for a node to turn off its radio wdio ensuring that global connectivity is not affected.
Phrase-final intonation was analysed in a subcorpus of Swedish computer-directed question utterances with the objective of investigating the extent to which final rises occur in spontaneous questions, and also to see if such rises might have pragmatic functions over and beyond the signalling of interrogative mode. Final rises occurred in 22 percent of the utterances. Final rises occurred mostly in conjunction with final focal accent. Children exhibited the largest percentage of final rises (32%), with women second (27%) and men lowest (17%). These results are viewed in relationship to results of related perception studies and are discussed in terms of Swedish question intonation and the pragmatic social function of rises in a biological account of intonation.
Voice-over-IP is expected to become a popular service offered by the internet. Thus, it is important to ensure high quality of service. In this paper, we look at two standards proposed for evaluating the intelligibility of Chinese speech. Adopting the philosophy and methodology of the Diagnostic Rhyme Test (DRT) for testing English speech, the Chinese Diagnostic Rhyme Test (CDRT) evaluates the six elementary phonemic attributes of Chinese words. Since Chinese is a tonal language, an extension of CDRT called CDRT-Tone evaluates the tonal attributes of Chinese speech. These two tests were used to evaluate the ITU-T G.728 speech coder as a VoIP codec for Chinese speech. Results are compared to the previous evaluations on a GSM 06.10 coder.
We present distributed regression, an effcient and general framework for in-network modeling of sensor data. In this framework, the nodes of the sensor network collaborate to optimally fit a global function to each of their local measurements. The algorithm is based upon kernel linear regression, where the model takes the form of a weighted sum of local basis functions; this provides an expressive yet tractable class of models for sensor network data. Rather than transmitting data to one another or outside the network, nodes communicate constraints on the model parameters, drastically reducing the communication required. After the algorithm is run, each node can answer queries for its local region, or the nodes can effciently transmit the parameters of the model to a user outside the network. We present an evaluation of the algorithm based upon data from a 48-node sensor network deployment at the Intel Research - Berkeley Lab, demonstrating that our distributed algorithm converges to the optimal solution at a fast rate and is very robust to packet losses.
Combined power control and interference cancellation in CDMA systems can be a very efficient resource management tool. While conventional power control tries to maintain equal received power or balanced SIR, Successive Interference Cancellation (SIC) in Multi-User Detection (MUD) relies more on the disparities between the powers of the different users. The combination can save more power into the system and thus a room for better capacity.
ARToolKit programmers are familiar with the kanji symbols supplied with the distribution. Most of them have do not know what these kanji symbols mean. We propose a piece of educational software that uses collaborative Augmented Reality (AR) to teach users the meaning of kanji symbols. The application is laid out as a two player Augmented Reality computer game. The novelty of our approach is that we do not use regular workstations or laptops to host the AR (Augmented Reality) application. Instead we use fully autonomous PDAs, running the application together with an optical marker-based tracking module that makes this application not only available for a broad audience but also optimally mobile.
Specifying the motion of an animated linked figure such that it achieves given tasks (e.g., throwing a ball into a basket) and performs the tasks in a realistic fashion (e.g., gracefully, and following physical laws such as gravity) has been an elusive goal for computer animators. The spacetime constraints paradigm has been shown to be a valuable approach to this problem, but it suffers from computational complexity growth as creatures and tasks approach those one would like to animate. The complexity is shown to be, in part, due to the choice of finite basis with which to represent the trajectories of the generalized degrees of freedom. This paper describes new features to the spacetime constraints paradigm to address this problem.
This paper considers the problem of improving the automatic speech recognition of audio fragments containing background music. This problem is put in the framework of linear source separation, where the music component is subtracted from the signal, thereby aiming at a better speech recognition, not necessarily a better subjective audio quality
An autonomous neurobiological mechanism is proposed for attention and its drift and switch without recourse to any metaphysical agency. This mechanism makes use of widely available empirical findings on the functions of the neuronal units of the brain such as the thalamus, hippocampus, basal ganglia and thalamocortical pathways. This formulation has been prompted by Abhidhamma and Vipassana Meditation and stands as an alternative to Francis Crick&apos;s &apos;Searchlight Hypothesis&apos;.
At the end of 1997, the foreign companies listed in the U.S. have a Tobin&apos;s q ratio that exceeds by 16.5% the q ratio of firms from the same country that are not listed in the U.S. The valuation  difference is statistically significant and largest for exchange-listed firms, where it reaches 37%. The difference persists even after controlling for a number of firm and country characteristics. We propose a theory that explains this valuation difference. We hypothesize that controlling shareholders of firms listed in the U.S. cannot extract as many private benefits from control  compared to controlling shareholders of firms not listed in the U.S., but that their firms are better  able to take advantage of growth opportunities. Consequently, the cross-listed firms should be  those firms where the interests of the controlling shareholder are better aligned with the interests  of other shareholders. The growth opportunities of cross-listed firms will be more highly valued  than those of firms not listed in the U.S. both because cross-listed firms are better able to take advantage of these opportunities and because a smaller fraction of the cash flow of these firms is expropriated by controlling shareholders. We find that our theory explains the greater valuation of cross-listed firms. In particular, we find expected sales growth is valued more highly for firms listed in the U.S. and that this effect is greater for firms from countrie s with poorer investor  rights.
This article discusses the integration of traditional abductive and inductive reasoning methods in the development of machine learning systems. In particular, it reviews our recent work in two areas: 1) The use of traditional abductive methods to propose revisions during theory refinement, where an existing knowledge base is modified to make it consistent with a set of empirical data; and 2) The use of inductive learning methods to automatically acquire from examples a diagnostic knowledge base used for abductive reasoning. Experimental results on real-world problems are presented to illustrate the capabilities of both of these approaches to integrating the two forms of reasoning.
Multiple shell planetary nebulae (MSPNe) are those which, in addition to the main bright inner shell, show one or two additional external shells. These faint external shells often have aspherical geometries, indicating an interaction with the surrounding interstellar medium (ISM). Our aim is to investigate, first of all, if the current theoretical stellar evolutionary models are able to reproduce the different observed shells and secondly, which ISM conditions are necessary in order to see the effect of a moving central star in the PN halo. We have performed hydrodynamical simulations following the evolution of the stellar winds predicted by the theoretical stellar evolutionary models. The study has been done in two parts: first, we approach the problem of the formation of multiple shells in the case of a star at rest, then we investigate the interaction of the different shells with the ISM due to a moving central star.
In this paper we investigate the datapath merging problem  (DPM) in reconfigurable systems. DPM is in    and it is described  here in terms of a graph optimization problem. We present an Integer  Programming (IP) formulation of DPM and introduce some valid inequalities  for the convex hull of integer solutions. These inequalities form  the basis of a branch-and-cut algorithm that we implemented. This algorithm  was used to compute lower bounds for a set of DPM instances,  allowing us to assess the performance of the heuristic proposed by Moreano  et al. [1] which is among the best ones available for the problem.
A comprehensive power analysis, keeping integrated circuits in mind, is presented while highlighting all conduction, switching, and dynamic power losses in a DC-DC converter. Synchronous rectification, zero-voltage switching, mode-hopping, and variable frequency operation are evaluated. The efficiency for constant frequency CCM, constant frequency DCM, and constant on-time, variable frequency DCM techniques is analyzed and the optimum technique is derived. It is concluded that a mode-hopping converter employing an asynchronous, constant on-time, variable frequency DCM operation for low output currents and a synchronous, constant frequency CCM operation for high load currents yields the best efficiency performance.
We present a novel technique for texture mapping on arbitrary surfaces with minimal distortions, by preserving the local and global structure of the texture. The recent introduction of the fast marching method on triangulated surfaces, made it possible to compute a geodesic distance map from a given surface point in O(n lg n) operations, where n is the number of triangles that represent the surface. We use this method to design a surface flattening approach based on multi-dimensional scaling (MDS). MDS is a family of methods that map a set of points into a finite dimensional flat (Euclidean) domain, where the only given data is the corresponding distances between every pair of points. The MDS mapping yields minimal changes of the distances between the corresponding points. We then solve an `inverse&apos; problem and map a flat texture patch onto the curved surface while preserving the structure of the texture. Keywords--- Texture mapping, multi-dimensional scaling, fast marching method, Geodesic distance, Euclidean distance. I. 
We present a series of three analyses of young children&apos;s linguistic input to determine the distributional information it could plausibly offer to the process of grammatical category learning. Each analysis was conducted on four separate corpora from the CHILDES database (MacWhinney, 2000) of speech directed to children under 2;5. We show that, in accord with other findings, a distributional analysis which categorizes words based on their co-occurrence patterns with surrounding words successfully categorizes the majority of nouns and verbs. In Analyses 2 and 3, we attempt to make our analyses more closely relevant to natural language acquisition by adopting more realistic assumptions about howyoung children represent their input. In Analysis 2, we limit the distributional context by imposing phrase structure boundaries, and find that categorization improves even beyond that obtained from less limited contexts. In Analysis 3, we reduce the representation of input elements which young children might not fully process and we find that categorization is not adversely affected: Although noun categorization is worse than in Analyses 1 and 2, it is still good; and verb categorization actually improves. Overall, successful categorization of nouns and verbs is maintained across all analyses. These results provide promising support for theories of grammatical category formation involving distributional analysis, as long as these analyses are combined with appropriate assumptions about the child learner&apos;s computational biases and capabilities.
The context of this paper is Enterprise Architecture (EA).
Several studies on the variations of the crossover operator  have shown that each of them presents specic properties that  are interesting under particular circumstances. The advantages  of each operator over others are often contradictory and  the best operator depends on the problem being solved. This  paper is based on the assumption that a combination of several  crossover operators can take advantage of their respective  qualities and power. Thus, we propose several combination  models based on four crossover operators: the 1-point,  2-point, uniform and dissociated. We test the performance  of these models through an experimental approach using the  problem of the Hamiltonian circuit.
We introduce a numerical measure on sets of partitions of finite sets that is linked to the Goodman-Kruskal association index commonly used in statistics. This measure allows us to define a metric on such partions used for constructing decision trees. Experimental results suggest that by replacing the usual splitting criterion used in C4.5 by a metric criterion based on the Goodman-Kruskal coefficient it is possible, in most cases, to obtain smaller decision trees without sacrificing accuracy.
This paper describes ImageSurfer, a volume-visualization system aimed at the study of nerve-cell dendritic spines imaged using confocal microscopy. ImageSurfer&apos;s final design can create convincing visualizations capable of answering our specific scientific questions through its fourwindow interface, featuring a volume and surface rendering, a 2D graphical-user interface, a magic cut plane with a height field, and a graph. Furthermore, this paper describes the three design iterations that led to ImageSurfer&apos;s final design, the lessons learned through the design process, and encouraging informal feedback. Through the informal feedback, users reported that ImageSurfer enabled them to explore more dendritic spines than in prior techniques and understand the layout of where proteins of interest lay with respect to the dendritic spines.
We propose a relaxation of Kleene algebra by giving up strictness and right-distributivity of composition. This allows the subsumption of Dijkstra&apos;s computation calculus, Cohen&apos;s omega algebra and von Wright&apos;s demonic refinement algebra. Moreover, by adding domain and codomain operators we can also incorporate modal operators. Finally, it is shown that the predicate transformers form lazy Kleene algebras again, the disjunctive and conjunctive ones even lazy Kleene algebras with an omega operation.  
The AutoMed project uses a hypergraph common data model (the HDM) to integrate heterogeneous data sources. Previous work has shown how data models including the relational, ER and XML models, can be mapped on to HDM to facilitate schema integration. RDF is a general-purpose language for representing information on the World Wide Web. It provides a data model for describing properties of resources in the web and their interrelationships. RDF Schema provides a basic type system for RDF models which is implemented in terms of classes and properties. These technologies are increasingly used in the semantic web effort in particular for defining ontologies. This technical report shows how RDF and RDF Schema can be modeled in the HDM thereby allowing data in these formats to be treated as an AutoMed datasource. 1 RDF  The Resource Description Framework (RDF) [6] is a language for representing information on the World Wide Web. RDF allows properties of Web resources to be stated in the form of Subject, Predicate and Object triples. A statementsuch as &quot;Dean Williams is the author of the webpage http://// tml&quot; can be represented by a triple where the subject is the webpage URL, the predicate is `author&apos; and the object is `Dean Williams&apos;. Resources are identified using the Uniform Resource Identifier (URI) [8] web standard. URI&apos;s are a more general identifier than the Uniform Resource Locator (URL) used in the Web. While URIs cover defined, centralised schemes (such as the http part of a URL) they also allowforanyone to create their own URI naming schemes. In the example statement, the concept of `author&apos; could be used in different ways by different systems. A web site maintenance system might use it in a different way from a book publishing house system. Referring to the concep...
Many applications employ sensors for monitoring entities such as temperature and wind speed. A centralized database tracks these entities to enable query processing. Due to continuous changes in these values and limited resources (e.g., network bandwidth and battery power), it is often infeasible to store the exact values at all times. A similar situation exists for moving object environments that track the constantly changing locations of objects. In this environment, it is possible for database queries to produce incorrect or invalid results based upon old data. However, if the degree of error (or uncertainty) between the actual value and the database value is controlled, one can place more confidence in the answers to queries. More generally, query answers can be augmented with probabilistic estimates of the validity of the answers. In this paper we study probabilistic query evaluation based upon uncertain data. A classification of queries is made based upon the nature of the result set. For each class, we develop algorithms for computing probabilistic answers. We address the important issue of measuring the quality of the answers to these queries, and provide algorithms for effciently pulling data from relevant sensors or moving objects in order to improve the quality of the executing queries. Extensive experiments are performed to examine the effectiveness of several data update policies. 1. 
We construct  2ff      combinatorial types of triangulated 3-spheres on n vertices. Since by
ulation synthesis program of different ages and upper stellar mass limits. We found that the index ! is primarily dependent on the effective temperature of the hottest star in the cluster.  Universidade Federal de Santa Maria, Brazil  2  University of Cincinnati, USA  We also considered the statistical use of ! for estimating the ages of the ionizing clusters of H II regions. As a check, we used our models to fit diagnostic diagrams for 50 H II regions in the Magellanic Clouds whose spectral data were available in the literature. We have derived the effective temperatures of the ionizing stars and the ages of some H II regions with the use of the index !. The results were compatible with those obtained from other methods.  -2-1 0 1 2 3  Ne=100  -3 -3        4  log(3727/5007)  Fig. 1. ! vs T eff for electron density N e varying from 10 to 6 000 cm  \Gamma3  and a mean value of the chemical composition of the Magellanic Clouds.  REFERENCES  
Interactive mediated environments (e.g. virtual reality/environments, computer games, the Internet, multi-media, interactive television) have the potential to induce experience in users. Engagement, involvement, agency, immersion and presence are widely used terms to describe the situation in which the inducement of experience in users of these media occurs. Implied in these terms is that experience presupposes that participants are absorbed in the illusion of interacting within the context created by these media. Without absorption, users&apos; attention is shifted from the mediated to the real world, disrupting the illusion. Deriving from an acting term (UK) to denote falling out of character, virtual corpsing is used to describe this shift in attention. So instead of talking about `being there&apos; this chapter focuses on maintaining the illusion and is referred to herein as staying there. This is a consequence of two things, firstly, transparency of equipment and secondly, continuity of interacting within the social and cultural environment depicted virtually. Extending concepts from activity theory, this chapter describes a conceptual framework to inform and guide analysis and design of interactive mediated environments. To this end, models and tools are developed in which to reason about human practice and experience in mediated environments from low-level operations through actions/tasks to a holistic activity-based scenario/narrative perspective characterised by objective and motive. The degree to which objective outcome coincides with the motive that stimulates the user to a mediated encounter provides a measure of success. Furthermore, stimulating mediated encounters may generate new motives, encouraging users to stay there.
In this paper, we generalize the lower bound on the number of rounds for Consensus  algorithms assuming that processes fail dependently. This lower bound is general in  the sense that it is independent of the failure assumptions for processes. In order to  instantiate it, one needs to provide a necessary condition on process replication for a  given failure model in terms of our abstractions to represent dependent failures. A  surprising corollary of our generalization is that the lower bound on the number of  rounds, in general, diers between the crash and the arbitrary failure models.
In this paper we present a general framework of logic programming allowing for the combination of several adjoint lattices of truth-values. The main
We give the  rst characterization of Turing machines that  run in polynomial-time on average. We show that a Turing machine M  runs in average polynomial-time if for all inputs x the Turing machine  uses time exponential in the computational depth of x, where the computational  depth is a measure of the amount of \useful&quot; information in  x.
This paper examines the main structural and performance features of European banking. It shows that while banking markets have become increasingly concentrated and bank numbers  have fallen, competition appears to have intensified. Given the large number of banks and branches in many countries there still remain indicators of excess capacity in the system and  that the consolidation trend, especially with the advent of EMU, will continue. A major theme  of this paper is that market concentration and bank size are poor indicators of market power. There is also 
In many real world planning scenarios, agents often do  not have enough resources to achieve all of their goals. Hence, this
In this paper we study the impact of the medium access control (MAC)  layer and the routing layer on the performance of a multi-hop wireless network.
The purpose of this study was to explore the relationship between student leadership and alcohol use. Previous literature had examined alcohol use of leaders and non-leaders in high-use organizations -- Greeks and athletes. This study extends that literature by focusing on leaders and non-leaders in low-use organizations, and by examining students with multiple leadership roles. The research used existing data from the Core Alcohol and Drug Survey. A random sample of 2,000 respondents was obtained from the Core Institute at Southern Illinois University -- Carbondale. Respondents were leaders and non-leader members of minority and ethnic organizations and religious and interfaith groups. From this total sample, 624 students were active in minority organizations only, 865 were involved in religious groups only, and 511 were active in both. Dependent variables were drawn from four questions on the Core Survey concerning average number of drinks per week, consumption of five or more drinks at one sitting, negative consequences of alcohol use, and alcohol-related beliefs. No statistically significant differences were found in the alcohol use of leader and nonleaders who were active only in minority groups. Significant differences were found however, between leaders and non-leaders who were active only in religious groups. For these groups, leaders consumed alcohol, engaged in high-risk drinking, experienced negative consequences, and ascribed to alcohol-related myths at a lower rate than those not in leadership positions. Student in dual leadership positions across the whole sample reported significantly higher alcohol use than student involved in one leadership position. Students with leadership roles in iii both minority and religious organizations drank approximately thre...
Learning to program involves the application of programming language features to the solving of novel problems, and the experience of educators suggests that it is this factor that causes novice programmers the most diffculty. Because software patterns are descriptions of common problems and their solution written in a standardised format that facilitates reuse, their use in the novice context is indicated. An earlier paper (Porter &amp; Calder 2003) suggested and demonstrated a process for applying patterns to problems that derives from the relationships between patterns in a pattern language suitable for novice programmers. This paper reports on the feasibility of testing the idea.
Logic programming with abductive reasoning is used during the realization of  a data dictionary with a particular methodology. Some methodological steps are  represented by means of a set of rules augmented with integrity constraints which  capture the presence of incompatible concepts. Explanations for incompatibility can  be inferred by exploiting abductive reasoning. To this end a new proposal for the  computation of hypotheses in an abductive framework where the theory is a general  logic program, is presented. It is based on a suitable manipulation of minimal threevalued  models of the logic program. A method to compute three-valued minimal  models of a general logic program is given.
Development of national Information and Communication Technology (ICT)  infrastructure is a kind of activity that government engage to in order to create a  rubric of progress and to promote diffusion of information technology (IT)  revolution. ICT infrastructure is perplexing because we do not have a good model  to use to analyze its development. Besides, the modernist style of regulation in  the countries promoting diffusion of IT revolution does not require the technology  developers to consider the impacts of technology systematically, which creates  substantial uncertainty in regard to possible trajectory of technological  development. To improve the construction of effective policy planning for large  information infrastructures, the conceptual link between the abstract enabling  structures and concrete technologies -- the building blocks of infrastructure --  must be established.
It is generally assumed that consumers in electronic shops feel more comfortable engaging in a dialog regarding their needs as opposed to inspecting detailed product features. However, empirical evaluations of needs-based recommender systems indicate that consumers are only confident about their buying decision if they can compare detailed information about product features. Ideally consumers should be supported in learning the relationship between their needs and product features. Current eCommerce-recommender systems do not support the required smooth transition between needs-oriented and feature-oriented interaction. In this paper we present a novel, homogeneous representation model for needs and feature preferences. The model enables a recommendation process that guides consumers from a needs-oriented to a feature-oriented interaction and thereby enables consumer learning and fosters confidence building. We illustrate our proposal with an eCommerce recommender system demonstrator for digital cameras. 1. 
We present two new methods to determine contraction kernels for the construction of graph  pyramids. The first method is restricted to undirected graphs and yields a reduction factor of at  least 2.0. This means with our method the number of vertices in the subgraph induced by any set  of contractible edges reduces to half or less by a single parallel contraction. Our second method  also works for directed graphs. Our methods yield better reduction factors than Meer&apos;s stochastic  decimation algorithm, in all tests. The lower bound of the reduction factor becomes crucial with  large images. We also give a method to compare the structure of the image pyramid.
This paper draws together existing data with recent survey results and compares  the development of local government GIS with the evolution of Information  Systems (IS). These comparisons are made using the philosophy that  organisational GIS can be modelled. Using this model, various stages of GIS  maturity are evaluated.
We consider an environment where distributed  data sources continuously stream updates to a  centralized processor that monitors continuous  queries over the distributed data. Significant communication  overhead is incurred in the presence of  rapid update streams, and we propose a new technique  for reducing the overhead. Users register  continuous queries with precision requirements at  the central stream processor, which installs filters  at remote data sources. The filters adapt to changing  conditions to minimize stream rates while  guaranteeing that all continuous queries still receive  the updates necessary to provide answers of  adequate precision at all times. Our approach enables  applications to trade precision for communication  overhead at a fine granularity by individually  adjusting the precision constraints of continuous  queries over streams in a multi-query workload.
This paper presents a reconfigurable platform as a tool for a virtual worldwide laboratory. The platform has been developed for teaching and research purposes. It is a practical approach to the Internet reconfigurable logic and it allows the rapid prototyping and test of digital systems.
A carrier separated OFDM system is proposed to support  high data rates in mobile multiple access environments. OFDM  is considered to support high data rates and carrier separation is introduced  to combat multipath fading. The performance is comparable  to that of a frequency hopped OFDM system. While promising robustness against multipath fading, the computational complexity of a  carrier separated OFDM system can be made to be as low as twice the  complexity of a OFDM system without carrier separation by maximum  uniform carrier separation.
Mobile ad hoc networks are characterized by multi-hop wireless links, absence of any cellular infrastructure, and frequent host mobility. Design of efficient routing protocols in such networks is a challenging issue. A class of routing protocols called on-demand protocols has recently found attention because of their low routing overhead. The on-demand protocols depend on query floods to discover routes whenever a new route is needed. Such floods take up a substantial portion of network bandwidth. We focus on a particular ondemand protocol, called Dynamic Source Routing, and show how intelligent use of multipath techniques can reduce the frequency of query floods. We develop an analytic modeling framework to determine the relative frequency of query floods for various techniques. Our modeling effort shows that while multipath routing is significantly better than single path routing, the performance advantage is small beyond a few paths and for long path lengths. It also shows that providing all intermediate nodes in the primary (shortest) route with alternative paths has a significantly better performance than providing only the source with alternate paths. We perform some simulation experiments which validate these findings.
this paper. Four soils of Shikama-2, Shizukuishi, Mouka and Naruko satisfied the critical values of Andosols, which is Alo + 1/2 Feo of 20 g kg    or more (FAO, 1998), and showed higher value of total carbon conent than Fulvisols. Shizukuishi and Mouka soils showed greater amounts of sulfate and Feo than other Andosols and Fulvisols. They were Silic Andosols, in which allophane and similar minerals are predominant in the clay fraction (FAO, 1998)
We prove super-linear lower bounds for some shortest path problems in directed graphs, where no such bounds were previously known. The central problem in our study is the replacement paths problem: Given a directed graph G with non-negative edge weights, and a shortest path P = {e1, e2, ..., ep} between two nodes s and t, compute the shortest path distances from s to t in each of the p graphs obtained from G by deleting one of the edges e i. We show that the replacement paths problem requires &amp;Omega;(m&amp;radic;n) time in the worst case whenever m = O(n&amp;radic;n). Our construction also implies a similar lower bound for the k shortest paths problem for a broad class of algorithms that includes all known algorithms for the problem. To put our lower bound in perspective, we note that both these problems (replacement paths and k shortest paths) can be solved in near linear time for undirected graphs.
This paper considers an example of Object-Oriented Programming (OOP) leading to subtle errors that break separation of  interface and implementations. A comprehensive principle that guards against such errors is undecidable. The paper introduces  a set of mechanically verifiable rules that prevent these insidious problems. Although the rules seem restrictive, they are  powerful and expressive, as we show on several familiar examples. The rules contradict both the spirit and the letter of the  OOP. The present examples as well as available theoretical and experimental results pose a question if OOP is conducive to  software development at all.
This paper reports on the word sense disambiguation of Korean noun by using co-occurrence information in context. For a given noun, its local contextual word distribution is not enough to express their semantic characteristics for noun sense disambiguation. This paper proposes a cluster-based sense as a base vector. Contextual noise is...
Video games are of increasing importance, both as a cultural phenomenon and as an application of collaborative technology. In particular, many recent online games feature persistent collaborative virtual environments (CVEs), with complex social organisation and strong social bonds between players. This paper presents a study of `There&apos;, one such game, focusing on how There has been appropriated by its players. In particular we describe how its flexibility has allowed players t o develop their own forms of play within the game. Three aspects of There are discussed: first, how the environment supports a range of social activities around objects. Second, how the chat environment is used to produce overlapping chat and how the game itself provides topics for conversation. Lastly, how the `place&apos; of There is a fluid interaction space that supports safe interactions between strangers. The paper concludes by drawing design lessons concerning the importance of supporting shared online activity, interaction between strangers, and the difficulties of designing for play.
We investigate a sub-optimal reducedcomplexity iterative technique for joint detection and estimation for sets of constrained sequences and derive analytical results concerning convergence regions and  xed points. We apply this theory to the problem of multiple-user decoding for multi-path fading channels.
In this paper, we introduce a generative probabilistic  optical character recognition (OCR)  model that describes an end-to-end process in  the noisy channel framework, progressing from  generation of true text through its transformation  into the noisy output of an OCR system.
Plan-Edit is a decision support tool that assists machinists and manufacturing students in creating efficient, small-batch manufacturing plans. Plan-Edit helps planners attain improved efficiency by reducing setups. Specifically, it assists them in merging squaring and feature cutting setups. Squaring setups prepare the stock so that all sides are smooth and either perpendicular or parallel to each other. Feature cutting setups shape the stock into its final form. Although planning becomes conceptually simpler when these two processes are separated, there are large efficiency gains of 10 to 40 percent to be obtained in small batch manufacturing when the two are combined. Unfortunately, the plan merging process is cognitively difficult for fairly advanced machinists, especially when they are simultaneously juggling many other planning decisions. Only the most experienced machinists master this plan merging skill (7 - 10 years experience) [6]. Plan-Edit helps users to manage the merging process by providing them with 1) a planning environment in which they can merge feature cutting and squaring setups, and 2) a plan critic which informs them when they have violated feasibility constraints. By doing so we hope to help users achieve levels of plan efficiency closer to those of experts.
Scene interpretation, in the sense of detecting and localizing instances from multiple object classes,  is formulated as a two-step process in which non-contextual detection primes global interpretation. During
In recent years, real world objects have been used to reflect information previously shown on the computer screen. While most earlier efforts have required significant developer knowledge and skills to construct and program the displays, our approach enables programmers to use real world objects in much the same way that they would typical user interface widgets. The programming interfaces leverage existing paradigms, simplifying the integration of off-the-desktop display and interaction techniques into standard programs. The APIs are developed using the X10 protocol for controlling power flow to electrical devices, thus avoiding engineering issues that make construction difficult and time-consuming for typical programmers. Sample programs are described that use the programming interfaces.
Fair bandwidth management for multipoint-to-point ABR connections is an extremely important problem. In  multipoint-to-point connections,  the traffic at the root (destination) is the sum of all traffic originating at the leaves. The most crucial concern in the case of multiple senders is how to define fairness within a multipoint group and among multipoint groups and point-to-point connections [3]. This can be complicated since the multipoint connection can have the same identifier (VPI/VCI) on  each link,  and senders might not be distinguishable in this case. We give various possibilities for  defining fairness,  and show the tradeoffs involved. In addition, 
We investigate the relative complexity of two free-variable labelled modal tableaux (KEM and Single Step Tableaux, SST). We discuss the reasons why p-simulation is not a proper measure of the relative complexity of tableaux-like proof systems, and we propose an improved comparison scale (p-search-simulation). Finally we show that KEM p-search-simulates SST while SST cannot p-search-simulate KEM.
The integration of preferences into answer set programming constitutes an important practical device for distinguishing certain preferred answer sets from non-preferred ones. To this end, we elaborate upon rule dependency graphs and their colorings for characterizing different preference handling strategies found in the literature. We start from a characterization of (three types of) preferred answer sets in terms of totally colored dependency graphs. In turn, we exemplarily develop an operational characterization of preferred answer sets in terms of operators on partial colorings for one particular strategy. In analogy to the notion of a derivation in proof theory, our operational characterization is expressed as a (non-deterministically formed) sequence of colorings, gradually turning an uncolored graph into a totally colored one. 1 
We present a new approach to summary evaluation which combines two novel aspects, namely (a) content comparison between gold standard summary and system summary via factoids, a pseudo-semantic representation based on atomic information units which can be robustly marked in text, and (b) use of a gold standard consensus summary, in our case based on 50 individual summaries of one text. Even though future work on more than one source text is imperative, our experiments indicate that (1) ranking with regard to a single gold standard summary is insuffcient as rankings based on any two randomly chosen summaries are very dissimilar (correlations average ff = 0.20), (2) a stable consensus summary can only be expected if a larger number of summaries are collected (in the range of at least 30-40 summaries), and (3) similarity measurement using unigrams shows a similarly low ranking correlation when compared with factoid-based ranking.
When establishing lightpaths in an all-optical DWDM network, it is possible that concurrent lightpath requests will block one another if the lightpaths attempt to reserve the same wavelength on the same link. In this paper, we propose a novel signaling mechanism, referred to as label prioritization, which attempts to reduce the backward-link blocking in GMPLS-centric all-optical networks by assigning different priorities to the suggested wavelengths (labels) of each connection request. The prioritization of wavelength encourages concurrent lightpath requests to choose different wavelengths, thereby reducing the possibility that the requests will be blocked. The label prioritization mechanism consists of a signaling extension to GMPLS to support the label prioritization and a modification in the optical switch controller to support the signaling extension. Simulation results show that the label prioritization method can effectively reduce wavelength conflicts.
TFRC (TCP Friendly Rate Control) is a rate-based congestion  control protocol for non-TCP flows. TFRC controls the sending rate  by using the TCP throughput equation considering the triple duplicate  ACKs and the timeouts. However, this equation provides the conservative  throughput bound of TCP. This conservativeness causes lower  throughput and slower response time for TFRC than those for TCP.
Operating systems are complex pieces of software. The usage environment puts challenges on systems causing them to uncover previously unknown defects. Some of these defects are concerned with the security requirements they are referred to as vulnerabilities. In this work, we attempt to study some reported data and try to identify some repeatable patterns. This is currently an initial work to put bases for further work in this area. Our ultimate goal is to try to model vulnerability growth and provide an estimation formula that will help us determine the present and the future.
This paper explores how people make sense of ambiguity caused by newly introduced information technology in organizations. A better understanding of these sensemaking processes might provide research as well as practice some basis for improving how information technology is adapted in organizations. On the basis of an interpretive study, the paper identifies how certain attention-structures facilitate as well as restrict how IT is constructed meaningful in a particular organizational context - social work. In this context, previous experiences of how IT-introductions have coincided with increasing administrative workload had considerable implications on how First Class was adapted. At a general level, it is concluded that resolving ambiguity is an important ingredient in successful IT-adaptation.
This paper addresses one aspect of scaling-up KBSE: enabling domain experts to construct and maintain their own domain-specific KBPS systems. The META-AMPHION system, currently under development, is designed to provide the KBSE analogue of application-generator generator technology. A key component of META-AMPHION is a subsystem to automatically operationalize a declarative domain theory for efficient deductive program synthesis. This paper describes extensions of a previous system, DRAT, that speeds up a theorem-prover for analytical reasoning problems by substituting decision procedures for axioms in a theory. An experiment with AMPHION (a real-world KBPS system) demonstrated that extending DRAT to deductive synthesis would be successful. The design for these extensions is described, and are currently being implemented. In parallel work we are also exploring suitable user interfaces for META-AMPHION that will guide domain experts in developing and maintaining domain theories. There appears to be a useful synergy with the component described in this paper: the same process described in section 6 for operationalizing axioms in an existing domain theory might be useful in eliciting axioms from a domain expert. Instead of using a theorem-prover to answer questions when searching the hierarchy, the domain expert would be used as an oracle to construct portions of a domain theory by traversing the same hierarchy. In previous work on DRAT, the work reported in [7] was extended to show that DRAT&apos;s attachment of literal satisfiability procedures to a theorem-prover was sound and complete. These results must be extended to DSDRAT and we are considering the framework reported in [2]. The extension of DRAT&apos;s classification procedure to DSDRAT&apos;s classification procedure is related...
An efficient use of background music is a very powerful tool  to achieve high levels of immersion and to induce moods within interactive  settings. Although this property is clear, the way in which action  and music should be connected, so as to have such an efficient emotion  delivery tool, is still to be fully understood. An action-music
In this paper we consider discrete approximations of a Dirichlet problem  for the quasilinear parabolic equation L(u(x; t))  f&quot;@      u(x; t)@=@x @=@t  c(x; t)gu(x; t) = f(x; t), that is, the viscous Burgers equation. The singular perturbation  parameter &quot; takes arbitrary values from the half-interval (0,1]. The initial  condition has a discontinuity of the  rst kind at the point S    = (0; 0) such that  &apos; 0 (+0) &apos; 0 ( 0) &gt; 0, where u(x; 0) = &apos; 0 (x); thus, we have the Riemann problem.
rd to z, and then we may think of x as the decoding of z.  There are two dual views of the Hadamard code, based on two different interpretations of  i=1 x i y i .  1. View the x i as coefficients and the y i as variables. Then the codeword E(x) can be viewed as the linear function f x =  i=1 x i y i evaluated on all possible inputs.  2. View the y i as coefficients and the x i as variables. Then the codeword E(x) can be viewed as evaluating all possible linear functions (over GF (2)    ) at the point x 2 f0; 1g    .  1.2 The linearity test  Given a string z 2 f0; 1g    , we would like to test whether it is (close to) a codeword of the Hadamard code. As noted in Section 1.1, valid codewords can be viewed as linear functions f x . Likewise, we view z as a Boolean function f , and accessing the bit at location y 2 f0; 1g    can be viewed as getting the value of f(y). For two strings x; y 2 f0; 1g    , let x \Phi y 2 f0; 1g    denote their bitwise exclusive or.  The linearity test: Choose 
Recent research has demonstrated quite convincingly that accurate cancer diagnosis can be achieved by constructing classifiers that are designed to compare the gene expression profile of a tissue of unknown cancer status to a database of stored expression profiles from tissues of known cancer status. This paper introduces the JCFO, a novel algorithm that uses a sparse Bayesian approach to jointly identify both the optimal nonlinear classifier for diagnosis and the optimal set of genes on which to base that diagnosis. We show that the diagnostic classification accuracy of the proposed algorithm is superior to a number of current state-of-the-art methods in a full leave-one-out cross-validation study of two widely used benchmark datasets. In addition to its superior classification accuracy, the algorithm is designed to automatically identify a small subset of genes (typically around twenty in our experiments) that are capable of providing complete discriminatory information for diagnosis. Focusing attention on a small subset of genes is not only useful because it produces a classifier with good generalization capacity, but also because this set of genes may provide insights into the mechanisms responsible for the disease itself. A number of the genes identified by the JCFO in our experiments are already in use as clinical markers for cancer diagnosis; some of the remaining genes may be excellent candidates for further clinical investigation. If it is possible to identify a small set of genes that is indeed capable of providing complete discrimination, inexpensive diagnostic assays might be widely deployable in clinical settings.
It is known that many integrable systems can be reduced from self-dual Yang-Mills  equations. The formal solution space to the self-dual Yang-Mills equations is given by  the so called ADHM construction, in which the solution space are graded by vector  spaces with dimensionality concerning topological index. When we consider a reduced  self-dual system such as the Bogomol&apos;nyi equations, in terms of ADHM construction,  we need to incorporate an infinite dimensional vector space, in general. In this paper,  we reformulate the ADHM construction by introducing various infinite dimensional  vector spaces taking into account the reduction of self-dual system.
Cyclic debugging requires repeatable executions. As non-deterministic or real-time systems typically do not have the potential to provide this, special methods are required. One such method is replay, a process that requires monitoring of a running system and logging of the data produced by that monitoring. We shall discuss the process of preparing the replay, a part of the process that has not been very well described before. KEYWORDS: Debugging; replay; starting replay; recording; monitoring; logging; FIFO  1 
The United States has one of the most technically advanced, most expansive, most evenly distributed, and most freely accessed communication system on the planet. Yet Americans are simultaneously one of the most poorly informed populations (in terms of diversity of opinions/sources, depth and breadth of knowledge, etc.). The proliferation of personalized information services, photo news galleries, computer simulations, and a host of interactive media links on commercial Internet news sites have been hailed recently as one remedy for this troubling statistic. By 2005 the nations comprising Western Europe will represent the largest concentration of netizens in the world with more than 300,000,000 people connected to the Net, many seeking the same conveniences enjoyed by their American counterparts. This paper examines the relationship between technical features and usage patterns on several of the leading Internet news sites. I argue that as the Internet becomes more technically sophisticated, a proportionate, though inverse trend in the epistemological sophistication of its user base will be inevitable. Finally, I discuss the implications this trend holds for the future of a &quot;global citizenry.&quot;  Keywords : internet, news, information, knowledge, hyper-utilization, decontextualization, epistemological, technological determinism, citizen 
We derive, for a binary antipodal input signal, the optimal uncoded regenerator function when the channels at the ingress and at the egress of the regenerator are degraded by AWGN. We show that the optimal function is a Lambert W function parametrized on the energies of the noises and the input. For comparison, we derive the performance of systems in which the regenerator uses a hard limiter or an amplifier.
The effect of different rice plantation periods on the properties of selected soils in alluvial plain was studied. The soils have been under continuous rice cultivation for less than 10 years, 10 - 20 years, 20-30 years, and over thirty-years period. In each rice cultivated and non-rice cultivated fields a soil profile and two nearby auger holes were studied. Soil samples were taken from different horizons and layers of the soil profiles and auger holes. Morphological features of each soil profile were determined and soils were classified according to USDA, soil taxonomy. Soil samples were analyzed by conventional methods and clay minerals were identified by X-ray diffraction analysis. This study indicated that continuous rice cultivation changed soil moisture regime from xeric to aquic and soil color also became greyish, surface horizons changed from mollic to ochre epipedons. With increasing period of cultivation soil mottle increased and soil structure became massive. Therefore the soil classificationmodified from mollisols to inceptisols. No illuviation and eluviation of clay minerals occurred as a consequence periods of rice cultivation. X-ray diffraction analysis showed that clay minerals in nonrice cultivated field were illite, vermiculite, montmorillonite, kaolinite, chlorite and in rice field were illite, montmorillonite, kaolinite and chlorite respectively. But with increasing the period of cultivation, the amount of illite decreased and montmorillonite increased.
Electronic equipment can emit unintentional signals from which eavesdroppers may reconstruct processed data at some distance. This has been a concern for military hardware for over half a century. The civilian computer-security community became aware of the risk through the work of van Eck in 1985. Military &quot;Tempest&quot; shielding test standards remain secret and no civilian equivalents are available at present. The topic is still largely neglected in security textbooks due to a lack of published experimental data. This report documents eavesdropping experiments on contemporary computer displays. It discusses the nature and properties of compromising emanations for both cathode-ray tube and liquid-crystal monitors. The detection equipment used matches the capabilities to be expected from well-funded professional eavesdroppers. All experiments were carried out in a normal unshielded office environment. They therefore focus on emanations from display refresh signals, where periodic averaging can be used to obtain reproducible results in spite of varying environmental noise. Additional experiments described in this report demonstrate how to make information emitted via the video signal more easily receivable, how to recover plaintext from emanations via radio-character recognition, how to estimate remotely precise video-timing parameters, and how to protect displayed text from radio-frequency eavesdroppers by using specialized screen drivers with a carefully selected video card. Furthermore, a proposal for a civilian radio-frequency emission-security standard is outlined, based on path-loss estimates and published data about radio noise levels. Finally, a new optical eavesdropping technique is demonstrated that reads CRT displays at a distance. It observes high-frequency variations of the light emitted, even after diffuse reflection. Experiments with a typical monitor show that enough video signal remains in the light to permit the reconstruction of readable text from signals detected with a fast photosensor. Shot-noise calculations provide an upper bound for this risk.
We evaluate the English---French word alignment  data of the shared tasks from a phrase  alignment perspective. We discuss peculiarities  of the submitted data and the test  data. We show that phrase-based evaluation is  closely related to word-based evaluation. We  show examples of phrases which are easy to  align and also phrases which are difficult to  align.
systems designers. It entails the determination of optimal buffer allocation plans in production lines  with the objective of maximizing their throughput. We present and compare two stochastic approaches  for solving the buffer allocation problem in large reliable production lines. The allocation plan is calculated  subject to a given amount of total buffer slots using simulated annealing and genetic algorithms.
Many declarative query languages for object-oriented (oo) databases allow nested  subqueries. This paper contains (1) the first algebra which is capable of handling  arbitrary nested queries and (2) the first complete classification of oo nested queries  and the according unnesting strategies.
Attempts to extend process management to support dynamic, knowledge intensive activities have not been as successful as workflow for routine business processes. In part this is due to the dynamic nature of knowledge-intensive work: the tasks performed change continuously in response to the knowledge developed by those tasks. Also, knowledge work involves significant informal communications, which are difficult to capture.
The new crystal structure of 5,6-dibromoacenaphthene has been successfully determined  from X-ray powder diffraction data with monoclinic crystals, P2 1 /n, a = 7.88(1) , b = 11.64(1) , c =  11.62(1) ,  =107.09(9), V = 1019.9(1)     and Z = 4. Individual reflection intensities were extracted  by means of Le Bail&apos;s method. The molecular structure of 1 was solved by direct methods and refined  using the combination of least-squares and Rietveld methods. The final Rietveld refinement converged  to the value of R p = 0.114, R wp = 0.155, Bragg-R = 5.28 for 8532 data points and 933 extracted reflections  collected in the 2 &lt; 2 &lt; 95 range. The molecules have an interplanar - interaction between the  acenaphthene planes with the average distance of 3.57(9) . Moreover, the molecular dimer-pairwise  packing diagram has an unexpected straight stacking, as opposed to the usual lateral stacking normally  occured in 5,6-dihaloacenaphthene.
While state of the art techniques can address the problem of automatically detecting negated medical observations, negation using the word `not&apos; presents a harder problem than other kinds of negation. We apply machine learning techniques to distinguish sentences where the word `not&apos; does and does not negate a medical observation. Our corpus contains hospital reports such as progress notes and emergency room notes. We use two different machine learning algorithms, Naive Bayes and Decision Trees, and both achieve significant improvement over the baseline. We also analyze the data and the classifiers&apos; behavior and output to learn more about the problem and the usefulness of various features in our feature vector.
This paper shows that deterministic consensus in synchronous distributed systems with link faults is possible, despite the impossibility result of (Gray, 1978). Instead of using randomization, we overcome this impossibility by moderately restricting the inconsistency that link faults may cause system-wide. Relying upon a novel hybrid fault model that provides different classes of faults for both nodes and links, we provide a formally verified proof that the    -round Byzantine agreement algorithm OMH (Lincoln &amp; Rushby, 1993)  requires  14519              &quot;!ff  nodes for transparently masking at most $   broadcast and  $   receive link faults (including at most    arbitrary ones) per node in each round, in addition to     %    &quot;!  arbitrary, symmetric, omission, and manifest node faults, provided that  &apos;&amp;(    )*   . Our approach to modeling link faults is justified by a number of theoretical results, which include tight lower bounds for the required number of nodes and an analysis of the assumption coverage in systems where links fail independently with some probability  +  .
This paper presents a novel approach for model-based realtime tracking of highly articulated structures such as humans. This approach is based on an algorithm which efficiently propagates statistics of probability distributions through a kinematic chain to obtain maximum a posteriori estimates of the motion of the entire structure. This algorithm yields the least squares solution in linear time (in the number of components of the model) and can also be applied to non-Gaussian statistics using a simple but powerful trick. The resulting implementation runs in real-time on standard hardware without any pre-processing of the video data and can thus operate on live video. Results from experiments performed using this system are presented and discussed.
We present an Augmented Reality system that relies on purely passive techniques to solve the real-time registration problem. It can run on a portable PC and does not require engineering of the environment, for example by adding markers.
We study partiality in propositional logics containing formulas with either unde  ned or over-de  ned truth-values. Unde  ned values are created by adding a four-place connective W termed transjunction to complete models which, together with the usual Boolean connectives is shown to be functionally complete for all partial functions. Transjunction is seen to be motivated from a game-theoretic perspective, emerging from a two-stage extensive form semantic game of imperfect information between two players. This game-theoretic approach yields an interpretation where partiality is generated as a property of non-determinacy of games. Over-de  ned values are produced by adding a weak, contradictory negation or, alternatively, by relaxing the assumption that games are strictly competitive. In general, particular forms of extensive imperfect information games give rise to a generalised propositional logic where various forms of informational dependencies and independencies of connectives can be studied.
References  
This paper analyzes the potential of asynchronous, decoupled pipelines from an architectural viewpoint. Previous work in this area has been implementation oriented and has concentrated on developing circuit models and design tools needed to actually build a simple clockless processor. This past work has shown that asynchronous processing is a viable approach, with several inherent advantages over synchronous processing. This paper breaks from this implementation focus and instead considers what is gained architecturally when the globally synchronous clock is removed. We first present the design of a decoupled, self-timed asynchronous architecture in which each pipeline component is able to determine its  own operating speed. Our design includes micropipelines between stages to provide extra elasticity, allowing the pipeline to dynamically adjust to long latency operations. We then show how this organization enables a new style of prediction-based optimizations that is not possible in a synchronous processor. Finally, we provide preliminary results of a  complete VHDL simulator which demonstrate that these optimizations can carry a significant performance advantage.
Dynamical searches  nd central dark objects | candidate supermassive black holes (BHs) | in 22 galaxies. Their demographics lead to the following conclusions: (1) BH mass is consistent with predictions based on quasar energetics.
The advantage of cognitively motivated automatic summarizing is that human users can better understand what happens. This improves  acceptability. The basic empirical finding in human summarizers is that they  combine a choice of intellectual strategies. We report here on SummIt-BMT  (Summarize It in Bone Marrow Transplantation), a prototype system that  applies a subset of human strategies to a real-world task: fast information  supply for physicians in clinical bone marrow transplantation. The human  strategies are converted to knowledge-based agents and integrated into a system  environment inspired by user-centered information seeking research. A domain  ontology provides knowledge shared by human users and system players. Users&apos; query formulation is supported through empirically founded scenarios. Incoming retrieval
A new technique for the determination of extrinsic and intrinsic camera parameters is presented. Instead of searching for a limited number of discrete feature points of the calibration test object, the entire image captured with the camera is exploited to robustly determine the unknown parameters. Shape and texture of the test object are described by a 3-D computer graphics model. With this 3-D representation, synthetic images are rendered and matched with the original frames in an analysis by synthesis loop. Therefore, arbitrary test objects with sophisticated patterns can be used to determine the camera settings. The scheme can easily be extended to deal with multiple frames for a higher intrinsic parameter accuracy. 1 
We consider the problem of estimating the magnitude of the error of an iterative linear solver after k iterations. Assuming that the initial error can be described using a probability distribution we derive L2-estimates for the magnitude of the error in the average case. In Part 1 the ideas are presented and applied to a simple splitting method, while Part 2 extends the same ideas to the Conjugate Gradient method.
In ESPRIT project no. EP5570 called IPTES a methodology and a supporting  environment for incremental prototyping of embedded computer systems is developed. As a patr
Functional magnetic resonance imaging (fMRI) measures the hemodynamic response in the brain that signals neural activity. The purpose is to detect those regions in the brain that show significant neural activity upon stimulus presentation. Most statistical fMRI tests used for this purpose rely on the assumption that the noise disturbing the data is Gaussian distributed. However, the majority of fMRI studies employ magnitude image reconstructions that are known to be Rician distributed, and hence corrupted by nonGaussian distributed noise. In this work, we propose a Generalized Likelihood Ratio Test (GLRT) for magnitude MRI data that exploits the knowledge of the Rician distribution. The performance of the proposed GLRT is evaluated by means of Monte Carlo simulations. 1. 
Resolving communication is not enough to address distribution because it is not the most dicult issue of developing distributed applications. The hard problems of distributed computing are not the problems of how to get things on and o the wire, but dealing with partial failure, lack of a central resource management, concurrency, and dierences in memory access on local and remote resources [17]. Jini is a distribution platform that recognizes the dierences between building stand-alone and really distributed systems. Its architecture provides a programming model, which enables developers to handle the hard aspects of distributed computing. However, Jini only provides a tool, and developers must apply such a tool to best address their needs to build distributed systems. This work presents the design and implementation of a framework aimed at building reliable Jini services on large-scale component networks.
In this paper, we introduce a new electronic money methodology: sub-contracting the blinding to a trustee and using an Identity-based piece of information to achieve provable privacy and security. This variation on the Brickel, Gemmel and Kravitz paradigm [2] offers protection against various attacks minimizing user&apos;s computational requirement. Furthermore, our scheme offers...
Software component run-time characteristics are largely dependent on their actual deployment situation. Validating software components i.e., confirming that they meet functional and nonfunctional property requirements, is time-consuming and for some properties quite challenging. We describe the use of &quot;validation agents&quot; to automate the testing of deployed software components to verify that they have the non-functional properties required. Our validation agents utilise &quot;component aspects&quot; that describe functional and non-functional cross-cutting concerns impacting on software components. Aspect information is queried by our validation agents and these construct and run automated tests on the deployed software components. The agents then determine if the deployed components meet their aspect-described requirements. Some agents deploy existing performance testbed generation tools to run realistic loading tests on components. We describe the motivation for our work, how component aspects are designed and encoded, our automated agent-based testing process, the architecture and implementation of our validation agents, and our experience in using them.
We present a theoretical and computational framework for nonrigid multimodal registration. We proceed by maximization of statistical similarity criteria (global and local) in a variational framework, and use the corresponding gradients to drive a flow of diffeomorphisms allowing large deformations. This flow is introduced through a new template propagation method, by composition of small displacements. Regularization is performed using fast filtering techniques. This approach yields robust matching algorithms offering a good computational efficiency. We apply this method to compensate distortions between EPI images (fMRI) and anatomical MRI volumes.
The Semantic Web is still a web, a collection of linked nodes. Navigation of links is currently, and will remain for humans if not machines, a key mechanism for exploring the space. The Semantic Web is viewed by many as a knowledge base, a database or an indexed and searchable document collection; in the work discussed here we view it as a hypertext.
This paper introduces a computational  framework for reasoning in Bayesian belief  networks that derives significant advantages  from focused inference and relevance reasoning. This framework
This paper presents a new approach to fuel-optimal path planning of multiple vehicles using a combination of linear and integer programming. The basic problem formulation is to have the vehicles move from an initial dynamic state to a final state without colliding with each other, while at the same time avoiding other stationary and moving obstacles. It is shown that this problem can be rewritten as a linear program with mixed integer /linear constraints that account for the collision avoidance. A key benefit of this approach is that the path optimization can be readily solved using the CPLEX optimization software with an AMPL/Matlab interface. An example is worked out to show that the framework of mixed integer/linear programming is well suited for path planning and collision avoidance problems. Implementation issues are also considered. In particular, we compare receding horizon strategies with fixed arrival time approaches.
The main purpose of this paper is to present an overview of the progress of  a modeling technique which is known as Total Least Squares (TLS) in computational  mathematics and engineering, and as Errors-In-Variables (EIV)  modeling or orthogonal regression in the statistical community. The basic  concepts of TLS and EIV modeling are presented. In particular, it is shown  how the seemingly different linear algebraic approach of TLS, as studied in  computational mathematics and applied in diverse engineering fields, is related  to EIV regression, as studied in the field of statistics. Computational  methods, as well as the main algebraic, sensitivity and statistical properties  of the estimators, are discussed. Furthermore, generalizations of the basic  concept of TLS and EIV modeling, such as structured TLS, Lp approximations,  nonlinear and polynomial EIV, are introduced and applications of the  technique in engineering are overviewed.
A self-consistent scenario to explain the morphology of planetary nebulae is presented. This scenario is consistent with the Galactic distribution of dierent nebular types. This work addresses several controversial features that appear in planetary nebulae, which are easily solved by the inclusion of MHD eects. These features include the presence of axisymmetric and collimated outows with linearly increasing kinematics, and the existence of asymmetrical morphologies such as point-symmetric nebulae.
This paper presents a simple multimembered evolution strategy (SMES) to solve global nonlinear optimization problems. The approach does not require the use of a penalty function and it does not require any extra parameters (besides those used with an evolution strategy). Instead, it uses a simple diversity mechanism based on allowing infeasible solutions to remain in the population This technique helps the algorithm to find the global optimum despite reaching reasonably fast the feasible region of the search space. Some simple selection criteria are used to guide the process to the feasible region of the search space. Also, the initial step size of the evolution strategy is reduced in order to perform a finer search and a combined (discrete/intermediate) recombination technique improves its exploitation capabilities. The approach was tested with a well-known benchmark. The results obtained are very competitive, when comparing the proposed approach against other state-of-the art techniques and its computational cost (measured by the number of fitness function evaluations) is lower than the required cost of the other techniques compared. 1 
the important parameters of disagreement are. Dennett (1988), for example, assumes that it is of the essence of qualia to be intrinsic (in the sense of atomic, unanalyzable and non-relational), private (in the sense that any objective test would miss the target), incorrigible (to believe one has one is to have one) and non-physical. Dennett says there are no qualia. Hence the title of his paper, &quot;Quining Qualia&quot;. (To Quine, according to the Philosophical Lexicon, http://www.blackwellpublishing.com/lexicon/, is &quot;to deny resolutely the existence of importance of something real or significant&quot;.) Of course, Dennett is free to use `qualia&apos; as he likes, but a defender of a scientific approach to qualia (the point of view of the author of this entry) will prefer a definition of `qualia&apos; that allows that science can investigate qualia, that qualia may turn out to be physical, and even that we may discover aspects of introspective beliefs about one&apos;s qualia can be mistaken. Indeed, I don&apos;t see
Pass transistor logic (PTL) has been recently proposedas  an alternative to standard MOS for aggressive circuit design. Even though PTL has been successful in a few handcrafted designs, its acceptance into mainstream digital design critically depends on the availabilityoftools for logic and physical synthesis and optimization. The automatic synthesis of pass transistor circuits starting from BDDs has been intensively studiedinthepast with promising results, but back-end tools for PTL cell generation are still missing. We describe an automatic layout generator that has  been designed for seamless integration in a library-free PTL design flow. The generator exploits the distinctive characteristics of pass transistor networks produced by synthesis to achieve quality of results comparable with state-of-the  art commercial cell generation tools in a fraction of the execution time.
The algorithm selection problem aims at selecting the best algorithm for a given computational problem instance according to some characteristics of the instance. In this dissertation, we first introduce some results from theoretical investigation of the algorithm selection problem. We show, by Rice&apos;s theorem, the nonexistence of an automatic algorithm selection program based only on the description of the input instance and the competing algorithms. We also describe an abstract theoretical framework of instance hardness and algorithm performance based on Kolmogorov complexity to show that algorithm selection for search is also incomputable. Driven by the theoretical results, we propose a machine learning-based inductive approach using experimental algorithmic methods and machine learning techniques to solve the algorithm selection problem. Experimentally, we have
Devices]: Automata; F.4.1 [Mathematical Logic and Formal Languages]: Computational Logic; F.4.3 [Math- ematical Logic and Formal Languages]: Classes defined by grammars or automata; H.2.3 [Database Management]: Query languages; I.7.2 [Document Preparation]: Markup languages General Terms: Theory, Languages, Algorithms  Additional Key Words and Phrases: Complexity, Expressiveness, HTML, Information Extraction, Monadic Datalog, MSO, Regular Tree Languages, Web Wrapping  1. 
The multiplicative fragment of Non commutative Logic (MNL) has a proof nets theory [AR00] with a correctness criterion based on long trips for cut-free proof nets. Recently, R.Maieli has developed another criterion in the Danos-Regnier style [Mai00]. Both are in exponential time. We give a quadratic criterion in the Danos contractibility criterion style.
In wireless sensor networks, a possibility to reduce the amount of data to be transmitted, and therefore to conserve energy, is to combine several sensor readings in intermediate nodes along the way towards the requester. This process is known as data aggregation.
ASSIST is a parallel programming environment aimed at providing programmers of complex parallel application with a suitable and e ective programming tool. Being based on algoritmical skeletons and coordination languages technologies, the programming environment relieves the programmer from a number of cumbersome, error prone activities that are required when using traditional parallel programming environments. ASSIST has been speci cally designed to be easily customizable in order to experiment different implementation techniques, solutions, algorithms or back-ends any time new features are required or new technologies become available. In this work we discuss how this goal has been achieved and how the current ASSIST programming environment has been already used to experiment solutions not implemented in the rst version of the tool.
As the logical next step after sequencing the mouse genome, biologists have developed laboratory methods for  rapidly determining where each of the 30K genes in the mouse genome is synthesizing protein. Applying these  methods to the mouse brain, biologists are currently generating large numbers of 2D cross-sectional images that  record the expression pattern for each gene in the mouse genome. In this paper, we describe the structure of a  geometric database for the mouse brain that allows biologists to organize and search this gene expression data. The
Many settings of unsupervised learning can be viewed as quantization problems - the minimization  of the expected quantization error subject to some restrictions. This allows the  use of tools such as regularization from the theory of (supervised) risk minimization for  unsupervised learning. This setting turns out to be closely related to principal curves, the  generative topographic map, and robust coding.
Any data exchanged between the processor and main memory uses the memory bus, sharing it with data exchanged between I/O devices and main memory. If the processor and a device try to transfer data at the same time, an impact can be seen on the processor as well as on the device. As a result, the execution time of an application on the processor may increase due to the memory-bus load generated by I/O devices. In real-time environments, this impact can result in missed deadlines and a behavior that is different to that intended by the designer of the system. This paper
Many address lookup methods on the IP routers have been recently proposed to improve the packet forwarding capability; nevertheless, their performance prediction is very limited because of lack of considering actual traffic characteristics in their evaluations. It is necessary to consider actual traffic to predict more realistic performances on routers, specially in case of layers 3 and 4 switches whose performances are more influenced by flow characteristic. In this paper, we propose new methods for predicting the router&apos;s performance based on the statistical analysis of the Internet traffic. We also present an example of its application to the existing table lookup algorithm, and show that simulation results based on our method can provide accurate performance prediction. I. 
Development and assessment of safety critical software is governed by many standards. Given the growing dependence on software in a range of industries, one might expect to see these standards reflecting a growing maturity in processes for development and assessment of safety critical software, and an international consensus on best practise. In reality, whilst there are commonalities in the standards, there are also major variations in the standards among sectors and countries. There are even greater variations in industrial practices. This leads us to consider why the variation exists and if any steps can be taken to obtain greater consensus.
The ebXML framework consists of eight specifications for conducting eBusiness.
ch have not been able to produce explosions.  It must be stressed, however, that these simulations do not conflict with each other. They were performed with largely different numerical descriptions and the discrepant results simply demonstrate the sensitivity of the delayed explosion mechanism to variations at the level of the different approaches.  2.1 Successful explosions on the one hand...  Wilson and collaborators (2) found explosions in one-dimensional simulations by assuming that neutron finger convection below the neutrinosphere boosts the neutrino emission from the nascent neutron star and thus increases the neutrino heating behind the stalled supernova shock. Neutron finger convection, however, requires a faster exchange of energy than lepton number between fluid elements, an assumption that could not be confirmed by detailed analysis of the multi-flavor neutrino transport (3). Another ingredient to the energetic explosions of Wilson&apos;s group is a nuclear equation of state (Eo
The ability of bifurcating processing units and their networks to rapidly switch between different dynamic modes has been used in recent research efforts to model new computational properties of neural systems. In this spirit, we devise a bifurcating neuron based on control of chaos collapsing to a period-3 orbit in the dynamics of a quadratic logistic map (QLM). Proposed QLM3 neuron is constructed with the third iterate of QLM and uses an external input, which governs its dynamics. The input shifts the neuron&apos;s dynamics from chaos to one of the stable fixed points. This way the inputs from certain ranges (clusters) are mapped to stable fixed points, while the rest of the inputs is mapped to chaotic or periodic output dynamics. It has been shown that QLM3 neuron is able to learn a specific mapping by adaptively adjusting its bifurcation parameter, the idea of which is based on the principles of parametric control of logistic maps [Proceedings of the International Symposium on Nonlinear Theory and its Applications (NOLTA&apos;97), Honolulu, HI, 1997; Proceedings of SPIE, 2000]. Learning algorithm for the bifurcation parameter is proposed, which employs the error gradient descent method.
: We consider a cross-section topology which is defined on grayscale images. The main interest of this topology is that it keeps track of the grayscale informations of an image. We define some basic notions relative to that topology. Furthermore, we indicate how to get an homotopic kernel and a leveling kernel. Such kernels may be seen as &quot;ultimate&quot; topological simplifications of an image. A kernel of a real image, though simplified, is still an intricated image from a topological point of view. We introduce the notion of irregular region. The iterative removal of irregular regions in a kernel allows to selectively simplify the topology of the image. Through an example, we show that this notion leads to a method for segmenting some grayscale images without the need of defining and tuning parameters.  Keywords: discrete topology, cross-section topology, topological numbers, regularization, segmentation.   1 Introduction  The topology of discrete binary two-dimensional images has receiv...
This paper presents the results of a literature survey, undertaken by the National Research  Council of Canada, on the efforts to move from prescriptive building regulations to performancebased  regulations. This survey has revealed that, in recent years, in many countries around the   world, building codes are moving from prescriptive- to performance-based requirements. This   increasing world-wide tendency to move toward performance-based codes is due, in part, to the  negative aspects of the prescriptive codes, to advances made in fire science and engineering, to  the need for codes to use fire safety engineering principles within the context of their regulations,  and to the global harmonization of regulation systems. In addition, a performance-based code  approach improves the regulatory environment by establishing clear code objectives and safety   criteria and leaving the means of achieving these objectives to the designer. Hence, the codes   will be more flexible in allowing innovation and more functional. Performance-based codes will   also permit the use of modelling tools for measuring the performance of any number of design  alternatives against the established safety levels. In this way, improved fire safety designs at  reduced costs might be achieved.   This paper also describes the required steps for developing performance-based codes. The  description outlines a set of objectives formulated based on a combination of international  formulations. Also presented are some of the performance design criteria for quantifying the  desired fire safety objectives and some of the existing fire safety design tools for quantifying the   performance objectives. The full utilization of the existing tools in performance-based design   will depend on the systems in place...
WITH SPECIAL REFERENCE TO HINDI  Rashmi Prasad  Supervisor: Ellen F. Prince  This dissertation makes a progress towards the generation of referring expressions in Hindi. We first make a proposal to exploit a combination of Gricean implicatures (Grice, 1975) and Centering theory constraints (Grosz et al., 1995) to formulate a generation algorithm for referring expressions whose domain of application is defined in terms of the Centering Transitions. The formulated algorithm is an abstraction over the cross-linguistic variability observed across languages. To set the language-specific parameters of the algorithm, in particular the parameter that decides the relative salience of the discourse entities in an utterance, we propose a corpus-based methodology to identify the ways in which discourse salience is realized linguistically in any language. We apply this method to a Hindi corpus to investigate three possible linguistic reflexes of discourse salience: grammatical role, word order, and information status, and show that Hindi does not display exhibit any correlation between discourse salience and either word order or information status, and that grammatical function emerges as the primary determinant of salience. Using the results of the proposed methodology for Hindi, we provide an analysis of Hindi zero pronouns. We argue that the constraints on the use of zeros in Hindi are neither syntactic (Kameyama, 1985) nor explicable purely in terms of the singular notion of the topic (Butt &amp; King, 1997). Our analysis, provided in terms of Centering transition preferences, shows that pronouns can be dropped in Hindi only when they occur in an utterance following a CONTINUE or a SMOOTH-SHIFT transition, thus demonstrating the importance of the Preferred Center for zero pronoun re...
... advent of new technologies and user-centred concerns, the user interface portion of interactive systems is becoming increasingly large and complex. In this article, we discuss agent-based architectural styles and show how sound tradeoffs between conflicting requirements and properties can be done using the PAC-Amodeus conceptual model.
From proplyds to champagne flows, spanning 4 orders of magnitude in size  and density, photoevaporation flows are ubiquitous in H II regions. Such flows are  also found in planetary nebulae, in the form of cometary knots, although in this case  the flows are often &quot;advection--dominated&quot;, unlike the &quot;recombination--dominated&quot;  flows that predominate in H II regions. The general properties of such flows are  discussed, with particular reference to recent results on the Orion proplyds and the  Helix knots.
This paper presents a novel protocol for a spatiotemporal variant of multicast called mobicast,  designed to support message delivery in sensor and mobile ad hoc networks. The  spatiotemporal character of mobicast relates to the obligation to deliver a message to all  the nodes that will be present at time t in some geographic zone Z, where both the location  and shape of the delivery zone are a function of time over some interval (t start , t end ). The  protocol, called Face-Aware Routing (FAR), exploits ideas adapted from existing applications  of face routing to achieve reliable mobicast delivery. The key features of the protocol  are a routing strategy, which uses information confined solely to a node&apos;s immediate spatial  neighborhood, and a forwarding schedule, which employs only local topological information.
Distributed Data Mining (DDM) is the process of mining distributed  and heterogeneous datasets. DDM is widely seen as a means of addressing the  scalability issue of mining large data sets. Consequently, there is an emerging  focus on optimisation of the DDM process. In this paper we present cost  formulae for estimating the communication and computation time for different  distributed data mining scenarios.
In this paper we present an approach to the transcription  of musical queries based on a hidden Markov  model (HMM). The HMM is used to model the audio  features related to the singing voice, and the transcription  is obtained through Viterbi decoding. We report  our preliminary work on evaluation of the system.
This paper provides evidence of a causal and economically important effect  of financial development on volatility. In contrast to the existing literature,  the identification strategy is based on the differences in sensitivities to financial  conditions across industries. The results show that sectors with larger  liquidity needs are more volatile and experience deeper crises in financially underdeveloped  countries. At the macro level, the results suggest that changes in  financial development can generate important differences in aggregate volatility. An additional
this paper is to increase the understanding and facilitate comparison and evaluation of the most commonly refered framworks. The paper provides a survey of the architecture and message definition of BizTalk, cXML, eCo Framework, ICE (Information and Content Exchange), IOTP (Internet Open Trading Protocol), OAG (Open Applications Group), RosettaNet, xCBL, ebXML and ontology.org. The relationships between these frameworks are cooperative and competitive and thus the merger and change are unavoidable. At present eCo Framework and xCBL are tightly cooperative and supported by others. The competing initiative is centered around Microsoft`s BizTalk, supported by cXML and OAG. The future will probably see closer cooperation to make formats compatible. Microsoft is both promoting BizTalk and is a member of eCo Framework
Multi-machine scheduling, that is, the assigment of jobs to machines such that certain performance demands like cost and time effectiveness are fulfilled, is a ubiquitous and complex activity in everyday life. This paper presents an approach to multi-machine scheduling that follows the multiagent learning paradigm known from the field of Distributed Artificial Intelligence. According to this approach the machines collectively and as a whole learn and iteratively refine appropriate schedules. The major characteristic of this approach is that learning is distributed over several machines, and that the individual machines carry out their learning activities in a parallel and asynchronous way.
The task of causal structure discovery from empirical data is a fundamental problem in many areas. Experimental data is crucial for accomplishing this task. However, experiments are typically expensive, and must be selected with great care. This paper
Mobile agent technology has gained more importance recently due to the rapid advance of high-performance mobile devices with high-speed wireless telecommunications technology such as 3G. However design methodologies for mobile agent applications are not mature enough to support their software development process.
The paper briefly presents a methodology for mapping surface UV radiation that uses a radiative transfer model and satellite data to quantify the influencing factors. TOMS, TOVS and GOME data are used for the total column ozone. The cloud optical thickness is estimated using METEOSAT/MVIRI data. Other influencing factors taken into account include tropospheric aerosols, snow cover and surface elevation. The resulting products are maps of surface dose rates and daily doses, covering Europe with a spatial resolution of 0.05 deg. On this basis, it has been undertaken to build a European UV climatology, with the purpose of supporting impact studies on the environment and human health. As of today, the data set covers the period from January 1      2002. A comparison between the satellite estimates and the measurements in Ispra is briefly presented. Finally, examples of how the climatological data set can document the geographical distribution and year-to-year variability in surface UV radiation are presented.
In this paper, we survey the stochastic programming models developed to deal with financial optimization problems. A few methods are introduced in details to generate reasonable scenarios which are of much importance for a successful model. Besides, computation aspect as well as some open problems in this area are addressed.
to assume that every interesting device has communication facilities. The same  assumption, however, cannot yet be made about the availability of location tracking information.
Maintaining and evolving large software systems is hard. One underlying cause is that existing modularisation mechanisms are inadequate to handle crosscutting concerns. We propose intentional software views as an intuitive and lightweight means of modelling such concerns. They increase our ability to understand, modularise and browse the implementation by grouping together source-code entities that address a same concern. Alternative descriptions of the same intentional view can be provided and checked for consistency. In addition, the model supports the declaration, verification and enforcement of relations among intentional views. This facilitates software evolution by providing the ability to detect invalidation of important intentional relationships among concerns when the software is modified.
Our purpose in this work is to track agents&apos; activities and to  issue a report in terms of plans or procedures that are likely to  be in progress, for applications such as surveillance, situation  assessment, hazard detection.
The infrastructure of the Internet consists of many interconnected networks. The free market for Internet connections has led to an enormous growth of the Internet. The maturing of this market may lead to competition issues like those in the telecommunications market. Moreover, there is little understanding on how the interconnection policies of the Internet&apos;s constituents (Internet Service Providers or ISPs) determine the topology of the Internet. We have developed a method to create a map of the Internet on the level of ISPs. This map can also be viewed as a map of the interconnection agreements between ISPs, measured from the outside, without asking each and every ISP what other ISPs they interconnect with. When viewed over time, the maps can be used to assess current trends in the Internet infrastructure regarding issues like market power, concentration, dependence upon one or few parties for Internet service, and so on.
A key advantage for the use of a Domain-Specific Language (DSL) is the leverage that can be captured from a concise representation of a programmer&apos;s intention. This paper reports on three different DSLs that were developed for two different projects. Two of the DSLs assisted in the specification of various modeling tool ontologies, and the integration of models across these tools. On another project, a different DSL has been applied as a language to assist in aspect-oriented modeling. Each of these three languages was converted to C++ using different code generators. These DSLs were concerned with issues of traversing a model and performing transformations. The paper also provides quantitative data on the relative sizes of the intention (as expressed in the DSL) and the generated C++ code. Observations are made regarding the nature of the benefits and the manner in which the conciseness of the DSL is best leveraged.
During the past decade, many algorithms have been proposed to solve the frequent itemset mining problem, i.e. find all sets of items that frequently occur together in a given database of transactions. Although very effcient techniques have been presented, they still suffer from the same problem. That is, they are all inherently dependent on the amount of main memory available. Moreover, if this amount is not enough, the presented techniques are simply not applicable anymore, or significantly need to pay in performance. In this paper, we give a rigorous comparison between current state of the art techniques and present a new and simple technique, based on sorting the transaction database, resulting in a sometimes more effcient algorithm for frequent itemset mining using less memory.
The aim of this paper is to apply the method proposed by Denuit, Genest and Marceau (1999) for deriving stochastic upper and lower bounds on the present value of a sequence of cash flows, where the discounting is performed under a given stochastic return process. The convex approximation provided by Goovaerts, Dhaene and De Schepper (2000) and Goovaerts and Dhaene (1999) is then compared to these stochastic bounds. On the basis of several numerical examples, it will be seen that the convex approximation seems reasonable.
Putting in more security measures and access controls within an organisation runs  contrary to operational efficiency and convenience. Although the balance between security  and operational efficiency is critical to making the combination works for the  better, most administrators will rather forego security than performance if either has  to be traded off. To address this problem, we propose a complementary infrastructure  to manage and monitor the existing network. This infrastructure is called the  ShadowNetowrk Management System (SNMS). It integrates seamlessly with the network  infrastructure without degrading network performance and user convenience. The  underlying paradigm is one of making it possible for security and operational efficiency  to co-exist, rather than trading off one for the other. Weshow that SNMS can provide  the necessary assurance to prevent intra-network misuse without affecting the flowof  network traffic and access.
Tactile feedback is a modality that has become more common in user interfaces due to overall development of haptic feedback hardware. However, it is still not well understood how to get benefit from this modality in graphical user interfaces. Answering this need of knowledge we present two experiments on how tactile feedback could be used in target selection tasks when using a tactile mouse. In the first experiment twelve subjects tested four different feedback conditions: (1) mouse vibrates when the cursor is on the target, (2) mouse vibrates when the cursor is near the target so that tremble is more powerful when the mouse is near the target, (3) mouse vibrates when the cursor is far from the target so that tremble is more powerful when the mouse is far the target, and (4) normal feedback in which the mouse does not vibrate at all. In the second experiment we used the best method from the first experiment and had different target sizes. We did not find significant differences in selection times. However, we got interesting results on how people liked to use tactile feedback.
A prevailing feature of mobile telephony systems is that the cell where a mobile user is located may be unknown. Therefore when the system is to establish a call between users it may need to search, or page, all the cells that it suspects the users are located in, to find the cells where the users currently reside. The search consumes expensive wireless links and so it is desirable to develop search techniques that page as few cells as possible.
In everyday conversations, we frequently &quot;give an example&quot;. Yet this is seldom accompanied by any reflection on what is going on when we do so. This report tries to contribute such a reflection. It shows how examples may be marked and used in a particular discourse: oral discourse on &quot;others &quot;. The empirical material is a transcribed focus group interview with a group of Swedish students, engaged in discussing a recent trip to Warsaw. Examples may be looked upon as relatively specific. They are sometimes marked in explicit ways (&quot;for example&quot;, &quot;for instance&quot;), sometimes in implicit ways (&quot;like this...&quot;; &quot;look at...&quot;, &quot;take...&quot;). Their functions are numerous. They may specify or objectify an argument, as well as mobilise associations, display attitudes, or indicate &quot;types&quot; of persons or items. Some examples are &quot;virtual&quot;; they exemplify what could happen, or what never happened. Typically, examples confirm, challenge or in other ways elaborate an argument.
Many applications are characterized by having naturally incomplete data on customers -- where data on only some fixed set of local variables is gathered. However, having a more complete picture can help build better models. The nave solution to this problem -- acquiring complete data for all customers -- is often impractical due to the costs of doing so. A possible alternative is to acquire complete  data for &quot;some&quot; customers and to use this to improve the models built. The data acquisition problem is determining how many, and which, customers to acquire additional data from. In this paper we suggest using active learning based approaches for the data acquisition problem. In particular, we present initial methods for data acquisition and evaluate these methods experimentally on web usage data and UCI datasets. Results show that the methods  perform well and indicate that active learning based methods for data acquisition can be a promising area for data mining research.
this paper, a new buckling model based on immediate buckling assumption is proposed. A cloth element is assumed to reach a stable configuration immediately once it begins to buckle. This assumption makes it possible to simulate the fabric buckling stably without introducing any fictitious damping force. Consequently, it produces highly responsive cloth motion and allows the use of a large fixed time step in simulation of cloth
Allowing higher-priority requests to preempt ongoing disk IOs is of particular benefit to delay-sensitive multimedia and real-time systems. In this paper we propose  Semi-preemptible IO, which divides an IO request into small temporal units of disk commands to enable preemptible disk access. We present main design strategies to allow preemption of each component of a disk access---seek, rotation, and data transfer. We analyze the performance and describe implementation challenges. Our evaluation shows that Semipreemptible IO can substantially reduce IO waiting time with little loss in disk throughput. For example, expected waiting time for disk IOs in a video streaming system is reduced 2.1 times with the throughput loss of less than 6 percent.
We present a new performance model for a prioritized optical burst switch architecture employing fiber delay lines (FDLs) as optical buffers to reduce the burst loss probability. The performance of such an architecture cannot be captured accurately using traditional queueing models since FDLs behave fundamentally differently from conventional electronic buffers. We formulate a Markovian model to evaluate the system performance when the burst arrival process is Poisson and the burst lengths are exponentially distributed. Both the balking and bounded delay characteristics of FDLs are captured in the model. A conservation law is used to extend the analysis to a system implementing differentiated services with two prioritized traffic classes. The extended model captures the system dynamics for high priority traffic and yields a good approximation for low priority traffic. We also find that the previously developed models are approximations of our general model in the regimes of short and long FDLs. Our numerical results validate the accuracy of our modeling approach and demonstrate significant performance gains when FDLs are employed as optical buffers.
This paper describes a neuro-fuzzy modeling framework for predicting the properties of ashes originated from combustion processes for electric generation. The prediction problem is tackled by means of a neuro-fuzzy system in which a neural network and a fuzzy system are combined in a fused architecture, so that the structure and the parameters of the fuzzy rule base are determined via a two-phase learning of the neural network. The modeling framework is composed of two modeling strategies that enable development of both MIMO and MISO neuro-fuzzy models. Experimental results demonstrate that models derived by the proposed framework delivered satisfactory results in spite of the significant complexity of the considered problem.
This paper describes our research on building an adaptive, WWW-based, agent-based, long-distance learning environment for academic English.
vii Povzetek ix Znanstveno podrocjeinproblematika................... x  Deformabilnaporavnavaslik..................... xii  Vecmodalnomerjenjelokalnepodobnosti.............. xiii  Modelideformacij........................... xiv  Izvirniprispevkikznanosti ........................ xvi  Tockovnemerepodobnosti...................... xvi  Simetricnaporavnava.........................xvii  Modelideformacij...........................xviii  Vrednotenje vecmodalnihnetogihporavnav............. xix  Sistemzaporavnavomedicinskihslik................ xx  1. 
Overlay networks among cooperating hosts have recently emerged as a viable solution to several challenging problems, including multicasting, routing, content distribution, and peer-to-peer services. Application-level overlays, however, incur a performance penalty over router-level solutions. This paper characterizes this performance penalty for overlay multicast trees via experimental data, simulations, and theoretical models. We compare three overlay multicast protocols with respect to latency, bandwidth, router degrees, and host degrees. Experimental data and simulations illustrate that (i) the average delay and the number of hops between parent and child hosts in overlay trees generally decrease, and (ii) the degree of hosts generally decreases, as the level of the host in the overlay tree increases. Overlay multicast routing strategies, overlay host distribution, together with power-law and smallworld Internet topology characteristics, are identified as causes of the observed phenomena. We show that these phenomena are directly related to the overlay tree cost. Our results reveal that the normalized cost   for small n, where L(n) is the total number of hops in all overlay links, U(n) is the average number of hops on the source to receiver unicast paths, and n is the number of members in the overlay multicast session.
This document describes a new approach that explores the use of Semantic Web languages in building an architecture for supporting context-aware systems. This new architecture called Context Broker Architecture (CoBrA) differs from other architectures in using the Web Ontlogy Language OWL for modeling ontologies of context and for supporting context reasoning. Central to our architecture is a broker agent that maintains a shared model of context for all computing entities in the space and enforces the privacy policies defined by the users. We also describe the use of CoBrA and its associated ontologies in prototyping an intelligent meeting room.
The basic distinction between already known algorithmic characterizations of matroids and antimatroids is in the fact that for antimatroids the ordering of elements is of great importance. While antimatroids can also be characterized by...
The last few years have seen an explosion in the amountof  text becoming available on the World Wide Web as online communities of users in diverse domains emerge to share documents and other digital resources. In this paper we explore the issue of howtoprovide a low-level information extraction tool based on hidden Markov models that can identify and classify terminology based on previously marked-up examples. Such a tool should provide the basis for a domain portable information extraction system, that when combined with search technology can help users to access information more eectively within their document collections than today&apos;s information retrieval engines alone. We present results of applying the model in twodiverse domains: news and molecular biology and discuss the model and term markup issues that this investigation reveals.
We describe and develop a close relationship between two problems  that have customarily been regarded as distinct: that of maximizing entropy,  and that of minimizing worst-case expected loss. Using a formulation  grounded in the equilibrium theory of zero-sum games between  Decision Maker and Nature, these two problems are shown to be dual to  each other, the solution to each providing that to the other. Although  Topse described this connection for the Shannon entropy over 20 years  ago, it does not appear to be widely known even in that important special  case.
Carrying out empirical studies is slowly becoming widely held to be of importance bythe  software engineering community. A view perhaps less widely held is that experiments should be  replicated externally to both verify and generalise the original results.
In this paper we give a robust logical and computational characterisation of peer-to-peer (p2p) database systems. We first define a precise model-theoretic semantics of a p2p system, which allows for local inconsistency handling. We then characterise the general computational properties for the problem of answering queries to such a p2p system.
We consider queries which originate from a mobile unit and whose  result depends on the location of the user who initiates the query. Example of  such a query is How many people are living in the region I am currently in?&quot;  We execute such queries based on location-dependent data involved in their  processing. We build concept hierarchies based on the location data. These  hierarchies define mapping among different granularities of locations. One such  hierarchy is to generate domain knowledge about the cities that belong to a  state. The hierarchies are used as distributed directories to assist in finding the  database or relation that contains the values of the location-dependent attribute  in a particular location. We extend concept hierarchies to include spatial  indexes on the location-dependent attributes. Finally, we discuss how to  partition and replicate relations based on the location to process the queries  efficiently. We briefly discuss the implementation issues.
UML is the most widely accepted formalism for the analysis and design  of software and one of its most important components are UML class  diagrams. In this paper we discuss how to encode UML class diagrams in  the Description Logics DLR ifd and ALCQI: the  rst fully captures the  semantics of UML class diagrams, while the second is directly supported  by state-of-the-art DL reasoning systems. We also show some results  obtained by reasoning on UML class diagrams of industrial interest.
The aim of this paper is to outline a perceptual approach to a computational colourtexture representation based on some colour induction phenomena. The extension of classical grey level methods for texture processing to the RGB channels of the corresponding colour texture is not the best solution to simulate human perception. Chromatic induction mechanisms of the human visual system, that has been widely studied in psychophysics, play an important role when looking at scenes where the spatial frequency is high as it occurs on texture images. Besides others, chromatic induction includes two complementary effects: chromatic assimilation and chromatic contrast. While the former has been measured by Wandell et al. in [1] and extended to computer vision by Petrou et al. in [2] as a perceptual blurring, some aspects on the last one still remain to be measured, but it has to be a computational operator that simulates the contrast induction phenomenon performing a perceptual sharpening that preserves the structural properties of the texture. Applying both, the perceptual sharpening and the perceptual blurring, we propose to build a tower of images as an induction front-end that can be the basis of a perceptual representation of colour textures.
We report our experience in implementing UbiCrawler, a scalable distributed web crawler,  using the Java programming language. The main features of UbiCrawler are platform independence,  linear scalability, graceful degradation in the presence of faults, a very effective assignment  function (based on consistent hashing) for partitioning the domain to crawl, and more in general  the complete decentralization of every task. The necessity of handling very large sets of data has  highlighted some limitation of the Java APIs, which prompted the authors to partially reimplement  them.
Relevance feedback is a powerful technique for image retrieval and has been an active research direction for the past few years. Various ad hoc parameter estimation techniques have been proposed for relevance feedback. In addition, methods that perform optimization on multilevel image content model have been formulated. However, these methods only perform relevance feedback on low-level image features and fail to address the images&apos; semantic content. In this paper, we propose a relevance feedback framework to take advantage of the semantic contents of images in addition to low-level features. By forming a semantic network on top of the keyword association on the images, we are able to accurately deduce and utilize the images&apos; semantic contents for retrieval purposes. We also propose a ranking measure that is suitable for our framework. The accuracy and effectiveness of our method is demonstrated with experimental results on real-world image collections.
In the paper we present a novel algorithm for collision detection between complex geometric objects represented  by polygonal models and undergoing rigid motions and deformations. Most algorithms described in the literature  deal with rigid bodies and are based on some kind of hierarchical representations. We present the alternative  approach. The algorithm relies on the idea of &quot;sensor particles&quot;: interacting particles distributed on a surface.
This paper describes an implementation of a surface based rendering algorithm used for virtual colonoscopy. The colon surface data is acquired by extracting a surface from computer tomography data sets. The connectivity meshes of the resulting triangles are reconstructed to determine which parts of the iso-surface belong to the colon. Further the colon is intersected into a number of pieces along its center-line. Then these pieces are rendered depending on their visibility using graphics hardware capable of triangle rasterisation. The implementation of the rendering algorithm is embedded into the virtual endoscopy environment VirEn.
An effective requirements engineering (RE) approach must harmonise the need to achieve separation of concerns with the need to satisfy broadly scoped requirements and constraints. Techniques such as use cases and viewpoints help achieve separation of stakeholders&apos; concerns but ensuring their consistency with global requirements and constraints is largely unsupported. In this paper we propose an approach to modularise and compose such crosscutting, aspectual requirements. The approach is based on separating the specification of aspectual requirements, non-aspectual requirements and composition rules in modules representing coherent abstractions and following welldefined templates. The composition rules employ informal, and often concern-specific, actions and operators to specify how an aspectual requirement influences or constrains the behaviour of a set of non-aspectual requirements. We argue that such modularisation makes it possible to establish early trade-offs between aspectual requirements hence providing support for negotiation and subsequent decision-making among stakeholders. At the same time early separation of crosscutting requirements facilitates determination of their mapping and influence on artefacts at later development stages. A realisation of the proposed approach, based on viewpoints and the eXtensible Markup Language (XML), supported by a tool called ARCaDe and a case study of a toll collection system is presented.
The dynamical behaviour of two infinitely long adjacent parallel polymer  threads (dispersed phase) immersed in a different polymeric fluid (continuous  phase) is considered. This behaviour is due to small initial perturbations. Assuming  the polymer fluids to behave Newtonian, we used the creeping flow  approximation, which resulted in Stokes equations. Applying separation of  variables on the basis of cylindrical coordinates and writing the dependence on  the azimuthal direction in the form of a Fourier expansion, we obtained a general  solution of these equations for both the dispersed and continuous phase. Substitution of
Thanks to its increasing availability, electronic literature can now be a major source of information when developing complex statistical models where data is scarce or contains much noise. This raises the question of how to integrate information from domain literature with statistical data. Because quantifying similarities or dependencies between variables is a basic building block in knowledge discovery, we consider here the following question. Which vector representations of text and which statistical scores of similarity or dependency support best the use of literature in statistical models? For the text source, we assume to have annotations for the domain variables as short free-text descriptions and optionally to have a large literature repository from which we can further expand the annotations. For evaluation, we contrast the variable similarities or dependencies obtained from text using different annotation sources and vector representations with those obtained from measurement data or expert assessments. Specifically, we consider two learning problems: clustering and Bayesian network learning. Firstly, we report performance (against an expert reference) for clustering yeast genes from textual annotations. Secondly, we assess the agreement between text-based and data-based scores of variable dependencies when learning Bayesian network substructures for the task of modeling the joint distribution of clinical measurements of ovarian tumors.
Within this paper a new framework for Bayesian tracking is presented, which approximates the posterior distribution at  multiple resolutions. We propose a tree-based representation  of the distribution, where the leaves define a partition of the state space with piecewise constant density. The advantage of this representation is that regions with low probability mass can be rapidly discarded in a hierarchical search, and the distribution can be approximated to arbitrary precision. We demonstrate the effectiveness of the technique by using it for tracking 3D articulated and non-rigid motion in front of cluttered background. More specifically, we are interested in estimating the joint angles, position and orientation of a 3D hand model in order to drive an avatar.
Despite the expectations of the benefits of this tool, the adoption of Electronic Commerce (EC) by small and medium firms of the agro-food sector in Italy is still not frequent, however, the understanding of opportunities it could create and how they can be exploited remains a relevant issue.
The incremental, &quot;construct by correction&quot; design methodology has become widespread in constraint-dominated DSM design. We study the problem of ECO for physical design domains in the general context of incremental optimization. We observe that an incremental design methodology is typically built from a full optimizer that generates a solution for an initial instance, and an incremental optimizer that generates a sequence of solutions corresponding to a sequence of perturbed instances. Our hypothesis is that in practice, there can be a mismatch between the strength of the incremental optimizer and the magnitude of the perturbation between successive instances. When such a mismatch occurs, the solution quality will degrade -- perhaps to the point where the incremental optimizer should be replaced by the full optimizer. We document this phenomenon for three distinct domains -- partitioning, placement and routing -- using leading industry and academic tools. Our experiments show that current CAD tools may not be correctly designed for ECO-dominated design processes. Thus, compatibility between optimizer and instance perturbation merits attention both as a research question and as a matter of industry design practice.
The purpose of this study was to identify variables that explain the job satisfaction of assistant principals of secondary schools. If such variables are identified, efforts can be made to eliminate or reduce the effects of those variables which lead to dissatisfaction and enhance those which lead to satisfaction. The participants were 291 respondents to a survey distributed to a systemic sample of 400 assistant principals who were members of the National Association of Secondary School Principals in 1996. Participants completed the short form of the Minnesota Satisfaction Questionnaire and a questionnaire developed by the researcher. Participants&apos; job satisfaction had three measures: extrinsic, intrinsic, and general job satisfaction. The variables believed to explain job satisfaction of assistant principals (age, opportunity for advancement, career aspirations, compensation, feelings of compensation fairness, supervisor relations, and iii  ability utilization) were analyzed through path analysis to determine the effects of the independent variables on the three measures of job satisfaction. Results revealed that assistant principals are only marginally satisfied with their jobs. Assistant principals are not as interested in advancing their careers as reported in prior studies. Assistant principals also feel that their responsibilities are extending beyond the routine maintenance of discipline and attendance programs. Examination of the data revealed that the hypothesized models did not fit the data. Of the variables theorized to explain job satisfaction, age, compensation, and opportunity for advancement were found to have no significant effect on intrinsic, extrinsic, or general job satisfaction. However, supervisor relations was found to have a significant effect ...
An incremental approach to system verification is proposed, for system behaviours and safety properties described by means of finite-string languages and finite-state automata. Properties are verified with respect to subsystems of the overall system, nevertheless allowing assertions to be made about the entire system satisfying such properties. The proposed approach considers satisfaction of properties, controllability, and synthesis as successive verification steps. Furthermore, it allows the incremental augmentation of the system to be verified: after each verification step, either the desired property is verified, or a counter example is obtained, which, together with heuristics, provides the basis for the augmentation of a given subsystem for the next verification step.
Interactive program analysis tools are often tailored to one particular representation of programs, making adaptation to a new language costly. One way to ease adaptability is to introduce an intermediate abstraction---an adaptation layer--- between an existing language representation and the program analysis tool. This adaptation layer translates the tool&apos;s queries into queries on the particular representation.
Introduction: All-optical wavelength converters with 2R regeneration capabilities, that allow operation at speeds beyond the limits of electronic devices, will be essential in future wavelength division multiplexing (WDM) networks. Of particular interest are simple and compact wavelength converters that help to avoid wavelength blocking and facilitate WDM network management. All-optical wavelength conversion has successfully been demonstrated with semiconductor optical amplifier (SOA) devices exploiting the cross-gain modulation (XGM) effect as well as the cross-phase modulation effect (XPM). In the XGM scheme a strong input signal is used to saturate the gain of an SOA and thereby to modulate a CW signal carrying the new wavelength. Although XGM is limited by the relatively slow carrier recovery time within the SOA, impressive wavelength conversion of up to 40Gbit/s [1], and with some degradation even up to 100Gbit/s [2], has been demonstrated. Unfortunately, XGM is accompanied by lar
Survivability in the dynamic IP over WDM network may be guaranteed by the Shared Path Protection (SPP) scheme implemented at the optical layer. The advantages of this scheme are fast recovery obtained at the lowest possible layer and efficient bandwidth utilization obtained by sharing spare resources.
The problem of finding documents written in a language  that the searcher cannot read is perhaps the most challenging application  of cross-language information retrieval technology. In interactive  applications, that task involves at least two steps: (1) the machine locates  promising documents in a collection that is larger than the searcher  could scan, and (2) the searcher recognizes documents relevant to their  intended use from among those nominated by the machine. The goal of  the 2001 Cross-Language Evaluation Forum&apos;s experimental interactive  track was to explore the ability of present technology to support interactive  relevance assessment. This paper describes the shared experiment  design used at all three participating sites, summarizes preliminary results  from the evaluation, and concludes with observations on lessons  learned that can inform the design of subsequent evaluation campaigns.
 Simulation-based design has become a major tool in the design of automotive, aerospace and consumer products. Designers are faced with the continuous challenge of reducing manufacturing costs and design cycle times while improving the system&apos;s performance and reliability. Simulation-based design plays an increasingly prominent role in facilitating the conceptualization and realization of products under these competitive conditions. Single discipline simulations used for analysis are being coupled together to create complex coupled simulation systems. This investigation focuses on the development of methodologies that help designers reduce the cost of using optimization to manage the simulation based design process. The computational cost of executing a single complex coupled simulation and the total number of simulations required per iteration are the two factors which most influence an optimization framework&apos;s design. Original contributions
Multiuser CDMA systems with aperiodic spreading codes have received considerable attention. One major difficulty in multiuser detection is the user&apos;s time-varying signature. In a multipath communication environment, this signature is not a priori known, making direct design of the detector more intractable. In this paper, multiuser detection is performed by a two-step approach: estimate the unknown channel coefficients and construct the detector based on the MMSE criterion. We model the interference as colored Gaussian  noise with unknown correlations and estimate the channel parameters based on the correlation matching idea. The spreading codes are assumed random with known up to the fourth order statistics, resulting in computationally efficient method. In the case of unknown statistics however, the method is still applicable by estimating those statistics from given codes. To guarantee identifiability, both the channel&apos;s output and the output of the matched filter for the desired user are considered in one cost function. Comparisons with previously proposed method and other existing methods are made by simulations.
We investigate a duopsonistic wage-setting game in which the firms have a limited number of workplaces. We assume that the firms have heterogeneous productivity, that there are two types of workers with different reservation wages and that a worker&apos;s productivity is independent of his type. We show that equilibrium unemployment arises in the wage-setting game under certain conditions, although the effcient allocation of workers would result in full employment.
Electronic commerce (e-commerce) is purported to be a &quot;silver bullet&quot; for businesses whether they are large or small to medium enterprises (SMEs). With the use of e-commerce it is said that geographic boundaries are removed, new global markets can be opened, and the Internet is the vehicle in which to do e-commerce. However, with the affects on Internet stock prices during the first quarter of 1999, and the slow uptake by many SMEs the question begs asking: is e-commerce really a &quot;silver bullet&quot; phenomenon and it is for all businesses?
The paper develops a post-keynesian macromodel of capacity utilization and growth in which the supply of credit-money is endogenous and firms&apos; debt position -- and thus the financial fragility of the economy -- is explicitly modeled. Both the influence of interest rate and indebtedness on capacity utilization and the rates of profit and growth, on the one hand, and the effect of the parameters of the saving and investment functions on financial fragility, on the other hand, are carefully analyzed.
This paper presents an original approach for the detection of auto-organizated coherent structures in a fluid flow simulation. This method is based on vortex methods and multiagent systems. We then use automata to simulate the interactions between these structures and the induced evolution of their stability.
Exchange of digitally signed certificates is often used to establish mutual trust between strangers that wish to share resources or to conduct business transactions. Automated Trust Negotiation (ATN) is an approach to regulate the flow of sensitive information during such an exchange. Previous work on ATN are based on access control techniques, and cannot handle cyclic policy interdependency satisfactorily. We show that the problem can be modelled as a 2-party secure function evaluation (SFE) problem, and propose a scheme called oblivious signature-based envelope (OSBE) for efficiently solving the SFE problem. We develop a provably secure and efficient OSBE protocol for certificates signed using RSA signatures. We also build provably secure and efficient one-round OSBE for Rabin and BLS signatures from recent constructions for identity-based encryption. We also discuss other applications of OSBE.
Today&apos;s car navigation systems have reached a high level of maturity, using huge map databases with a high coverage and up-to-dateness. However, as additional applications gain importance, such as advanced driver information and warning systems, more detailed and accurate information on the true road geometry has to be incorporated into those databases. Properties like height, longitudinal and transversal slope, curvature, and width which are currently not present, have to be acquired and integrated. This article shows how existing databases either from public authorities or from private map providers can be used in combination with aerial laser scan data to derive such properties. Apart from a general discussion of the problem and our approach, first results are presented and discussed.
Contents  1 Collisions 4 1.1 Environmentalprogramarvo...................... 4  1.1.1 Execution............................ 4  1.1.2 Communication......................... 5  1.1.3 Algorithm............................ 6  1.1.4 Sampleobjects ......................... 6  1.2 Environmental program collisions . . ................. 7  1.2.1 Execution............................ 7  1.2.2 Communication......................... 7  1.2.3 Algorithm............................ 7  1.2.4 Sampleobjects ......................... 8  1.3 Environmentalprogramecosystem................... 9  1.3.1 Execution............................ 9  1.3.2 Communication......................... 9  1.3.3 Algorithm............................ 10  1.3.4 Sampleobjects ......................... 10  1.4 Environmental program honda81 . . . ................. 11  1.4.1 Execution............................ 11  1.4.2 Communication......................... 11  1.4.3 Algorithm............................ 12  1.4.4 Sampleobjects .
IP Header compression mechanisms have always been an important topic for the research community to save bandwidth in the Internet. Due to the high license fees of 3G bands and the upcoming integration of IP based multimedia services, it is of particular importance to reduce the IP header overhead even in the wireless format. Reducing the IP overhead gives the network providers the possibility for a faster return of investment on their 3G networks and simultaneously enables real--time services by improving the latency of the IP packets over bandwidth limited links. Many compression methods exist already but they are either not designed for multimedia services, or not robust in the presence of error--prone links and therefore not suitable for wireless communication. For wireless environments robust header compression was introduced. Robust header compression was standardized by the Internet Engineering Task Force in RFC 3095 and will be an integral part of the 3GPP--UMTS specification. This compression scheme was designed to operate in error--prone environments by providing error detection and correction mechanisms in combination with robustness for IP based data streams. A connection oriented approach removing packet inter-- and intra--dependencies reduces the IP header significantly. This paper gives a solid performance evaluation for robust header compression showing both the bandwidth savings for the IP protocol stack and the quality of services at the application layer by the means video services.
The relentless scaling of CMOS technology has provided a steady increase in processor performance for the past two decades. However, increased power densities (hence temperatures) and other scaling effects have an adverse impact on long-term processor lifetime reliability. This paper represents a first attempt at quantifying the impact of scaling on lifetime reliability due to intrinsic hard errors, taking workload characteristics into consideration.
A secret sharing scheme is a cryptographic protocol by means of which a dealer shares a  secret among a set of participants in such a way that it can be subsequently reconstructed  by certain qualified subsets. The setting we consider is the following: in a first phase, the  dealer gives in a secure way a piece of information, called a share, to each participant.
In IP based wireless access networks, the flow of packets to the mobile host (MH) is diverted at the cross-over node (CoN) to the new location of the MH in order to reduce handover delay and packet loss. As the depth of the network increases so does the round trip time to the CoN, in the worst case this is the time to the gateway or root node of the domain. This paper proposes an architectural modification to micro mobility protocols employing virtual or static tree topologies such as Cellular IP (CIP). We propose an architectural extension to the path update process by adding multihomed base stations (MHOBS) at certain points in the wireless access network where the path update delay becomes severe. There are two situations where this may arise. The first is where the path update is due to a distant gateway, or other CoN, as a result of an increase in the depth of the topology. The second situation is more likely to arise, with virtual tree topologies on mesh networks, when the new path, between new base station (BS) and the CoN, is longer than the old path to the old BS. The path update minimization proposed here will eliminate the effect of path update delay at these points in the network, and consequently reduces packet loss due to the handover process.
The aim of this paper is to explain why time use data are essential for analyzing issues of gender equity and the intra-household allocation of resources, comparing living standards, and estimating the behavioral effects of changes in policy variables. The first step in the exposition is to show that the neglect of these data in much of the literature on household behavior, in both developed and developing economies, can be traced to unrealistic assumptions on domestic production and the mistaken idea that non-market time can be viewed as leisure. It is argued that an approach is required that makes explicit the need for data on the time family members spend on domestic work as well as on labor supply. An approach of this kind is outlined and used to identify the specialized assumptions that are employed when they are missing. The paper also discusses the limitations of available time use survey datasets that are due to deficiencies in survey design. The more serious and common problems are illustrated using as case studies the Statistics South Africa 2000 Time Use Survey and the time use module included in the Nicaraguan 1998 Living Standards Measurements Survey.  
We are exposed to physical and virtual systems every day. They consist of computers, PDAs, wireless devices and increasingly, robots. Each provides services to individual or groups of users whether they are local or remote to the system. Services offered by these systems may be useful beyond these users to others, however connecting many of these systems to more users presents a challenging problem. The primary goal of the research presented in this paper is to demonstrate a scalable approach for connecting multiple users to the services provided by multiple systems. Such an approach must be simple, robust and general to contend with the heterogeneous capabilities of the services. An infrastructure is presented that addresses these scalability requirements and establishes the foundation for contending with heterogeneous services. Additionally, it allows services to be linked to form higher-level abstractions. The infrastructure is demonstrated in simulation on several similar multirobot systems with multiple users. The results propose it as a solution for large-scale human-system interaction.
We describe an autonomous Byzantine fault tolerant public key authentication architecture. It aims to satisfy the authentication requirements of large distributed systems consisting of semitrusted peers. The distributed trust model does not demand the existence of prede  ned trusted parties and provides authentication if more than a threshold of the peers are honest.
Introduction  In future accelerator developments neutrino factories are an investigated issue [1]. Experimental requirements for neutrino physics and muons lifetime impose a set of constrains which are challenging for the design of the front-end linac after the target [2]. In order to meet design constrains, the large 6D beam emittance obtained after the target needs an eective fast cooling. Ionization cooling has been proposed as a method to cool transverse 4D emittance [2] while longitudinal cooling might be reached through a transverse-longitudinal emittance [3] exchange where the longitudinal emittance is transferred to the transverse plane and there cooled out. The main issue for an ionization cooling scheme become its eectiveness and optimization. In the international collaboration [4] eorts both theoretical [5, 6] and computational [7] have been dedicated to investigate beam dynamics in ionization cooling channels.  We report in this note a theoretical investigation based on 
The Caltech Multi-Vehicle Wireless Testbed (MVWT) is a platform designed to explore theoretical advances in multi-vehicle coordination and control, networked control systems and high confidence distributed computation. The contribution of this report is to present simulation and experimental results on the generation and implementation of optimal trajectories for the MVWT vehicles. The vehicles are nonlinear, spatially constrained and their input controls are bounded. The trajectories are generated using the NTG software package developed at Caltech. Minimum time trajectories and the application of Model Predictive Control (MPC) are investigated.
We investigate the use of certain data-dependent estimates of the complexity of a function  class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we  prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the  Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines.
The question of how to combine monads arises naturally  in many areas with much recent interest focusing on the coproduct  of two monads. In general, the coproduct of arbitrary monads does  not always exist. Although a rather general construction was given  by Kelly [15], its generality is reflected in its complexity which limits  the applicability of this construction. Following our own research [19],  and that of Hyland, Plotkin and Power [12], we are looking for specific  situations when simpler constructions are available. This paper uses  fixed points to give a simple construction of the coproduct of two ideal  monads.
The purpose of this study is to develop a wind spectrum model for the site  of the new Mexican Optical-Infrared Telescope (TIM) to be installed at the Sierra  of San Pedro Martir. This model will be useful in the study and design of the  structure and building that will contain the telescope. This paper summarizes the  results of our initial analysis of selected wind data obtained at the site with a  sampling frequency of 40 Hz. For each data set we calculated the power spectral  density. A power law / f    for the high frequency part of the spectrum was  tted  and the spectral index  was found to be in the range of 0.625 to 0.969, and to  depend on the wind direction. The frequency for the maximum of the spectrum  lies in the range of 0.008 to 0.017 Hz. These studies should be repeated once the  building is installed at the site due to changes in the site topography.
The wavelet based ECW image compression is compared with older compression techniques and other wavelet compression methods. The ability to compress images without intermediate tiling or intermediate disk storage is a big advantage of the ECW compression especially for the compression of big remote sensing data sets.
A well known theorem Hajós claims that every graph with chromatic number greater than k can be constructed from disjoint copies of the complete graph K k+1 by repeated application of three simple operations. This classical result has been extended in 1978 to coloring the hypergraphs by C. Benzaken and in 1996 to list coloring graphs by S. Gravier. In this note, we capture both variations to extend Hajós theorem to list coloring hypergraphs.
Motivation: We investigate two new Bayesian classification algorithms incorporating feature selection. These algorithms are applied to the classification of gene expression data derived from cDNA microarrays.
Cycorp has developed a knowledge acquisition  system, based on Cyc, that can engage a user in a  natural-language mixed-initiative dialogue. In  order to achieve a intelligent dialogue with the  user, it employs explicit topic- and user-modeling,  a system of prioritized interactions, and a  transparent agenda to which either the user or the  system can add interactions at any time.
Discretization of singular functions is an important component in many problems  to which level set methods have been applied. We present two methods for constructing  consistent approximations to Dirac delta measures concentrated on piecewise  smooth curves or surfaces. Both methods are designed to be convenient for level  set simulations on Cartesian grids and are introduced to replace the commonly used  but inconsistent regularization technique that is solely based on the distance to the  singularity with a regularization parameter proportional to the mesh size. The first  algorithm is based on a tensor product of regularized one-dimensional delta functions.
The problem of solving System of Linear Algebraic Equations  (SLAE) by parallel Monte Carlo numerical methods is considered.
A new algorithm is proposed to obtain very high resolution time--frequency analysis of signal components with curved time-- frequency supports. The proposed algorithm is based on fractional Fourier domain warping concept introduced in this work. By integrating this warping concept to the recently developed directionally smoothed Wigner distribution algorithm [1], the high performance of that algorithm on linear, chirp--like components is extended to signal components with curved time--frequency supports. The main advantage of the algorithm is its ability to suppress not only the cross--cross terms, but also the auto--cross terms in the Wigner distribution. For a signal with ff samples duration, the computational complexity of the algorithm is ffffff ffffff ffff flops for each computed slice of the new time--frequency distribution.
Programming interface in general and particularly for XML data manipulation  should be simple and flexible. For this purpose, we introduce light-weight and flexible  programming interface for XML that provides only some basic operations such as  controlling XML parser and XML document generator. Our programming toolkit represents  XML documents as S expressions internally; therefore, XML application programs  can be simply coded as list processing, and we can make use of advantage of using Lisp  such as treating data and programs uniformly.
this article, I will consider only a gaussian distribution of membrane depolarizations of magnitude p.h/    p 2    .h N h/ 2  2 2  (2.13) or the nonstochastic limit     0, in which case      h/; (2.14) where .x/ is the Dirac distribution. I will refer to 
This paper concerns control of stochastic networks using state-dependent safetystocks. Three examples are considered: a pair of tandem queues; a simple routing model; and the Dai-Wang re-entrant line. In each case, a single policy is proposed that is independent of network load ff  .
We describe a method for performing trustpreserving set operations by untrusted parties. Our motivation for this is the problem of securely reusing content-based search results in peer-to-peer networks. We model search results and indexes as data sets. Such sets have value for answering a new query only if they are trusted. In the absence of any system-wide security mechanism, a data set is trusted by a node a only if it was generated by some node which is trusted by a.
Poor accuracy of frequency references used in GSM Base Transceiver Stations (BTS) can lead to dropped calls, slow handover between cells and even co-channel interference. In this paper, we report some results of an experimental trial carried out in the Omnitel-Vodafone test plant in Milano (Italy). In authors&apos; knowledge, this is the first paper confirming with experimental data that lack of BTS synchronization affects GSM handover performance, leading to some degradation of quality of service.
Many evaluations of cognitive models rely on data that have been averaged  or aggregated across all experimental subjects, and so fail to consider the possibility  that there are important individual differences between subjects. Other  evaluations are done at the single-subject level, and so fail to benefit from the  reduction of noise that data averaging or aggregation potentially provides. To  overcome these weaknesses, we develop a general approach to modeling individual  differences using families of cognitive models, where different groups  of subjects are identified as having different psychological behavior. Separate  models with separate parameterizations are applied to each group of subjects,  and Bayesian model selection is used to determine the appropriate number of  groups. We demonstrate the general approach in a concrete and detailed way  using the ALCOVE model of category learning, and data from four previously  analysed category learning experiments. Meaningful individual differences are  found for three of the four experiments, and ALCOVE is able to account for  this variation through psychologically interpretable differences in parameterization.
Nowadays research and development activities are accompanied by  an increasing focus on future user needs in the field of multimedia retrieval.
In this paper, we present a new approach for the automatic construction of video summaries. We introduce the Simulated User Principle to evaluate the quality of a video summary in a way which is automatic, yet related to user perception. We present experimental results to support our ideas.
The Center of Instructional Technology and Multimedia at the Universiti Sains Malaysia (Science University of Malaysia) has for the past three years conducted a Multimedia Development course for the Masters of Education program. This paper will present experiences and opinions from both the instructor &apos;s and students&apos; of the course. The course objective is to provide both theoretical and practical experiences in Multimedia Courseware development to be used in the classroom or training facilities. Students are required to submit a four-research article revie w as well as produce two-multimedia courseware as their practical component for the course. They are also required to undertake a final examination. From the evaluation and assessment of the course the students are very positive in the experiences that the y have undergone and their major complaint is a lack of time to complete the practical component of the course. The instructor utilized PowerPoint and Microsoft Publisher as the introductory software for Multimedia and Web Page development.
IP telephony is currently evolving from a more or less still experimental towards a carrier grade service which has the potential of extensive use both within the Internet as well as in Intranets. Currently we see the two signaling protocol families H.323 and SIP existing and further evolving simultaneously. For both, efforts are done to not only establish basic calls but to enable so called Supplementary Services. This is generally considered one precondition for replacing the functionality of existing conventional PBXs on top of a standard protocol. Nevertheless solutions that support more then just basic call scenarios are at the moment still often based on proprietary protocols or protocol extensions.
The current generation of ad hoc networks relies on other nodes in the network for routing information and for routing the packets. These networks are based on the fundamental assumption that the nodes will cooperate and not cheat. This assumption becomes invalid when the network nodes have tangential or contradictory goals.
A malleable parallel task is one whose execution time is a function of the number of  (identical) processors allotted to it. We study the problem of scheduling a set of n independent  malleable tasks on an arbitrary number m of parallel processors and propose an asymptotic  fully polynomial time approximation scheme. For any  xed  &gt; 0, the algorithm computes a  non-preemptive schedule of length at most (1 + ) times the optimum (plus an additive term)  and has running time polynomial in n; m and 1=.
Gate-level voltage scaling is an approach that allows different supply voltages for different gates in order to achieve power reduction. Previous researches focused on determining the voltage level for each gate and ascertaining the power saving capability of the approach via logic-level power estimation. In this paper, we present the layout techniques that feasiblize the approach in cell-based design environment. A new block layout style is proposed to support the voltage scaling with conventional standard cell libraries. The block layout can be automatically generated via a simulated annealing based placement algorithm. In addition, we propose a new cell layout style with built-in multiple supply rails. Using the cell layout, gate-level voltage scaling can be immediately embedded in a typical cell-based design flow. Experimental results show that proposed techniques produce very promising results.
The aim of this work is to give a contribution to the identification and analysis of the degree of the  Engineering students&apos; learning of the concepts of Mechanics of Structures providing the necessary help for the planning and possible modifications in the organization of the existing courses. The teaching of Engineering over the time was studied, aiming at verifying if any procedures to increase the students&apos; motivation could be recovered from the past. The  problems that could eventually concur to the high degree of students&apos; failure in the discipline Strength of Materials taught at Escola Politcnica da Universidade de So Paulo (USP) were raised. The reasons which could lay behind the different degrees of stimulus in the two types of courses have also been studied. Finally the article refers to the planning and adoption of changes made to improve the discipline Strength of Materials by means of the introduction of new strategies for the development of the desirable profile for the engineer. This improvement aims at attaining four characteristics: a solid professional formation, the conscience of being an agent of evolution, the ability to cope  with new situations and an ethical consciousness.
We will discus a method for determining location shifts in normally distributed matrices for use with apsorption matrices. A testing methodology will be produced and argued for.
This paper describes a real-valued representation for the negative selection algorithm and its applications to anomaly detection. In many anomaly detection applications, only positive (normal) samples are available for training purpose. However, conventional classification algorithms need samples for all classes (e.g. normal and abnormal) during the training phase. This approach uses only normal samples to generate abnormal samples, which are used as input to a classification algorithm. This hybrid approach is compared against an anomaly detection technique that uses self-organizing maps to cluster the normal data sets (samples). Experiments are performed with different data sets and some results are reported.
Introduction  Although the processing power of uniprocessor systems is significantly increasing every year, there is a never ending quest for peak performance. The so called grand challenges in computing require more computing power than a single uniprocessor machine can ever offer. Recentadvances in parallel processing however has brought the solution of grand challenges into reach.  What is usually neglected when reporting on peak performance and Teraflops is the effort whichwould be necessary for implementing an efficient application. Handling all the communication, whichisneededina  parallel algorithm, is probably not the most difficult part for the programmer, but often the most annoying part. To support or even automate this task a variety of programming tools and libraries has been developed.  An interesting question is nowtoinvestigate and compare the performance obtained when using such tools and the effort for using these tools. For that purpose an evaluation project has been
Current aspect-oriented languages express aspects in many different ways. This diversity contributes to several problems when trying to model aspects in an early stage of the software lifecycle. This paper discusses a software modelling approach, called CoCompose, which supports aspect-oriented mechanisms without committing to one specific mechanism. A model made with CoCompose can be translated into several implementation languages using an automated process.
The Intelligent Broadband Network [1] enables efficient provision of advanced multimedia services to users, exploiting the benefits given by ATM technology and hiding the limitations of current signalling systems to IN service designers. The  availability of new technologies like DOT (Distributed Object Technology) and MAT (Mobile Agent Technology) opens new horizons within the framework of Intelligent Broadband Networks, providing the basis for a more flexible architecture, where network intelligence (i.e., IN services) can be dynamically relocated in network nodes where they are actually needed. These topics are analysed by ACTS project AC340-MARINE (Mobile Agent enviRonments in Intelligent NEtworks). This paper provides a highlight on the main research activities carried out within this framework  KEYWORDS:  INTELLIGENT NETWORK, DISTRIBUTED OBJECT TECHNOLOGY, MOBILE AGENT TECHNOLOGY.
It is the author&apos;s experience that science and engineering students following a conventional degree course, overwhelmingly demand that they be taught in the lecture theatre where the immediacy of being able to question the lecturer is of prime importance. If information technology is exploited in the lecture theatre, then that material can also be made available to students twenty-four hours a day via a personal computer. If the overhead projection slides are electronically bound together with an audio narrative that is synchronised to appropriate animation, then an on-line lecture is created that can be presented as if it were a video. While a definition for quality is elusive, this paper identifies how the specification and generation mechanisms impose an element of quality on the resulting on-line lectures and identifies what characteristics of the presentation can be measured for their contribution to the overall quality.
We are dealing with the optimal, i.e. densest packings of congruent circles into a unit square. In the recent years we built a numerically reliable method using interval arithmetic computations, which can be regarded as a `computer assisted proof&apos;. A very efficient algorithm for eliminating large sets of suboptimal points is well known from earlier, non-interval computer methods. The paper presents an interval-based version of this tool, implemented as an accelerating device of an interval branch-and-bound optimization algorithm. In order to satisfy the requirements of a computer proof, detailed algorithmic descriptions and a proof of correctness are provided. The elimination method played a key role in solving the earlier open problems of packing 28, 29, and 30 circles.
The need for sharing is well known in a large number of distributed collaborative applications. These applications are difficult to develop for wide area (possibly mobile) networks because of slow and unreliable connections. For this purpose
Distributed sensor networks are quickly gaining recognition as viable embedded computing platforms. Current techniques for programming sensor networks are cumbersome, inflexible, and low-level. This paper introduces EnviroTrack, an object-based distributed middleware system that raises the level of programming  abstraction by providing a convenient and powerful interface to the application developer geared towards tracking the physical environment. EnviroTrack is novel in its seamless integration of objects that live in physical time and space into the computational environment of the application. The performance of an initial implementation of the system is evaluated on an actual sensor network based on MICA motes. Results demonstrate the ability of the middleware to track realistic targets.
The first step in the analysis of video content is the partitioning of a long video sequence into short homogeneous temporal segments. The homogeneity property ensures that the segments are taken by a single camera and represent a continuous action in time and space. These segments will then be used as atomic temporal components for higher level analysis like browsing, classification, indexing and retrieval. The novelty of our approach is to use color information to cut down the video into segments dynamically homogeneous using a criterion inspired by compact coding theory. First, we use a statistical detection framework to detect abrupt &quot;shot&quot; transitions (strong discontinuities in the data stream), then, we perform an information-based segmentation inside each &quot;shot&quot; using a Minimum Message Length (MML) criterion and minimization by a Dynamic Programming Algorithm (DPA). We show
this paper and the latter is currently under investigation for a different model. A `feature&apos;, by its nature, is a general term and is specified by the analyst. Under this guideline the traditional update choices of eigen-parameters would qualify as features, though their application is only meaningful for linear systems
We present a design of a distributed publish-subscribe  system that extends the functionality of messaging  middleware with &quot;relational subscriptions&quot;, to support  timely updates to state derived from published messages  while preserving high throughput, scalability, and  reliability.
We introduce the use of negative preferences to produce solutions that are acceptable to a group of users. Using negative preference profiling, a system determines which solutions are unsatisfactory to individual users, and it is assumed that the remaining solutions are satisfactory. To satisfy all members of the group, the system can propose solutions that are not unsatisfactory to any of the group&apos;s members. This approach can find a large set of solutions that are acceptable to a group and simplify user profiling. To demonstrate these benefits, we implemented Adaptive Radio, a system that selects music to play in a shared environment. Rather than attempting to play the songs that users want to hear, the system avoids playing songs that they do not want to hear. Negative preferences can potentially be applied to other domains, such as information filtering, intelligent environments, and collaborative design.
VECTOR ...................... 77  4.2 SEGREGATING P AND S CROSSTALK . .................. 77  4.3 References .................................... 85  4.4 HOW TO DIVIDE NOISY SIGNALS . . . .................. 85  4.5 NONSTATIONARITY . . ........................... 91  4.6 DIP PICKING WITHOUT DIP SCANNING . . . .............. 94  5 Adjoint operators 101 5.1 FAMILIAR OPERATORS ........................... 102  5.2 ADJOINT DEFINED: DOT-PRODUCT TEST . . .............. 109  5.3 NORMAL MOVEOUT AND OTHER MAPPINGS .............. 111  5.4 DERIVATIVE AND INTEGRAL ....................... 122  5.5 CAUSAL INTEGRATION RECURSION . .................. 123  5.6 UNITARY OPERATORS . ........................... 125  5.7 VELOCITY SPECTRA . . ........................... 126  5.8 
. This paper presents a new logic based framework for the formal treatment of graph rewriting systems as special cases of programmed rewriting systems for arbitrary relational data structures. Considering its expressive power, the new formalism surpasses almost all variants of nonparallel algebraic as well as algorithmic graph grammar approaches by offering set-oriented pattern matching facilities as well as nonmonotonic reasoning capabilities for checking pre- and postconditions of rewrite rules. Furthermore, the formalism closes the gap between the operation-oriented manipulation of data structures by means of rewrite rules and the declaration-oriented description of data structures by means of logic based languages. Finally, the formalism even offers recursively defined (sub-)programs, by means of which the application of rewrite rules may be regulated. A denotational semantics definition for these (sub-)programs relies on a special variant of fixpoint theory for noncontinuous but m...
Digital Clay is a term that signifies a computercontrolled physical surface, capable of taking any of a wide variety of possible shapes in response to changes in a digital 3D model or changes in the pressure exerted upon it by bare hands. The physical properties of such a device impose design and user-interface constraints not encountered in traditional, tracker-based software for the manipulation of virtual models. This paper describes the interaction techniques we have developed to work with this future medium. In particular, we present our solution for tracking the user&apos;s fingers using a local deformation of the surface, which we call a blister, that senses the tangential and normal displacements of the finger. We also present a solution for creating variable-height bosses and creases with the simple sweep of a finger. Since the Digital Clay hardware is not yet operational, we have have implemented a haptic simulation framework based on a PHANTOM device.
In this paper we integrate colour, texture, and motion into a segmentation  process. The segmentation consists of two steps, which both combine the  given information: a pre-segmentation step based on nonlinear diffusion for improving  the quality of the features, and a variational framework for vector-valued  data using a level set approach and a statistical model to describe the interior  and the complement of a region. For the nonlinear diffusion we apply a novel  diffusivity closely related to the total variation diffusivity, but being strictly edge  enhancing. A multi-scale implementation is used in order to obtain more robust  results. In several experiments we demonstrate the usefulness of integrating many  kinds of information. Good results are obtained for both object segmentation and  tracking of multiple objects.
In this paper, we present a synthetic traffic model for the Universal Mobile Telecommunication Systems (UMTS) based on measured trace data. The analysis and scaling process of the measured trace data with respect to different bandwidth classes constitutes the basic concept of the UMTS traffic characterization. Furthermore, we introduce an aggregated traffic model for UMTS networks that is analytically tractable. The key idea of this aggregated traffic model lies in customizing the batch Markovian arrival process (BMAP) such that different packet sizes of IP packets are represented by rewards (i.e., batch sizes of arrivals) of the BMAP. The effectiveness of the customized BMAP for modeling UMTS traffic is illustrated using the synthetic traffic model previously presented.
We showed in previous work that weighted finite-state transducers  provide a common representation for many components of a speech recognition system and described general algorithms for combining these representations to build a single optimized and compact transducer integrating all these components, directly mapping from HMM states to words. This approach works well for certain well-controlled input transducers, but presents some problems related to the efficiency of composition and the applicability of determinization and weight-pushing with more general transducers. We generalize our prior construction of the integrated speech recognition transducer to work with an arbitrary number of component transducers and, to a large extent, release the constraints imposed to the type of input transducers by providing more general solutions to these problems. This generalization allowed us to deal with cases where our prior optimization did not apply. Our experiments in the AT&amp;T HMIHY 0300 task and an AT&amp;T VoiceTone task show the efficiency of our generalized optimization technique. We report a 1.6 recognition speed-up in the HMIHY 0300 task, 1.8 speed-up in a VoiceTone task using a word-based language model, and 1.7 using a class-based model.
This paper proposes a framework for segmenting different textured areas over synthetic or real textured frames by curves propagation. We assume that the system has the ability to be taught over different texture prototypes. For each prototype a global statistical model is generated, as a set of probability density functions attributes from a multi-valued frame analysis, where different filter responses are used to create this multi-valued frame. Then, each prototype is represented by a reliable statistical model. Given an input frame composed of different texture types, the same bank of filters is applied. Over the generated multi-valued frame, we deffne an energy as a special form of a geodesic active contour model, a Geodesic Active Region Model, where we integrate boundary finding and region based segmentation approaches. This energy is minimized using a steepest gradient descend method, where smoothing, edge-based, and region statistics forces, move the curve toward the minimum of the designed objective function. Using the level set formulation scheme, complex curves can be detected, while topological changes for the evolving curves are naturally managed. In order to deal with the problem of noise influence, as well as to reduce the required computational cost, a multi-grid approach has also been considered. Finally,two different methods are used for the level set implementation, the Narrow Band and the Hermes Algorithm. Very promising experimental results are provided using synthetic and real textured frames.
In: F. Esposito (Eds.), Congress of the Italian Association for Artificial  Intelligence (AI*IA 2001), LNAI 2175, pp. 99-110, Springer, 2001.
The interaction between tones in a corpus is analyzed using conditional probability and mutual information, and a probabilistic model of intonation in American English is presented. The last pitch accent in the final intermediate phrase is found to be a strong predictor of boundary tone; this is modeled as a second order Markov process. The implications of these results for the compositional theory of intonational meaning of Pierrehumbert and Hirschberg (1990) are discussed. The conclusion is reached that the tones in a tune are interrelated in a way that a model that assigns separate meaning to each tone cannot capture.
This work is a study of a hybrid adaptive controller that blends fixed feedback control and adaptive feedback control techniques. This type of adaptive controller removes the requirement that information about the disturbance is known apriori. Additionally, the control structure is implemented in such a way that as long as the adaptive controller is stable during adaptation, the system consisting of the controller and plant remain stable. The objective is to design and implement an adaptive controller that damps the structural vibrations induced in a multi-modal structure. The adaptive controller utilizes an adaptive infinite impulse response lattice filter for improved damping over the fixed feedback controller alone. An adaptive finite impulse response LMS filter is also implemented for comparison of the ability for both algorithms to reject harmonic, narrow bandwidth and wide bandwidth disturbances. It is demonstrated that the lattice filter algorithm performs slightly better than the LMS filter algorithm in all three disturbance cases. The lattice filter also requires less than half the order of the LMS filter to get the same performance. iii  Acknowledgements I would like to thank Dr. Baumann for his guidance as my committee chair and for his infinite patience. Without his help, I would probably still be scratching my head in the lab wondering why things are not working properly. I would also like to thank Dr. Bay and Dr. Vanlandingham for their suggestions and serving on my committee. Special thanks go to my family. To my brothers, Mike and Matt, for their understanding when I was unable to spend time with them because I was too busy. To my mother and father, who always seem to know just what to say to lift my spirits. To Donna and Wade, for always supporting an...
  This thesis consists of three parts. Even though each part is self-contained, a common theme runs through all of them: data compression and its implications for statistical inference. In particular, we consider the following three questions. How can we quantify the effect of compression on statistical inference? How should a compression scheme be designed such that the effect of compression on inference is minimal? How can the Minimum Description Length (MDL) principle be used for model selection with an extraordinary number of dependent predictors? In this thesis, we attempt to answer these three questions in a general setting, and with a specific application in the compression and analysis of microarray images. In the first
this paper begins to address. Techniques for removing image distortions, recovering 3-D shape, and correcting for lighting imbalances are discussed. A complete reconstruction of the tomb of Sennedjem is shown
The need to reprogramme a wireless sensor network may arise from changing application requirements, bug fixes, or during the application development cycle. Once deployed, it will be impractical at best to reach each individual node. Thus, a scheme is required to wirelessly reprogramme the nodes. We present an energy-efficient code distribution scheme to wirelessly update the code running in a sensor network. Energy is saved by distributing only the changes to the currently running code. The new code image is built using an edit script of commands that are easy to process by the nodes. A small change to the programme code can cause many changes to the binary code because the addresses of functions and data change. A naive approach to building the edit script string would result in a large script. We describe a number of optimisations and present experimental results showing that these significantly reduce the edit script size.
Using empirical data from an Internet/WAN distributed Web response time measurement system, this paper explores the relative applicability and usefulness of the geometric mean and geometric standard deviation, and introduces the lognormal distribution for quantifying the response time measurements. These statistics are particularly useful in the areas of Web content performance comparison and SLA monitoring of service providers such as Content Providers, CDNs, ASPs, MSPs, and Web Hosting companies. Some surprisingly counter-intuitive results are identified and discussed. Warning: improper use of traditional averages and standard deviations can financially hurt you, if not simply mislead you.
This paper presents a comparative performance study between the recently proposed time-varying LMS (TVLMS) algorithm and other two main adaptive approaches: the least-mean square (LMS) algorithm and the recursive leastsquares (RLS) algorithm. Three performance criteria are utilized in this study: the algorithm execution time, the minimum meansquared error (MSE), and the required filter order. The study showed that the selection of the filter order is based on a trade-off between the MSE performance and algorithm executive time. Results also showed that the execution time of the RLS algorithm increases more rapidly with the filter order than other algorithms.
The effect of Multipacket Reception (MPR) on stability and delay of slotted ALOHA based random  access systems is considered. A general asymmetric MPR model is introduced and the MAC capacity  region is specified. An explicit characterization of the ALOHA stability region for the two user system  is given. It is shown that the stability region undergoes a phase transition from a concave region to a  convex region bounded by lines as the MPR capability improves. It is also shown that after this phase  transition, slotted ALOHA is optimal i.e., the ALOHA stability region coincides with the MAC capacity  region. Further, it is observed that there is no need for transmission control when ALOHA is optimal  i.e., ALOHA with transmission probability one is optimal. These results are extended to a symmetric  N &gt; 2 user ALOHA system, where it is shown that for a large class of symmetric MPR channels no  transmission control is optimal from a stability viewpoint. This finding suggests that if the physical  layer is even reasonably good, there is no need for sophisticated Medium Access Control protocols.
VHDL is frequently used for describing purely synchronous circuits. However, the underlying model of VHDL is much more expressive than it need be. In this report, a synchronous subset of VHDL named ABC-VHDL is introduced. ABC-VHDL is dedicated towards logical argumentation and correct circuit synthesis based on VHDL descriptions. Although being conform with the standard VHDL semantics, the semantics of ABC-VHDL is based on a far simpler model: synchronous circuit descriptions at the RT-level formalized within higher order logic. This article describes the syntactical aspects of ABC-VHDL, and it also defines the semantics of ABC-VHDL by a mapping between ABC-VHDL structures and the corresponding formulae in higher order logic.
This paper  discusses  the  basic  role of the  trifocal tensor in scene recff424ffP4HVE0  from three views. This 3    3 tensor plays a role in the analysis ofscff4 es from three views analogous to the role played by the fundamental matrix in the  two-viewc  ase. In particff4H r, the  trifoc  al tensor may  bec  omputed by a linear algorithm from a set of 13  linec orrespondencff0  in three views. It is further shown in this paper, that the trifoc  al tensor is essentially identicff l to a set  ofc  oeffcff4A ts introducff0 by Shashua to effecff point transfer in the three  viewc  ase. This observation means that the 13-line algorithm may be extended to allow for  thec  omputation of the  trifoc  al tensor given any mixture of sufficiently many line and  point correspondencffMff  From the  trifoc  al tensor the fundamental matricffM of  eac  h pair of images may be cff mputed, and the scff4A may be reconstrucffrffE For unrelated uncalibrated cameras, this recff nstr ucffrff n will be unique up to  projecffHk4A  y. Thus, projecff4V e  recffk04ffP00AHE  of a set of lines and points may be recffM2ffP00AHE  linearly from three views. If the  treec  ameras have the  samec  alibration, then reconstruction up to a similarity transform may be achieved (though not linearly).  
Most applications driving the advancements in microarchitecture and memory  system research have a non-negligible interaction with the operating system. Yet, most  architectural investigations are based on user-level simulators in which operating system  activity is not modelled. This has motivated us to design SimWattch, a microarchitectural  modeling infrastructure. SimWattch is based on Simics -- a system-level  simulation tool -- and Wattch (SimpleScalar extended with power modeling) -- a flexible  user-level simulation tool. As a result, it can analyze performance and power dissipation  in microarchitectures at the cycle level for complex workloads running on commodity  operating systems.
We introduce a probabilistic formalism subsuming  Markov random fields of bounded  tree width and probabilistic context free  grammars. Our models are based on a representation  of Boolean formulas that we call  case-factor diagrams (CFDs). CFDs are similar  to binary decision diagrams (BDDs) but  are concise for circuits of bounded tree width  (unlike BDDs) and can concisely represent  the set of parse trees over a given string under  a given context free grammar (also unlike  BDDs). A probabilistic model consists of a  CFD defining a feasible set of Boolean assignments  and a weight (or cost) for each individual  Boolean variable. We give an insideoutside  algorithm for simultaneously computing  the marginal of each Boolean variable,  and a Viterbi algorithm for finding the mininum  cost variable assignment. Both algorithms  run in time proportional to the size of  the CFD.
This paper presents a low-power encoding technique, called chromatic encoding, for the Digital Visual Interface standard (DVI), a digital serial video interface. Chromatic encoding reduces power consumption by minimizing the transition counts on the DVI. This technique relies on the notion of tonal locality, i.e., the observation that the signal differences between adjacent pixels in images follow a Gaussian distribution. Based on this observation, an optimal code assignment is performed to minimize the transition counts. Furthermore, the three color channels of the DVI may be reciprocally encoded to achieve even more power saving. The idea is that given the signal values from the three color channels, one or two of these channels are encoded by reciprocal differences with a number of redundant bits used to indicate the selection. The proposed technique requires only three redundant bits for each 24-bit pixel. Experimental results show up to a 75% transition reduction.
Introduction  Ad hoc networks have the potential to increase the flexibility of wireless communication systems. They, however, also require novel operating principles. In particular, due to the absence of fixed infrastructure, most of the functions (routing, mobility management, in some cases even security) rely on the cooperation between the nodes.  In civilian scenarios, the selfishness of the participants might be a motivation for non-cooperation. Over the last few years, several researchers have proposed incentive techniques to encourage nodes to collaborate, be it by making use of a reputation system [2, 6], or by relating the right to benefit from the network to the contribution to the common interest of a node provided thus far [3]. These proposals have been based on heuristics, and are therefore rather diffcult to compare with each other. Srinivasan et al. [7] have proposed a formal framework, based on game theory, to study cooperation with the emphasis on energy-effciency. They 
Architecture selection is one of the key steps in a multi-standard front-end design flow. By choosing a suitable front-end architecture, problems in the building block design (DC-offsets, flicker noise, high power consumption) can be avoided. In this paper the selection of the most suitable architecture for a multi-standard front-end for DECT and Bluetooth is presented. An important selection criterion is the flexibility of an architecture for different wireless standards, but some other selection criteria are also used: robustness of an architecture to the technology scaling, level of integration, image rejection, power consumption and chip area. Taking into account the fact that modern CMOS technologies migrate fast towards deep submicron processes and in order to alleviate a front-end redesign in new CMOS technologies, special attention is paid to develop a realistic scaling scenario end to evaluate their consequences on the architecture selection. Starting from a generic front-end architecture, six architectures are evaluated. According to a qualitative analysis, a quadrature low-IF architecture with adaptive image rejection shows the best performance considering the selection criteria.
One of today&apos;s hottest IT topics is integration, as bringing together  information from different sources and structures is not completely solved. The  approach outlined here wants to illustrate how ontologies [Gr93] could help to  support the integration process.
The growing interest in ontologies is concomitant with the increasing use of agent systems in user environment. Ontologies have established themselves as schemas for encoding knowledge about a particular domain, which can be interpreted by both humans and agents to accomplish a task in cooperation. However, construction of the domain ontologies is a bottleneck, and planning towards reuse of domain ontologies is essential. Current methodologies concerned with ontology development have not dealt with explicit reuse of domain ontologies. This paper presents guidelines for systematic construction of reusable domain ontologies. A purpose-driven approach has been adopted. The guidelines have been used for constructing ontologies in the Experimental High-Energy Physics domain.
The capability of the scintimammography to diagnose subcentimeters sized tumors was increased by the employment of a dedicated gamma camera. The introduction of small field of view camera, based on pixellated scintillation array and position sensitive photomultiplier, allowed to enhance the geometric spatial resolution and contrast of the images due to reduced collimator-tumor distance.
Garbage collection relieves the programmer of the burden of  managing dynamically allocated memory, by providing an automatic way  to reclaim unneeded storage. This eliminates or lessens program errors  that arise from attempts to access disposed memory, and generally leads  to simpler programs. One might therefore expect that reasoning about  programs in garbage collected languages would be much easier than in  languages where the programmer has more explicit control over memory.
We present approximation algorithms for maintaining various descriptors of the  extent of moving points in R    . We  rst describe a data structure for maintaining the  smallest orthogonal rectangle containing the point set. We then use this data structure  to maintain the approximate diameter, and smallest enclosing disk of a set of moving    so that the number of events is only a constant. This contrasts with      ) events that data structures for the maintenance of those exact properties have  to handle.
The purpose of this work is to investigate the use of machine learning approaches for confidence estimation within a statistical machine translation application. Specifically, we attempt to learn probabilities of correctness for various model predictions, based on the native probabilites (i.e. the probabilites given by the original model) and on features of the current context. Our experiments were conducted using three original translation models and two types of neural nets (single-layer and multilayer perceptrons) for the confidence estimation task.
We study the use of kernel subspace methods for learning low-dimensional representations for classification. We propose a kernel pooled local discriminant subspace method and compare it against several competing techniques: Principal Component Analysis (PCA), Kernel PCA (KPCA), and linear local pooling in classification problems. We evaluate the classification performance of the nearest-neighbor rule with each subspace representation. The experimental results demonstrate the effectiveness and performance superiority of the kernel pooled subspace method over competing methods such as PCA and KPCA in some classification problems. 1. 
User-adaptive systems are interactive software systems that spontaneously adapt to their individual users-- for example, to their interests or their work habits. First, we briefly characterize this type of system and consider its relationship to the broader category of smart adaptive systems. We then give an overview of the computational techniques that are used to realize user-adaptation, ranging from data-based machine learning techniques to theory-based decisiontheoretic models. Special attention is given to the question of possible synergies: What methods that have been developed for user-adaptive systems might profitably be transferred to other smart adaptive systems? And what techniques from the latter field deserve increased attention in connection with user-adaptivity?  KEYWORDS: User-adaptive systems, user models, machine learning, Bayesian networks, decision making, transparency, controllability  USER-ADAPTIVE SYSTEMS IN THE EUNITE TOPIC AREAS  One subclass of the class of smart adaptive systems is that of user-adaptive systems. A user-adaptive system can be defined as an interactive system that adapts its behavior to individual users on the basis of processes of user model acquisition and application that involve some form of learning, inference, or decision making (cf. [1]).
Vision is the main sensory modality employed in learning. Teaching materials in the areas of information technology and computer engineering are highly visual in nature and vision impaired students find it increasingly difficult to access and process these visuo-centric learning materials and on-line delivery.
We investigate rule dependency graphs and their colorings for characterizing the computation of answer sets of logic programs. We start from a characterization of answer sets in terms of totally colored dependency graphs. To a turn, we develop a series of operational characterizations of answer sets in terms of operators on partial colorings. In analogy to the notion of a derivation in proof theory, our operational characterizations are expressed as (non-deterministically formed) sequences of colorings, turning an uncolored graph into a totally colored one. This results in an operational framework in which different combinations of operators result in different formal properties. Among others, we identify the basic strategy employed by the noMoRe system and justify its algorithmic approach. Also, we distinguish Fitting&apos;s and well-founded semantics. 1 
This paper explores the interdependencies between userdetermined interaction in the authoring process of the complex multimedia presentation and the semantic descriptions of tasks and content that allow active system support for the user during the various process phases. The domain we have chosen is the one of the fine arts. The process phases in which we are particularly interested include events such as decision making about form and function, establishing the logical structure, material collection and selection. We introduce the design of an experimental framework for semi-automatic authoring, SampLe. This exploits large mediaaware semantic spaces through semantic-sensitive authoring methods to support users mainly during the early stages of presentation design. We discuss the relations between interactionbased interface design for the system and the underlying semantic structures.
In this paper we prove that bounded Hua-harmonic functions on tube domains that satisfy some boundary regularity condition are necessarily pluriharmonic. In doing so, we show that a similar theorem is true on one-dimensional extensions of the Heisenberg group or equivalently on the Siegel upper half-plane.
Form-based visual programming languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that form-based visual programs often contain faults. We would like to provide at least some of the benefits of formal testing methodologies to the creators of these programs. This paper presents a testing methodology for form-based visual programs. To accommodate the evaluation model used with these programs, and the interactive process by which they are created, our methodology is validationdriven and incremental. To accommodate the users of these languages, we provide an interface to the methodology that does not require an understanding of testing theory. We discuss our implementation of this methodology and empirical results achieved in its use.
Failures in cast iron water mains are more complex and diverse than is widely understood in the industry. This paper discusses the modes and causes of pipe failures that have been encountered during a three year investigation by the National Research Council Canada. In addition to corrosion, manufacturing defects, human error and unexpected levels of pipe loading all play a role in the large number of pipe failures that occur each year.
Work to date on algorithms for message-passing systems has explored a wide variety of types of faults, but corresponding work on shared memory systems has usually assumed that only crash faults are possible. In this work, we explore situations in which processes accessing shared objects can fail arbitrarily (Byzantine faults).
Distributed Fault Tolerance entails detecting errors, confining the damage caused, recovery from the errors, and providing continued service on a network of co-operating machines. Functional languages potentially offer benefits for distributed fault tolerance: many computations are pure, and hence have no side-effects to be reversed during error recovery. Moreover functional languages have a high-level runtime system (RTS) where computations and data are readily manipulated. We propose a new RTS level of fault tolerance for distributed functional languages, and outline a design for its implementation for the GdH language. Glasgow distributed Haskell is a small extension to the Haskell language and the fault tolerance design utilises existing distributed graph reduction mechanisms. The design distinguishes between pure and impure computations; impure or side effecting computations must be recovered using conventional exceptionbased techniques, but the RTS attempts implicit backward recovery of pure computations.
Let (a 1 ; a 2 ;    ; a k ) denote the graph obtained by connecting two distinct vertices  with k independent paths of lengths a 1 ; a 2 ;    ; a k respectively. Assume that  2  a 1  a 2      a k . We prove that the graph (a 1 ; a 2 ;    ; a k ) is chromatically  unique if a k &lt; a 1 + a 2 , and  nd examples showing that (a 1 ; a 2 ;    ; a k ) may not be  chromatically unique if a k = a 1 + a 2 .
Wavelet packet division multiplexing (WPDM) is a high-capacity, flexible and robust orthogonal multiplexing scheme in which the message signals are waveform coded onto wavelet packet basis functions for transmission. However, WPDM suffers from severe performance degradation in the presence of high-power amplifier (HPA) nonlinearities. In this paper, data predistortion using the pth-order Volterra inverse is proposed to combat the amplifier nonlinearities in a WPDM system. A 5th-order Volterra inverse with truncated memory length is designed based on the Volterra series channel model. Computer simulations are presented to demonstrate the capability of the proposed technique in compensating amplifier nonlinearities even under system parameter discrepancy. Guidelines are also proposed for designing wavelet filter which leads to better predistortion with the truncated Volterra inverse.
Usually, objects to be classified are represented by features. In this paper, we discuss an  alternative object representation based on dissimilarity values. If such distances separate  the classes well, the nearest neighbor method offers a good solution. However, dissimilarities  used in practice are usually far from ideal and the performance of the nearest neighbor rule  suffers from its sensitivity to noisy examples. We show that other, more global classification  techniques are preferable to the nearest neighbor rule, in such cases. For classification
The variable length code (VLC) tables in the MPEG-1/2/4 and H.263 are fixed and optimized for a limited range of bit-rates, and they cannot handle a variety of applications. The universal variable length code (UVLC) is a new scheme to encode syntax elements and has some configurable capabilities. It is also being considered in the ITU-T H.26L. However, the configurable feature of the UVLC has not been well explored. In this paper we propose configuring the UVLC with the additional code configuration (ACC). The ACC is used to adapt UVLC to different symbol distributions by adjusting the partitioning of the symbols into different categories, and the code size assignment to different categories. Experimental results show that the UVLC with ACC outperforms the current proposed scheme in H.26L and the VLC tables of existing standards, while drastically simplifying the encoding and decoding process, and is applicable to a variety of applications.
Most programming languages adopt static binding, but for distributed programming an exclusive reliance on static binding is too restrictive: dynamic binding is required in various guises, for example when a marshalled value is received from the network, containing identifiers that must be rebound to local resources. Typically it is provided only by ad-hoc mechanisms that lack clean semantics.
A number of results have bounded generalization of a classifier in terms of its margin on the training points. There has been some debate about whether the minimum margin is the best measure of the distribution of training set margin values with which to estimate the generalization. Freund and Schapire [8] have shown how a different function of the margin distribution can be used to bound the number of mistakes of an on-line learning algorithm for a perceptron, as well as an expected error bound. We show that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size. Algorithms arising from the approach are related to those of Cortes and Vapnik [5]. We generalise the basic result to function classes with bounded fat-shattering dimension and the 1-norm of the slack variables which gives rise to Vapnik&apos;s box constraint algorithm. We also extend the results to the reg...
The study of self-replicating structures in Computer Science has been taking place for more than half a century, motivated by the desire to understand the fundamental principles and algorithms involved in self-replication. The bulk of the literature explores self-replicating forms in Cellular Automata. Though trivially self-replicating programs have been written for dozens of languages, very little work exists that explores self-replicating forms in programming languages. This paper reports initial investigations into selfreplicating expressions in the Lambda Calculus, the basis for functional programming languages. Mimicking results from the work on Cellular Automata, selfreplicating Lambda Calculus expressions that also allow the application of an arbitrary program to arbitrary data are presented. Standard normal order reduction, however, will not reduce the sub-expression representing the program application. Two approaches of dealing with this, hybrid reduction and parallel reduction, are discussed, and have been implemented in an interpreter.
This paper gives an overview of methods used for Design Space Exploration (DSE) at the  system- and micro-architecture levels. The DSE problem is considered to be two orthogonal  issues: (I) How could a single design point be evaluated, (II) how could the design space be  covered during the exploration process? The latter question arises since an exhaustive exploration  of the design space by evaluating every possible design point is usually prohibitive due to  the sheer size of the design space. We therefore reveal trade-offs linked to the choice of appropriate  evaluation and coverage methods. The designer has to balance the following issues: the  accuracy of the evaluation, the time it takes to evaluate one design point (including the implementation  of the evaluation model), the precision/granularity of the design space coverage,  and last but not least the possibilities for automating the exploration process. We also list common  representations of the design space and compare current system and micro-architecture  level design frameworks. This review thus eases the choice of a decent exploration policy by  providing a comprehensive survey and classification of recent related work. It is focused on  System-on-a-Chip designs, particularly those used for network processors. These systems are  heterogeneous in nature using multiple computation, communication, memory, and peripheral  resources.
Dot maps -- drawings of point sets -- are a well known cartographic method to visualize density functions over an area. We study the problem of simplifying a given dot map: given a set P of points in the plane, we want to compute a smaller set Q of points whose distribution approximates the distribution of the original set P. We formalize this using the concept of...
of environment be modeled accurately and that appropriate texturing be laid down on top of that geometry. We describe an approach for automatic coloring of panchromatic aerial orthoimagery. The method is able to remove shading and shadowing effects in the original image so that shading and shadowing appropriate to variable times of day and year can be added. The method we present is based on pattern recognition principles. It classifies regions in the original panchromatic orthoimagery into classes that are subsequently colored with user selected color palette. The method requires very little user interaction and is robust. The user only needs to select a few training point for each class that are later used in the pattern recognition and classification step. We also present an alternative method that is even simpler and requires no user intervention.
In this paper , we descr ibe the deployment of lar ge mobile ad-hoc networ ks using standar d components. The design, implementation and oper ation of distr ibuted, self-or ganized,lar ge scale, mobile communication and infor - mation systems poses many inter estingr esear ch pr oblems. While a lot of questions devoted to algorO hmic and ar chitectur al aspectsar e alr eady being pur sued, few have actually deployed such systems to the extents envisioned. Wepr esent Bluetooth Smar Nodes, each of which can stor e inforfi8 ion, compute and communicate using standar  d wir eless interfiJ es on a limitedr esour ce platfor8 These wir eless enabled small devices can inter act in a heter ogeneous envir onment consisting of differ ent types of networfiR g nodes as well as with other wir eless enabled appliances. Impor tantr equir ements and design tr adeoffs to be able to suppor t multiple communication inter faces, handle limitedr esour ces, power awar e oper ation and effcient testbed deploymentar e discussed. The BTnodesar e integr ated into our MANET application and networ kingfr amewor k. Demo applications give an insight into usage scenar ios envisioned for futur ear chitectur al explor ations. 1 
This paper first extends the result of Blakley and Kabatianski  [3] to general non-perfect SSS using information-theoretic arguments.
Any significant real-world application of mobile augmented reality will require a large model of location-bound data. While it may appear that a natural approach is to develop application-specific data formats and management strategies, we have found that such an approach actually prevents reuse of the data and ultimately produces additional complexity in developing the application. In contrast we describe a three-tier architecture to manage a common data model for a set of applications. It is inspired by current Internet application frameworks and consists of a central storage layer using a common data model, a transformation layer responsible for filtering and adapting the data to the requirements of a particular applications on request, and finally of the applications itself. We demonstrate our architecture in a scenario consisting of two multi-user capable mobile AR applications for collaborative navigation and annotation in a city environment.
This study examined the relationship between social affiliation and school violence among male public high school students in Kuwait. Specifically, this study investigated the violent behavior characteristics of tribal and non-tribal male public high school students in Kuwait and the relationship between family structure, family type, and student age of those students and school violence. A one-way ANOVA was conducted to test the first null hypothesis: there are no significant differences in mean subscale scores between the four characteristics of violent behavior and the social affiliation of male public high school students in Kuwait. Multiple linear regression was used to develop a predictive linear model for the relationship between violence and household size, family structure, and student age among tribal and non-tribal male public high school students in Kuwait. Six hundred male public high school students were given the Aggression Questionnaire which consisted of four subscales: physical aggression, verbal aggression, anger, and hostility. Tribal participants reported more violent behavior characteristics than non-tribal participants. These results supported previous research regarding a relationship between culture and school violence. The results supported those studies that had previously found a significant relationship between student age and the prediction of school violence as indicated by the Aggression Questionnaire (Al Dokhy&apos;s;2003). However, the findings indicated that family structure and family size were not significant predictors of violent behavior for the study sample. iii A linear regression model for predicting scores for violent behaviors of male public high school students in Kuwait was proposed. iv DEDICATION To the soul of my father in his ...
Hierarchical dynamic simplification (HDS) is a new approach to the problem of simplifying arbitrary polygonal environments. HDS operates dynamically, retessellating the scene  continuously as the user&apos;s viewing position shifts, and  adaptively, processing the entire database without first decomposing the environment into individual objects. The resulting system allows real-time display of very complex  polygonal CAD models consisting of thousands of parts and hundreds of thousands of polygons. HDS supports various preprocessing algorithms and various run-time criteria, providing a general framework for dynamic view-dependent simplification.
The goal of this paper is to derive a measure of utility for questions and answers from  a game theoretic model of communication. We apply this measure to account for a number  of judgements about the appropriateness of partial and mention--some answers, e.g. that a  partial answers to a question can be as appropriate as a strongly exhaustive answer. Under  the assumption that interlocutors are Bayesian utility optimisers we see questioning and  answering as a two-person sequential decision problem with complete coordination of preferences.
This module zooms in from the company scope to the architecting scope. The business is described by a simplified decomposition in four processes. The Product Creation Process is described and decomposed further in three processes. The architecting process is described in relation to the four business processes. Distribution  This article or presentation is written as part of the Gaud project. The Gaud project philosophy is to improve by obtaining frequent feedback. Frequent feedback is pursued by an open creation process. This document is published as intermediate or nearly mature version to get feedback. Further distribution is allowed as long as the document remains complete and unchanged. All Gaud documents are available at:  http://www.extra.research.philips.com/natlab/sysarch/ version: 1 status: draft 24th March 2004 Contents 1 Process Decomposition of a Business 1  1.2 Process Decomposition . . . . . . . . . . . . . . . . . . . . . . . 1 1.3 Process versus Organization . . . . . . . . . . . . . . . . . . . . 4 1.4 Value Chain and Feedback . . . . . . . . . . . . . . . . . . . . . 4 1.5 Decomposition of the Customer Oriented Process . . . . . . . . . 5 1.6 Extended Process Decomposition; Generic Developments . . . . . 5  2 What is a Process? 7  2.2 What is a process . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 The relation between Processes and Organizations . . . . . . . . . 8 2.4 Process Improvement . . . . . . . . . . . . . . . . . . . . . . . . 10  3 The Product Creation Process 12  3.2 The Context of the Product Creation Process . . . . . . . . . . . . 12 3.3 Phases of the Product Creation Process . . . . . . . . . . . . . . . 13 3.4 Milestones and Decisions . . . . . . . . . . . . . . . . . . . . . . 16 3.5 Organization of the Product Creation Pr...
Prefetching has been widely used to improve system performance in mobile environments. Since prefetching also consumes system resources such as bandwidth and power, it is important to consider the system overhead when designing a prefetching scheme. This paper proposes a cache-miss-initiated prefetch (CMIP) scheme to address this issue. The CMIP scheme relies on two prefetch sets: the always-prefetch set and the miss-prefetch set. The always-prefetch set consists of data that should always be prefetched if possible. The miss-prefetch set consists of data that are closely related to the cache-missed data item. When a cache miss happens, instead of sending an uplink request to ask for the cache-missed data item only, the client also requests for the data items which are within the miss-prefetch set. This reduces not only future cache misses but also the number of uplink requests. Note that the client can ask for several data items in one uplink request with little additional cost. We propose novel algorithms to mine the association rules and use them to construct the two prefetch sets. Simulation results show that our CMIP scheme can greatly improve the system performance in terms of improved cache hit ratio, reduced uplink requests, and negligible additional traffic.
this paper we  continue that work by considering analytic solutions for evolution on the  half line x  0. We use an exact evolutionary operator to derive finite difference  approximation schemes which maintain accuracy near a boundary
A foveated image is a non-uniform resolution image whose resolution is highest at a point (fovea) but falls off away from the fovea. It can be obtained from a uniform image through a space-variant smoothing process, where the width of the smoothing function is small near the fovea and gradually expanding as the distance from the fovea increases. We treat this process as an integral operator and analyze its kernel. This kernel is dominated by its diagonal in the wavelet bases and thus permits a fast algorithm for foveating images. In addition, the transformed kernel takes a simple form which can be easily computed using a look-up table. This is useful since in applications, the fovea changes rapidly. We describe an application of our approximation algorithm in image visualization over the Internet. 
Nowadays, biologists use a number of large biological databanks to find relevant information for their research. Users of these databanks face a number of problems. One problem is that users are required to have good knowledge about the contents, implementations and conceptual models of many databanks to be able to ask precise and relevant questions. Further, the terminology that is used in the different databanks may be different. Also, when asking complex queries to multiple databanks, users need to construct a query plan on their own possibly leading to poor performance or not even obtaining results. To alleviate these problems we define an architecture for systems that deal with these problems by allowing for a transparent and integrated way to query the multiple sources. The contribution of this paper is threefold. First, we describe a study of current biological databanks. Then, we propose a base query language that contains operators that should be present in any query language for biological databanks. Further, we present an architecture for a system supporting such a language and providing integrated access to the highly distributed and heterogeneous environment of biological databanks.
This paper explores the application of information theory notions to  description logics. The goal of the paper is to present a formal framework  in which the problem of theory learning or TBox acquisition from a set  of models could be defined and boarded. TBox
An important building block for intelligent mobile robots is the ability to track people moving around in the environment. Algorithms for person-tracking often incorporate motion models, which can improve tracking accuracy by predicting how people will move. More accurate motion models produce better tracking because they allow us to average together multiple predictions of the person&apos;s location rather than depending entirely on the most recent observation. Many implemented systems, however, use simple conservative motion models such as Brownian motion (in which the person&apos;s direction of motion is independent on each time step). We present an improved motion model based on the intuition that people tend to follow efficient trajectories through their environments rather than random paths. Our motion model learns common destinations within the environment by clustering training examples of actual trajectories, then uses a path planner to predict how a person would move along routes from his or her present location to these destinations. We have integrated this motion model into a particle-filter-based person-tracker, and we demonstrate experimentally that our new motion model performs significantly better than simpler models, especially in situations in which there are extended periods of occlusion during tracking.
In this paper we describe a system that allows a power wheelchair user to drive through a virtual architectural environment. The system allows architects and designers to visualize an environment, test the environment for handicapped accessibility, and determine whether the environment meets design standards such as the Americans with Disabilities Act of 1990. First we describe the system itself, including its hardware and software components and its user interface. Then we discuss architectural and design considerations. KEYWORDS  Virtual environment, adaptive technology, octree, rendering, stereopsis, accessibility.
Conversation agents present a challenging agenda for  research and application. We describe the development,  evaluation, and application of Baldi, a computer animated  talking head. Baldi&apos;s existence is justified by the important  contribution of the face in spoken dialog. His actions are  evaluated and modified to mimic natural actions as much as  possible. Baldi has the potential to enrich human-machine  interactions and serve as a tutor in a wide variety of  educational domains. We describe one current application of  language tutoring with children with hearing loss.
A classical theorem by Block and Levin says that certain variants of the relaxation method for solving systems of linear inequalities produce bounded sequences of intermediate solutions even when running on inconsistent input data. Using a new approach, we prove a more general version of this result and answer an old open problem of quantifying the bounds as a function of the input data.
Modern radio terminals for high data rate communication require wideband receivers. The well-known receiver architectures, such as zero-IF-receivers usually require very accurate component matching. In order to relax these requirements digital techniques are used on one side. On the other side alternative receiver architectures such as the six-port are investigated. The six-port has been introduced as a very flexible and elegant means for microwave measurements in the 1960s and 1970s. Later on it has been used in radar applications. It was not until recently that communication receivers have been built upon the six-port principle. In order to generate the required IQ-signal from the output signals of a six-port, knowledge of the six-port structure and/or of the signal statistics must be available. The first can be obtained by means of calibration procedures, while the latter can be used in blind techniques. Classical calibration procedures employ probes that are connected to the six-port. This is not applicable in communication receivers. Alternatively, a method is presented that uses the incoming signal as the calibration-driving probe and moreover uses the fact that the I-component and the Q-component of the signal are uncorrelated in most practical scenarios.
The View Generator    (VG) is a system that provides the necessary components for the definition, materialization, storage, maintenance and re-use of views over remote web-accessible databases. Through the system, clients identify web databases of interest, access their metadata and create personalized views that can be shared by other clients. Creating personalized views provides a more efficient way of data processing than directly issuing complex queries to the data sources especially in the case of light-weight and wireless clients that suffer from scarce local resources, limited bandwidth and high communication costs. VG&apos;s multi-tier architecture implemented using mobile Java agents allows automatic code deployment as well as dynamic relocation of views.
We introduce the notion of certificate-based encryption. In  this model, a certificate -- or, more generally, a signature -- acts not only  as a certificate but also as a decryption key. To decrypt a message, a  keyholder needs both its secret key and an up-to-date certificate from  its CA (or a signature from an authorizer). Certificate-based encryption  combines the best aspects of identity-based encryption (implicit certification)  and public key encryption (no escrow). We demonstrate how  certificate-based encryption can be used to construct an effcient PKI  requiring less infrastructure than previous proposals, including Micali&apos;s  Novomodo, Naor-Nissim and Aiello-Lodha-Ostrovsky.
Introduction  In his laws of perceptual organization, Wertheimer included the tendency for similar stimulus elements  to group together [22]. This law of similarity grouping  has intrigued many vision scientists because the idea seems so simple, and yet a precise formulation has proven elusive.  The basic phenomenon is shown in Figure 1a. Each side of this stimulus is textured with a pattern of repeating lines. The lines differ in orientation, and observers report seeing them as forming two distinct groups separated by a perceptual boundary. In other words, the textures appear to segment. Early experiments [1, 15] showed that differences in orientation are especially potent for producing this effect. But textures that differ in other ways may also segment; for example, patterns of &quot;+&quot;s and &quot;L&quot;s segment even though they have the same component orientations.  Early attempts to explain texture segmentation were couched in terms of abstract features [10, 3, 20]. Thus, the segmentation of &quot;
In this paper we describe GPlan, a new approach to the application of Genetic Programming (GP) to planning. This approach starts with a traditional AI planner (prodigy) and uses GP to acquire control rules to improve its eciency. We also analyze two ways to introduce domain knowledge acquired by another method (hamlet) into GPlan: seeding the initial population and using a new operator (knowledge-based Crossover). This operator combines genetic material from both an evolving population and a non-evolving population containing background knowledge. We tested these ideas in the blocksworld domain and obtained excellent results.
In this work we introduce a new method for computing Form Factors in Radiosity. We demostrate how our method improves on existing projective techniques such as the hemicube. We use the Nusselt analog to directly compute form factors by projecting the scene onto the unit circle. We compare our method with other form factor computation methods. The results show an improvement in the quality/speed ratio using our technique.
Internet application performance over wireless links is disappointing due to wireless impairments that adversely affect higher layers. This paper focuses on link layer enhancement mechanisms that hide wireless errors from the rest of the Internet. We simulated file transfer and WWW browsing over TCP and continuous media distribution over UDP, in conjunction with various link layer schemes. Our results reveal that WWW browsing has substantially different behavior than file transfer, that existing TCP enhancement schemes have limited applicability and that UDP applications are best served by schemes inappropriate for TCP. Therefore, multiple link layer solutions are needed to optimize the performance of diverse applications.
Recurrent neural network processing of regular language is reasonably well understood. Recent work has examined the less familiar question of context-free languages. Previous results regarding the language a      suggest that while it is possible for a small recurrent network to process context-free languages, learning them is diffcult. This paper considers the reasons underlying this diffculty by considering the relationship between the dynamics of the network and weightspace. We are able to show that the dynamics required for the solution lie in a region of weightspace close to a bifurcation point where small changes in weights may result in radically different network behaviour. Furthermore, we show that the error gradient information in this region is highly irregular. We conclude that any gradient-based learning method will experience diffculty in learning the language due to the nature of the space, and that a more promising approach to improving learning performance may be to make weight changes in a nonindependent manner.
In this paper, we study the problem of solving integer range constraints  that arise in many static program analysis problems. In particular, we  present the first polynomial time algorithm for a general class of integer range  constraints. In contrast with abstract interpretation techniques based on widenings  and narrowings, our algorithm computes, in polynomial time, the optimal  solution of the arising fixpoint equations. Our result implies that &quot;precise&quot; range  analysis can be performed in polynomial time without widening and narrowing  operations.
representation of the query space. The image set I forms a true subset of the full set of images in the database. The feature space F is composed of two color features. Similarity S corresponds to Euclidean distance. Finally, with the images, two labels in Z are assigned with their associated probabilities P .
The current situation in PCP is marked by development of complex products taking into account quality, costs, time and innovation aims in a concurrent environment. Unidentified mistakes in early phases cause high expenses and time delays in late phases.
This document present the C source-code for the implementation of the logging strategy the Extended Constant Eviction Scheduler (ECETES) [1]. In addition, also the source-code to the simulator and the FIFO logging strategy that was used to evaluate the performance of the ECETES implementation is provided. 1 C-source for the ECETES-implementation /* FILE ecetes.h Written 2003-03-31 by Joel Huselius joel.huselius@mdh.se M\&quot;{a}lardalen University at the Department of Computer Science and Engineering The Extended Constant Execution Time Eviction Scheduler (ECETES) */ //ffdefine ECETES_DEBUG_CHK //ffdefine ECETES_DEBUG /* UN-COMMENT LINE FOR AUX-OUTPUTS */ ffifdef ECETES_DEBUG ffinclude &lt;stdio.h&gt; ffdefine ECETES_DPRINT printf ffdefine ECETES_DPRINTN // ffelse ffinclude &lt;stdio.h&gt; ffdefine ECETES_DPRINTN printf ffdefine ECETES_DPRINT // ffendif //ffdefine ECETES_DEBUG /* UN-COMMENT LINE FOR AUX-OUTPUTS */ ffifdef ECETES_DEBUG_DEBUG ffinclude &lt;stdio.h&gt; ffdefine ECETES_DDPRINT printf ffdefine ECETES_DDPRINTN /...
. This paper presents an environment supporting collaborative object-oriented development in a distributed, large scale, situation. The environment provides a fine-grained version control mechanism which enables explicit representation of development histories. The  wait-freedom principle underlying our environment ensures that a developer can always work locally without waiting for any remote communication and despite network and node failures. 1 Introduction  Although object-orientation encourages the design and implementation of applications by composing various components developed by different users, very few environments support collaborative object-oriented development. The availability of world-wide networks adds a dimension of geographical distribution to the picture and makes the problem even more urgent. Existing techniques for version control building on locking of checked out  files do not scale up to many people and geographic distribution. The problems created with locki...
The Batinah Region of Oman consists of extensive alluvial fans incised by ephemeral streams (wadis) and accumulation of fine textured coastal soils. Soils have been cultivated near the coast for time immemorial using, formerly, traditional and, latterly, mechanized water lifting systems. Rapid agricultural expansion in since 1970 has led to widespread cultivation of alluvial fan soils. This trend is projected to intensify as irrigated soils near the coast become increasingly salinized. Land use planners need to decide whether to encourage farming activities on either alluvial fans or in wadi areas. Two transects were selected across examples of both these areas, commencing beyond the area of cultivation inland and terminating at the coast. Soils along the transects were described and classified according to Soil Taxonomy. The objectives were to 1) assess the presence of a wadi on soil properties, 2) determine pattern of soil organic carbon and sand particle size distributions with depth, and 3) identify characteristics of soils in the interaction zone where fan and plain soils meet. Soils of the alluvial fans were Gypsids and Orthents, whereas in the wadi and the coastal plain Fluvents and Salids dominated. Distinguishing between Orthents and Fluvents on the basis of regular or irregular decreases in organic carbon with depth was not satisfactory, since organic carbon contents were very low and there was no consistent patterns of accumulation between soil types. Soils showed a high proportion of fine and very fine particles in the sand fraction, especially when capped by a desert pavement, suggesting that sand deposition was more related to dust entrapment than the action of flowing water. The boundary between fan and coastal plain deposits was more distinct along the f...
Research on mobile augmented reality (AR) so far concentrated on building and deploying prototypes and investigating technical and user interface aspects. To create plausible application scenarios other concerns also need to be addressed. In particular, the size of the physical world made accessible by mobile AR raises new challenges in terms of scalability: the system will among others need a very large 3D world model, display this model efficiently without visual clutter or graphics overload, should operate in networked mode with arbitrary collaborators, provide tracking anywhere etc. In this paper we address some of these scalability issues, namely data management for very large geographic 3D models, and efficient techniques for collaborative outdoor user interfaces. As an application scenario, we use a tourist group guide application operating in the city of Vienna.
We suggest a new approach to the problem of dimensional reduction of initial/boundary  value problems for evolution equations in one spatial variable. The approach is  based on higher-order (generalized) conditional symmetries of the equations involved.
radius. C will indeed be dubious if data are recorded so fre-  quently that the animal is still standing and chewing at both t and t + At. For C to be useful, At must be sufficiently large so that successivecirclesdonot overlap when the animal is foraging, though clearly  no such condition is relevant if the predator uses a perch. The objection of Gautestad and Mysterud that `C is a dependent variable even when the loca-  tions are non-autocorrelated. The reason is that, for large a new (non-autocorrelated)location has a tendency to increase the local plot density within A at a given box resolution, rather than contribut-  ing to a larger A. Thus, more overlap is generated with increasing making C n-dependent.&apos;  In reply, I note first that the degree of overlap with previous circles is precisely what C is meant to measure: how much the same area is used (searched) repeatedly. It is true that C will decrease as increases, but C should stabilize at some mini-  mum n that is sufficiently
An extension of univariate quantiles in the multivariate set-up has been proposed and studied. The proposed approach is ane equivariant, and it is based on an adaptive transformation retransformation procedure. Bahadur type linear representations of the proposed quantiles are established and consequently asymptotic distributions are also derived. As applications of these multivariate quantiles, we develop some ane equivariant quantile contour plots which can be used to study the geometry of the data cloud as well as the underlying probability distribution and to detect outliers. These quantiles can also be used to construct ane invariant versions of multivariate Q-Q plots which are useful in checking how well a given multivariate probability distribution ts the data and for comparing the distributions of two data sets. We illustrate these applications with some simulated and real data sets. We also indicate a way of extending the notion of univariate L-estimates and trimmed means in the multivariate set-up using these ane equivariant quantiles. 
We investigate an integration of the first-order method of proof  by consistency (PBC), also known as term rewriting induction, into theorem  proving in higher-order specifications. PBC may be seen as well-founded induction  over an ordering which contains the rewrite relation, and in this paper  we extend this method to the higher-order rewrite relation due to Nipkow. This yields
As High Performance Fortran (HPF) is being both developed and redesigned  by the HPF Forum, it is important to provide comprehensive criteria for analyzing  HPF features. This paper presents such criteria related to three aspects: adequacy  to applications, aesthetic and soundness in a language, and implementability. Some  features already in HPF or being currently discussed are analyzed according to  these criteria. They are shown as not balanced. Thus new or improved features  are suggested to solve the outlined deficiencies: namely a scope provider, multiple  mapping declarations and simpler remappings.
We extend the automata-theoretic framework for reasoning about infinitestate  sequential systems to handle also the global model-checking problem. Our framework  is based on the observation that states of such systems, which carry a finite but  unbounded amount of information, can be viewed as nodes in an infinite tree, and  transitions between states can be simulated by finite-state automata. Checking that  the system satisfies a temporal property can then be done by a two-way automaton  that navigates through the tree. The framework is known for local model checking.
In Augmented Reality, see-through HMDs superimpose virtual 3D objects on the real world. This technology has the potential to enhance a user&apos;s perception and interaction with the real world. However, manyAugmented Reality applications will not be accepted until we can accurately register virtual objects with their real counterparts. In previous systems, such registration was achieved only from a limited range of viewpoints, when the user kept his head still. This paper offers improved registration in two areas. First, our system demonstrates accurate static registration across a wide variety of viewing angles and positions. An optoelectronic tracker provides the required range and accuracy. Three calibration steps determine the viewing parameters. Second, dynamic errors that occur when the user moves his head are reduced by predicting future head locations. Inertial sensors mounted on the HMD aid head-motion prediction. Accurate determination of prediction distances requires low-overhead operating systems and eliminating unpredictable sources of latency. On average, prediction with inertial sensors produces errors 2-3 times lower than prediction without inertial sensors and 5-10 times lower than using no prediction at all. Future steps that may further improve registration are outlined.
The paper considers the statistical analysis of fire-interval charts 8 based on fire-scar data. Estimation of the fire interval (expected time 9 between scar-registering fires at any location) by maximum likelihood 10 is presented. Because of the fact that fires spread, causing a lack of 11 independence in scar registration at distinct sites, an over-dispersed 12 binomial model is used leading to a two-variable quasi-likelihood func- 13 tion. From this, point estimates, standard errors and approximate 14 confidence intervals for fire interval and related quantities can be de- 15 rived. Methods of testing for the significance of spatial and temporal 16 differences are also discussed. A simple example using artificial data 17 is given to illustrate the computational steps involved, and an analysis 18 of real fire-scar data is presented.
Motivated by existing models used for soft body simulation which are rather complex to implement, we present a novel technique which is based on simple laws of physics and gives high quality results in real-time. We base the implementation on simple thermodynamics laws and use the Clausius-Clapeyron state equation for pressure calculation. In addition, this provides us with a pressure force that is accumulated into a force accumulator of a 3D mesh object by using an existing spring-mass engine. Finally after integration of Newtons second law we obtain the behavior of a soft body with fixed or non-fixed air pressure inside of it.
Spring embedding is a widely used method for producing automated layouts of graphs. We present a preprocessor that improves the performance of the classical spring embedder, which can be used in conjunction with other optimization and approximation techniques. It creates an initial graph layout with edge lengths that are approximately equal and with a minimum node separation from which the spring embedder typically needs far fewer iterations to produce a well laid out graph.
The ISO/IEC MPEG-4 multimedia content creation, management and distribution framework is foreseen to be an important component of many forthcoming Wireless IP Multimedia services. However, current approaches for transporting MPEG-4 Elementary streams over IP networks are not optimized for wireless group communications. To address this issue we propose a novel Real-Time Transport Protocol (RTP) payload, called RTP4mux, that provides better data multiplexing and flow aggregation over shared wireless IP links. RTP4mux offers the following transport features (1) an MPEG-4 elementary streams interleaving mechanism that minimize temporal coding dependencies between adjacent RTP packets and then improving burst packet loss tolerance (2) a configurable two levels Access Units multiplexing scheme, that optimizes wireless bandwidth utilization and reduces end-to-end transmission delays through a lower data control overhead and a shorter packetization latency.
In this paper we introduce the recent normal inverse Gaussian (NIG) probability density as a new model for sparsely coded data. The NIG density is a flexible, four-parameter density, which is highly suitable for modeling unimodal  super-Gaussian data.
this paper we describe the design, implementation and  performance of the cluster, as well as some scientific application results obtained  with the machine
g a novel security approach that addresses these challenges. If successful, this approach will significantly benefit not only future systems but also various emerging mobile computing applications. It could also benefit collaborations over the Internet where correspondents&apos; identities and intentions are difficult to establish with certainty.  Our approach applies the human notion of trust. This naturally leads to a decentralized security management approach that can tolerate partial information, albeit one that has inherent risks for the trusting entity. Fundamentally, the ability to reason about trust and risk is what lets entities accept risk when interacting with other entities. The SECURE project seeks a formal basis for reasoning about trust and risk and for deploying verifiable security policies, embodied in a computational framework that is adaptable to various application scenarios.  For example, consider the problem of routing messages in an ad hoc wireless network. An entity,
governments has continued to diminish due to various administrative, political, socio-cultural, economic, and mass media causes. Focusing on the administrative dimension, this study explores selected administrative factors to the declining of public trust, including public perception of administrative corruption (lack of transparency), inefficiency (wastefulness), ineffectiveness, and policy alienation. We argue that information technology (IT) can offer potentially useful tools to governments and help them to restore public trust by enhancing transparency, cost efficiency, effectiveness, and policy participation. This argument is illustrated by four selected mini cases (OPEN system in Seoul, eVA in Virginia, eFiling for IRS tax returns, and online policy forums in Seoul and Pennsylvania). Despite a generalizability problem, this study offers a cautious but positive view on the potential contribution of IT in restoring pubic trust.
The situation calculus, as proposed by McCarthy and Hayes,  and developed over the last decade by Reiter and co-workers,  is reconsidered. A new logical variant is proposed that captures  much of the expressive power of the original, but where  certain technical results are much more easily proven. This is  illustrated using two existing non-trivial results: the regression  theorem and the determinacy of knowledge theorem of  Reiter. We also obtain a regression theorem for knowledge,  and show how to reduce reasoning about knowledge and action  to non-epistemic non-dynamic reasoning about the initial  situation.
In this paper, we introduce our Message Passing Interface (MPI) based Object-Oriented parallel discrete event simulation framework. The framework extends the capabilities of the OMNeT ++ simulation system. In conjunction with this project, our research efforts also include the development of synchronization methods suitable for architectural properties of the distributed-memory and shared-memory parallel computer systems. We intend to harness the computational capacity of these parallel systems for simulation, and to use this framework for modeling very large scale telecommunication networks to investigate protocol performance and rare event failure scenarios.
This paper presents a novel theoretical approach to calculating the apparent contour of a smooth surface. The problem is formulated as a dual space intersection of algebraic tangent cones, which we will consider to be the members of degree d hypersurfaces. The well known thereotical foundation for multi-view geometry is extended in light of this to solve the problems of triangulation and forming multi-view matching constraints for degree d apparent contours. 1 
We report on a series of human evaluations of the task of sentence fusion. In this task, a human is given two sentences and asked to produce a single coherent sentence that contains only the important information from the original two. Thus, this is a highly constrained summarization task. Our investigations show that even at this restricted level, there is no measurable agreement between humans regarding what information should be considered important. We further investigate the ability of separate evaluators to assess summaries, and find similarly disturbing lack of agreement.
Currently, clusters of Personal Computers ffPCsff with sharing Fast Ethernet may offer a signiffcant priceffperformance ratio in parallel processing. This paper presents a study of the promising possibilities of parallelism in this environment, using both a classical numerical method to solve a differential equation and an innovative algorithm to solve the statistical problem of density estimation. In addition, and to compare, some comparisons with an IBM SP2 are carried out.
This paper describes the application of the optimal nonlinear/non-Gaussian filtering theory to the radar signal processing problem. This approach, made feasible by a new technique named Particle Filtering, may cope with nonlinear models as well as nonGaussian dynamic and observation noises. TheParticle Filter constructs the conditional probability of the state variables, with respect to the measurements, through a random exploration of the state space by particles, which obey the conditional probability generator. The application of this new filter to the inverse synthetic aperture radar (ISAR) technique allows the joint estimation of the path and the image of a maneuvering target in weak signal to noise ratio situations.
We show how a tableaux algorithm for    can be extended to  support role boxes that include range and domain axioms, prove that the extended  algorithm is still a decision procedure for the satisfiability and subsumption of    concepts w.r.t. such a role box, and show how support for range and  domian axioms can be exploited in order to add a new form of absorption optimisation  called role absorption. We illustrate the effectiveness of the optimised  algorithm by analysing the perfomance of our FaCT++ implementation when  classifying terminologies derived from realistic ontologies.
In recent studies, the orthogonality property of certain bases, such as wavelets, has been claimed to produce insuperable disadvantages to some signal processing tasks, e.g., feature extraction. To overcome these disadvantages research interest has turned towards redundant bases, frames, and shiftability concept has been proposed as the preferred property to be achieved by relaxing the orthogonality requirement. The presented results in this work continue the preliminary studies on shiftability. Previous definition is extended to frames of arbitrary regular translates and shiftability measures are proposed for such frames. The shiftability is also considered for finite dimensional discrete frames for which numerical shiftability values are computed.
Embedded systems of today pose difficult dependability challenges. Hardware and software requirements as well as human interface components all contribute to or detract from the overall dependability of a system. Assigning a `dependability number&apos; to a system is becoming increasingly subjective due to the confluence of these three areas. In particular it is important to go beyond composing individual component reliability predictions, and additionally consider factors such as ease of user workaround in the face of a partial system failure. In this paper we shall present an approach that attempts to detect these dependability bottlenecks within embedded systems and investigate its ability to represent users&apos; ability to interact with partially failed systems. We propose a graph-based approach that is partially based on composing and extending Unified Modeling Language (UML) standards. This `mission graph&apos; concept is meant to take advantage of the user&apos;s perspective to help system designers understand what is really going on in complex systems. We apply this approach to an example embedded system, and examine the experimental results to determine the feasibility of the proposed approach. Finally, the mission graph approach is used to further investigate the workaround concept and how it applies to users attempting to accomplish their goals even in the face of component failures.
The established theory of the resultant of two polynomials assumes that they are expressed in the power (monomial) basis, and a basis transformation is therefore necessary if the resultant of two Bernstein polynomials is required. In this paper, a resultant matrix for two scaled Bernstein polynomials (polynomials of degree n  whose basis functions are (1 x)     ; i = 0 : : : n) is constructed. In particular, a companion matrix M for a scaled Bernstein polynomial r(x) is developed, and this is used to form a resultant matrix s(M ), where s(x) is a scaled Bernstein polynomial. Key words: Bernstein basis, resultants  AMS classi cation : 11C08, 11C20 1 
A characterization of the impact of interconnects at the system level is necessary to decide early in the design flow on &quot;how&quot; and not just on &quot;whether&quot; all the devices can be connected. How the devices are connected depends on the interconnect architecture, which seems to be determined in a non transparent and sometimes ad hoc fashion by technologists. However, future DSM designs would benefit from a connection between high level design properties and interconnect architecture definition. This work will identify suitable and relevant models and methodologies which will help in systematically defining an optimal interconnect architecture for global wires under optimal repeater insertion.
The performance loss due to separation of detection and decoding on the binary-input additive white Gaussian noise channel is quantified in terms of mutual information. Results are reported for both the code-division multiple-access (CDMA) channel in the large system limit and the intersymbol interference (ISI) channel. The results for CDMA rely on the replica method developed in statistical mechanics. It is shown that a previous result in [1] found for Gaussian input alphabet holds also for binary input alphabets. For the ISI channel, the performance loss is calculated via the BCJR algorithm. Comparisons are made to the capacity of separate detection and decoding using suboptimum detectors such as a decision-feedback equalizer.
Although the ability to notice a change in the visual scene  has been used as an investigative tool to gain new insights on visual  system mechanisms, it has recently become an issue itself. In the last  decade, many studies have addressed the flip side of change detection,  namely the change blindness phenomenon. Change blindness refers  to functional blindness for an otherwise visible change in the scene  an observer is looking at when visual continuity is briefly interrupted  by extraneous visual events. Here we provide a review of the main  findings and notions emerged from the change blindness studies, both  from behavioral and electrophysiological experiments. Mechanisms of  change detection and change blindness are discussed, highlighting the  role of visual attention for conscious change perception.
The Narayana numbers are N(n; k) =    n      . There  are several natural statistics on Dyck paths with a distribution given  by N(n; k). We show the equidistribution of Narayana statistics by  computing the ag h-vector of J(2n) in dierent ways. In the process  we discover new Narayana statistics and provide co-statistics for which  the Narayana statistics in question have a distribution given by Frlinger  and Hofbauers q-Narayana numbers. We also interpret the h-vector in  terms of semi-standard Young tableaux, which enables us to express the  q-Narayana numbers in terms of Schur functions.
This paper presents a method for simulating decorative tile mosaics. Such mosaics are challenging because the square tiles that comprise them must be packed tightly and yet must follow orientations chosen by the artist. Based on an existing image and user-selected edge features, the method can both reproduce the image&apos;s colours and emphasize the selected edges by placing tiles that follow the edges. The method uses centroidal voronoi diagrams which normally arrange points in regular hexagonal grids. By measuring distances with an manhattan metric whose main axis is adjusted locally to follow the chosen direction field, the centroidal diagram can be adapted to place tiles in curving square grids instead. Computing the centroidal voronoi diagram is made possible by leveraging the z-buffer algorithm available in many graphics cards.
Autonomy is a very important property for a robot  to have, yet implementing it in a robot is far from  trivial, particularly when one requires the meaning of  autonomy to include self-motivation, instead of mere  automaticity.
This paper investigates the notion of dialgebra, which generalises the notions of algebra and coalgebra. We show that many (co)algebraic notions and results can be generalised to dialgebras, and investigate the essential differences between (co)algebras and arbitrary dialgebras.
Super-resolution algorithms reconstruct a high resolution image from a set of low resolution images  of a scene. If those low resolution images are undersampled and have aliasing artifacts, the performance of  the super-resolution algorithm decreases. We propose a frequency domain technique to precisely register  a set of aliased images, based on their low-frequency, aliasing-free part. A high resolution image is then  reconstructed using cubic interpolation. Our algorithm is compared to other algorithms in simulations and  practical experiments using real images. Both show very good visual results and prove the superiority  of our approach in case of aliased images. A possible application is to digital cameras where a set of  rapidly acquired images can be used to recover a higher resolution final image.
In the characterization of multivariate extreme indices of multivariate stationary processes,  multivariate maxima of moving maxima processes, or M4 processes for short, have been introduced  by Smith and Weissman. Central to the introduction of M4 processes is that the extreme  observations of multivariate stationary processes may be characterized in terms of a limiting  max-stable process under quite general conditions and a max-stable process can be arbitrarily  closely approximated by a M4 process. In this paper, we derive some additional basic probabilistic  properties for a finite class of M4 processes of which each contains finite range clustered  moving patterns, called signature patterns, when extreme events occur. We use these properties  to construct statistical estimation schemes for model parameters.
Branch prediction reversal has been proved to be an effective alternative  approach to dropping misprediction rates by means of adding a Confidence  Estimator to a correlating branch predictor. This paper presents a Branch  Prediction Reversal Unit ( BPRU) especially oriented to enhance correlating  branch predictors, such as the gshare and the Alpha 21264 metapredictor. The  novelty of this proposal lies on the inclusion of data values in the confidence  estimation process. Confidence metrics show that the BPRU can correctly tag  43% of branch mispredictions as low confident predictions, whereas the SBI (a  previously proposed estimator) just detects 26%. Using the BPRU to reverse the  gshare branch predictions leads to misprediction reductions of 15% for the  SPECint2000 (up to 27% for some applications). Furthermore, the  BPRU+gshare predictor reduces the misprediction rate of the SBI+gshare by  an average factor of 10%. Performance evaluation of the BPRU in a superscalar  processor obtains speedups of up to 9%. Similar results are obtained when the  BPRU is combined with the Alpha 21264 branch predictor.
This paper describes conditional-probability training of Markov random fields using combinations of labeled and unlabeled data. We capture the similarities between instances learning the appropriate distance metric from the data. The likelihood model and several training procedures are presented.
This paper presents an empirical evaluation of a pronoun resolution algorithm augmented with discourse segmentation information. Past work has shown that segmenting discourse can aid in pronoun resolution by making potentially erroneous candidates inaccessible to a pronoun&apos;s search. However, implementing this in practice has been difficult given the complexities associated with deciding on a useful scheme and then generating the segmentation reliably. In our study, we investigate whether or not shallow schemes that are  easy to generate will improve pronoun resolution in a task-oriented corpus of dialogues. Our results show that incorporating this  shallow segmentation at best marginally improves pronoun resolution performance.
The incorporation of multithreading in Java may be considered a  significant part of the Java language, because it provides rudimentary facilities  for concurrent programming. However, we belief that the use of channels is a  fundamental concept for concurrent programming. The channel approach as  described in this paper is a realization of a systematic design method for  concurrent programming in Java based on the CSP paradigm. CSP requires the  availability of a Channel class and the addition of composition constructs for  sequential, parallel and alternative processes. The Channel class and the  constructs have been implemented in Java in compliance with the definitions in  CSP. As a result, implementing communication between processes is facilitated,  enabling the programmer to avoid deadlock more easily, and freeing the  programmer from synchronization and scheduling constructs. The use of the  Channel class and the additional constructs is illustrated in a simple application.
The Kohonen Self-Organizing Feature Map algorithm is compared to the K-Means vector quantization algorithm. Computation and storage requirements are calculated for both algorithms. A new algorithm which takes advantage of the structured code book produced by Kohonen&apos;s algorithm is introduced. This algorithm offers a significant computational savings over full-search vector quantization without imposing a storage cost penalty. The results of simulation studies are presented and the performance of the algorithms is compared.  1 Introduction  This paper evaluates the use of neural network algorithms for vector quantization. Section 2 gives a brief overview of vector quantization, followed by descriptions of the K-Means algorithm in Section 3 and the Kohonen SelfOrganizing Feature Map in Section 4. Computation and storage costs of these two algorithms are compared in Section 5. Simulation results demonstrating the performance of the algorithms are presented in Section 6. Sec-    This repor...
this paper is to show how to construct discrete subgroups of the Lorentz group and then embed these discrete subgroups in a quasidiscrete Poincare subgroup, in order to construct interacting four-momentum operators. The context for this work is point form relativistic quantum mechanics [1], wherein all interactions are put in the four-momentum operators, and the Lorentz generators are free of interactions. The Lorentz generators are readily exponentiated to give global Lorentz transformations; this is important since discrete subgroups do not have an associated Lie algebra. The point form is to be contrasted with the more usual instant form of dynamics, where interactions are present in the Hamiltonian and boost generators, and the momentum and angular momentum generators are free of interactions (for a discussion of the various forms of dynamics, see for example [2]). One of the main reasons for introducing discrete subgroups is that the creation and annihilation operators that are used to build the interacting four-momentum operators then have discrete momenta in their arguments, and for bosonic creation and annihilation operators, can be realized as multiplication and differentiation operators acting on a holomorphic Hilbert space. In this representation matrix elements of the relativistic Schrodinger equation then tak e on the form of an infinite coupled set of first order partial differential equations. 2 Point form quantum mechanics  In order to have a relativistic theory it is necessary to satisfy the commutation relations of the Poincare algebra. In quantum field theory this is done by integrating the stress-energy tensor -- made up of polynomials of field operators -- over a time constant surface (see for 318 W.H.Klink example [3]). It is however also possible to...
The study of biogenious elements, particularly phosphorus, is connected with the fact that biological productivity of water bodies, and the water quality depends on their concentration. The union of phosphors coming into this system &quot;water-silt&quot; with the help anthropologic factor can influence on the processes of soil-formation.
In a recent Bayesian model by Weiss, Simoncelli and Adelson, motion perception is biased by a prior favoring slow speeds. This model predicts qualitativelyan impressive variety ofphenEfi*:E inenEfi* the depenkk*: of perceived speedon cond*EL9 We show that the modelcan alsogen*Okk quankkkk*:E predictionE for adriftin gratin withcon*k&amp;G c, perceived speed isproportionL to    /(k       ), with k, qconkOE*:E  We tested thisexpression on measuremenE of perceived speed as afunfiGE* ofcon&amp;Gz&amp;* Observers inserver the slower of twodriftin gratinO  testan  a standard. For each testcon*kLO wefoun the test speed that appeared to match thestan)fiz speed. The model fits the data, but on* if q is lessthan 2, the value it would have if thein*EELG represen:k9fi9 of conLEE&amp; werelin*9k TheBayesian modelcan make correctquant*fi&amp;GOk predictionk butnt*k to beextenLk toin&amp;9O&amp;*:kz a more realistic, nealisti representation of contrast.
Coordinated action for a team of robots is a challenging problem, especially  in dynamic, unpredictable environments. In the context of robot soccer,  a complex domain with teams of robots in an adversarial setting, there is a great  deal of uncertainty in the opponent&apos;s behavior and capabilities. We introduce the  concept of a play as a team plan, which combines both reactive principles, which  are the focus of traditional approaches for coordinating actions, and deliberative  principles. We introduce the concept of a playbook as a method for seamlessly  combining multiple team plans. The playbook provides a set of alternative team  behaviors which form the basis for our third contribution of play adaptation. We  describe how these concepts were concretely implemented in the CMDragons  robot soccer team. We also show empirical results indicating the importance of  adaptation in adversarial or other unpredictable environments.
This thesis examines programming language concepts that facilitate fault-tolerant distributed programming. New language primitives are introduced for whole-process migration, which allows an active process to be transferred from one machine to another, and speculative execution, which enables optimistic computing based on an unverified assumption. These primitives are developed in the context of the Mojave Compiler Collection, a multi-language multi-architecture compiler with ties to the MetaPRL theorem prover. The new primitives
We explore a variety of interaction and visualization techniques for fluid navigation, segmentation, linking, and annotation of digital videos. These techniques are developed within a concept prototype called LEAN that is designed for use with pressure-sensitive digitizer tablets. These techniques include a transient position+velocity widget that allows users not only to move around a point of interest on a video, but also to rewind or fast forward at a controlled variable speed. We also present a new variation of fish-eye views called twist-lens, and incorporate this into a position control slider designed for the effective navigation and viewing of large sequences of video frames. We also explore a new style of widgets that exploit the use of the pen&apos;s pressure-sensing capability, increasing the input vocabulary available to the user. Finally, we elaborate on how annotations referring to objects that are temporal in nature, such as video, may be thought of as links, and fluidly constructed, visualized and navigated.
Some recent Artificial Life models have attempted to explain the  origin of linguistic diversity with varying conclusions and explanations. We  posit, contrary to some existing Artificial Life work, that linguistic diversity  should naturally emerge in spatially organised populations of language learners,  and this is supported by our experimental work and by recent literature.
The effects of technology scaling on three run-time leakage reduction techniques (Input Vector Control, Body Bias Control and Power Supply Gating) are evaluated by determining their limits and benefits, in terms of the potential leakage reduction, performance penalty and area and power overhead in 0.25um, 0.18um, 0.07um and 0.065um technologies. HSPICE simulation results and estimations with various function units and memory structures are presented to support a comprehensive analysis.
Deductive Object-Oriented Databases are excellent modelling tools that provide good support for modelling through object-oriented features and also for reasoning capabilities about the data stored in the objectbase through logic. ConceptBase is an example of such a system that implements the ISO-IRDS. But it has a severe limitation in the form of absence of support for stored procedures or any similar powerful mechanism for server-side processing of data.
In many applications of pattern recognition, patterns appear in groups (fields) that have a common origin. For example, a printed word is a field of character patterns printed in the same font. A common origin induces consistency of style among features measured on patterns. In the presence of multiple styles, the features of co-occurring patterns are statistically dependent through the underlying style. Modeling such dependence among constituent patterns of a field increases classification accuracy. Effects of style consistency on the distributions of field-features (concatenation of pattern features) are modeled by hierarchical mixtures. Each field derives from a mixture of styles, while within a field a pattern derives from a class-style conditional mixture of Gaussians. An optimal (least error) style-conscious classifier processes entire fields of patterns rendered in a consistent but unknown style, based on the model. In a laboratory experiment style-conscious classification reduced errors on fields of printed digits by nearly 25% over singlet classifiers. Longer fields favor our classification method, because they furnish more information about the underlying style.
Nuclear) Magnetic Resonance ((N)MR) is a non-invasive technique that has been used to acquire spatially resolved images of living organisms and to monitor changes in the metabolism. An application of clinical MR is MR spectroscopy (MRS) in which chemical information can be obtained from a well-dened region in for example the human brain. The parameters of the MRS signal provide direct information about the molecules of the organism under investigation: the frequency of the spectral components characterizes the identity of the molecules, the damping characterizes the mobility of the molecules and the intensity (amplitude) is directly proportional to the number of molecules. Accurate quantication of MRS signals is the essential step prior to the conversion of the estimated signal parameters into biochemical quantities (e.g. concentration, pH). In this paper an overview of modern analysis methods is given.
In a natural variant of the comparison model, we show that there exists a constant ! &lt; 1  such that the fully-dynamic d-dimensional orthogonal range reporting problem for d  2 can be  solved in time O(log    n) for updates and time O((log n= log log n)    + r) for queries. Here  n is the number of points stored and r is the number of points reported. The space usage is    n). In the standard comparison model the result holds for d  3.
Using our classification of separable Schrodinger equations with two space dimensions  published in J.Math.Phys., 36, N 10, 1995 we give an exhaustive description  of the coordinate systems providing their separability. Furthermore, we apply  these results to separate variables in the heat, Hamilton-Jacobi and Fokker-Plank  equations.
Within the SAF on Climate Monitoring cloud parameters such as cloud cover, cloud type, cloud top height, cloud top temperature, cloud phase, cloud optical thickness and cloud water path will be determined over a long time. The cloud parameters will be derived for boxes of 15*15 km    covering time scales such as daily averages, monthly averages for AVHRRF and SEVIRI based products and monthly mean diurnal cycle only when using SEVIRI data. For cloud type and cloud phase frequency distributions for the already outlined time and space periods will be provided. The basic cloud parameters will be calculated applying part of the cloud detection algorithms, which were developed within the NWCSAF. This paper will show first results of the application of the Polar Package System (PPS) for calculating time and space averages of cloud parameters covering a larger area. The calculations were done for the summer month June 2003 and the autumn month October 2002. Examples will show the effect of different calculation methods of fractional cloud cover. Based on a monthly average the performance of PPS will be shown for early morning conditions with low sun elevation and for midday. In the end satellite based fractional cloud cover will be validated with Synop observations.
This paper describes a novel knowledge-based methodology and toolset for helping business  process designers and participants better manage exceptions (unexpected deviations  from an ideal sequence of events caused by design errors, resource failures, requirement  changes, etc.) that can occur during the enactment of a process. This approach is based on  an on-line repository exploiting a generic and reusable body of knowledge, which describes  what kinds of exceptions can occur in collaborative work processes, how these exceptions  can be detected, and how they can be resolved. This work builds upon previous efforts  from the MIT Process Handbook project and from research on conflict management in  collaborative design.
In this paper, we consider the execution of a complex application on  a heterogeneous &quot;grid&quot; computing platform. The complex application  consists of a suite of identical, independent problems to be solved. In  turn, each problem consists of a set of tasks. There are dependences  (precedence constraints) between these tasks. A typical example is the  repeated execution of the same algorithm on several distinct data samples. We use a
Protocol State Machines (PSM) in UML 2.0 [13] describe valid sequences of operation calls. To support modeling components, UML 2.0 introduces a Port associated with a set of provided and required interfaces. Unfortunately, a PSM is applicable only to a single interface, either a provided or required one; moreover, nested calls cannot be modeled with a PSM. Furthermore, the definition of protocol conformance is rather fuzzy and reasoning on this relation is not possible in general; thus reasoning on consistency in component composition is not possible with PSMs. Behavior Protocols [17] capture the behavior of a component via a set of traces. A textual notation similar to regular expressions is provided to approximate the behavior with a regular language. In [1,17], the compliance relation and consent operator are defined to reason on consistency of component composition; a verifier tool [18] is available for the compliance relation.
This paper shows how mathematical morphological operators can be applied to computer go. On one hand, mathematical morphology is a very powerful tool within image processing community. On the other hand, the Zobrist&apos;s model is well-known within the computer go community for its &quot;influence&quot; recognition. We present a model, derived from the closing operator of mathematical morphology and from the Zobrist&apos;s model, which yields very good results for &quot;territory&quot; recognition. Moreover, we give efficient implementations of the dilation operator and territory recognition for computer go. This model was found when developing Indigo, our go playing program, and is now used with success in GnuGo, the go playing program of the Free Software Foundation.
As the majority of content-based image retrieval systems operate on full images in pixel domain, decompression is a prerequisite for the retrieval of compressed images. To provide a possible on-line indexing and retrieval technique for those jpg image files, we propose a novel pseudo-pixel extraction algorithm to bridge the gap between the existing image indexing technology, developed in the pixel domain, and the fact that an increasing number of images stored on the Web are already compressed by JPEG at the source. Further, we describe our Web-based image retrieval system, WEBimager, by using the proposed algorithm to provide a prototype visual information system toward automatic management, indexing, and retrieval of compressed images available on the Internet. This provides users with efficient tools to search the Web for compressed images and establish a database or a collection of special images to their interests. Experiments using textureand colour-based indexing techniques support the idea that the proposed algorithm achieves significantly better results in terms of computing cost than their full decompression or partial decompression counterparts. This technology will help control the explosion of media-rich content by offering users a powerful automated image indexing and retrieval tool for compressed images on the Web.
The task of a fast correlation attack is to eciently restore  the initial content of a linear feedback shift register in a stream cipher  using a detected correlation with the output sequence. We show that  by modeling this problem as the problem of learning a binary linear  multivariate polynomial, algorithms for polynomial reconstruction with  queries can be modied through some general techniques used in fast  correlation attacks. The result is a new and ecient way of performing  fast correlation attacks.
While developing the Simple Gesturing User Interface (SGUI) API for incorporating simple gestures into personal digital assistant (PDA) user interfaces, we developed an object-based HCI model and design process. The model assists designing direct manipulation interfaces by proposing that all interface objects are amenable to user manipulation. The model extends object-action interface models by emphasizing the relationship between the interface objects and their actions, object-action associations. We delineate three association properties: directness, localness, and appropriateness. We illustrate the design process and the utility of SGUI by developing Experimental Assistant, a graphing software for high school science experiments on the PDA  Categories and Subject Descriptors  H.5.2 [User Interfaces]: Theory and methods  General Terms  Design, Human Factors, Theory.
In this paper, we present the method we have developed to analyze socio-technical  aspects of software problem management in F/OSS communities, based on large  corpora of problem reports; and we report on early results we obtained using our  framework. Given the amount of data available, computational techniques to  scalably extract event data and model the collectives&apos; behaviors are needed. We are  using a variety of techniques that couple human-based qualitative analysis with  computational extraction and modeling to generate models of the processes involved  in software problem management.
We present here a generalization of the work done by Rabin and Ben-Or in [16]. We give a protocol for multiparty computation which tolerates any   active adversary structure based on the existence of a broadcast channel, secure communication between each pair of participants, and a monotone span program with multiplication tolerating the structure. The secrecy achieved is unconditional although we allow an exponentially small probability of error. This is possible due to a protocol for computing the product of two values already shared by means of a homomorphic commitment scheme which appeared originally in [8].
Adding adaptation capabilities to existing distributed systems is a major concern. The question addressed here is how to retrofit existing systems with self-healing, adaptation and/or selfmanagement capabilities. The problem is obviously intensified for &quot;systems of systems&quot; composed of components, whether new or legacy, that may have been developed by different vendors,  mixing and matching COTS and &quot;open source&quot; components. This  system composition model is expected to be increasingly common in high performance computing. The usual approach is to train technicians to understand the complexities of these components and their connections, including performance tuning parameters, so that they can then manually monitor and reconfigure the system as needed. We envision instead attaching a &quot;standard&quot; feedbackloop  infrastructure to existing distributed systems for the purposes  of continual monitoring and dynamically adapting their activities and performance. (This approach can also be applied to &quot;new&quot;  systems, as an alternative to &quot;building in&quot; adaptation facilities, but we do not address that here.) Our proposed infrastructure consists of multiple layers with the objectives of probing, measuring and reporting of activity and state within the execution of the legacy system among its components and connectors; gauging, analysis  and interpretation of the reported events; and possible feedback to focus the probes and gauges to drill deeper, or -- when necessary - direct but automatic reconfiguration of the running system.
This paper describes a novel approach to an objective measurement of aorta samples of rats and a quantitative evaluation of aorta-related drugs. Two-photon fluorescence microscopy is used for recording image sequences of deforming aorta. Time sequence snake models are used to track the structural deformations of aorta walls caused by drug stimulation of the elastic lamina in the aorta. Several objective and quantitative biomarkers extracted from these models are used as diagnostic indicators. In a preliminary study, the technique was successfully used for evaluating the effect of a newly developed drug---human erythrocyte-derived depressing factor quantitatively and objectively.
We investigate the problem of decoupling capacitance allocation for power supply noise suppression at floorplan level. Decoupling capacitance budgets for the circuit modules are calculated based on the power supply noise estimates. A linear programming technique is used to maximize the allocation of the existing white space in the floorplan for the placement of decoupling capacitors. An incremental heuristic is proposed to insert more white space into the existing floorplan to meet the remaining demand required for decoupling capacitance fabrication. Experimental results on six MCNC benchmark circuits show that the white space allocated for decoupling capacitance is about 6%ff12% of the chip area for the 0ff25m technology, and the power supply noise can be kept below 10%Vdd.
The visually impaired experience serious difficulties in leading an independent life, due to their reduced perception of the environment. However, we believe that ubiquitous computing can significantly improve the perception of the surrounding reality for the blind and visually impaired. In this paper we describe the Chatty Environment, a system that addresses this problem and has been developed after a series of interviews with potential users. The system, which reveals the surroundings to the user by speech output, is usable in both indoor and outdoor contexts.
The Programming by Demonstration paradigm promises to reduce the complexity incurred in programming robot tasks. Its aim is to let robot systems learn new behaviors from a human operator demonstration. In this paper, we argue that while providing demonstrations in the real environment enables teaching of general tasks, for tasks whose essential features are known a priori demonstrating in a virtual environment may improve efficiency and reduce trainer&apos;s fatigue. We next describe a prototype system supporting Programming by Demonstration in a virtual environment and we report results obtained exploiting simple virtual tactile fixtures in pick-and-place tasks.
We propose a novel alternative to application-level overlays called VIOLIN, or Virtual Internetworking on OverLay INfrastructure. Inspired by recent advances in virtual machines, VIOLINs are virtual and isolated networks created on top of an overlay infrastructure such as PlanetLab. Entities in a VIOLIN include virtual routers, switches, and end-hosts, all implemented in software and hosted by physical overlay hosts. The salient features of VIOLIN include: (1) Each VIOLIN is a `virtual world&apos; with its own IP address space. And its activities and communications are strictly confined within the VIOLIN. (2) All VIOLIN entities can be created, deleted, or migrated on-demand. (3) It provides a new playground to deploy, leverage, and evaluate value-added network services which are not widely deployed in the real Internet. An application can simply connect to a VIOLIN and leverage the network services provided. (4) It releases application developers from network service implementation details, resulting in easier application implementation and maintenance. We have designed and implemented a prototype of VIOLIN in PlanetLab.
We present two broadcast algorithms that can be used on top of distributed hash tables (DHTs) to perform group communication and arbitrary queries. Unlike other P2P group communication mechanisms, which either embed extra information in the DHTs or use random overlay networks, our algorithms take advantage of the structured DHT overlay networks without maintaining additional information. The proposed algorithms do not send any redundant messages. Furthermore the two algorithms ensure 100% coverage of the nodes in the system even when routing information is outdated as a result of dynamism in the network. The first algorithm performs some correction of outdated routing table entries with a low cost of correction traffic. The second algorithm exploits the nature of the broadcasts to extensively update erroneous routing information at the cost of higher correction traffic. The algorithms are validated and evaluated in our stochastic distributed-algorithms simulator.
This paper investigates a multipleaccess communication receiver system that receives coded data modulated using DS-CDMA. We specifically test the receiver on Rayleigh distributed, correlated fading channel with multi-path. We describe an iterative receiver that improves the initial signal estimates, and therefore reduces the multiple access interference(MAI). Simulation results show that the performance approaches that obtained when only one users signal is received.
This paper presents a hybrid evolutionary algorithm (EA) for developing locomotion gaits of Sony legged robots. An online training algorithm is used for generating gaits for quadruped walking robots based on a hybrid approach that changes the probability of genetic operators in respect to the performance of the operator&apos;s offspring. The probability of applying an operator changes in proportion to the observed performance of the individuals created by that operator in the course of a run. The selection of EA parameters such as the population size and recombination methods and mutation parameters are made to be flexible and strive towards optimal performance autonomously. An overhead CCD camera is used to evaluate the performance of the generated gaits on-line while the robot is playing a football game. Robot is learning to walk on its own without any human interference.
We describe a head-tracking system that harnesses Bayesian modality fusion, a technique for integrating the analyses of multiple visual tracking algorithms within a probabilistic framework. At the heart of the approach is a Bayesian network model that includes random variables that serve as context-sensitive indicators of reliability of the different tracking algorithms. Parameters of the Bayesian model are learned from data in an offline training phase using ground-truth data from a Polhemus tracking device. In our implementation for a real-time head tracking task, algorithms centering on color, motion, and background subtraction modalities are fused into a single estimate of head position in an image. Results demonstrate the effectiveness of Bayesian modality fusion in environments undergoing a variety of visual perturbances.
The current technical note gives an overview of the available literature on the soft error rate (SER) of semiconductor devices at sea level. The main focus is on SRAM circuits, but other memories and logic are considered also. The radiation sources causing ionizing particles in semiconductors are discussed and the physical mechanisms of the upset of data bits are treated. Reported work on soft error modeling and simulation is reviewed and test methods are summarized. The impact of technology scaling on SER is discussed. Several techniques are treated that were proposed to improve the SER sensitivity of circuit designs. The report includes a discussion about the impact of the SER issue on deep-submicron design in coming years.
This paper studies the interplay between functional application and nondeterministic  choice in the context of untyped λ-calculus. We introduce an operational semantics which is based on the idea of must preorder, coming from the theory of  process algebras. To characterize this relation, we build a model using the classical inverse limit construction, and we prove it fully abstract using a generalization of  Böhm trees.
We describe an approach for compiling dynamic preferences into logic programs under the answer set semantics. An ordered  logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of atoms of the form s    t where s and t are names. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Our approach allows the specification of static orderings (in which preferences are external to a logic program), as well as dynamic orderings (in which preferences can appear within a program), and orderings on sets of rules. In large part then, we are interested in describing a general methodology  for uniformly incorporating preference information in a logic program. Since the result of our translation is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a compiler, available on the web, as a front-end for these programming systems.
The particle filter offers a general numerical tool to approximate the posterior density function for the state in nonlinear and non-Gaussian filtering problems. While the particle filter is fairly easy to implement and tune, its main drawback is that it is quite computer intensive. However, due to faster computers this drawback can be overcome and as a result the particle filter has quickly become a popular tool in signal processing applications. The computational complexity increases quickly with the state dimension for the problem at hand. One remedy to this problem is a technique known as RaoBlackwellization, where states appearing linearly in the dynamics are marginalized out. The result of this is that one Kalman filter is associated with each particle. Our main contribution in this article is to derive the details for the marginalized particle filter for a general nonlinear state-space model. We will also discuss some important special cases occurring in typical signal processing applications. The marginalized particle filter is applied to an integrated navigation system for aircraft. It is demonstrated that the complete high-dimensional system can be based on the particle filter using marginalization for all but three states. Excellent performance on real flight data is reported.
Subtyping relations for object-oriented formalisms describe relationships between super- and subclasses which satisfy the substitutability requirement imposed on types and their subtypes. Behavioural subtyping is concerned with subtypes for active classes with an explicit dynamic behaviour, specifiable for instance by object-oriented formal methods combining state-based with behavioural formalisms. In this
The Boolean formula value problem is in alternating log time and,  more generally, parenthesis context-free languages are in alternating log  time. The evaluation of reverse Polish notation Boolean formulas is also in  alternating log time. These results are optimal since the Boolean formula  value problem is complete for alternating log time under deterministic  log time reductions. Consequently, it is also complete for alternating log  time under AC    reductions.
Mobile service robots are recently gaining increased attention from industry  as they are envisaged as a future market. Such robots need natural interaction capabilities  to allow unexperienced users to make use of these robots in home and office  environments. In order to enable the interaction between humans and a robot, the detection  and tracking of persons in the vicinity of the robot is necessary. In this paper  we present a person tracking system for a mobile robot which enables the robot to  track several people simultaneously. Our approach is based on a variety of input cues  that are fused using a multi-modal anchoring framework. The sensors providing input  data are two microphones for sound source localization, a pan-tilt camera for face  and torso recognition, and a laser range finder for leg detection. Through processing  camera images to extract the torso position, our robot can follow a person guiding the  robot that is not oriented towards the robot and that is not speaking. In this case the  torso information is especially important for robust tracking during temporary failure  of the leg detection.
In this report, we introduce the MOLEN -coded processor which comprises of hardwired and microcoded reconfigurable units. At the expense of three new instructions, the proposed mechanisms allow instructions, entire pieces of code, or their combination to execute in a reconfigurable manner. The reconfiguration of the hardware and the execution on the reconfigured hardware are performed by -microcode (an extension of the classical microcode to allow reconfiguration capabilities) . We include fixed and pageable microcode hardware features to extend the flexibility and improve the performance. The scheme allows partial reconfiguration and includes caching mechanisms for non-frequently used reconfiguration and execution microcode. Using simulations, we establish the performance potential of the proposed processor assuming the JPEG and MPEG-2 benchmarks, the ALTERA APEX20K boards for the implementation, and hardwired superscalar processor. After implementation, cycle time estimations and normalization, our simulations indicate that the execution cycles of the superscalar machine can be reduced by 30% for the JPEG benchmark and by 32% for the MPEG-2 benchmark using the proposed processor organization.
We consider the problem of  nding, for a given n point set P in the plane and  an integer k  n, the smallest circle enclosing at least k points of P . We present a  randomized algorithm that computes in O(nk) expected time such a circle, improving  over previously known algorithms.
We provide designs for the first autonomous DNA nanomechanical devices that execute cycles of motion without external environmental changes. These DNA devices translate along a circular strand of ssDNA and rotate simultaneously. The designs use various energy sources to fuel the movements, include (i) ATP consumption by DNA ligase in conjunction with restriction enzyme operations, (ii) DNA hybridization energy in trapped states, and (iii) kinetic (heat) energy. We show that each of these energy sources can be used to fuel random bidirectional movements that acquire after n steps an expected translational deviation of O( ff n). For the devices using the first two fuel sources, the rate of stepping is accelerated over the rate of random drift due to kinetic (heat) energy. Our first DNA device, which we call walking DNA, achieves random bidirectional motion around a circular ssDNA strand by use of DNA ligase and two restriction enzymes. Our other DNA device, which we call rolling DNA, achieves random bidirectional motion without use of DNA ligase or any restriction enzyme, and instead using hybridization energy. We also describe how to modify the design for the rolling DNA device to include a &quot;latching mechanism&quot; that fixes the wheels position at specified location along the road, so as to provide for overall unidirectional translational movement.
This report presents a formal methodology of formalizing and verifying the  Transaction Layer Protocol (WTP) design in the Wireless Application Protocol  (WAP) architecture. Corresponding to the Class 2 Transaction Service (TRService)  definition and the Protocol (TR-Protocol) design, two models at different  abstraction levels are built with a finite state automaton (FSA) formalism. By  using the model checker SPIN, we uncover defects in a latest approved version of  the TR-Protocol design, which can lead to deadlock, channel buffer overflow and  unfaithful refinement of the TR-Service definition. As an extended result, a set of  safety, liveness and temporal properties is verified for the WTP to be operating in  a more general environment which allows for loss and re-ordering messages.
To find evaluating method of soil fertility by the application of the geographic information system (GIS), field experiments were conducted on 23 tobacco fields located at Cheongweon and Eumseong counties in 1996, 25 fields at Boeun and Goesan counties in 1997, and 19 fields at Jincheon and Chungju counties in 1998 to cover the wide range of distribution in landscape and soil attributes in Chungbuk Province, Korea. Tobacco was grown on a field that on fertilizer was applied to. Dry weight of tobacco leaves (DWTL) showed high variations by five times in different plots with minimum yield and maximum yield indicating the diverse soil fertility among the experimental fields. Dry weight and nutrients (N, P, and K) uptake of tobacco leaves were considered as basic fertility of the soil (BFS). The BFS was estimated by twenty-five independent variables including 13 chemical properties and 12 GIS data. Twenty-five independent variables were classified into two groups, 15 quantitative variables and 10 qualitative variables, and were analyzed by multiple linear regressions (MLR) of REG and GLM models of SAS. Evaluation for the BFS by the MLR including independent variables was better than that by simple regression showing gradual improvement by adding chemical properties, quantitative variables, and qualitative variables of the GIS. The variability in the DWTL by MLR was explained 34.2% by only chemical properties, 35.0% by adding quantitative variables, and 72.5% by adding both the quantitative and qualitative variables of the GIS compared with 21.7% by simple regression with NO 3 -N content in soil. Consequently, it is assumed that this approach by the MLR including both the quantitative and qualitative variables was available as an evaluation model of soil fertility for tobacc...
Applying Zeilberger&apos;s algorithm of creative telescoping to a family of certain very-well-poised hypergeometric series involving linear forms in Catalan&apos;s constant with rational coefficients, we obtain a second-order difference equation for these forms and their coefficients. As a consequence we derive a new way of fast calculation of Catalan&apos;s constant as well as a new continued-fraction expansion for it. Similar arguments are put forward to deduce a second-order difference equation and a new continued fraction for (4)  =  4
Field programmable gate arrays are an ideal substrate for computer architecture project courses. FPGAbased processor development offers some learning opportunities that pure simulation approaches cannot rival. This paper first introduces the XSOC Project, a free kit that includes the xr16 RISC CPU core, system-on-a-chip infrastructure, peripheral cores, C compiler, and simulator. Such kits have numerous applications in a first architecture course. It then suggests that FPGA-based processors can also be applied to the study of advanced architecture topics including memory systems, multithreading, LIWs, chip multiprocessors, and architectural support for programming languages and networking.
this paper determined the optimal fire management strategy for B. ornata by maximizing the expected number of years during which the species would persist in the next century. Other authors have attempted to determine optimal fire regimes of Banksia species using different methods and with different objectives. Enright, Lamont &amp; Marsula (1996) estimated the optimal fire frequency for B. hookeriana, a serotinous fire-killed shrub of western Australia, by determining the fire interval that maximized the finite rate of population increase. Focusing on the finite rate of population increase ignores risks associated with prescribed fires, such as the chance of germination failure after a fire or the chance of a subsequent fire killing immature plants. When risks are appreciable, the optimal management strategy is to attempt to burn the population as infrequently as possible but before the plants senesce and density becomes low ( Fig. 2). The optimal age of burning varied between 12 and 52 years depending on abundance ( Fig. 2). However, the influence of considering population size was small compared with the influence of considering population age (Figs 3 and 4), suggesting that the age of the population rather than abundance was the most important management variable for B. ornata. Nevertheless, changing from an age-dependent strategy to one based on age and abundance (the SDP solution in Figs 3 and 4) could lead to an approximate halving of the 100-year extinction risk ( Figs 3 and 4)
In this paper we consider a waveform relaxation method of the form        + f; A = P \Gamma Q    (0) = u 0 ;  where P is Toeplitz or block Toeplitz matrix for a numerical solution of the equation    + Au = f  u(0) = u 0  We show that under suitable conditions this method converges and apply this result  to linear parabolic equations with variable coefficients.
Consider a CIA agent who wants to authenticate herself to a server, but does not want to reveal her CIA credentials unless the server is a genuine CIA outlet. Consider also that the CIA server does not want to reveal its CIA credentials to anyone but CIA agents -- not even to other CIA servers.
A supply chain can be defined as a set of relationships among suppliers, manufacturers, distributors and retailers that facilitates the transformation of raw materials into final products. As such any examination of a supply chain system cannot be divorced from the consideration of the types of relationships that exist between the players in the chain. Such strategic analysis will also help supply chain members in deciding who to partner with and what type of relationship would be most useful for the player or the supply chain. Although supply chain management principles advocate close collaboration among all supply chain players this is far from the practice especially where resources and trust are scarce and the winner takes all attitude prevails. The question of whether to integrate or not to integrate and with whom can be discussed by investigating effects and trade-offs from such a venture. Other factors like the impact of the existence and position of a powerful player in the chain can also be investigated.
This study investigates the linguistic realization of information structure (IS) in Turkish. Following Vallduvi and Engdahl [Linguistics 34 (1996) 459], Hoffman (Hoffman, B., 1995. The Computational Analysis of the Syntax and Interpretation of ‘‘Free’’ Word Order in Turkish. Unpublished PhD dissertation, University of Pennsylvania), and Kilicaslan (Kilicaslan, Y., 1994. Information Packaging in Turkish. Unpublished MSc dissertation, University of Edinburgh), it is assumed that IS has a tripartite structure, consisting of topic, tail, and focus. The main claim of this paper is that syntax and phonology, by means of word order and prosody, are both responsible for the realization of the IS units. Thus, neither syntax nor phonology can be reduced to a secondary role. The word order–prosody interface reveals that presentational-focus and contrastive-focus are two distinct phenomena in Turkish, which are marked by different focusing strategies, i.e. syntactic and prosodic. It is shown that without drawing the distinction between the two types of focus, focusing phenomena in Turkish cannot be explained. This study also provides a schema representing the surface level structuring of IS in Turkish. At the same time, it is brought to light that in the interaction between specificity and IS, word order is employed in an extremely ‘free ’ way to mark the ground elements. This empirical fact suggests that the relation between specificity and IS is far more complex in Turkish than suggested in the previous literature.
In this paper, we present a new method to improve global routing results. By using an amplified congestion estimate to influence a rip-up and reroute approach, we obtain substantial reductions in total congestion. In comparisons with a recently published tool on publicly available benchmarks, our new router is roughly twice as fast, obtains 15.1% reductions in total wire length, and 65.2% reductions in the number of overcongested graph edges. A direct implementation of an old approach also performs extremely well, indicating that some known techniques have been overlooked.
number of usernames and passwords that must be remembered would become a problem [1]. This is considered a major usability problem, with password problems representing 30% of helpdesk calls    and up to 90% of users reporting they cannot cope with remembering and using passwords [1].  User identification and authentication can only become worse as we move to a pervasive computing environment, where users have access to a large range of devices and networks. For example, it would be deeply frustrating for users to have to keep entering passwords as they change devices or use different networks, especially when in a pervasive computing world, in which computing resources are intended to become `invisible&apos; [3].  Current main network security models use IP addressing identification and authentication, which are really of the device and not the user. This model works reasonably well for the traditional deskbound PC user, who typically works at a specific machine with an associated IP-addres
Most current tools for data mining lack support  for intelligent analysis and filtering of  mined patterns. Dividing interesting mining  results from uninteresting ones still is a  laborious task mainly performed by human  users. We propose to employ formalized domain  knowledge for assessing the interestingness  of mining results. We present considerations  and ideas as foundations of the design of  an intelligent data mining environment.
The goal of this FYI memo is to provide an update to FYI 2, RFC 1147 [1], which provided practical information to site administrators and network managers. New and/or updated tools are listed in this RFC. Additonal descriptions are welcome, and should be sent to: noctoolsentries @merit.edu. 
The problem of recovering a high-resolution image from a sequence of low-resolution DCT-based compressed observations is considered in this paper. The introduction of compression complicates the recovery problem. We analyze the DCT quantization noise and propose to model it in the spatial domain as a colored Gaussian process. This allows us to estimate the quantization noise at low bit-rates without explicit knowledge of the original image frame, and we propose a method that simultaneously estimates the quantization noise along with the high-resolution data. We also incorporate a nonstationary image prior model to address blocking and ringing artifacts while still preserving edges. To facilitate the simultaneous estimate, we employ a regularization functional to determine the regularization parameter without any prior knowledge of the reconstruction procedure. The smoothing functional to be minimized is then formulated to have a global minimizer in spite of its nonlinearity by enforcing convergence and convexity requirements. Experiments illustrate the benefit of the proposed method when compared to traditional high-resolution image reconstruction methods. Quantitative and qualitative comparisons are provided.
The use of consistent hashing to construct a distributed hash table (DHT) has recently gained significant interest from the distributed computing community, particularly because algorithms have been devised which implement a DHT using peer-to-peer (P2P) technology. However a common assumption is that every node that participates in the DHT is within the same transport domain. Some solutions make to this problem make use of an intermediate overlay network which itself can become messy. Other solutions rely on the widespread use of IPv6 or &quot;work-arounds&quot; for the limited IPv4. We propose a solution that uses a multi-space DHT algorithm to manifest a DHT space in each transport domain, with bridge nodes linking DHT spaces between transport domains. The ostensive problem with this solution is to ensure that key requests are evenly distributed over all spaces. We provide a framework to solve this problem and give an example solution for a simple 2-space DHT network.
As we move towards nanometer technology, manufacturing problems become overwhelmingly difficult to solve. Presently, optimization for manufacturability is performed at a post-synthesis stage and has been shown capable of reducing manufacturing cost up to 10%. As in other cases, raising the abstraction layer where optimization is applied is expected to yield substantial gains. This paper focuses on a new approach to design for manufacturability: logic synthesis for manufacturability. This methodology consists of replacing the traditional area-driven technology mapping with a new manufacturability-driven one. We leverage existing logic synthesis tools to test our method. The results obtained by using STMicroelectronics 0.13m library confirm that this approach is a promising solution for designing circuits with lower manufacturing cost, while retaining performance. Finally, we show that our synthesis for manufacturability can achieve even larger cost reduction when yield--optimized cells are added to the library, thus enabling a wider area-yield tradeoff exploration.
The use of information technologies (IT) to collect personal health information is growing in popularity via computer-assisted interviewing and a wide variety of healthcare Web sites. However, a review of the literature on computer-assisted interviewing exhibits confounding and equivocal results regarding the effects of IT on individuals&apos; willingness to disclose socially sensitive health information. Some studies revealed individuals&apos; heightened concerns about entering their health information into a computer, while other studies exhibited greater willingness to enter sensitive information into a computer than to give it to a personal physician.
In order to reduce coupling effects between bitlines in static or dynamic RAMs, bitline twisting can be used in the design. For testing, however, this has consequences for the to-be-used data backgrounds. A generic twisting scheme is introduced and the involved fault models are identified.
The bias-variance decomposition of error provides useful insights into the error performance of a classifier as it is applied to different types of learning task. Most notably, it has been used to explain the extraordinary effectiveness of ensemble learning techniques. It is important that the research community have effective tools for assessing such explanations. To this end, techniques have been developed for estimating bias and variance from data. The most widely deployed of these uses repeated sub-sampling with a holdout set. We argue, with empirical support, that this approach has serious limitations. First, it provides very little flexibility in the types of distributions of training sets that may be studied. It requires that the training sets be relatively small and that the degree of variation between training sets be very circumscribed. Second, the approach leads to bias and variance estimates that have high statistical variance and hence low reliability. We develop an alternative method that is based on cross-validation. We show that this method allows far greater flexibility in the types of distribution that are examined and that the estimates derived are much more stable. Finally, we show that changing the distributions of training sets from which bias and variance estimates are drawn can alter substantially the bias and variance estimates that are derived.
This contribution deals with a fully digital multirate radio receiver suitable for vehicular applications. Timing correction and sample rate conversion are performed by a polynomial interpolator. Three different receiver configurations are considered in terms of computational complexity and BER performance. Careful selection of the intermediate frequency turns out to play a crucial role. System parameters are provided yielding good BER performance for all considered symbol rates. Results are verified by computation of the BER degradation as compared to an analog receiver with synchronized symbol-rate sampling.
IP Telephony recently  nds a lot of attention and will be used in IP based networks and in combination with the existing conventional telephone system. There is a multitude of competing signaling protocol standards, interfaces and implementation approaches. A number of basic functions can be found throughout all of those, though. This includes the addressing of participants using symbolic names, the negotiation of connections and their parameters as well as the enforcement of a dedicated handling of data streams by means of QoS signaling activities. Thus, a generic abstraction hiding underlying protocol speci  cs is very desirable and useful. The Delivery Multimedia Integration Framework DMIF - as part of the MPEG approach towards distributed multimedia systems - forms a general and comprehensive framework that is applicable to a wide variety of multimedia scenarios.
The paper presents a system architecture for  the automatic generation of interface specifications  from ontologies. The ensuing
Knowledge discovery in databases (KDD) is not a straightforward application of a single method, but rather a long lasting, iterative and interactive process during which the user has to model a multitude of data derivation processes, execute them and interpret the results in order to formulate new derivation processes. Having a single tool that supports the user in the selection of data, their preprocessing and transformation, as well as their mining, is an important requirement of KDD practitioners. Moreover, it is important to assist the user of a KDD system in the selection of the right parameters or methods.
This paper reports work done as part of the Large Interactive Display Surface(LIDS) project at the University of Waikato. One application of the LIDS equipment is distributed meeting support. In this context large display surfaces are used as shared workspaces by people at collaborating sites. A meeting will start with a shared presentation document, typically an agenda document with summary and detail on agenda items as required. During the meeting, annotations will be made on the shared document, and new pages will be added with notes and drawings.
In recent years, the field of comparative economics refocused on the comparison of capitalist economies. The theme of the new research is that institutions exert a profound influence on economic development. We argue that, to understand capitalist institutions, one needs to understand the basic tradeoff between the costs of disorder and those of dictatorship. We apply this logic to study the structure of efficient institutions, the consequences of colonial transplantation, and the politics of institutional choice.  
A new method for detecting shot boundaries in video sequences using metrics based on information theory is proposed. The method relies on the mutual information and the joint entropy between frames and can detect cuts, fade-ins and fade-outs. The detection technique was tested on TV video sequences having different types of shots and significant object and camera motion inside the shots. It was favorably compared to other recently proposed shot cut detection techniques. The method is proven to detect both fades and abrupt cuts very effectively.
In this paper, we present techniques for provisioning CPU and network resources in shared hosting platforms running potentially antagonistic third-party applications. The primary contribution of our work is to demonstrate the feasibility and benefits of overbooking resources in shared platforms, to maximize the platform yield: the revenue generated by the available resources. We do this by first deriving an accurate estimate of application resource needs by profiling applications on dedicated nodes, and then using these profiles to guide the placement of application components onto shared nodes. By overbooking cluster resources in a controlled fashion, our platform can provide performance guarantees to applications even when overbooked, and combine these techniques with commonly used QoS resource allocation mechanisms to provide application isolation and performance guarantees at run-time. When compared to provisioning based on the worst-case, the efficiency (and consequently revenue) benefits from controlled overbooking of resources can be dramatic. Specifically, experiments on our Linux cluster implementation indicate that overbooking resources by as little as 1% can increase the utilization of the cluster by a factor of two, and a 5% overbooking yields a 300-500% improvement, while still providing useful resource guarantees to applications.
etry confirmed the anomalies of the left and right superior temporal sulci and of the right intraparietal sulcus. Our results thus provide converging evidence of regionally specific structural changes in TS that are highly consistent with the hallmark symptoms associated with TS.  Keywords: developmental dyscalculia, diffusion tensor imaging, MRI, social  cognition, sulcal morphometry, Turner syndrome, voxel-based morphometry  Introduction  Turner syndrome (TS) is a genetic condition resulting from a partial or complete absence of one of the two X chromosomes in a phenotypic female (Turner, 1938). TS occurs in approximately one in 2000 liveborn females and affects an estimated 3% of all females conceived (Saenger, 1996; Ranke and Saenger, 2001). TS is associated with a well-documented physical phenotype including a short stature, ovarian failure and abnormal pubertal development and a variety of other features (webbed neck, renal dysgenesis, cardiac malformation) (Ranke and Saenger, 20
Many mechanical and electromechanical devices work by the impact principle. A movable part in the device switches between two stable positions and each working cycle causes an impact inside the device. This process can be found for example in relays and valves.
HST observations provided us with the spatial resolution required to explore in detail the properties of the Narrow Line Region of Seyfert galaxies. In Seyferts with extended radio structures the HST images revealed a very close connection between radio and optical line emission. We interpret this result as strong evidence that the line-emitting gas is compressed by the shocks created by the passage of the radio-emitting outow. The increase in the density due to the shocks causes the line emission to be highly enhanced in the region where this interaction occurs. Spectroscopy of the NLR con  rms the presence of a strong interaction: in the region co-spatial with the radio-jets, the gas velocity  eld is highly perturbed and shows two velocity systems separated by as much as 1700km s    . In several locations the split lines form an almost complete velocity ellipsoid, implying that we are seeing an expanding shell of gas. We conclude that the morphology and the kinematics of the NLR in these sources are dominated by the presence of radio outows, which might also have a signi  cant role in the ionization balance. Observations of Seyfert galaxies represent a unique tool to study energetics and evolution of ISM/jets systems.
We present a framework for mining association rules from transactions consisting of categorical items where the data has been randomized to preserve privacy of individual transactions. While it is feasible to recover association rules and preserve privacy using a straightforward &quot;uniform&quot; randomization, the discovered rules can unfortunately be exploited to findprivacy breaches. We analyze the nature of privacy breaches andpropose a class of randomization operators that are much more effective than uniform randomization in limiting the breaches. We derive formulae for an unbiased support estimator and its variance, which allow us to recover itemset supports from randomized datasets, and show how to incorporate these formulae into mining algorithms. Finally, we present experimental results that validate the algorithm by applying it on real datasets.
Today&apos;s technology offers a wide variety of sensors. Although many sensing applications have been produced, there is no support for the design of applications offering physical interaction. In order to make a step towards such a design framework this paper analyzes different means of sensing of humans and human activity. In particular we identify six sensing goals, referred to dimensions of sensing: ID (1), Object Use (2), Location (3), Bio Signs/Emotions (4), Activity (5) and Interaction among humans (6). Those dimensions together with different sensor placements are used to review and analyze ubiquitous computing research related to physical interaction and sensing. The final discussion draws conclusions from this analysis with respect to appropriateness of sensors and sensor placement for different sensing dimensions.
Quadrupole fringe fields can limit the dynamic aperture of muon storage rings. Using the computer code COSY INFINITY for particle tracking and normal-form analysis, we evaluate the importance of fringe fields in the FNAL muon storage ring, and identify the regions of the machine where they are most critical. Dynamic aperture and linear tune shifts with amplitude are calculated considering an ideal machine without any errors or misalignments. We also explore the efficiency of various nonlinear correction schemes, study the momentum acceptance, and evaluate the spin decoherence over the transverse phase space. Geneva, Switzerland May 4, 2000   FNAL, Batavia, Illinois, USA.
We demonstrate how relational algebra and its mechanization through the software  system RelView can be used for solving practical problems in the design of objectoriented  software. The examples we present range from the search of improper code pieces  to the detection of code parts indicating some design pattern structure.
Once the decision to outsource an activity has been made, managers are faced with issues related to the management of the relationship with their service providers. A critical element of relationship management is the contract itself, which defines, more or less completely, the nature of the services to be rendered and of the relationship itself. In this paper, we rely on transaction  cost theory to develop a series of propositions on the relationship between the characteristics of  the transaction - asset specificity, number of suppliers, measurement problems, uncertainty, and permanent character of the contract - and the level of contract completeness. A survey of 200 firms was conducted to test the propositions.
A family of gradient descent algorithms for learning linear functions in an online setting is  considered. The family includes the classical LMS algorithm as well as new variants such as  the Exponentiated Gradient (EG) algorithm due to Kivinen and Warmuth. The algorithms  are based on prior distributions de  ned on the weight space. Techniques from dierential  geometry are used to develop the algorithms as gradient descent iterations with respect to  the natural gradient in the Riemannian structure induced by the prior distribution. The  proposed framework subsumes the notion of \link-functions&quot;.
this paper is to present a model to structure and query document databases, following the new trend of mixing content and structure in queries. The model is shown to be expressive and efficiently implementable. There is not at  ProximalNodes: A Model to Query Document Databases by Contents and Structure \Delta 3 this time, to the best of our knowledge, any approach satisfying both goals (see, however, a recent work [Dao et al. 1996]). We first concentrate on text and later show how to integrate other types of data (e.g. audio, images, etc.). These media are becoming more and more common in document databases. It has been argued that is better to put a layer integrating a traditional database system with a textual one, than trying to design a language comprising all the features [Sacks-Davis et al. 1994]. Each subsystem focuses on a different part of the query (e.g. integration of an object-oriented database with a structured text engine [Consens and Milo 1994]). We rely on this approach. We design a language which is focused on exploiting structure- and content-related features. Other features, such as tuples and joins, should be added by integrating this language with another one oriented to that kind of operations, e.g. a relational database. We point out what are we not covering in this work. First, we do not cover languages to describe document structure, such as SGML [International Standards Organization 1986], DSSSL [International Standards Organization 1994], SPDL [International Standards Organization 1991], HyTime [International Standards Organization 1992], etc. We cover structuring models (e.g. hierarchical). A given structuring model may or may not be expressed using a given language to describe structure. Second, we concentrate more on querying than on inde...
The QoS Steiner Tree Problem asks for the most costefficient way to multicast multimedia to a heterogeneous collection of users with different consumption rates. We assume that the cost of using a link is not constant but rather depends on the maximum bandwidth routed through the link. Formally, given a graph with costs on the edges, a source node and a set of terminal nodes, each one with a bandwidth requirement, the goal is to find a Steiner tree containing the source, and the cheapest assignment of bandwidth to each of its edges so that each source-to-terminal path in the tree has bandwidth at least as large as the bandwidth required by the terminal. Our main contributions are: (1) new covering-type integer linear program formulations for the problem; (2) two new heuristics based on the primal-dual framework; (3) a primaldual constant-factor approximation algorithm; (4) an extensive experimental study of the new heuristics and of several previously proposed algorithms.
This paper describes the integration of analog, digital, and mixed-signal IC design in the undergraduate ECE curriculum at Lafayette College. This integration is being accomplished by adding IC design coverage to the required electronics sequence and including analog and mixed-signal coverage in a what was previously an &quot;alldigital &quot; VLSI Design elective. As a result, all ECE students at Lafayette receive some basic instruction on IC design, including layout. Students who take advanced courses receive more coverage of analog and mixed-signal design.
Network processors (NPs) implement a balance between hardware and software that addresses the demand of performance and programmability in active networks (AN). We argue that this makes them an important player in the implementation and deployment of ANs. Besides a general introduction into the relationship of NPs and ANs, we describe the power of this combination in a framework for secure and safe capsule-based active code. We also describe the advantages of offloading AN control point functionality into the NP and how to execute active code in the data path efficiently. Furthermore, the paper reports on experiences about implementing active networking concepts on the IBM PowerNP network processor.
In this paper, we present parallel algorithms for the coarse  grained multicomputer (CGM) and the bulk synchronous parallel computer   (BSP) for solving two well known graph problems: (1) determining  whether a graph G is bipartite, and (2) determining whether a bipartite  graph G is convex.
We have previously developed a distributed routing algorithm which minimizes the number of overhead messages necessary to maintain the minimum-power multi-hop routes in a mobile ad-hoc network, assuming a piece-wise linear model for the motion of the nodes. In this paper we extend the routing algorithm to include clustering as well, to reduce further the number of overhead messages at the expense of sub-optimal routes. A single parameter controls the degree of clustering, and consequently the degree of sub-optimality, rather than arbitrary parameters such as maximum cluster size or maximum distance between nodes. The proposed algorithm converges in a nite number of iterations to both stable routes and stable clusters, and by setting the cluster parameter to 0 collapses to the original routing algorithm with no clusters and optimum routes.
With development of modern geoscience, particularly of environmental sciences, the contemporary soil science is undergoing great changes in both research contents and scope. Soil is not only a certain substance or a certain independent natural historical body, but also a spheric layer with peculiar structure and functions in the earth system. From the viewpoint of the geo-biosphere system of the earth, soil science does not only deal with the soil substances per se, but also more importantly with the relationship between soil, the other spheres and the human survivorship environment in view of the pedosphere. This is the new orientation of soil science today and will affect profoundly the studies on the human survivorship environment and global changes. To throw more light on this subject, the present paper intends to address the conception of pedosphere and its role in global changes and environmental quality of soil. Also addressed are series of environmental issues in China and their relations to the global change. Moreover, research orientation and priorities are discussed in the field of pedosphere for agriculture and environment.
A flexible, parameterizable simulator of pipelined processors is presented. The simulator  allows to configure several (micro-)architectural features such as the pipeline depth, the stage  in which branch execution occurs, whether or not register file forwarding is performed, and the  number of branch delay slots. We use the simulator to perform experiments with three synthetic  benchmarks: vector addition, vector summation, and sum of absolute differences. These kernels  are representative for data parallel loops, reduction operations, and benchmarks containing many  hard to predict branches, respectively.
This paper presents weighted importance sampling techniques for  Monte Carlo form factor computation and for stochastic Jacobi radiosity system  solution. Weighted importance sampling is a generalisation of importance  sampling. The basic idea is to compute a-posteriori a correction factor to the  importance sampling estimates, based on sample weights accumulated during  sampling. With proper weights, the correction factor will compensate for statistical  fluctuations and lead to a lower mean square error. Although weighted  importance sampling is a simple extension to importance sampling, our experiments  indicate that it can lead to a substantial reduction of the error at a very low  additional computation and storage cost.
One of the challenges in bio-computing is to enable the efficient use of a wide variety of rapidly evolving computational methods to simulate, analyze and understand complex interactions of molecular systems. Our laboratory is interested in the development of novel computational technologies and in the application of these technologies to the analysis and understanding of complex biological systems. We have been using the Python programming language as a platform to develop reusable and interoperable components dealing with different aspects of structural bioinformatics. These components are the basic building blocks from which several domain specific applications have been developed. In this paper we describe the integration of two applications developed in our laboratory: PMV and a visual-programming environment. PMV is a general purpose, command-driven molecular visualization and manipulation program built from reusable software components. The visual-programming environment enables a user to build interactively networks describing novel combinations of computational methods. We describe several applications demonstrating the synergy created by combining these two programs.
It is known that the set of permutations, under the pattern containment ordering, is not a partial well-order. Characterizing the partially well-ordered closed sets (equivalently: down sets or ideals) in this poset remains a wide-open problem. Given a 0/&amp;plusmn;1 matrix M, we define a closed set of permutations called the profile class of M. These sets are generalizations of sets considered by Atkinson, Murphy, and Ruskuc. We show that the profile class of M is partially well-ordered if and only if a related graph is a forest. Related to the antichains we construct to prove one of the directions of this result, we construct exotic fundamental antichains, which lack the periodicity exhibited by all previously known fundamental antichains of permutations.
We prove the theorem from the title: the acyclic edge chromatic  number of a random d-regular graph is asymptotically almost surely  
Feature modeling is a popular domain analysis method for describing the commonality and variability among the domain products. The current formalisms of feature modelling do not have enough support for automated domain product configuration and validation. We have developed a theory of feature modeling: a feature model is analogous to a definition of a language; a particular feature composition instance (domain product) is analogous to a program written in that language; and the way the features can be assembled to form a product is analogous to the way various tokens can be assembled to form a program. To apply this theory, we have developed a meta-language Two-Level Grammar++ to specify feature models. The interpreter derived from the feature model specification performs automated product configuration and product quality validation.
The conversion from quarters to semesters presents a unique opportunity to revamp the curricula. In the fall of 2000 this is exactly what occurred at Michigan Tech. The conversion involved overhauling much of the engineering curricula. One major change in the engineering curricula was the implementation of the Engineering Enterprise Program. Prior to the calendar conversion students had only one pathway toward graduation. This pathway involved the students taking technical courses in one of three flavors: theory, application, or design. Little interconnectivity was ever established between these courses. Additionally, student design projects, while of high quality were often completed with minimal student interest. To help build student interest and interconnect the theory, application, and design courses, a novel pathway was developed. This pathway, the Enterprise Program, takes students on a three-year educational journey using topics of self-selected student interests. Students in this program join a pseudo company or &quot;enterprise&quot;. From day one, entering sophomores start working with juniors and seniors from multiple disciplines as well as with as with people from industry. This allows the younger students to immediately see the application of their theory course and reinforce &quot;just  learned&quot; topics. This further allows the juniors and seniors to excel in design as they have self-selected to join a particular enterprise. This paper elaborates on the Enterprise Program and details the process of turning a well established club organization into an enterprise using the  example of Michigan Tech&apos;s SAE Mini-Baja Team / Enterprise.
this paper are not necessarily those of the department as a whole. The accuracy of the information presented in this paper is the sole responsibility of the authors
We consider the problem of scheduling n independent multiprocessor  tasks with release dates on a  xed number of processors, where  the objective is to compute a non-preemptive schedule minimizing the  average weighted completion time. For each task, in addition to its processing  time and release date, there is given a prespeci  ed, dedicated  subset of processors which are required to process the task simultaneously.
. We present a novel and uniform technique for normal logic program declarative error diagnosis. We lay down the foundations on a general approach to diagnosis using logic programming, and bring out the close relationship between debugging and fault--finding. Diagnostic debugging can be enacted by contradiction removal methods. It relies on a simple program transformation to provide a contradiction removal approach to debugging, based on revising the assumptions about predicates&apos; correctness and completeness. The contradiction removal method is justified in turn in terms of well-- founded semantics. The loop detection properties of well--founded semantics will allow in the future for a declarative treatment of otherwise endless derivations. The debugging of programs under well--founded semantics with explict negation is also foreseen. Here, we apply our techniques to finite SLDNF derivations, whose meaning coincides with the well--founded model, for which our contradiction removal meth...
Consider an Ornstein-Uhlenbeck process with reflection at the origin.
The model-based approaches for tracking of human bodies in image sequences can becategorized into two types; fitting model to body frame by frame, and accumulating estimatedpose displacements in successive frames after model fitting at the initial frame. The latter has an inherent drawback as accumulation of tracking errors while the one has a great advantage as small computational efforts compared with the former. This paper proposes a new method which can correct the tracking errors by propagation from fitting model to body at a few key-frames. The propagation makes it possible to establish tracking of bodies under occlusion. Capturing the actor&apos;s motions in real old movies is presented. 1. 
In mobile ad-hoc networks, nodes act both as terminals and information  relays, and participate in a common routing protocol, such as Dynamic  Source Routing (DSR). The network is vulnerable to routing misbehavior, due to  faulty or malicious nodes. Misbehavior detection systems aim at removing this  vulnerability. In this paper we investigate the use of an Artificial Immune System  (AIS) to detect node misbehavior in a mobile ad-hoc network using DSR. The  system is inspired by the natural immune system of vertebrates. Our goal is to be  able to build a system that, like its natural counterpart, automatically learns and  detects new misbehavior. We describe the first step of our design; it employs negative  selection, an algorithm used by the natural immune system. We define how  we map the natural immune system concepts such as self, antigen and antibody to  a mobile ad-hoc network, and give the resulting algorithm for misbehavior detection.
This thesis investigates security in multi-agent systems for mobile communication. Mobile as well as non-mobile agent technology is addressed.
Motivated by problems that arise in computing degrees of belief, we consider the problem of computing asymptotic conditional probabilities for first-order sentences. Given first-order sentences &apos; and `, we consider the structures with domain f1; : : : ; Ng that satisfy `, and compute the fraction of them in which &apos; is true. We then consider what happens to this fraction as N gets large. This extends the work on 0-1 laws that considers the limiting probability of first-order sentences, by considering asymptotic conditional probabilities. As shown by Liogon&apos;kii[31] and Grove, Halpern, and Koller [22], in the general case, asymptotic conditional probabilities do not always exist, and most questions relating to this issue are highly undecidable. These results, however, all depend on the assumption that ` can use a nonunary predicate symbol. Liogon&apos;kii [31] shows that if we condition on formulas ` involving unary predicate symbols only (but no equality or constant symbols), then the asymptotic conditional probability does exist and can be effectively computed. This is the case even if we place no corresponding restrictions on &apos;. We extend this result here to the case where ` involves equality and constants. We show that the complexity of computing the limit depends on various factors, such as the depth of quantifier nesting, or whether the vocabulary is finite or infinite. We completely characterize the complexity of the problem in the different cases, and show related results for the associated approximation problem.
Designingcomponents and composing them into an architecture inherentlyinvolves describing their behavior. The classical software engineering approach to specifying requirements for large-scale components is to start with use cases. However, employing use cases to component architectures triggers the need of (i) assembling the behavior specified by several use cases, (ii) composing the behavior of communicating entities, and (iii) reasoning on consistency of the composed behavior. Applying a modeling language, such as UML,while dealing with these issues is desirable.
The most popular data mining techniques consist in searching databases  for frequently occurring patterns, e.g. association rules, sequential patterns.
Introduction  Ubiquitous computing is beginning to generate interest in the computer world, and new applications for this technology are being found daily. One facet of ubiquitous computing is the concept of distributed sensor networks: groups of sensors that communicate with each other for tasks such as monitoring inventory (RFID tags and infrastructure in a convenience store), security (security cameras in a warehouse), or watching environmental conditions (a series of temperature sensors in a greenhouse). One major obstacle blocking the adoption of distributed sensor networks is the issue of maintenance. Installing infrastructure to supply power to the sensors would be a robust solution, always keeping them powered, but it is an expensive option. On the other side of the spectrum, one could simply use disposable battery packs to power the sensors. However, these battery packs would have be tested and replaced regularly, a labor intensive process if done by hand. Combining robotics w
A problem of great interest in the control of hybrid systems  is the design of least restrictive controllers for reachability specifications.
Learning from past experience allows a problem solver to increase its solvability horizon from simple to complex problems. For planners, learning involves a training phase during which knowledge is extracted from simple problems. But how are these simple problems constructed? All current learning and problem solving systems require the user to provide the training set. However it is rarely easy to identify problems that are both simple and useful for learning, especially in complex applications. In this paper, we present our initial research towards the automated or semiautomated identification of these simple problems. From a difficult problem and a corresponding partially completed search episode, we extract auxiliary problems with which to train the learner. We motivate this overlooked issue, describe our approach, and illustrate it with examples. Introduction and Problem Formulation  Researchers in Machine Learning and in Planning have developed several systems that use simple plan...
The most important task of Legal Knowledge Based Systems (LKBS) is to `assess&apos; cases.
Introduction  Over the last years, highly sensitive magnetic field sensors, SQUIDs, have been employed to measure the magnetocardiogram (MCG), which are generated by ionic currents flowing in myocardium when it is active.  The MCG method provides additional information in comparison to surface electric potential distribution. The obtained information is complementary in nature because the magnetic field is related to the curl of the impressed current sources into the human heart while the electric field is related to its divergence [1]. It is known that the myocardium has many kinds of anisotropy. One of them is determined by an anisotropy of conductivity (uniform anisotropy), which plays a key role in realistic propagation of the electrical excitation in the myocardium. This was indicated by computer simulation of normal MCG based on a propagation model of an excitation wave front in threedimensional arrays of cardiac cells within the cellular automata theory [2]. Animal studies and t
The set   of ff-integers is a Meyer set when ff is a Pisot number, and thus there exists a finite set F such that Zff  -Zff ff  Zff +F . We give finite automata describing the expansions of the elements of  Zff and of Zff   Zff . We present a construction of such a finite set F , and a method to minimize the size of F . We obtain in this way a finite transducer that performs the decomposition of the elements of Zff    as a sum belonging to Zff + F . 1 
We propose a practical and efficient method for adding security to network-attached disks (NADs). In contrast to previous work, our design requires no changes to the data layout on disk, minimal changes to existing NADs, and only small changes to the standard protocol for accessing remote block-based devices. Thus, existing NAD file systems and storage-management software could incorporate our scheme very easily. Our design enforces security using the well-known idea of self-describing capabilities, with two novel features that limit the need for memory on secure NADs: a scheme to manage revocations based on capability groups, and a replay-detection method using Bloom filters.
Recent high-throughput techniques have allowed the identification of many DNA target sequences specifically bound by transcription factors at a genomic scale. UV or formaldehyde X-ChIP  experiments performed on Drosophila embryos using Engrailed or Trithorax-like antibodies have led to the cloning and sequencing  vivo Engrailed or Trl-recognized DNA fragments. The mining of such datasets had involved the development of dedicated tools to localize the DNA regions on the genome sequence, to assign putative target genes and to process the associated Gene Ontology terms. This analysis points on overrepresentations of several functions for Engrailed and Trithorax-like targets. Sequence analysis of the fragments has been performed with motif discovery programs. In these sequences, we found that the strongest motif in the sequences extends the known Engrailed or Tritorax-like consensus. These motifs are further accompanied by several additional motifs, indicating putative co-acting factors binding sites.
We present a noun phrase coreference system  that extends the work of Soon et  al. (2001) and, to our knowledge, produces  the best results to date on the MUC6  and MUC-7 coreference resolution data  sets -- F-measures of 70.4 and 63.4, respectively. Improvements
Continuous queries are queries for which responses given to users must be continuously updated, as the sources of interest get updated. Such queries occur, for instance, during on-line decision making, e.g., traffic flow control, weather monitoring, etc. The problem of keeping the responses current reduces to the problem of deciding how often to visit a source to determine if and how it has been modified so that a user response can be updated accordingly. On the surface, this seems to be similar to the crawling problem since crawlers attempt to keep indexes up-to-date as users pose search queries. We show that this is not the case, both due to the inherent differences between the nature of the two problems as well as the performance metric. We also develop and evaluate a novel multi-phase (Continuous Adaptive Monitoring) (CAM) solution to the problem of maintaining the currency of query results. Some of the important phases are: The tracking phase, in which changes, to an initially identified set of relevant pages, are tracked. From the observed change characteristics of these pages, a probabilistic model of their change behaviour is formulated and weights are assigned to pages to denote their importance for the current queries. During the next phase, the Resource Allocation phase, based on these statistics, resources, needed to continuously monitor  these pages for changes, are allocated. Given these resource allocations, the scheduling phase produces an optimal achievable schedule for the monitoring tasks. An experimental evaluation of our approach compared to prior approaches for crawling dynamic web pages shows the effectiveness of our approach to monitoring dynamic changes. For example, by monitoring just 5% of the page changes, CAM is able to return 90% of the cha...
We study the interdomain traffic as seen by a non-transit ISP, based on a six days trace covering all the interdomain links of this ISP. Our analysis considers the relationships between the interdomain traffic and the interdomain topology. We first discuss the day...
This paper focuses on load management in looselycoupled federated distributed systems. We present a distributed mechanism for moving load between autonomous participants using bilateral contracts that are negotiated offline and that set bounded prices for moving load. We show that our mechanism has good incentive properties, efficiently redistributes excess load, and has a low overhead in practice.
We are rapidly approaching a time when large-scale shared memory supercomputers will have remote memory latencies measured in the thousands of cycles and cross-section bandwidth will be a limiting performance factor. For these machines to scale, mechanisms that minimize interprocessor communication will be essential. We propose one such mechanism, active memory, which allows operations to be sent to and executed on the home memory controller of particular data items. Performing the operations near where the data resides, rather than moving it across the network, operating on it, and moving it back, eliminates significant network traffic, introduces opportunities for additional parallelism, and hides high remote memory latencies. Active memory provides many of the benefits of PIMs without the need for non-standard DRAMs, and enables significantly better application scaling than conventional shared memory synchronization and range operations.
The Internet routing system today is divided into two views:...
Mark de Berg Jit Bose David Bremner    William Evans Lata Narayanan 1 Introduction  Suppose the only access we have to an arrangement of n input lines is to &quot;probe&quot; the arrangement with horizontal lines. A probe returns the set of probe points  which are the intersections of the probe&apos;s horizontal line (the probe line) with all input lines. We assume that none of the input lines is horizontal, so a probe line intersects every input line. Our goal is to reconstruct the set of input lines using a small number of probes. Aoki, Imai, Imai, and Rappaport [1] observed that if one is allowed to place the probe lines after seeing the results of previous probes, then the number of probes required is at most three. This paper addresses the problem of fixed probes; in our setting the locations of the probes must be chosen before the input is examined. We show that for each natural number n there is a set of n + 1 probe lines that will serve to determine any arrangement of  n input lines and that ...
Relevance feedback (RF) has been an effective query modification  approach to improving the performance of information retrieval  (IR) by interactively asking a user whether a set of documents are relevant  or not to a given query concept. The conventional RF algorithms  either converge slowly or cost a user&apos;s additional efforts in reading irrelevant  documents. This paper surveys several RF algorithms and introduces  a novel hybrid RF approach using a support vector machine  (HRFSVM), which actively selects the uncertain documents as well as  the most relevant ones on which to ask users for feedback. It can efficiently  rank documents in a natural way for user browsing. We conduct  experiments on Reuters-21578 dataset and track the precision as  a function of feedback iterations. Experimental results have shown that  HRFSVM significantly outperforms two other RF algorithms.
A window-based flow control mechanism is a sort of feedback-based congestion control mechanisms, and has been widely used in current TCP/IP networks. Recently, use of an ECN (Explicit Congestion Notification) mechanism as congestion indication from the network to source hosts has been actively discussed in the IETF (Internet Engineering Task Force). In this paper, we focus on a window-based flow control mechanism, which cooperates with routers supporting the ECN mechanism. The first part of this paper discusses how the ECN mechanism can be incorporated into the TCP/IP network when all source hosts respond to ECN messages. The second part of this paper gives a control theoretic approach to the window-based flow control mechanism, which cooperates with ECN routers. We derive a stability condition of the window-based flow control mechanism, and show that system stability is affected by the router&apos;s buffer size as well as the bandwidth of the bottleneck router. We also show that the number of TCP connections is unrelated to the system stability. We further design a regulator for the window-based flow control mechanism, which utilizes the current window size and the estimated number of packets at the router&apos;s buffer as a feedback input. We show that the transient performance is significantly improved by applying the regulator. Several practical issues are also discussed. 1 
For a stationary two-dimensional random eld evolving in time, we derive the intensity distributions of appropriately dened velocities of crossing contours. The results are based on a generalization of the Rice formula. The theory can be applied to practical problems where evolving random elds are considered to be adequate models. We study dynamical aspects of deep sea waves by applying the derived results to Gaussian elds modeling irregular sea surfaces. In doing so, we obtain distributions of velocities for the sea surface as well as for the envelope eld based on this surface. Examples of wave and wave group velocities are computed numerically and illustrated graphically.
This paper addresses the problem of efficient image retrieval from a compressed image database, using information derived from the compression process. Images in the database are compressed applying two approaches: Vector Quantization (VQ) and Quadtree image decomposition. Both are based on Konohen&apos;s Self-Organizing Feature Maps (SOFM) for creating vector quantization codebooks. However, while VQ uses one codebook of one resolution to compress the images, Quadtree decomposition uses simultaneously 4 codebooks of four different resolutions. Image indexing is implemented by generating a Feature Vector (FV) for each compressed image. Accordingly, images are retrieved by means of FVs similary evaluation between the query image and the images in the database, depending on a distance measure. Three distance measures have been analyzed to assess FV index similarity: Euclidean, Intersection and Correlation distances. Distance measures efficiency retrieval is evaluated for different VQ resolutions and different Quadtree image descriptors. Experimental results using real data, esophageal ultrasound and eye angiography images, are presented.
Interface agents are computer programs that provide personalized assistance to users with their  computer-RMxL  tasks. Most interface agents achieve personalization by learning a user&apos;s preferences in a given application domain and assisting him according to them. In this work we adopt a different approach to personalization: how to personalize the interaction between interface agents and users in a  mixed-initiative interaction  context. We have empirically studied a set of interaction issues that agents have to take into account to achieve this goal and we present our results in this article. Some of these personalization issues are: discovering the type of assistant a user wants, learning when (and if) to interrupt the user, discovering how the user wants to be assisted in different contexts. As a result of our experiments, we have defined the components of a user interaction profile that models a user&apos;s interaction and assistance preferences. This profile will enable interface agents to enhance and personalize their interaction with users by discovering how to provide each user assistance of the right sort at the right time.
... this article, new tighter sufficiency conditions for slicibility of rectangular graphs are postulated and utilized in the generation of area-optimal floorplans. These graph-theoretic conditions not only capture a larger class of slicible rectangular graphs but also help in reducing the total effort for topology generation, and in solving problems of larger size.
Typically wire antenna structures are modeled by approximating curved structures with straight wire segments. The straight wire approximation yields accurate results, but often requires a large number of segments to adequately approximate the antenna geometry. The large number of straight wire segments or unknowns requires a large amount of memory and time to solve for the currents on the antenna. By using curved segments which exactly describe the contour of the antenna geometry the number of unknowns can be reduced, thus allowing for bigger problems to be solved accurately. This thesis focuses on the analysis of a helix antenna. The Method of Moments is used to solve for the currents on the antenna, and both the triangle basis and pulse testing functions exactly follow the contour of the helix antenna. The thin wire approximation is used throughout the analysis. The helix is assumed to be oriented along the z-axis with an optional perfect electric conductor (PEC) ground plane in the x-y plane. For simplicity, a delta gap source model is used. Straight feed wires may also be added to the helix, and are modeled similarly to the helix by the Method of Moments with triangular basis and pulse testing functions. The primary validation of the curved wire approach is through a comparison with MININEC and NEC of the convergence properties of the input impedance of the antenna versus the number of unknowns. The convergence tests show that significantly fewer unknowns are needed to accurately predict the input impedance of the helix, particularly for the normal mode helix. This approach is also useful in the analysis of the axial mode helix where the current changes significantly around one turn. Because of the varying current distribution, the improvement of impedance converg...
Interface (VCI) Standard. This paper reports recent experiences in formally verifying a few properties of a VCI-compliant PCI 2.1 bus wrapper model in the formal verification tool, FormalCheck. Though we chose to only verify three liveness properties and three safety properties, the verification highlighted issues buried deeply within the model, quickly. We found eight issues in our model in four person-weeks of verification effort.
This paper proposes an architecture design for a tool suitable for emulating DA-TDMA (demand-assignment-time division multiple access) satellite access schemes. The tool presented, named FRACAS    , is particularly suitable for comparing the performance of  different satellite channel allocation policies. Using FRACAS, a service provider can choose from different policies for sharing a satellite channel among a number of users. Some allocation policies, selected from those available in the literature, are built-in, while others can be designed from scratch and added without much effort. The parameters of the built-in allocation policies can easily be changed in order to exploit the full potential of the allocation schemes. FRACAS&apos;s features permit the optimisation of satellite resource usage in accordance with the traffic pattern supported. FRACAS enables  research teams and students to explore and compare different multiple access schemes,  and to develop simulation runs for various kinds of service-induced traffic, including  aggregate traffic, which is typical in a local area network (LAN) interconnection environment.
In this paper, we study the incremental deployability of the CoreStateless  Fair Queuing (CSFQ) approach to provide fair rate allocations in backbone  networks. We define incremental deployability as the ability of the approach  to gracefully provide increasingly better quality of service with each additional  QoS-aware router deployed in the network. We use the ns2 network simulator  for the simulations. We conclude that CSFQ does not exhibit good incremental  deployability.
We show that an analogue of the domain-theoretic least fixpoint operator can be defined in a purely set-theoretic framework. It can be formalized in classical higher order logic, serving as a solid foundation for proving termination of (possibly nested) recursive programs in a variety of mechanized proof systems.
The interpretation by Duskin and Glenn of abelian sheaf cohomology as  connected components of a category of torsors is extended to homotopy classes. This is  simultaneously an extension of Verdier&apos;s version of  Cech cohomology to homotopy.
Just as the social science literature is rich in models relevant to the software development process, these fields could benefit greatly from advances in knowledge representation methods used within software engineering and related disciplines. Here, we advocate the use of formal conceptual modelling techniques, commonly employed in software engineering work, for representing models and theories developed within the social sciences and, especially, management science fields. We illustrate our thesis through a brief description of an instance where a formal representation of activity theory was employed to capture critical &apos;softer&apos; factors within a system development project.
In Duan, Gauthier and Simonato (1999), an analytical approximate formula for European options in the GARCH framework was developed. The formula is however restricted to the nonlinear asymmetric GARCH model. This paper extends the same approach to two other important GARCH specifications - GJR-GARCH and EGARCH. We provide the corresponding formulas and study their numerical performance.
We consider the unique Hermitian connection with totally skew-symmetric torsion on a Hermitian manifold. We prove that if the torsion is parallel and the holonomy is Sp(n)U(1)      U(1), then the manifold is locally isomorphic to the twistor space of a quaternionic Kahler manifold with positive scalar curvature. If the manifold is complete, then it is globally isomorphic to such a twistor space. Keywords: Hermitian manifold, nearly Kahler manifold, quaternionic Kahler manifold, twistor space, connection with totally skew-symmetric torsion, holonomy group  MSC 2000: 53B05; 53B35; 53C26; 53C55 1 
This chapter deals with the cognitive underpinnings of voluntary action, here defined as goal-directed behaviour. It is delineates how voluntary action emerges through the automatic acquisition of bilateral associations between cognitive codes of movement patterns and sensory movement effects. Once acquired, these associations can be used in the backward direction to choose movement patterns by activating codes of intended outcomes (the LotzeHarle  principle). Actions are planned by specifying the features of intended outcomes, binding the activated feature codes, and integrating them with features of anticipated trigger stimuli. Integrated action plans are then carried out automatically as soon as the trigger stimulus is encountered.
this paper, we propose a set of techniques to largely automate the process of KA, by using technologies based on Information Extraction (IE), Information Retrieval and Natural Language Processing. We aim to reduce all the impeding factors mention above and thereby contribute to the wider utility of the knowledge management tools. In particular we intend to reduce the introspection of knowledge engineers or the extended elicitations of knowledge from experts by extensive textual analysis using a variety of methods and tools, as texts are largely available and in them -- we believe -- lies most of an organization&apos;s memory
This paper introduces a new probabilistic specification of reliable broadcast communication primitives, called ff-Reliable Broadcast. This specification captures in a precise way the reliability of practical broadcast algorithms that, on the one hand, were devised with some form of reliability in mind but, on the other hand, are not considered reliable according to &quot;traditional&quot; reliability specifications.
We study the Chvatal rank of polytopes as a complexity measure of unsatisfiable sets of clauses. We  discuss the relationship between the Chvatal rank and the minimum refutation length and height in the  cutting planes proof system. The main technical result is a general technique for deriving Chvatal rank  lower bounds directly from the syntactical form of the inequalities. We apply this technique to show  that the polytope of the Pigeonhole Principle requires logarithmic Chvatal rank. The bound is tight since  we also prove a logarithmic upper bound. We also apply the technique to the polytope of the Ramsey  Principle and obtain a loglogarithmic rank lower bound.
In the absence of an equalization grant system, like we justify occurs in  Spain at the local level of government, those governments that bear a decrease in their level  of tax capacity will have to adjust their budget either by increasing their level of tax effort,  by reducing their level of public services and/or by incurring in a deficit. By means of a  dynamic panel data analysis, and using a database from municipalities of the province of  Barcelona (1993-99), we describe that process of fiscal adjustment. About a 25% of the  shock is internalized through an increase in tax effort, a 35% through a reduction in public  expenditure (mainly investment), while the rest (40%) is covered by an increase in the level  of debt (i.e., the adjustment is delayed). However, this process of fiscal adjustment is very  much influenced by the political situation of the municipality. Coalition and minority  governments (&quot;weak&quot; governments) tend to delay the (unavoidable) fiscal adjustment, and  a 70% of their shock is covered by an increase in the level of debt, while the rest of  municipalities (&quot;strong&quot; governments) adjust immediately (80% reduction in public  expenditure and 20% increase in tax effort). Leftist governments tend to react mostly  through increases in tax effort, while rightist governments tend to reduce public  expenditure to a greater extent. Finally, we find that municipalities tend to react differently  in front of a negative (28%) and a positive (26%) shock with respect to the level of tax  effort. Hence, municipalities are relatively reluctant to decrease taxes.
An approach to system verification is described in which design artefacts produced during forward engineering are automatically compared to corresponding artefacts produced during reverse engineering. The goal is to automatically determine if an implementation is consistent with the original design. In the system described, XML Metadata Interchange (XMI) representations of Unified Modelling Language (UML) class diagrams are recovered from compiled Java class files. These are automatically compared with the corresponding diagrams produced during forward engineering by software engineers using CASE tools. Examples are provided in which reversed engineered UML class diagrams differ from those produced during forward engineering but are still faithful to the original design intent. Such differences are often due to more abstract system representations being captured in forward engineered design artefacts, the inclusion of design attributes and annotations that are not retained in the final implementation, and issues associated with the use of weakly typed containers. In other cases, differences indicate a deviation from the intended design. It is this latter type of difference that this paper is particularly interested in identifying. We advocate that an automated comparison of forward and reverse engineering artefacts should be performed during formal code inspection preparation and used to guide human review of the identified differences.
This paper addresses the problem of statically analyzing input command syntax as deffned in interface  and requirements speciffcations and then generating test cases for input validation testing. The IVT  ffInput Validation Testingfftechnique has been developed, a proof-of-concept tool ffMICASAff has been  implemented, and validation has been performed. Empirical validation on actual industrial software fffor  the Tomahawk Cruise Missileff shows that as compared with senior, experienced testers, MICASA found  more requirement speciffcation defects, generated test cases with higher syntactic coverage, and found  additional defects. Additionally, the tool performed at signiffcantly less cost.
this paper, we propose query optimization techniques for XML queries using DTDs. Our technique This work was supported by the Brain Korea 21 Project
The ability to estimate the Worst-Case Execution  Time (WCET) of software is essential for all forms  of timing analysis. In critical systems, there is a  need to obtain safe estimates which can only be  achieved via analysis. Traditionally, WCET analysis  approaches have been tightly coupled to specific  compilers and processors. The purpose of this  paper is to describe a framework that has been  developed for WCET analysis that is not tied to a  specific compiler or platform. Instead, existing  analysis has been generalised so that it can be  instantiated at run-time for a particular platform, in  this case using a purpose-built language. The  framework produced allows the tool to be retargeted  without (in most cases) modification/recompilation  of the analysis software.
GREP (Generalized Route Establishment Protocol) is a routing protocol for ad hoc networks that is particularly well suited to very mobile environments. In GREP, packets are routed through space and time, meaning that at each hop they advance either along their current route (thus advancing in space), or onto a fresher route (advancing in time) to their destination. We give a full definition of the protocol and prove that it is loop-free.
Network wide broadcasting is an energy intensive function. In this paper we propose a new method that performs transmission power adaptations based on information available locally, to reduce the overall energy consumed per broadcast. In most of the prior work on energy efficient broadcasting it is assumed that the originator of the broadcast has global network information (both topology information as well as the geographical distance between nodes). This can be prohibitive in terms of the consumed overhead. In our protocol, each node attempts to tune its transmit power based on local information (of up to two hops from the transmitting node). We perform extensive simulations to evaluate our protocol. Our simulations take into account the possible loss of packets due to collision effects and the additional re-broadcasts that are necessary due to lower power transmissions. We show that our protocol achieves almost the same coverage as other non power-adaptive broadcast schemes but with a reduction of approximately 40 % in terms of the consumed power as compared to a scheme that does not adapt its power.
The Distributed Computing Column covers the theory of systems that are composed of a  number of interacting computing elements. These include problems of communication and networking,  databases, distributed shared memory, multiprocessor architectures, operating systems,  veri  cation, internet, and the web.
PM, Propositional Model, is a highly interactive model of language comprehension which can be contrasted with language processing models which assume an autonomous syntactic component. Models assuming an autonomous syntax can be divided into two basic types: (a) Strong Autonomy Models which assume that only the part of speech of lexical items is available to the syntactic analyzer, and (b) Weak Autonomy Models which assume that information other than the part of speech, but of a purely syntactic nature, is available to syntactic analyzer. This paper presents the results of two experiments using the cumulative self-paced reading task which are intended to distinguish PM from both Strong and Weak Autonomy Models. The first experiment considers the influence of verb argument preferences on the initial determination of structure. The existence of such influences would argue against the Strong Autonomy Model and in favor of Weak Autonomy or Interactive Models. The second experiment considers the influence of object schematicity on the initial determination of structure. An influence of object schematicity argues against the Weak Autonomy Models and in favor of Interactive Models like PM. The results of the two experiments do indeed show effects of verb argument and object schematicity preferences. If the cumulative self-paced reading task is a legitimate measure of immediate, on-line processing, then Interactive Models are supported by these experiments and Autonomous models are not.
Economic theory deals with a complex reality, which may be seen through various perspectives, using different methods. Economics&apos; three major branches -- development economics, macroeconomics, and microeconomics -- cannot be unified because the former two use preferentially a historical-deductive, while the later, an essentially hypothetical-deductive or aprioristic method. While Smith, Marx and Keynes used the new historical facts method to develop their major theories, Walras adopted an aprioristic one to devise the neoclassical general equilibrium model. The historical-deductive method looks for the new historical facts that condition the new economic reality, and only then develop the stylized facts or economic model that generalizes the problem. Thus, economic theory remains central, but it is more modest, or less general, as the economist is content to develop short term macroeconomic models and long term growth models in the framework of a given historical phase or moment of the economic process. As a trade off, his models are more realistic and conducive to more effective economic policies, as long as he is not required to previously abandon, one by one, the unrealistic assumptions required by a excessively general theory, but already starts from more realistic ones. Classical development economics, founded by Smith and Ricardo, had in Marx and Schumpeter its two major followers.
Filtering of an image with rotated versions of an orientation selective filter yields a set of images which can be stacked to form an orientation space. Orientation space provides a means of analyzing overlapping and touching patterns, characterized by their orientation. In this paper we extend previous work and show that curved patterns may also be analyzed using orientation space. Orientation space allows us to decompose an image into a background and oriented components. The curvature of each of the individual components can be estimated by measuring the tilt of the locally planar response in orientation space. 1 
A lightweight collaborative web browsing system, that targets casual (non-technical) web users, is presented. This system allows remote participants to easily synchronize pointing, scrolling and browsing of uploaded content in their web browsers. Since instant messenging systems have become a very popular method for remote participants to engage in real-time text chat sessions, it is conjectured that this simple co-browsing system which allows remote participants to share and point at their pictures and web content (instead of just sharing text) could prove useful and fun, too. The collaboratively viewed web content could either pre-exist on a host web server or, in a more typical scenario, be dynamically uploaded by the remote participants themselves. A specific goal of this system is to keep the interactions extremely simple and safe. Any user should be able to use it intuitively with a single click; there are no pre-installation or pre-registration requirements. This system is based on simple polling-based scripting techniques that avoid intrusive mechanisms based on proxies or events. Most significantly, there is no reliance on any controls, applets, plug-ins or binary executables, since these would require the trust of participants and are virus-prone. It is the reliance upon such inconvenient helper programs, along with any pre-installation or pre-registration requirements, that makes existing co-browsing offerings more &quot;heavyweight&quot;, and limits their appeal for casual collaboration. Keywords Web collaboration, synchronized browsing, real-time distributed collaboration, shared web browsing, instant messenging, co-browsing Word count Approximately 7500 words 1. 
Answer Set Programming (ASP) is a formalism widely used  for knowledge representation since its introduction in 1988 by Gelfond  and Lifschitz [4]. Nowadays there are powerful implementations of this  paradigm, like DLV    and Smodels    . In order to increment the descriptive  power of this tools, several extensions to their languages have been done. For example
Shlomi Dolev    Seth Gilbert    Nancy A. Lynch    Elad Schiller    Alex A. Shvartsman  zy  Jennifer Welch  February 24, 2004  Abstract  One of the most significant challenges introduced by mobile networks is the difficulty in coping with  the unpredictable movement of mobile nodes. If, instead, the mobile nodes could be programmed to  travel through the world in a predictable and useful manner, the task of designing algorithms for mobile  networks would be significantly simplified. Alas, users of mobile devices in the real world are not  amenable to following instructions as to where their devices may travel.
In [KK], a new identification scheme based on the Gap Diffie-Hellman problem was proposed at SCIS 2002, and it is shown that the scheme is secure against active attacks under the Gap Diffie-Hellman Intractability Assumption. Paradoxically, this identification scheme is totally breakable under passive attacks. In this paper, we show that any adversary holding only public parameters of the scheme can convince a verifier with probability 1.
Ovarian masses are a common problem in gynaecology.A reliable test for preoperative discrimination between benign and mali gnant ovarian tumors is of considerable help for clinicians in choosing approp riate treatments for patients. This study was carried out to generate and evaluate both logistic regression models and artificial neural network (ANN) models to predict m alignancy of ovarian tumors, using patient data collected at the University Hosp itals of Leuven between 1994 and 1997. The first part of the report details the stat istical analysis of the ovarian tumor dataset, including explorative univariate and multi variate analysis, and the development of the logistic regression models. The input variable selection was conducted via logistic regression as well. In the s econd part of the report, we describe the development of several types of feed f orward neural networks such as multi-layer perceptrons (MLPs) and generalized regr ession networks (GRNNs). The issue of model validation is also addressed. Our ad apted strategy for model evaluation is to perform Receiver Operating Charate ristic (ROC) curve analysis, using both a temporal holdout cross validation (CV) and multiple runs of K-fold CV. The experiments confirm that neural network cla ssifiers have the potential to give a more reliable prediction of the malignancy o f ovarian tumors based on patient data. Contents  PART I Statistical Analysis  0. 
this paper.  the horizontal emittance growth induced by synchrotron radiation in the switchyard arcs must be tolerable
We present a calculus that integrates equality handling by superposition into a free variable tableau calculus. We prove completeness of this calculus by an adaptation of the model generation [1, 15] technique commonly used for completeness proofs of resolution calculi. The calculi and the completeness proof are compared to earlier results of Degtyarev and Voronkov [8].
Two-component theories of intellectual development over the life span postulate that fluid abilities develop earlier during child development and decline earlier during aging than crystallized abilities do, and that fluid abilities support or constrain the acquisition and expression of crystallized abilities. Thus, maturation and senescence compress the structure of intelligence by imposing age-specific constraints upon its constituent processes. Hence, the couplings among different intellectual abilities and cognitive processes are expected to be strong in childhood and old age. Findings from a populationbased study of 291 individuals aged 6 to 89 years support these predictions. Furthermore, processing robustness, a frequently overlooked aspect of processing, predicted fluid intelligence beyond processing speed in old age but not in childhood, suggesting that the causes of more compressed functional organization of intelligence differ between maturation and senescence. Research on developmental changes in functional brain circuitry may profit from explicitly recognizing transformations in the organization of intellectual abilities and their underlying cognitive processes across the life span.
The basic premise of vibration-based damage detection is that damage will significantly alter the stiffness, mass, or energy dissipation properties of a system, which, in turn, alter the measured dynamic response of the system. Although the basis for vibration-based damage detection appears intuitive, its actual application poses many significant technical challenges. A fundamental challenge is that in many situations vibration-based damage detection must be performed in an unsupervised learning mode. Here, the term unsupervised learning implies that data from damaged systems are not available. These challenges are supplemented by many practical issues associated with making accurate and repeatable vibration measurements at a limited number of locations on complex structures often operating in adverse environments. This paper will discuss two statistical methods for approaching the unsupervised learning damage detection problem. The first method is density estimation and significance testing. The second method is statistical process control. Examples of these methods are applied to data from an undamaged and subsequently damaged concrete column.
The quest for general theory of aging and longevity has become an important part of biodemographic studies (Carey &amp; Judge, 2001). In this paper we suggest to include the reliability theory in the theoretical arsenal of biodemographic research for developing a comprehensive theory of aging and longevity. Reliability theory is a general theory about systems failure. It allows researchers to predict the age-related failure kinetics for a system of given architecture (reliability structure) and given reliability of its components. Reliability theory predicts that even those systems that are entirely composed of non-aging elements (with a constant failure rate) will nevertheless deteriorate (fail more often) with age, if these systems are redundant in irreplaceable elements. Aging, therefore, is a direct consequence of systems redundancy. Reliability theory also predicts the late-life mortality deceleration with subsequent leveling-off, as well as the late-life mortality plateaus, as an inevitable consequence of redundancy exhaustion at extreme old ages. The theory explains why mortality rates increase exponentially with age (the Gompertz law) in many species, by taking into account the initial flaws (defects) in newly formed systems. It also explains why organisms &apos;prefer&apos; to die according to the Gompertz law, while technical devices usually fail according to the Weibull (power) law. Theoretical conditions are specified when organisms die according to the Weibull law: organisms should be relatively free of initial flaws and defects. The theory makes it possible to find a general failure law applicable to all adult and extreme old ages, where the Gompertz and the Weibull laws are just special cases of this more general failure law. The theory explains why relative difference...
This essay considers the problem of defining the context that context aware systems should pay attention to from a human perspective. In particular, we argue that there are human aspects of context that cannot be sensed or even inferred by technological means, so context aware systems cannot be designed simply to act on our behalf. Rather they will have to be able to defer to users in an efficient and non-obtrusive fashion. Our point is particularly relevant for systems that are constructed such that applications are architecturally isolated from the sensing and inferencing that governs their behavior. We propose a design framework that is intended to guide thinking about accommodating human aspects of context. This framework presents four design principles that support  intelligibility of system behavior and accountability of human users and a number of human-salient details of context that must be accounted for in context aware system design. 
The purpose of this document is to advance NewReno TCP&apos;s Fast  Retransmit and Fast Recovery algorithms in RFC 2582 from Experimental to Standards Track status.
Semi-automatic data mining approaches often yield better results than  plain automatic methods, due to the early integration of the user&apos;s goals. For  example in the medical domain, experts are likely to favor simpler models instead  of more complex models. Then, the accuracy of discovered patterns is often not  the only criterion to consider. Instead, the simplicity of the discovered knowledge  is of prime importance, since this directly relates to the understandability and the  interpretability of the learned knowledge.
This paper discusses the use of spatial methods for the display of non-Geographic data
In [1], we studied amplitude estimation of one-dimensional (1-D) sinusoidal signals from measurements corrupted by possibly colored observation noise. We herein extend the results for twodimensional (2-D) amplitude estimation. In particular, we investigate the 2-D sinusoidal amplitude estimation within the general frameworks of least squares (LS), weighted least squares (WLS), and MAtched FIlterbank (MAFI) estimation. A variety of 2-D amplitude estimators are presented, which are all asymptotically statistically efficient. The performances of these estimators in finite samples are compared numerically with one another. Making use of amplitude estimation techniques, we introduce a new scheme for 2-D system identification, which is shown to be computationally simpler and statistically more accurate than the conventional output error method (OEM), when the observation noise is colored.
We propose a novel information visualization approach for an analytical method applied in the social sciences. In social network analysis, social structures are formally represented as graphs, and structural properties of these graphs are assumed to be useful in the explanation of social phenomena. A particularly important such property is the relative status of actors in a given network. Since operationalists...
This essay proposes a new approach to risk evaluation using disease  mathematical modeling. The mathematical model is an algebraic equation of the  available database attributes and is used to evaluate the patient condition. If its  value is greater than zero it means that the patient is ill (or in risk condition),  otherwise healthy. In practice risk evaluation has been a very difficult problem  mainly due its sporadic behavior (suddenly, the patient has a stroke, etc as a  condition aggravation) and its database representation. The database contains,  under the label of risk patient data, information of the patient condition that  sometimes is in risk condition and sometimes is not, introducing errors in the  algorithm training. The study was applied to Atherosclerosis database from  Discovery Challenge 2003 - ECML/PKDD 2003 workshop.
We theoretically investigate vertical high-field transport in semiconductor superlattices...
This paper describes ways of cooperation between the Department of Control Systems and Instrumentation and industry, and with companies representing a main role in industrial automation from the point of view of their contribution to education and ways of motivating students. This cooperation has been developing through the last decade by various means implemented into teaching. One of them is a competition of students from the field of Control Systems and Applied Informatics who present their projects, which are leading in their diploma thesis. Further, this paper presents an overview of some of the students&apos; projects, their techniques on how to complete a  project, with results which can develop teaching curriculum and be used during teaching again, as well as the results  developing skills and abilities, which students take with them after graduating from University, put them into practice and perhaps coming back in the future as specialists and role models for the next student generation.
To determine the dynamics of the spatial extent of gallery forest on Konza Prairie Research Natural Area (KPRNA), aerial photographs taken over a 46 year time frame were digitized into anARC-INFO Geographic Information System A Global Positioning System (GPS) was used to collect ground control points to co-register the photographs for each year. Gallery forest areas for the three major drainage boundaries (Kings Creek, Shane Creek, and White Pasture) were analyzed to assess the uniformity of change in the land-  scape system. Results indicated that the total gallery forest area on KPRNA has increased in area from 157  ha in 1939 to over 241 ha in 1985. During this time, there was an increase in the total number of patches and a decrease in the mean size of forest patches. However, the rate of increase was not consistent over this time period, nor was it uniform from one drainage basin or stream order to another. Detailed spatial analysis of the forested area with a geomorphology and digital elevation model of Konza Prairie showed that in 1985,  58% of the forest was on soil, yet only 15% of that soil type was forested. In addition, over of the forest was on the slope interval, but only of that slope interval was forested. These results may be attributed to a variety of factors such as changing management practices frequency of fires and herbicide spraying) and the temporal constraints on extent to which the gallery forest can expand across the landscape. 
We show a sharp dichotomy between systems of identical automata with a symmetric global control whose behavior is easy to predict, and those whose behavior is hard to predict. The division pertains to whether the global control rule is invariant with respect to permutations of the states of the automaton. On the other hand, we show that testing whether the global control rule has this invariance property is an undecidable problem.
In this paper, we propose Neighborhood Matchmaker Method  as a re-configration method for personal human network.
We present a method for automatically learning a set of discriminatory facial components for face recognition. The algorithm performs an iterative growing of components starting with small initial components located around preselected points in the face. The direction of growing is determined by the gradient of the cross-validation error of the component classifiers. In experiments we analyze how the shape of the components and their discriminatory power changes across different individuals and views.
We propose a Content Delivery Network (CDN) with servers arranged hierarchically in multiple tiers. Lower-tier servers are topologically closer to the clients, and hence can deliver better QOS in terms of end-to-end delay and jitter. On the other hand, highertier servers have a larger coverage area and hence their clients incur fewer server handoffs. We present a server selection scheme that reduces the number of server handoffs while meeting differentiated QoS requirement for each client. The scheme dynamically estimates the client&apos;s residence time and uses a simple algorithm to assign clients to the appropriate tier. The scheme also caters for traditional server selection criteria, such as the expected QoS from the servers, bandwidth consumption, and the server load. We show through simulations that the scheme can achieve up to 15% reduction in handoffs at the cost of minimal increases in delay and jitter while ensuring that clients of different QoS classes experience different delays.
This paper is concerned with probabilistic approximation of metric spaces. In previous work we introduced the method of ecient approximation of metrics by more simple families of metrics in a probabilistic fashion. In particular we study probabilistic approximations of arbitrary metric spaces by \hierarchically wellseparated tree&quot; metric spaces. This has proved as a useful technique for simplifying the solutions to various problems.
this paper we question the orthodoxy that ubiquitous computing systems should be . We briefly explore some notions of seamlessness and suggest, by example, why alternative approaches may be required. Rather than a fully-formed model, this is an initial step towards a new conceptual framework for understanding interaction to frame the design of ubiquitous computing. It is based on our development of a number of systems in the EQUATOR Interdisciplinary Research Collaboration
Decision tree construction is an important data mining problem. In this paper, we revisit this problem, with a new goal, i.e. Can we develop an efficient parallel algorithm for decision tree construction that can be parallelized in the same way as algorithms for other major mining tasks ?.
Introduction  1.1 Design  The objective of engineering analysis, the mathematical and computational modelling of an engineering product, is not to determine the behaviour of a single product; if this were the case it would be simpler and cheaper to take the first manufactured product and test it extensively. The real objective of engineering analysis is to predict the behaviour of a product, and then use that information to design a better product. Often a whole sequence of improved designs is created, with very few being actually manufactured and tested experimentally. Given that experimental testing is often very time-consuming and expensive, the extensive use of computational engineering analysis can greatly reduce the time and cost of the design process.  Although design is the ultimate objective of engineering analysis, most academic research in the past 20 years has been directed towards the simpler task of modelling the behaviour of specific aspects of the engineering system. Th
We introduce an architecture for &quot;Intradomain Overlays&quot;, where a subset of routers within a domain is augmented with a dedicated host. These strategically placed hosts form an overlay network, and we describe a number of applications for such overlays. These applications include efficient network monitoring, policyand load-based packet re-routing, and network resource accounting.
In this paper we address the subject of reducing the impact of phase noise on a QAM-OFDM system transmitting over an AWGN channel. Phase noise is known to have two effects on OFDM systems, rotating each symbol by a different common phase rotation (CPR) and producing an intercarrier interference term (ICI) that adds to the channel noise. We present two novel algorithms that remove CPR and reduce the amount of ICI using different approaches to the ICI problem. Their performance in terms of bit error rate (BER) are evaluated by simulation using symbol by symbol or maximum likelihood sequence detection (MLSD). The results show that these algorithms can significantly reduce the BER floor of the system while still maintaining an acceptable throughput.
We argue that in a distributed context, such as the Semantic Web, ontology  engineers and data creators often cannot control (or even imagine) the possible  uses their data or ontologies might have. Therefore ontologies are unlikely  to identify every useful or interesting classification possible in a problem domain,  for example these might be of a personalised nature and only appropriate for a  certain user in a certain context, or they might be of a different granularity than  the initial scope of the ontology. We argue that machine learning techniques will  be essential within the Semantic Web context to allow these unspecified classifications  to be identified. In this paper we explore the application of machine  learning methods to FOAF, highlighting the challenges posed by the characteristics  of such data. Specifically, we use clustering to identify classes of people  and inductive logic programming (ILP) to learn descriptions of these groups. We  argue that these descriptions constitute re-usable, first class knowledge that is  neither explicitly stated nor deducible from the input data. These new descriptions  can be represented as simple OWL class restrictions or more sophisticated  descriptions using SWRL. These are then suitable either for incorporation into  future versions of ontologies or for on-the-fly use for personalisation tasks.
A phylogenetic tree represents historical evolutionary relationships between different species or organisms. The space of possible phylogenetic trees is both complex and exponentially large. Here we study combinatorial features of neighbourhoods within this space, with respect to four standard tree metrics. We focus on the splits of a tree: the bipartitions induced by removing a single edge from the tree. We characterize those splits appearing in trees that are within a given distance of the original tree, demonstrating close connections between these splits, the Whitney number of a tree, and the binary characters with a given parsimony length.
There is a growing trend toward emotional intelligence in human-computer  interaction paradigms. In order to react appropriately to a human, the  computer would need to have some perception of the emotional state of the human. We assert
This paper discusses how stereo vision achieved through the use of omnidirectional sensors can help mobile robot navigation providing advantages, in terms of both versatility and performance, with respect to the classical stereo system based on two horizontally-displaced traditional cameras.
We describe the application of kernel methods to Natural Language Processing  (NLP) problems. In many NLP tasks the objects being modeled  are strings, trees, graphs or other discrete structures which require some  mechanism to convert them into feature vectors. We describe kernels for  various natural language structures, allowing rich, high dimensional representations  of these structures. We show how a kernel over trees can  be applied to parsing using the voted perceptron algorithm, and we give  experimental results on the ATIS corpus of parse trees.
We have implemented an application-independent collaboration manager, called Collagen,  based on the SharedPlan theory of discourse, and used it to build a software interface  agent for a simple air travel application. The software agent provides intelligent, mixed  initiative assistance without requiring natural language understanding. A key benefit of  the collaboration manager is the automatic construction of an interaction history which  is hierarchically structured according to the user&apos;s and agent&apos;s goals and intentions.
We present an efficient solution to the Eikonal equation on parametric manifolds, based on the fast marching approach. This method overcomes the problem of a non-orthogonal coordinate system on the manifold by creating an appropriate numerical stencil. The method is tested numerically and demonstrated by calculating distances on various parametric manifolds. It is further used for two applications: image enhancement and face recognition.  
Learning spatial models from sensor data raises the challenging data association problem of relating model parameters to individual measurements. This paper proposes an EM-based algorithm, which solves the model learning and the data association problem in parallel. The algorithm is developed in the context of the the structure from motion problem, which is the problem of estimating a 3D scene model from a collection of image data. To accommodate the spatial constraints in this domain, we compute virtual measurements as sufficient statistics to be used in the M-step. We develop an efficient Markov chain Monte Carlo sampling method called chain flipping, to calculate these statistics in the E-step. Experimental results show that we can solve hard data association problems when learning models of 3D scenes, and that we can do so efficiently. We conjecture that this approach can be applied to a broad range of model learning problems from sensor data, such as the robot mapping problem. 
We present strategies and results for identifying the symbol type of every character  in a text document. Assuming reasonable word and character segmentation for  shape clustering, we designed several type recognition methods that depend on cluster  n-grams, characteristics of neighbors, and within-word context. On an ASCII test  corpus of 925 articles, these methods represent a substantial improvementover default  assignmentofallcharacters to lower case.
We describe a method of visualising geometrically the dependency structure of a distributed representation. The mutual information between each pair of components is estimated using a nonlinear correlation coeff- cient, in terms of which a distance measure is defined. Multidimensional scaling is then used to generate a spatial configuration that reproduces these distances, the end result being a spatial representation of the dependency between the components, from which an appropriate topology for the representation may be inferred. The method is applied to ICA representations of speech and music.
Current methods for facial reconstruction are tedious and time-consuming, and require forensic  artists with years of practical experience. Furthermore, the complexity of the reconstruction  problem greatly increases when time-related factors come into play, such as those that  occur in missing children scenarios. This thesis describes a software system for simulating the  growth of the craniofacial skeleton. It is a first step towards our goal of a complete software  package for three-dimensional craniofacial reconstruction. There is a tremendous amount of  data on craniofacial growth in the form of studies that collect frontal and lateral cephalograms,  which can be used to generate three-dimensional coordinates of landmarks on the  craniofacial skeleton at various ages. We define a simplified model of bone growth that uses  these landmarks to drive the growth of the rest of the craniofacial skeleton. The inputs to our  growth model include a triangular mesh acquired from the bone to be grown (e.g. skull, mandible)  , a set of vertices on the mesh identified as landmarks, the coordinates of these landmarks  through time, and vertex weights which are a measure of the influence exerted by  landmarks on the rest of the vertices. The output is a triangular mesh, &quot;grown&quot; either forwards  or backwards in time to a specified age. An expert in craniofacial growth assigns these vertex  weights by using a specialized tool called Krayola. We also provide a tool for automatically  generating a first approximation for the vertex weights of a new mesh given the weights previously  assigned to a mesh of similar bone type (e.g. skull, mandible). Validation of our  growth model is an outstanding issue; we lack three-dimensional data (e.g. from CT scans)  for an individual through time...
Let ff be a triangulation of a topological n ball embedded in R    . Rose has conjectured that if the module C    ( ff) of piecewise polynomial functions of smoothness r on ff is free, then so is C    ( ff). For n = 2, we prove the conjecture, and we show that it is false in all higher dimensions.
A subset S = {s_1, ..., s_k} of an abelian group G is called an S_t-set of size k if al sums of t different elements in S are distinct. A function with applications in coding theory, v_i(k) denotes the order of the smallest cyclic group in which an S_2-set of size k exists. A lower bound for v_i(k) is given in this study, and exact values of v_i(k) are obtained for k &amp;le; 15. For the related problem in which all sums of any two, not necessarily distinct, elements in S are required to be different, values of the corresponding function v_&amp;delta;(k) for each k &amp;le; 14 are given.
Effective coordination in partially observable MAS requires agent actions to be based on reliable estimates of nonlocal states. One way of generating such estimates is to allow the agents to share state information that is not directly observable. To this end, we propose a novel strategy of delayed distribution of state estimates. Our empirical studies of this mechanism demonstrate that individual reinforcement-learning agents in a simulated network routing problem achieve a significant improvement in the overall success, robustness, and efficiency of routing compared with the standard Q-routing algorithm.
The Java Virtual Machine (JavaVM) has contributed greatly  to Java&apos;s success because it provides a common intermediate format  which can be shared across the Internet. Unfortunately, the JavaVM  has been optimized for an interpreted model, resulting in inferior performance  because its stack-based execution model cannot exploit instructionlevel  parallelism. The inherent serialization of the stack execution model  can be addressed either by using compilation techniques or by hardware. In this article
ATM (asynchronous transfer mode) aims at providing both guaranteed bandwidth to support real-time communications and dynamic bandwidth sharing to accommodate bursty data traffic. Achievement of this goal is vital to make ATM an enabling technology for the future integrated digital networks. To realize this objective, good algorithms for controlling network traffic are required. This paper proposes a traffic control scheme which uses a timed-round-robin cell transmission scheduling algorithm enhanced with a simple feedback flow control mechanism to realize the promised advantages of ATM networks. Specifically, with the proposed scheme, a network is able to provide each user with (1) a guaranteed bandwidth, (2) immediate access to the full link bandwidth when there is no contention from other users, and (3) free of cell losses. An ATM switch design is also presented which shows the feasibility of implementing the proposed scheme with today&apos;s switch architectures without adding much extra cost.
In a first successful attempt to rigorously derive the Boltzmann equation from a many-particle system, M. Kac introduced a stochastic model, in which particle velocities make jumps due to random interactions between pairs of particles. In this 
Magnetic force microscopy (MFM) is a widely used form of scanning probe microscopy (SPM) that is used for obtaining a magnetic image from a surface with nanoscale resolution. Currently, the resolution of MFM is limited to approximately 20 nm due to the long-range nature of magnetic interactions between the MFM probe&apos;s tip and the magnetic medium. This paper describes an optimal signal processing solution to the problem of achieving high resolution MFM with the goal of significantly exceeding the level at which MFM metrology currently performs, thus providing metrologists with a means of resolution increase that is believed will enable, for example, the rapid development of high-density magnetic recording media (&gt; 100GBit/in    ). This is achieved by using focused-ion beam trimming of a conventional tip to create a probe tip of predictable magnetic characteristics and then, provided that the sample could be approximated as a thin-film, using knowledge of this tip&apos;s sensitivity field for performing a deconvolution on the measured signal to better estimate the magnetic state of the surface under study. Stated differently, as the MFM measured signal is modeled as the convolution of the magnetised surface and the tip&apos;s sensitivity field, we exploit knowledge of the tip&apos;s properties to increase the resolution of the MFM image. Details of the deconvolution approach as well as images resulting from this processing are the focus of this paper.
effcientpr ocessing of similar ity joins is impor tant for a lar ge class of applications. The dimensionality of the data for these applicationsr angesfr om low to high. Most existing methods have focussed on the execution of high-dimensional joins over lar3 amounts of diskbased data. The incr easing sizes of main memor y available on cur r ent computer s, and the need for effcient pr ocessing of spatial joins suggest that spatial joins for a lar5 class of pr oblems can be pr ocessed in main memor . In this paper we develop two new spatial join algor thms, the Gr id-join and EGO*-join, and study their per o r ance in compar son to the state of the a r algor thm, EGO-join, and the RSJ algor thm. Thr ough evaluation we explor e the domain of applicability of each algor ithm and pr ovide r ecommendations for the choice of join algor ithm depending upon the dimensionality of the data as well as the cr itical ff  . We also point out the significance of the choice of this parI932C for ensur4I that the selectivity achieved isr easonable. The pr5 osed EGO*-join algor thm always, often significantly, outperor s the EGO-join. For low-dimensional data the Gr id-join outperor both the EGO- and EGO*- joins. An analysis of the cost of the Gr id-join is pr]11 ted and highly accuru e cost estimator functions a r developed. These a r used to choose an appr opr ate g r d size for optimal per o r ance and can also be used by a quer y optimizer to compute the estimated cost of theGr id-join. 1
A computational approach to the optimization of service properties of two-phase materials (in this case, fracture resistance of tool steels) by varying their microstructure is developed. The main points of the optimization of steels are as follows: (1) numerical simulation of crack initiation and growth in real microstructures of materials with the use of the multiphase finite elements (MPFE) and the element elimination technique (EET), (2) simulation of crack growth in idealized quasi-real microstructures (net-like, band-like and random distributions of the primary carbides in the steels) and (3) the comparison of fracture resistances of different microstructures and (4) the development of recommendations to the improvement of the fracture toughness of steels. The fracture toughness and the fractal dimension of a fracture surface are determined numerically for each microstructure. It is shown that the fracture resistance of the steels with finer microstructures is sufficiently higher than that for coarse microstructures. Three main mechanisms of increasing fracture toughness of steels by varying the carbide distribution are identified: crack deflection by carbide layers perpendicular to the initial crack direction, crack growth along the network of carbides and crack branching caused by damage initiation at random sites.
A gesture recognition system which can reliably recognize single-hand gestures  in real time on a 600Mhz notebook computer is described. The system has a  vocabulary of 46 gestures including the American sign language letterspelling  alphabet and digits. It includes mouse movements such as drag and drop, and is  demonstrated controlling a windowed operating system, editing a document and  performing file-system operations with extremely low error rates over long time  periods. Real-time
The New Jersey Institute of Technology (NJIT) has participated in the NSF/Gateway Engineering Education  Coalition since its inception in 1992. Through this program,  NJIT has improved its undergraduate programs and engineering education, both in quantifiable ways and through culture changes within the institution. By incorporating design work in the freshman year through its  Fundamentals of Engineering Design courses, NJIT has significantly improved its student retention and 6-year graduation rates for engineering students. Currently, NJIT is working with New Jersey&apos;s community colleges to incorporate some of these innovations into their curricula, reaching beyond the boundaries of the Gateway Coalition.
This paper presents results of a project initiated by ASHRAE and the National Research Council of Canada. The project applies both physical and numerical modeling techniques to atrium smoke exhaust systems to investigate the effectiveness of such systems and to develop guidelines for their design.
The availability of high-speed processors means that drives are increasingly able to perform traditional control tasks. Integrated technology functions such as position control, encoder synchronization and electronic gears have made it possible to replace mechanical components with electronically synchronized independent drives. On the other
Block coders are among the most common compression tools available for still images and video sequences. Their low computational complexity along with their good performance make them a popular choice for compression of natural images. Yet, at low bitrates, block coders introduce visually annoying artifacts into the image. One approach that alleviates this problem is to downsample the image, apply the coding algorithm, and interpolate back to the original resolution. In this paper, we consider the use of optimal decimation and interpolation filters in this scheme. We first consider only optimization of the interpolation filter, by formulating the problem as least-squares minimization. We then consider the joint optimization over both the decimation and the interpolation filters, using the Variable Projection method. The experimental results presented clearly exhibit a significant improvement over other approaches.
Existing navigation services, such as GPS, offer no signal-integrity (anti-spoof) protection for the general public, especially not with systems for remote attestation of location, where an attacker has easy access to the receiver antenna. With predictable broadcast signals, the antenna can be replaced with a signal generator that simulates a signal as it would be received elsewhere. With a symmetrically encrypted broadcast signal, anyone who can build or reverse engineer a receiver will know the secret key needed to spoof other receivers. Such encryption is only of use in closed user communities (e.g., military) or with highly tamper-resistant modules protecting the common key. In open user communities without common secret keys, integrity protection is needed instead, with properties similar to digital signatures. The ability to verify a navigation signal must be separate from the ability to generate a new one or to apply selective-delay attacks; but simply signing the broadcast signals will not protect their exact relative arrival times. This paper introduces a practical solution based on short-term information hiding.
We present deep Hubble Space Telescope single-star photometry of Leo A in B, V, and I. Our new field of view is offset from the centrally located field observed by Tolstoy et al. (1998) in order to expose the halo population of this galaxy. We report the detection of metal-poor red horizontal branch stars, which demonstrate that Leo A is not a young galaxy. In fact, Leo A is as least as old as metal-poor Galactic Globular Clusters which exhibit red horizontal branches, and are considered to have a minimum age of about 9 Gyr. We discuss the distance to Leo A, and perform an extensive comparison of the data with stellar isochrones. For a distance modulus of 24.5, the data are better than 50% complete down to absolute magnitudes of 2 or more. We can easily identify stars with metallicities between 0.0001 and 0.0004, and ages between about 5 and 10 Gyr, in their postmain -sequence phases, but lack the detection of main-sequence turnoffs which would provide unambiguous proof of ancient (&gt;10 Gyr) stellar generations. Blue horizontal branch stars are above the detection limits, but diffcult to distinguish from young stars with similar colors and magnitudes. Synthetic color-magnitude diagrams show it is possible to populate the blue horizontal branch in the halo of Leo A. The models also suggest  ff50%  of the total astrated mass in our pointing to be attributed to an ancient (&gt;10 Gyr) stellar population. We conclude that Leo A started to form stars at least about 9 Gyr ago. Leo A exhibits an extremely low oxygen abundance, of only 3% of Solar, in its ionized interstellar medium. The existence of old stars in this very oxygen-deficient galaxy illustrates that a low oxygen abundance does not preclude a history of early star formation. Subject headings: Galaxies: irregular --- galaxi...
It is not uncommon for oscillatory electric power system modes to move close to a resonance in which eigenvalues coincide. In a weak resonance the modes are decoupled and the eigenvalues do not interact. We analyze general perturbations of a weak resonance and find two distinct behaviors, including interactions near strong resonances in which the eigenvalues quickly change direction. The possible perturbations are illustrated with interactions between electromechanical modes in a 4 generator power system. Some of the interactions are similar to subsynchronous resonance and can lead to oscillatory instability. Index Terms--- power system dynamic stability, oscillations, resonance, root loci, eigenvalues and eigenfunctions, Hopf bifurcation, subsynchronous resonance  I. 
This paper describes a representation of plan cases as a structured  set of goals and actions. These goals and actions are the unit pieces that form a  case. These case pieces are related each other by hierarchical and temporal  links (explanations) forming a tree-like network. We give importance not just  to explicit links, i.e., links between case pieces which are concretely known,  but also to implicit ones, i.e., possibly unknown links between case pieces.
Java is becoming the main software platform for consumer and embedded devices such as mobile phones, PDAs, TV set-top boxes, and in-vehicle systems. Since many of these systems are memory constrained, it is extremely important to keep the memory footprint of Java applications under control.
This paper presents a design process for developing service management systems which are  required to integrate &amp; co-operate with management services across different service  provider administrations, organisations and technologies. The development of such cooperative  management systems is both heavily time consuming and very complex. This paper  describes an integrated design process which supports the rigorous design and rapid  implementation of such management services. The design process incorporates enterprise  modelling and object oriented design techniques for requirements capture and description,  system development, implementation and deployment. It also facilitates the generation of  service management system specifications based on Open Distributed Processing standards.
A representational gap exists between low-level measurements (segmentation, object classification, tracking) and high-level understanding of video sequences. In this paper, we propose a novel representation of events in videos to bridge this gap, based on the CASE representation of natural languages. The proposed representation has three significant contributions over existing frameworks. First, we recognize the importance of causal and temporal relationships between sub-events and extend CASE to allow the representation of temporal structure and causality between sub-events. Second, in order to capture both multi-agent and multi-threaded events, we introduce a hierarchical CASE representation of events in terms of sub-events and case-lists. Last, for purposes of implementation we present the concept of a temporal event-tree, and pose the problem of event detection as subtree pattern matching. By extending CASE, a natural language representation, for the representation of events, the proposed work allows a plausible means of interface between users and the computer. We show two important applications of the proposed event representation for the automated annotation of standard meeting video sequences, and for event detection in extended videos of railroad crossings.
this article should be addressed to E. Ruthruff, Department of Psychology, University of California--San Diego, La Jolla, CA 92093-0109, or to J. Miller, Department of Psychology, University of Otago, P.O. Box 56, Dunedin, New Zealand
images, videos or audio  les easy. While this enables numerous new applications, a certain loss of trust in digital media can be observed. In general, there is no guarantee that a digital image \does not lie&quot;, i.e., that the image content was not altered. To counteract this risk, fragile watermarks were proposed to protect the integrity of digital multimedia objects. In high security applications, it is necessary to be able to reconstruct the original object out of the watermarked version. This can be achieved by the use of invertible watermarks. While traditional watermarking schemes introduce some small non-invertible distortion in the digital content, invertible watermarks can be completely removed from a watermarked work. In the past, the security of proposed image authentication schemes based on invertible watermarks was only analyzed using ad-hoc methods and neglected the possibility of malicious attacks, which aim at engineering a fake mark so that the attacked object appears to be genuine. In this paper, we characterize and analyze possible malicious attacks against watermark-based image authentication systems and explore the theoretical limits of previous constructions with respect to their security.
This document describes PNG (Portable Network Graphics), an  extensible file format for the lossless, portable, well-compressed storage of raster images. PNG provides a patent-free replacement for  GIF and can also replace many common uses of TIFF. Indexed-color,  grayscale, and truecolor images are supported, plus an optional alpha channel. Sample depths range from 1 to 16 bits.
We present improved techniques for finding homologous regions in DNA and protein sequences. Our approach focuses on the core region of a local pairwise alignment; we suggest new ways to characterize these regions that allow marked improvements in both specificity and sensitivity over existing techniques for sequence alignment. For any such characterization, which we call a vector seed, we give an efficient algorithm that estimates the specificity and sensitivity of that seed under reasonable probabilistic models of sequence. We also characterize the probability of a match when an alignment is required to have multiple hits before it is detected. Our extensions fit well with existing approaches to sequence alignment, while still offering substantial improvement in runtime and sensitivity, particularly for the important problem of identifying matches between homologous coding DNA sequences.
This paper introduces a new method for representing two-dimensional maps, and shows how this representation may be applied to concurrent localization and mapping problems involving multiple robots. We introduce the notion of a manifold map; this representation takes maps out of the plane and onto a two-dimensional surface embedded in a higher-dimensional space. Compared with standard planar maps, the key advantage of the manifold representation is self-consistency: as we will show, manifold maps do not suffer from the `cross over&apos; problem that planar maps commonly exhibit in environments containing loops. This self-consistency facilitates a number of important autonomous capabilities, including robust retro-traverse, lazy loop closure, active loop closure using robot rendezvous, and, ultimately, autonomous exploration.
A low power wireless intercom system is designed and implemented. Two fully-operational ASICs, integrating custom and commercial IP, implement the entire digital portion of the protocol stack. Combined, the chips consume 13 mW on average when three nodes are connected to the network. A high-level design methodology was used to define the protocol stack and communication algorithms, select architectures, and minimize energy.
We provide a Hilbert-Mumford Criterion for actions of reductive  groups G on Q-factorial complex varieties. The result allows to construct open  subsets admitting a good quotient by G from certain maximal open subsets  admitting a good quotient by a maximal torus of G. As an application, we  indicate how to obtain all invariant open subsets with good quotient for a given  G-action on a complete Q-factorial toric variety.
The direction of a tube represents the principle direction of diffusion along the tube. Studies show that there is correlation between the structures of neural fibers in white matter and the tracts derived from the principle direction of diffusion in linear anisotropy regions [4]. The green stream surfaces are generated in planar anisotropy regions, where water diffuses primarily in a small plane at any given point. The surface structures could be the result of crossing fibers or laminar structures [3]. In addition to tubes and surfaces, we show anatomical features for context. The blue surface shows ventricles, and the images on the three orthogonal planes show slices of T2-weighted images collected with the DTI.  To create our color models we use Z-Corp&apos;s Z406 printer. The process is as follows: The digital model, in VRML format, is subdivided into horizontal layers by the printer software. These layers are then manufactured by putting down a thin layer of plaster powder and droppin
Wireless data broadcast has received a lot of attention from industries and academia in recent years. Access effciency and energy conservation are two critical performance concerns in a wireless data broadcast environment. To improve the effciency of energy consumption on mobile devices, traditional disk-based indexing techniques such as B    -tree have been extended to index broadcast data on a wireless channel. However, existing designs are mostly based on centralized tree structures. Most of these indexing techniques are not flexible in the sense that the trade-off between access efficiency and energy conservation is not adjustable based on application specific requirements. We propose in this paper a novel parameterized index, called the exponential index, which can be tuned to optimize the access latency with the tuning time bounded by a given limit, and vice versa. The proposed index is very effcient because it facilitates replication naturally by sharing links in multiple search trees and thus minimizes storage overhead. Experimental results show that the exponential index not only achieves better performance than the state-of-the-art indexes but also enables great flexibility in trade-offs between access latency and tuning time.
We propose a general operational and realistic framework that aims at a generalization of quantum  mechanics and relativity theory, such that both appear as special cases of this new theory. Our  framework is operational, in the sense that all aspects are introduced with specific reference to events  to be experienced, and realistic, in the sense that the hypothesis of an independent existing reality is  taken seriously. To come to this framework we present a detailed study of standard quantum mechanics  within the axiomatic approach to quantum mechanics, more specifically the Geneva-Brussels approach,  identifying two of the traditional 6 axioms as `failing axioms&apos;. We prove that these two failing axioms  are at the origin of the impossibility for standard quantum mechanics to describe a continuous change  from quantum to classical and hence its inability to describe macroscopic physical reality. Moreover  we show that the same two axioms are also at the origin of the impossibility for standard quantum  mechanics to deliver a model for the compound entity of two `separated&apos; quantum entities. We put  forward that it is necessary to replace these two axioms in order to proceed to the more general theory.
Wireless networks combined with location technology create new problems and call for new decision aids. As a precursor to the development of these decision aids, a concept of communication distance is developed and applied to six situations. This concept allows travel time and bandwidth to be combined in a single measure so that many problems can be mapped onto a weighted graph and solved through shortest path algorithms. The paper looks at the problem of intercepting an out-of-communication team member and describes ways of using planning to reduce communication distance in anticipation of a break in connection. The concept is also applied to ad hoc radio networks. A way of performing route planning using a bandwidth map is developed and analyzed. The general implications of the work to transportation planning are discussed.
This paper presents a practical system for automated 3-D road network reconstruction from aerial images using knowledge-based image analysis. The system integrates processing of color image data and information from digital spatial databases, extracts and fuses multiple object cues, takes into account context information, employs existing knowledge, rules and models, and treats each road subclass accordingly. The key of the system is the use of knowledge as much as possible to increase success rate and reliability of the results, working in 2-D images and 3-D object space, and use of 2-D and 3-D interaction when needed. Another advantage of the developed system is that it can correctly and reliably handle problematic areas caused by shadows and occlusions. This work is part of a project to improve and update the 1:25,000 vector maps of Switzerland. The system was originally developed to processed stereo images. Recently, it has been modified to work also with single orthoimages. The system has been implemented as a stand-alone software package, and has been tested on a large number of images with different landscape. In this paper, various parts of the developed system are discussed, and the results of our system in the tests conducted independently by our project partner in Switzerland, and the test results with orthoimages in a test site in the Netherlands are presented together with the system performance evaluation.
s that satisfy a director&apos;s constraints. The probability of an animation is simply the product of the probabilities of all the physical parameters used, according to the random fields. To enforce constraints, we weight this probability according to how well the resulting animation satisfies the constraints. Any type of constraint is possible, provided it can be expressed as a positive weight that increases as the animation comes closer to satisfying the constraints. In our examples, we can constrain one or more of time, position, velocity, orientation, angular velocity, and the number thrown by a roll of the die. To sample animations, we use a Markov chain Monte Carlo (MCMC) algorithm.  3  This generates an infinite number of animations, with the physical properties and resulting trajectory distributed according to the weighted probability described above. Hence, each animation is plausible according to our world model and satisfies the direction constraints. The accompanying image sho
R nn                    be classes of matrices associated with graph G.Heren is the number of vertices in graph G, and A(G) is the adjacency matrix of this graph. Denote r(G)=  min XffC(G) rank(X),r + (G)=min XffB(G) rank(X). We have shown previously that for every graph G, ff(G)     ff(G) holds and ff(G)=r+ (G) implies  ff(G)=ff(G). In this article we show that there is a graph G such that ff(G)=r(G)  but ff(G) &lt; ff(G). In the case when the graph G doesn&apos;t contain two chordless cycles  C 4 with a common edge, the equality ff(G)=r(G) implies ff(G)=ff(G). Corollary: the last statement holds for d(G) -- the minimal dimension of the orthonormal representation of the graph G.
We present a direct measurement of the parity-violating parameter A b  by analyzing the left-right forward-backward asymmetry of b quarks in      !Z   ! bb. The SLD experiment observes hadronic decays of Z   bosons produced at resonance in collisions of longitudinally polarized electrons and unpolarized positrons at the SLC. Heavy flavor decays of the Z   are identified by using the topologically reconstructed mass of B  hadrons. The asymmetry A b is measured with a self-calibrating technique employing momentum-weighted track charge from both hemispheres in the tagged events. From our 1994--1995 sample of 3.6 pb   of      annihilation data with a luminosity-weighted average e   polarization of 77%, and our 1993 sample of 1.8 pb   with a luminosity-weighted polarization of 63%, we obtain A b = 0:911 \Sigma 0:045(stat.) \Sigma 0:045(syst.). Submitted to the 1997 International Europhysics Conference on High Energy Physics (HEP97), Jerusalem, Israel, 19 \Gamma 26 August, 1997. Reference EPS-122  Work supported by U.S. Department of Energy contract ffDE-AC03-76SF00515. 1 
This paper studies local properties of Voronoi diagrams of sets of disjoint compact convex  sites in R    .
People use their awareness of others&apos; temporal patterns to plan work activities and communication. This paper presents algorithms for programatically detecting and modeling temporal patterns from a record of online presence data. We describe analytic and end-user visualizations of rhythmic patterns and the tradeoffs between them. We conducted a design study that explored the accuracy of the derived rhythm models compared to user perceptions, user preference among the visualization alternatives, and users&apos; privacy preferences. We also present a prototype application based on the rhythm model that detects when a person is &quot;away&quot; for an extended period and predicts their return. We discuss the implications of this technology on the design of computer-mediated communication.
... system. In a large-scale wide-area system such as the Grid, security is a prime concern. One approach is to be conservative and implement techniques such as sandboxing, encryption, and other access control mechanisms on all elements of the Grid. However, the overhead caused by such a design may negate the advantages of Grid computing. This study examines the integration of the notion of &quot;trust&quot; into resource management such that the allocation process is aware of the security implications. We present a formal definition of trust and discuss a model for incorporating trust into Grid systems. As an example application of the ideas proposed, a resource management algorithm that incorporates trust is presented. The performance of the algorithm is examined via simulations.
A bisection of an n-vertex graph is a partition of its vertices into two  sets S and T , each of size n/2. The bisection cost is the number of edges  connecting the two sets. In directed graphs, the cost is the number of  arcs going from S to T . Finding a minimum cost bisection is NP-hard  for both undirected and directed graphs. For the undirected case, an  approximation of ratio O(log    n) is known. We show that directed minimum  bisection is not approximable at all. More specifically, we show that  it is NP-hard to tell whether there exists a directed bisection of cost 0,  which we call oneway bisection. In addition, we study the complexity  of the problem when some slackness in the size of S is allowed, namely,  (1/2    ff)n    (1/2 + ff)n. We show that the problem is solvable  in polynomial time when ff  =ff  (1/ log n), and provide evidence that the  problem is not solvable in polynomial time when ff = o(1/(log n)    ).
The symbol-based, correspondence epistemology used in AI is  contrasted with the constructivist, coherence epistemology promoted by cybernetics. The latter
The widening gap between processor and memory speeds renders data locality optimization a very important issue in data-intensive embedded applications. Throughout the years hardware designers and compiler writers focused on optimizing data cache locality using intelligent cache management mechanisms and program-level transformations, respectively. Until now, there has not been significant research investigating the interaction between these optimizations. In this work, we investigate this interaction and propose a selective hardware/compiler strategy to optimize cache locality for integer, numerical (arrayintensive) , and mixed codes. In our framework, the role of the compiler is to identify program regions that can be optimized at compile time using loop and data transformations and to mark (at compile-time) the unoptimizable regions with special instructions that activate/deactivate a hardware optimization mechanism selectively at run-time. Our results show that our technique can improve program performance by as much as 60% with respect to the base configuration and 17% with respect to a non-selective hardware/compiler approach.
Both MaxNet and SumNet are distributed congestion control architectures suitable for the  Internet. MaxNet has recently been shown to have better fairness and scaling properties, but the  majority of existing Internet links use the SumNet paradigm. If MaxNet links are to be deployed,  they will need to be compatible with the existing SumNet infrastructure. This paper investigates  the fairness and utilisation of networks consisting of mixtures of MaxNet and SumNet links, in  different proportions.
Whenever people move through their environments they do not move randomly. Instead, they usually follow specific trajectories or motion patterns corresponding to their intentions. Knowledge about such patterns may enable a mobile robot to robustly keep track of persons in its environment. This paper proposes a technique to derive a Hidden Markov Model (HMM) from learned motion patterns of people. This HMM is able to estimate the current and future positions of persons given knowledge about their intentions. Experimental results obtained with a mobile robot using laser and vision data collected in a typical office building with several persons illustrate the reliability and robustness of the approach. We also demonstrate that our model provides better estimates than an HMM directly learned from the data.
In this paper we describe a computational approach to the optimal output feedback control of multi-model systems by means of a unique constant output feedback matrix. Extension of the method to output feedback control of multi-model periodic systems is also presented. The power of the proposed approach is illustrated by the simultaneous stabilization and optimization of a helicopter multi-model.
Introduction  Over the last ten years, genomic sequencing has started in over 600 organisms, and more than 50 complete genomes are publicly available. The DNA microarray technology permits the measurement of gene expression in cultured cells  1  . An increasing number of laboratories are using the combination of these two methods to study gene expression on a genomic scale. After all the genes from an organism are clustered based on their expression patterns  2  , an important next step is to examine the upstream region of genes in the same expression pattern group and look for sequence motifs. These motifs might be the regulatory signal (most likely a transcriptional regulatory site) that causes these genes to respond similarly to developmental or environmental changes. Information on expressions, regulatory motifs and functions provides substantial insight to the understanding of gene networks    . The motivation of this research is to provide a gene expression data analysis tool  to
... horizon. One of the main challenges in sensor networks is to process and aggregate data in the network rather than wasting energy by sending large amounts of raw data to reply to a query. Some effcient data dissemination methods, particularly data-centric storage and information aggregation, rely on effcient routing from one node to another. In this paper we introduce GEM (Graph EMbedding for sensor networks), an infrastructure for node-to-node routing and data-centric storage and information processing in sensor networks. Unlike previous approaches, it does not depend on geographic information, and it works well even in the face of physical obstacles. In GEM, we construct a labeled graph that can be embedded in the original network topology in an effcient and distributed fashion. In that graph, each node is given a label that encodes its position in the original network topology. This allows messages to be effciently routed through the network, while each node only needs to know the labels of its neighbors. To demonstrate
The way people respond to the chance that an unlikely event will occur depends on how the event is described. We propose that people attach more weight to unlikely events when they can easily generate or imagine examples in which the event has occurred or will occur than when they cannot. We tested this idea in two experiments with mock jurors using written murder scenarios. The results suggested that jurors attach more weight to the defendant&apos;s claim that an incriminating DNA match is merely coincidental when it is easy for them to imagine other individuals whose DNA would also match than when it is not easy for them to imagine such individuals. We manipulated the difficulty of imagining such examples by varying the description of the DNA-match statistic. Some of the variations that influenced the jurors were normatively irrelevant.
This paper reports on the development of a domestic user-interface robot that is able to have natural human interaction by speech and emotional feedback. Natural interaction with the user is achieved by means of a mechanical head able to express emotions. Additionally, our robot is aware not only of its position in the environment but also of the position and intentions of the users. The localization of the robot is achieved with an appearance-based localization method. To get information about the users, the robot is designed to operate in (and to cooperate with) an intelligent ambient that takes care of monitoring the users. The result is a service robot that departs from existing developments in the fields of interface and service robots that are mainly reactive and, thus, limited in functionality.
The definition of similarity measures is one of the most crucial  aspects when developing case-based applications. In particular, when  employing similarity measures that contain a lot of specific knowledge  about the addressed application domain, modelling similarity measures  is a complex and time-consuming task. One common element of the similarity  representation are local similarity measures used to compute similarities  between the values of single attributes. In this paper an approach  to learn local similarity measures by employing an evolution program ---  a special form of a genetic algorithm --- is presented. The goal of the approach  is to learn similarity measures that suffciently approximate the  utility of cases for given problem situations in order to obtain reasonable  retrieval results.
The nonlinear amplification process in the mammalian cochlea gives rise to a variety of phenomena, which manifest as two-tone suppression and combination tone generation. These nonlinear effects show that, besides mere mechanical-to-neural transduction, the cochlea performs significant information processing on a biophysical, pre-neural level. As nonlinear cochlear processing is a precondition for successful feature extraction at higher neural stages, its profound understanding is of interest for the design of intelligent acoustic sensors. In this contribution, we provide a thorough explanation of suppression and combination tone generation, where we rely on Hopftype cochlear amplifiers. The underlying cochlear model can be implemented as an electronic circuit.
Events in natural language semantics are characterized in terms of regular languages, each string in which can be regarded as a temporal sequence of observations. The usual regular constructs (concatenation, etc.) are supplemented with superposition, inducing a useful notion of entailment, distinct from that given by models of predicate logic.
This paper presents a formal `fragment&apos; of database semantics as a declarative model of a cognitive agent. It is called a SLIM machine and functionally integrates the procedures of natural language interpretation, conceptualization, and production as well as query and inference. Each of these functions is illustrated explicitly by a corresponding LA-grammar. In addition, a control structure based on the principle of balance is presented. This principle mediates between the knowledge of the SLIM machine and its current situation by selecting a suitable action. ff 2001 Elsevier Science B.V. All rights reserved.
Introduction  CHR Grammars (CHRG) are a recent constraint-based grammar formalism added on top of CHR analogously to the way Definite Clause Grammars are defined and implemented over Prolog. A CHRG executes as an error robust bottom-up parser, and the formalism provides several advantages for natural language analysis reported elsewhere [4--6].  A notable property is CHRG&apos;s inherent ability to handle abduction in a way that requires no metalevel interpreter which otherwise is a common way to implement abduction. The technique, first noticed in [3], consists of a straightforward reformatting of grammar rules so that abducibles are passed through the constraint store, and this allows other CHR rules to serve as integrity constraints. The principle works best for grammars without local ambiguity but this is of course too limited as our main target is natural language analysis. To do abduction with an ambiguous grammar, additional techniques are necessary to avoid mixing up different sets o
Distributed computing systems have been moving beyond the realm of LAN environments into WANs. This notion is reinforced by the NSF&apos;s recent $88 million funding of the TeraGrid Project. However, this trend of expansion into WANs calls us to reconsider the performance effects on runtime and network consumption by traditional consistency protocols. These protocols can present performance degradation that becomes increasingly significant in WANs because of the more limited bandwidth. To overcome some of these drawbacks, we developed a demand-update consistency protocol that not only provides better runtime performance, but also requires less network communication than traditional protocols in WANs.
In finite mixture of location-scale distributions maximum likelihood estimator does not exist because of the unboundedness of the likelihood function when the scale parameter of some mixture component approaches zero. In order to study the strong consistency of maximum likelihood estimator, we consider the case that the scale parameters of the component distributions are restricted from below by  c n , where     is a sequence of positive real numbers which tends to zero as the sample size n increases. We prove that under mild regularity conditions maximum likelihood estimator is strongly consistent if the scale parameters are restricted from below by c n = exp(-n   ), 0 &lt; d &lt; 1. 1 
In this dissertation, we exhibit work revolving around the FO    meta-logic. After going over some of the background material on FO    , we discuss work about induction, and give induction principles that are richer than mathematical induction, in that they yield shorter, simpler and more elegant proofs. We then define and study notions of entailment and equivalence between two FO    definitions. We make use of these notions by defining a framework, closely related to FO    , for performing program transformation on definitions, and proving it correct. Finally, we discuss implementations in Prolog of Iris, a theorem prover based on FO    tool based on the framework we built.
The international standard IEEE 802.11 was developed recently in recognition of the increased demand for wireless local area networks. Its medium access control mechanism is described according to a variant of the Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) scheme. Although collisions cannot always be prevented, randomized exponential backoff rules are used in the retransmission scheme to minimize the likelihood of repeated collisions. More precisely, the backoff procedure involves a uniform probabilistic choice of an integer-valued delay from an interval, where the size of the interval grows exponentially with regard to the number of retransmissions of the current data packet. We model the two-way handshake mechanism of the IEEE 802.11 standard with a fixed network topology using probabilistic timed automata, a formal description mechanism in which both nondeterministic choice and probabilistic choice can be represented. From our probabilistic timed automaton model, we obtain a finite-state Markov decision process via a property-preserving discrete-time semantics. The Markov decision process is then verified using Prism, a probabilistic model checking tool, against probabilistic, timed properties such as at most 5,000 microseconds pass before a station sends its packet correctly.
Frequent itemset mining was initially proposed and has been studied extensively in the context of association rule mining. In recent years, several studies have also extended its application to the transaction (or document) classification and clustering. However, most of the frequent-itemset based clustering algorithms need to first mine a large intermediate set of frequent itemsets in order to identify a subset of the most promising ones that can be used for clustering. In this paper, we study how to directly find a subset of high quality frequent itemsets that can be used as a concise summary of the transaction database and to cluster the categorical data. By exploring some properties of the subset of itemsets that we are interested in, we proposed several search space pruning methods and designed an effcient algorithm called SUMMARY. Our empirical results have shown that SUMMARY runs very fast even when the minimum support is extremely low and scales very well with respect to the database size, and surprisingly, as a pure frequent itemset mining algorithm it is very effective in clustering the categorical data and summarizing the dense transaction databases.
With billion-transistor chips on the horizon, single-chip multiprocessors (CMPs) are likely to become commodity components. Speculative CMPs use hardware to enforce dependence, allowing the compiler to improve performance by speculating on ambiguous dependences without absolute guarantees of independence. The compiler is responsible for decomposing a sequential program into speculatively parallel threads, while considering multiple performance overheads related to data dependence, load imbalance, and thread prediction. Although the decomposition problem lends itself to a min-cut-based approach, the overheads depend on the thread size, requiring the edge weights to be changed as the algorithm progresses. The changing weights make our approach different from graph-theoretic solutions to the general problem of task scheduling. One recent work uses a set of heuristics, each targeting a specific overhead in isolation, and gives precedence to thread prediction, without comparing the performance of the threads resulting from each heuristic. By contrast, our method uses a sequence of balanced min-cuts that give equal consideration to all the overheads, and adjusts the edge weights after every cut. This method achieves an (geometric) average speedup of 74% for floating-point programs and 23% for integer programs on a four-processor chip, improving on the 52% and 13% achieved by the previous heuristics.
We describe a new method for  nding  nite models of unsorted  rst-order logic clause sets. The method is a MACE-style method, i.e. it  &quot;attens&quot; the  rst-order clauses, and for increasing model sizes, instantiates  the resulting clauses into propositional clauses which are consecutively  solved by a SAT-solver. We enhance the standard method by using 4 novel  techniques: term de  nitions, which reduce the number of variables in attened  clauses, incremental SAT, which enables reuse of search information  between consecutive model sizes, static symmetry reduction, which reduces  the number of isomorphic models by adding extra constraints to the SAT  problem, and sort inference, which allows the symmetry reduction to be applied  at a  ner grain. All techniques have been implemented in a new model  nder, called Paradox, with very promising results.
Presently, there are user interfaces that allow multimodal interactions. Many existing research and prototype systems introduced embodied agents, assuming that they allow a more natural conversation or dialogue between user and computer. Here we will first take a look at how in general people react to computers. We will look at some of the theories, in particular the CASA (&quot;Computers Are Social Actors&quot;) paradigm, and then discuss how new technology, for example ambient intelligence technology, needs to anticipate the need of humans to build up social relationships. One way to anticipate is to do research in the area of social psychology, to translate findings there to the humancomputer situation and to investigate technological possibilities to include human-human communication characteristics in the interface. For that reason we will discuss embodied conversational agents, the role they can play in human-computer interaction (in face-to-face conversation), in ambient intelligence environments and in virtual communities.
This paper describes several aspects of our on-going research relating to technologies and techniques for collaborative learning. Our research is focused on the classroom-in-the-round, a CSCW enabled classroom that attempts to promote collaborative learning through novel room design, hardware, software and courseware. Encouraging preliminary feedback from courses we are teaching in the classroom is presented. An in-depth study conducted to evaluate the effectiveness of assigned roles on collaboration between pairs of users in a synchronous groupware simulation is then discussed. Statistically significant results show that assigned roles increase team interaction and improve group performance in subsequent unrelated group activities, when compared to a control group. The paper closes with some remarks on promising instructional techniques we are investigating for collaborative learning, including distributed grading, online presentation of team projects, and team-oriented presentation-style exams.
this paper describes the complete problems that motivate interest in these classes, discusses some surprising recent discoveries, and points out open problems where progress can reasonably be expected
This paper hopes to further our understanding of cross-modal processing of simple stimuli. Cross-modal matching tasks were conducted where the two objects to be matched were presented to vision and touch at different times. The stimuli used were `L-shaped&apos; objects with varying lengths in the X and Y dimensions. Participants were shown a visual and a haptic object and were asked whether they were the `same&apos; or `different&apos;. In experiment one, the objects were presented in a serial order, with an interstimulus interval (ISI) of either 0, 15 or 30 seconds. Visual-haptic and hapticvisual temporal decay of sensitivity to object change was found to be the same and a two-dimensional change was easier to detect than one-dimensional changes. Experiment two found no performance difference between synchronous and immediate serial object presentation. Thus, a temporal window of optimum performance may exist with presentation outside of this subject to memorial decay.
set-up to simplify data interpretation. For example, an antigen can often be made monomeric, and for an antibody, monovalent Fab and scFv fragments can be used. However, the aim of the work presented here is to understand the influence of bivalence on the binding. This is important for maximizing binding to cellular surfaces in therapeutic settings and for protein engineering, in order to optimize the geometry for multivalent proteins used as analytes or ligands. Thus, we primarily wish to study factors such as ligand density and binding site distance, rather than propose to extract intrinsic affinities from multivalent binding experiments. For this purpose, we have set up a microscopic kinetic model and applied it to the simulation of multivalent binding kinetics. The kinetics of multivalent binding to ligands fixed on a surface or in a three-dimensional network is of direct biological relevance, for example, when antibodies bind to epitopes on surfaces such as viruses and cells.  Bi
This paper proposes a class of nonlinear stochastic volatility models based on  the Box-Cox transformation which offers an alternative to the one introduced in  Andersen (1994). The proposed class encompasses many parametric stochastic  volatility models that have appeared in the literature, including the well known  lognormal stochastic volatility model, and has an advantage in the ease with which  different specifications on stochastic volatility can be tested. In addition, the functional  form of transformation which induces marginal normality of volatility is  obtained as a byproduct of this general way of modeling stochastic volatility. The  effcient method of moments approach is used to estimate model parameters. Empirical  results reveal that the lognormal stochastic volatility model is rejected for  daily index return data but not for daily individual stock return data. As a consequence,  the stock volatility can be well described by the lognormal distribution  as its marginal distribution, consistent with the result found in a recent literature  (cf Andersen et al (2001a)). However, the index volatility does not follow the  lognormal distribution as its marginal distribution.
Introduction  Multichannel MEG systems measure neuromagnetic fields simultaneously at many scalp locations. By solving inverse problem in adequate algorithm, the location of the current sources responsible for the measured field distribution can be determined noninvasively [1]. In order to study cortical activities of the human brain with MEG systems, the localization accuracy is an important figure of merit of a system.  In the following, we introduce the localization results of our 40-channel SQUID system. The difference between our system and others is that it has integrated DROS (double relaxation oscillation SQUID) planar gradiometers instead of axial gradiometers with conventional dc SQUID. The planar gradiometers are oriented to measure tangential components to the body surface [2]. In this configuration the magnetic field distribution has a peak just above the current dipole and therefore a less extensive sensor array is needed to get the essential field distribution. Also it i
Hierarchical task network and action-based planning  approaches have traditionally been studied  separately. In many domains, human expertise in  the form of hierarchical reduction schemas exists,  but is incomplete. In such domains, hybrid approaches  that use both HTN and action-based planning  techniques are needed. In this paper, we extend  our previous work on refinement planning to  include hierarchical planning. Specifically, we provide  a generalized plan-space refinement that is capable  of handling non-primitive actions. The generalization  provides a principled way of handling partially  hierarchical domains, while preserving systematicity,  and respecting the user-intent inherent  in the reduction schemas. Our general account  also puts into perspective the many surface differences  between the HTN and action-based planners,  and could support the transfer of progress between  HTN and action-based planning approaches.
A three dimensional vector finite element method (FEM) has been used to model the permittivity of rice using the scattered far field radiation. Scattering of incident plane wave radiation is analysed to extract the permittivity of granular material. Some reduced parameters obtained from the scattered electric field show a very good correlation with the permittivity of materials (both dielectric constant and loss factor). Although the present analysis is based upon the permittivity of granular materials having dielectric constant between 1 and 7 and loss factor between 0.001 and 1.0, similar techniques could be developed for materials having different ranges of permittivity. Such analysis might be useful in solving more cumbersome problem of inverse scattering.
Inamulti-transmitter broadcast system, the weightvector for each message signal can provide an additional degree of freedom for signal enhancement and interference suppression by taking advantage of the spatial diversity among the users. The design of optimal weightvectors that maximize the overall channel capacity is an open problem. Under certain power constraints, the channel capacity R is a highly nonlinear function of the M-dimensional weightvectors     where M is the number of transmitters. Hence, a closed-form algebraic solution that maximizes R over      does not seem to be tractable. In this paper, we decouple the weight vectors in R to simplify the optimization problem to a search for the maxima of a smooth multidimensional function. Based on this decoupling, we derive and evaluate two algorithms for computing weightvectors for the two-user and three-user cases: orthogonal and optimal. We also propose a near-optimum algorithm for the two-user case. The optimal algorithm requires an iterative search.
This paper describes the development of a speech recognition system for the processing of telephone conversations, starting with a state-of-the-art broadcast news transcription system. We identify major changes and improvements in acoustic and language modeling, as well as decoding, which are required to achieve state-of-theart performance on conversational speech. Some major changes on the acoustic side include the use of speaker normalization (VTLN), the need to cope with channel variability, and the need for efficient speaker adaptation and better pronunciation modeling. On the linguistic side the primary challenge is to cope with the limited amount of language model training data. To address this issue we make use of a data selection technique, and a smoothing technique based on a neural network language model. At the decoding level lattice rescoring and minimum word error decoding are applied. On the development data, the improvements yield an overall word error rate of 24.9% whereas the original BN transcription system had a word error rate of about 50% on the same data.
It has been shown that non-determinism, both angelic and demonic,  can be encoded in a functional language in different representation of  sets. In this paper we see quantum programming as a special kind of  non-deterministic programming where negative probabilities are allowed.
In many combinatorial domains, simple stochastic algorithms often exhibit superior performance when compared to highly customized approaches. Many of these simple algorithms outperform more sophisticated approaches on difficult benchmark problems; and often lead to better solutions as the algorithms are taken out of the world of benchmarks and into the real-world. Simple stochastic algorithms are often robust, scalable problem solvers.
The Sequoia and Kings Canyon National Parks&apos; natural fire management program is the oldest of its kind in the United States. Past fire suppression practices had produced an unnatural accumulation of fuel, increasing the risk of high-intensity fires. Subsequent research showed fire was important to many park ecosystems and that fire could be reintroduced without harm under prescribed conditions. Each Park must define the goals of its natural fire management program, monitor its effectiveness, and continuously reevaluate goals, objectives, and methods.
After almost a decade since the introduction of the earliest reliable CORBA implementation, and despite the recent adoption of the Fault Tolerant CORBA (FT-CORBA) standard by the Object Management Group, CORBA is still not widely adopted as the preferred platform for building reliable distributed applications. Among the obstacles to FT-CORBA&apos;s widespread deployment are the complexity of the new standard, the lack of understanding in implementing and/or deploying reliable CORBA applications, and the fact that current FT-CORBA implementations are not readily applicable to real-world complex applications. In this paper, we candidly share our independent experiences as developers of two separate reliable CORBA infrastructures (OGS and Eternal), and as contributors to the FT-CORBA standardization process. Our intention is to reveal the intricacies, challenges and strategies in developing fault-tolerant CORBA systems, including our own. We provide an overview of the new FT-CORBA standard, and discuss its limitations and techniques for best exploiting it. We reflect on the difficulties that we encountered in building dependable CORBA systems, the solutions that we developed to address these challenges, and the lessons that we learned as a result. Finally, we highlight some of the open issues, such as non-determinism and partitioning, along with some solutions for resolving these issues.
Our overall objective is to translate  a domain-specific document in a foreign  language (in this case, Chinese) to  English. Using automatically induced  domain-specific, comparable documents  and language-independent clustering, we  apply domain-tuning techniques to a bilingual  lexicon for downstream translation  of the input document to English. We will  describe our domain-tuning technique and  demonstrate its effectiveness by comparing  our results to manually constructed  domain-specific vocabulary. Our coverage  /accuracy experiments indicate that  domain-tuned lexicons achieve 88% precision  and 66% recall. We also ran a Bleu  experiment to compare our domain-tuned  version to its un-tuned counterpart in an  IBM-style MT system. Our domain-tuned  lexicons brought about an improvement  in the Bleu scores: 9.4% higher than a  system trained on a uniformly-weighted  dictionary and 275% higher than a system  trained on no dictionary at all.
The running times of many computational science applications are much longer than the mean-time-to-failure of current high-performance computing platforms. Therefore, to run to completion, these applications must tolerate hardware failures.
Growing data stockpiles and storage consolidation continue to be the trend. So does the need to provide secure yet unconstrained, high bandwidth access to such repositories by  geographically distributed users. Conventional data management approaches, both at the  local and wide area level, are viewed as potentially inadequate to meet these challenges. This paper explores methods deploying a new breed of Fibre Channel (FC) technology that leverages Internet Protocol (IP) infrastructures as the data transport mechanism, a step towards creating a &quot;storage area network (SAN) grid&quot;. These technologies include products using the FC Over IP (FCIP) and the Internet FC Protocol (iFCP) protocols. The effort draws upon earlier work that concentrated on standard FC and internet SCSI (iSCSI) technologies. In summary, the vendor offerings tested performed as expected and provided encouraging performance results. However, their operational readiness still  needs to be understood and demonstrated. Installing and configuring the products was  reminiscent of the early days of FC with driver and version compatibly issues surfacing  once again. Maturity will take some time.
Divergence time and substitution rate are seriously confounded in phylogenetic analysis, making it difficult  to estimate divergence times when the molecular clock (rate constancy among lineages) is violated. This problem can be  alleviated to some extent by analyzing multiple gene loci simultaneously and by using multiple calibration points. While  different genes may have different patterns of evolutionary rate change, they share the same divergence times. Indeed,  the fact that each gene may violate the molecular clock differently leads to the advantage of simultaneous analysis of  multiple loci. Multiple calibration points provide the means for characterizing the local evolutionary rates on the phylogeny.
The semi-automatic extraction of a semantic hierarchy from a machine  readable dictionary is investigated. This hierarchy could potentially be used  by a word sense disambiguation technique which measures semantic relatedness  of two senses using a semantic hierarchy and evaluates potential sense  con  gurations according to this measure of semantic relatedness.
The discovery of semantic relations from text  becomes increasingly important for applications  such as Question Answering, Information  Extraction, Text Summarization, Text Understanding,  and others. The semantic relations  are detected by checking selectional constraints.
Vulnerability to water-stress-induced embolism of stems, petioles, and leaf midribs was evaluated for two rubber clones (RRIM600 and RRIT251). The xylem conduits were relatively vulnerable to cavitation with 50% of embolism measured for xylem pressures between --1 and --2 MPa. This feature can be related to the tropicalhumid origin of the species. A distinct basipetal gradient of vulnerability was found, leaf midribs being the least vulnerable. Substantial variation in vulnerability to cavitation was found between the two clones only at the petiole level. A correlation was found between the stomatal behavior and the development of cavitation. Stomata were nearly closed when the xylem pressure reached the point of xylem dysfunction. Stomata may thus contribute to controlling the risk of cavitation. However, for one clone a poor correlation was found between stomatal regulation and petiole vulnerability. This was consistent with a high degree of embolism measured in the petioles after a soil drought event. Therefore, xylem cavitation might represent a promising criterion to evaluate the performance of rubber clones under drought conditions.
This paper reports on research which aims to test t he efficacy of applying automated evaluation techniques, originally designed for human second language learners, to machine translation (MT) system evaluation. We believe that such evaluation techniques will provide insight into MT evaluation, MT development, the human translation process and the human language learning process. The experiment described here looks only at the intelligibility of MT output. The evaluation technique is derived from a second language acquisition experiment that showed that assessors can differentiate native from non-native language essays in less than 100 words. Particularly illuminating for our purposes is the set of factor on which the assessors made their decisions. We duplicated this experiment to see if similar criteria could be elicited from duplicating the test using both human and machine translation outputs in the decision set. The encouraging results of this experiment, along with an analysis of language factors contributing to the successful outcomes, is presented here.
Introduction  In the last decades, particularly due to the proliferation of powerful molecular biological tools and the automation of some processes, we assisted to an explosion of biological data. More than ever, biological researchers need to integrate a tremendous quantity of information to be able to formulate ecient hypotheses.  To address this problem, a multitude of databases were created, and continue to be created in order to organize data for experimental as well as for theoretical research. However, in most cases, such databases are built for a specic and often narrow purpose and/or use idiosyncratic vocabulary. The formalism that should underly the databases is also weak, or altogether absent. This results in an overlap of the work that is being done, and may lead to inaccuracy or even loss of information. The need for databases designed around formal concepts is now starting to be accepted as a necessity[1].  Transcription regulation is one of the most important ways to 
In this paper we show that several known algorithms for sequential  prediction problems (including the quasi-additive family of Grove et al.
This technical note defines a standard set of document structuring conventions (DSC), which will help ensure that a PostScript document is device independent. DSC allows PostScript language programs to communicate their document structure and printing requirements to document managers in a way that does not affect the PostScript language page description
American higher education have been in pursuant of establishing virtual classrooms where learners  can obtain their education at the comfort of their homes and  via the Internet. However, one must ask,&quot; how can students  perform real lab experiments over the Internet?&quot; To examine this distance education (DE) challenge, two researchers from Florida Atlantic University (FAU), Drs. Alhalabi and Hamza, along with a team of graduate students in the fields of Computer Science and Engineering and Educational Technology and Research, have built a  proof of concept experiments that combine real  instrumentation, data acquisition equipment, and interfaces to different computer ports in an unprecedented fashion! This innovative set up allows, for example, the instructor to  set up a simple experiment the way it would be set up in a  real laboratory-- thus realistically providing experiments over the Internet.
Proxy-network based overlays have been proposed to protect Internet Applications against  Denial-of-Service (DoS) attacks by hiding an application&apos;s location. We study how a proxy  network&apos;s topology influences the effectiveness of location-hiding. We provide a general analysis  of system dynamics under attack, and study how the speed of attack, speed of defense, and  proxy network topology affect these dynamics. Our analysis characterizes when proxy networks  are robust against attacks (attackers&apos; impact can be quickly and completely removed), and when  they are vulnerable to attacks (attackers&apos; impact cannot be completely removed). We present  a general set of metrics for proxy network topologies which can be applied to evaluate them  with respect to robustness and vulnerability. We apply these metrics and analysis to a range  of popular overlay network topologies, showing that the Chord [18] used for location-hiding in  [27, 19], is not a good choice, and that topologies such as CAN [20] are a much better topology  for DoS resistance and location-hiding. In general proxy networks with lower vertex degrees and  balanced distribution of connectivity have better properties. Our results provide a set of sound  design principles for proxy network topologies for location-hiding and DoS-resistance.
This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically -motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches.
In the context of physical synthesis, large-scale standard-cell placement algorithms must facilitate incremental changes to layout, both local and global. In particular, flexible gate sizing, net buffering and detail placement require a certain amount of unused space in every region of the die. The need for &quot;local&quot; whitespace is further emphasized by temperature and power-density limits. Another requirement, the stability of placement results from run to run, is important to the convergence of physical synthesis loops. Indeed, logic resynthesis targetting local congestion in a given placement or particular critical paths may be irrelevant for another placement produced by the same or a different layout tool.
This paper introduces a study into the realization of physical interaction components, based on a technology for providing network connectivity and power to small objects via a layered surface. Small pin-like components can be activated and networked by attaching them to the same, augmented surface, and can be used to dynamically create an interlinked set of atomic interaction components. The physical connection becomes thus also a digital link between components. To demonstrate our proposed platform, we have built atomic interface components in the form of dials and multicolour lights that are activated and integrated in a network by simply pushing their pin connectors in an augmented surface. They allow to pick and mix colours using the red, green, and blue primaries, as a physical alternative for the traditional WIMP colour mixer tools.
this paper is to describe an approach based on GA for solving dynamic scheduling problems, where the products (jobs) to be processed have due dates. This paper starts by presenting a scheduling system, based on Genetic Algorithms for the resolution of the dynamic version of Single Machine Scheduling Problem (SMSP). The approach used adapts the resolution of the static problem to the dynamic one in which changes may occur continually. This takes into account dynamic occurrences in a system and adapts the current population to a new regenerated population. Then, it is proposed an approach for the resolution of the Job-Shop Scheduling Problem (JSSP) in dynamic environments
Selective attention is a mechanism used to sequentially select and process salient subregions of the input space, while suppressing inputs arriving from nonsalient regions. By processing small amounts of sensory information in a serial fashion, rather than attempting to process all the sensory data in parallel, this mechanism overcomes the problem of flooding limited processing capacity systems with sensory inputs. It is found in many biological systems and can be a useful engineering tool for developing artificial systems that need to process in real-time sensory data. In this paper we present a neuromorphic hardware model of a selective attention mechanism implemented on a very large scale integration (VLSI) chip, using analog circuits. The chip makes use of a spike-based representation for receiving input signals, transmitting output signals and for shifting the selection of the attended input stimulus over time. It can be interfaced to neuromorphic sensors and actuators, for implementing multichip selective attention systems. We describe the characteristics of the circuits used in the architecture and present experimental data measured from the system.
Semide  nite programming based approximation algorithms, such as the Goemans and Williamson approximation  algorithm for the MAX CUT problem, are usually shown to have certain performance  guarantees using local ratio techniques. Are the bounds obtained in this way tight? This problem was  considered before by Karlo and by Alon and Sudakov. Here we further extend their results and show,  for the  rst time, that the local analyses of the Goemans and Williamson MAX CUT algorithm, as well  as its extension by Zwick, are tight for every possible relative size of the maximum cut in the sense that  the expected value of the solutions obtained by the algorithms may be as small as the analyses ensure.
ih a NdstressbenddFo, E11dih,  E22dih, g12dih, n12, theta, th, r1, width, numberOfPlieshh`,      DodV  sigma a stressaxisdihddj, 1hh C stressesbenddihddj, 1hh,  Ifdsigma ` 0.0,  strength aEXcdjhdconcenddi, jhhh, strength a Xtdjhdconcenddi, jhhhh,  Fadi, jh a sigmastrength,  trdi, jh a trdjhdFadi, jhh,  sigma a stressaxisdihddj, 1hh C stressesbenddihddnumberOfPliesC j, 1hh,  Ifdsigma ` 0.0,  strength aEXcdjhdconcenddi, jhhh, strength a Xtdjhdconcenddi, jhhhh,  Fadi, numberOfPliesC jh a sigmastrength,  trdi, numberOfPliesC jh a trdjhdFadi, numberOfPlies C jhh`,      Faf a TabledFadi, jh,      Trupt a Tabledtrdi, jh,      ff Remaining Strength Fr Evalution  t a TabledViB timeInc`,    1, noSteps`h;  DodVIf  dFad1,  jh b 1, Frd1, jh a 0,  Frd1, jh a 1 E    E Fad1, jhv B  rtdd1,  1hhtrd1, jhv^kvh`,    1, 2 BnumberOfPlies`h;  i a 2;  Whiledi `a noSteps,    DodVIfdFadi, jh b 1, Frdi, jh a 0,    t0 a    E Frdi E 1, jhvr1 E Fadi, jhvv^r1  kv  Btrdi, jh,  ht a tddi, 1hh E tddi E 1, 1hh,  Frdi, jh 
This paper addresses the following robust scheduling problem: Given that only coarse-grained channel state information  (i.e., bounds on channel errors, but not the fine-grained error pattern) is available, how to design a robust  scheduler that ensures worst-case optimal performance? To solve this problem, we consider two coarse-grained channel  error models and take a zero-sum game theoretic approach, in which the scheduler and the channel error act as  non-cooperative adversaries in the scheduling process. Our results show that in the heavy channel error case, the optimal  scheduler adopts a threshold form. It does not schedule a flow if the price (the flow is willing to pay) is too small,  in order to maximize the system revenue. Among the scheduled flows, the scheduler schedules a flow inversely proportional  to the price (the flow is to pay) to minimize the risk of being caught by the channel error adversary. We also  show that in the mild channel error model, the robust scheduling policy exhibits a balanced trade-off between a greedy  decision and a conservative policy. The scheduler is likely to take a greedy decision if it evaluates the risk of encountering  the channel error adversary now to be small. Therefore, robust scheduling does not always imply conservative  decision. The scheduler is willing to take &quot;risks&quot; to expect higher gain in some scenarios. Our solution also shows  that probabilistic scheduling may lead to higher worst-case performance compared to traditional deterministic policies.
  Maintenance tends to degrade the structure of software, ultimately making maintenance more costly. At times, then, it is worthwhile to manipulate the structure of a system to make changes easier. However, it is shown that manual restructuring is an error-prone and expensive activity. By separating structural manipulations from other maintenance activities, the semantics of a system can be held constant by a tool, assuring that no errors are introduced by restructuring. To allow the maintenance team to focus on the aspects of restructuring and maintenance requiring human judgment, a transformation-based tool can be provided---based on a model that exploits preserving data flow-dependence and control flow-dependence---to automate the repetitive, errorprone, and computationally demanding aspects of re...
CASSIUS is an awareness server which assists users in designing subscriptions for maintaining awareness of events within work, physical and social environments. This environment is designed to work with a wide range of awareness tools using desktop computers, mobile devices and ambient fixtures[4]. This work investigates the requirements for creating ad-hoc subscriptions -- a subscription that is created either by the user or a software agent, and which only exists for a brief period of time. Design guidelines are proposed that help address the problem inherent in having users invest effort in creating a subscription which may last for only the few minutes in which they are in a specific location or context.
We study two-sided heat kernel estimates on a class of fractal  graphs which arise from a subclass of finitely ramified fractals. These fractal  graphs do not have spatial symmetry in general, and we find that there is a  dependence on direction in the estimates. We will give a new form of expression  for the heat kernel estimates using a family of functions which can be thought  of as a &quot;distance for each direction&quot;. As an application, we give a law of  the iterated logarithm which shows that the directional dependence leads to  non-uniform behaviour in the typical paths of the random walk.
Role-based access control (RBAC) is attracting increasing attention because it reduces the  complexity and cost of security administration by interposing the notion of role in the assignment  of permissions to users. In this paper, we present a formal framework relying on an extension  of the ff--calculus to study the behaviour of concurrent systems in a RBAC scenario. We define a  type system ensuring that the specified policy is respected during computations, and a bisimulation  to equate systems. The theory is then applied to three meaningful examples, namely finding the  `minimal&apos; policy to run a given system, refining a system to be run under a given policy (whenever  possible), and minimizing the number of users in a given system without changing the overall  behaviour.
Inferring types for polymorphic recursive function de  nitions (abbreviated to polymorphic recursion)  is a recurring topic on the mailing lists of popular typed programming languages. This is despite the  fact that type inference for polymorphic recursion using 8-types has been proved undecidable. This  report presents several programming examples involving polymorphic recursion and determines their  typability under various type systems, including the Hindley-Milner system, an intersection-type system,  and extensions of these two. The goal of this report is to show that many of these examples are typable  using a system of intersection types as an alternative form of polymorphism. By accomplishing this, we  hope to lay the foundation for future research into a decidable intersection-type inference algorithm.
This paper investigates the properties of argumentation-based dialogues  between agents. It takes a previously defined system by which agents can trade  arguments, and examines how different classes of protocols for this kind of interaction  can have profoundly different outcomes. Studying such classes of protocol,  rather than individual protocols as has been done previously, allows us to start to  develop a meta-theory of this class of interactions.
We present an algorithm for out-of-core simplification of large polygonal datasets that are too complex to fit in main memory. The algorithm extends the vertex clustering scheme of Rossignac and Borrel [13] by using error quadric information for the placement of each cluster&apos;s representative vertex, which better preserves fine details and results in a low mean geometric error. The use of quadrics instead of the vertex grading approach in [13] has the additional benefits of requiring less disk space and only a single pass over the model rather than two. The resulting linear time algorithm allows simplification of datasets of arbitrary complexity. In order
Local register allocation (LRA) assigns pseudo-registers to  actual registers in a basic block so as to minimize the spill cost. In this  paper, four different LRA algorithms are compared with respect to the  quality of their generated allocations and the execution times of the algorithms  themselves. The evaluation is based on a framework that views  register allocation as the combination of boundary conditions, LRA, and  register assignment. Our study does not address the problem of instruction  scheduling in conjunction with register allocation, and we assume  that the spill cost depends only on the number and type of load and store  operations, but not on their positions within the instruction stream.
We study the errors in particle filtering with incorrect system model parameters. The total error in approximating the posterior distribution of the actual process (state) given noisy observations, can be split into modeling error and particle filtering error in tracking with the incorrect model. We show that the bound on both errors is a monotonically increasing function of the error in the system model per time step. The bound on the particle filtering error blows up very quickly since it has increasing derivatives of all orders. We apply this result to bounding the errors in approximating our statistic for slow change detection in nonlinear systems.
In the northeast of Thailand, saline soils can be as large as 2.8 million hectares. Besides, another 3.04 million hectares are classified into potentially saline soils and increasing year by year. Saline areas are more or less barren land and almost useless for agriculture especially in the heavily salt affected soils. The objective of this study is to find ways and means to utilize these heavily salt affected soils for agriculture production such as growing one additional crop after paddy by the appropriate fertilizer management that can be done by farmers.
Multi-agent systems (MAS) differ from non-agent based systems because agents are intended to be autonomous units of intelligent functionality. As a consequence, agent-based software engineering methods must complement standard design activities and representations with models of the agent society. Some methods coming from artificial intelligence community address social knowledge and relationships but have high-level design abstractions as their end points. This paper describes PASSI a step-by-step requirement-to-code method for developing multiagent software that integrates design models and philosophies from both object-oriented software engineering and MAS using UML notation. The method has evolved through several stages; it has been previously applied in the synthesis of embedded robotics software and we are currently exploring its applications to the design of various agent-based information systems.
Wavelength-division multiplexing has emerged as an important physical layer technology. Optical
To produce fast, reasonably intelligible and easily correctable  translations between related languages, it suffces to use a machine translation  strategy which uses shallow parsing techniques to refine what  would usually be called word-for-word machine translation. This paper  describes the application of shallow parsing techniques (morphological  analysis, lexical disambiguation, and flat, local parsing) in a Portuguese--  Spanish, Spanish--Portuguese machine translation system which is currently  being developed by our group and is publicly and freely available  at http://copacabana.dlsi.ua.es.
this paper are entirely those of the authors. They do not necessarily represent the view of the World Bank, its Executive Directors, or the countries they represent. Policy Research Working Papers are available online at http://econ.worldbank.org
Recently Heteroscedastic Discriminant Analysis (HDA) has been proposed as a replacement for Linear Discriminant Analysis (LDA) in speech recognition systems that use mixtures of diagonal covariance Gaussians to model the data. Typically HDA and LDA involve a dimension reduction of the feature space. A specific version HDA that involves no dimension reduction, and is popularly known as Maximum Likelihood Linear Transform (MLLT) is often used on the feature space to give significant improvements in performance. MLLT approximately diagonalizes the class covariances, and in effect, tries to approximate the performance of a full-covariance system. However, the performance of a full-covariance system could in some cases be much better than using MLLT-based diagonal covariance system. We propose the method of Multiple Linear Transforms (MLT), that bridges this gap in performance, while maintaining the speed efficiency of a diagonal covariance system. This technique improves the performance of a diagonal covariance system, over what could be obtained from HDA or MLLT.
A bigraphical reactive system (BRS) involves bigraphs, in which the nesting of nodes represents locality, independently of the edges connecting them; it also allows bigraphs to reconfigure themselves. BRSs aim to provide a uniform way to model spatially distributed systems that both compute and communicate. In this memorandum we develop their static and dynamic theory. In part I, we illustrate...
This paper presents a novel Scalar-field based Free-Form Deformation (SFFD) technique founded upon general flow constraints and implicit functions. In contrast to the traditional lattice-based FFD driven by parametric geometry and spline theory, we employ scalar  fields as embedding spaces instead. Upon the deformation of the scalar field, the vertices will move accordingly, which result in freeform deformations of the embedded object. The scalar field construction, sketching, and manipulation are both natural and intuitive. By tightly coupling self-adaptive subdivision and mesh optimization with SFFD, versatile multi-resolution free-form deformations can be achieved because our algorithm can adaptively refine and  improve the model on the fly to improve the mesh quality. We can also enforce various constraints on embedded models, which enable our technique to preserve the shape features and facilitate more sophisticated design. Our system demonstrates that SFFD is very powerful and intuitive for shape modeling. It significantly enhances traditional FFD techniques and facilitates a larger number of shape deformations.
With modern software systems, an important requirement is the ability to be auto adaptive, i.e. being able to adjust itself to the changing environment. In line with this, a run time manager for dynamic feature integration of telecommunication systems, interaction detection and resolution is described in this paper with aspects being used to implement features. The manager manages the interaction of features/aspects by monitoring the managed program. The program is represented by a labelled transition system (LTS) model, stored in a flexible data structure, and executed by calling action subroutine represented by the label of the LTS model, forming a reflective facility for the composition and analysis of features. It is the reflective mechanism that makes dynamic feature addition, run time model checking, as well as adaptive interaction resolution possible. Runtime model checking is possible because the checked program is stored within itself, and the interaction resolution will be done by selecting behaviour traces according to the resolution rules.
Informally, a communication protocol is sender k - anonymous if it can guarantee that an  adversary, trying to determine the sender of a particular message, can only narrow down its  search to a set of k suspects. Receiver k-anonymity places a similar guarantee on the receiver:  an adversary, at best, can only narrow down the possible receivers to a set of size k. In this paper  we introduce the notions of sender and receiver k-anonymity and consider their applications.
It is possible to offer a high speed data transfer capability by employing a WDM technology. One promising method is to transfer the data through a fast wavelength reservation. However, the wavelength reservation time, including the propagation delay between source and receiver nodes, becomes large, which may lead to performance degradation. In this paper, we develop a new approximate analytical method, which incorporates wavelength reservation times. We consider two methods for wavelength reservation; a forward method in which wavelength reservation is performed along the forward path from source to receiver, and a backward method in which it is performed along the backward path. Based on our approximate analysis, we investigate the effects of propagation delays on both methods.
Two approaches to improving the accuracy of camera motion estimation from image sequences are the use of omnidirectional cameras, which combine a conventional camera with a convex mirror that magnifies the field of view, and the use of both image and inertial measurements, which are highly complementary. In this paper, we describe optimal batch algorithms for estimating motion and scene structure from either conventional or omnidirectional images, with or without inertial data.
Two-dimensional gel electrophoresis is the preferred method for simultaneously  separating and visualising thousands of proteins. An important  part of the computer aided analysis of the proteome is the ability to automatically  detect, identify, and quantify the proteins by means of automatic Image  processing. We present a fast and sensitive method for protein spot detection  using the Circular Symmetry Tensor. It is based upon the work of Bign on  Symmetry derivatives of Gaussians.
BGP, the current inter-domain routing protocol, assumes that the routing information propagated by authenticated routers is correct. This assumption renders the current infrastructure vulnerable to both accidental misconfigurations and deliberate attacks. To reduce this vulnerability, we present a combination of two mechanisms: Listen and Whisper. Listen passively probes the data plane and checks whether the underlying routes to different destinations work. Whisper uses cryptographic functions along with routing redundancy to detect bogus route advertisements in the control plane. These mechanisms are easily deployable, and do not rely on either a public key infrastructure or a central authority like ICANN.
Instructional Design, the pedagogical technique typically used to design Computer Based  Education software, including Intelligent Tutoring Systems, relies on a set of correctness metrics  called Instructional Integrity. Hereby, curricula should describe, in unambiguous terms, the structure  of the information any student must acquire. A time interval is expected to have elapsed upon each  transition among these states. It is assumed that knowledge must have been transferred to the student  at the end of each interval. This is clearly consistent with Instructivism, since knowledge is  quantifiable and incrementally administered. In reality, however, learning will occur holistically, in  time. As a result, agent measuring and tutoring of the learning process at the end of predetermined  causal-time intervals results in Temporal Holes where important events might be taking place. We  deal with these holes by modeling the student&apos;s learning process rather than the knowledge to be  learned. We attempt this by using Agent-prone Modal-Temporal Logic Specifications on a modified  Interface Model for a Datastructures Tutor not designed as an Instructional Graph. We demonstrate a  prototype software, and provide test examples within three well-known Instructional methods :  Didactic, Inquiry and Discovery. We conclude by presenting conclusions and further work strands.
This paper presents Threaded Multi-Path Execution (TME), which exploits existing hardware on a Simultaneous Multithreading (SMT) processor to speculatively execute multiple paths of execution. When there are fewer threads in an SMT processor than hardware contexts, threaded multi-path execution uses spare contexts to fetch and execute code along the less likely path of hard-to-predict branches. This paper
The Snort 2.0 Protocol Flow Analyzer classifies network application protocols into client and server data flows. Indepth analysis of these protocol data flows allows the Fusion Detection Engine to make intelligent decisions about protocol inspection, greatly enhances performance and efficiency, and helps to reduce false positives. Currently, the Fusion Detection Engine has an HTTP flow analyzer that is user-configurable and significantly reduces the Fusion Detection Engine&apos;s HTTP processing time. Protocol Flow Analyzer  What is a Protocol Flow?  A protocol flow refers to the client or server communication in an application protocol. For example, HTTP clientto -server communication is considered a flow and HTTP server-to-client communication is considered a separate flow. This allows the Fusion Detection Engine to break down a particular application protocol into two distinct flows, a client flow and a server flow. Application Protocol Breakdown SERVER FLOW CLIENT FLOW Server To Client Client To Server Protocol Flow Analysis  Protocol flow analysis is performed at a high level and is usually only concerned with a few important aspects of a particular protocol flow, such as a server response code or a client request type. Flow analysis does not replace other Snort 2.0 protocol inspection technologies; instead it compliments them. It is rather a generic analysis that allows an application protocol to be classified as a client or server flow. Once an application protocol is classified into a client flow and a server flow, it gives the Fusion Detection Engine useful knowledge as to the type of inspection and the regions of the protocol flow to inspect.
This paper presents a systematic approach to implementing fuzzy parsers. A fuzzy parser is a form of syntax analyzer that performs analysis on selected portions of its input rather than performing a detailed analysis of a complete source text. Fuzzy parsers are often components of software tools and also of program development environments that extract information from source texts. This paper clarifies the term fuzzy parser and introduces an object-oriented framework for implementing reusable and efficient fuzzy parsers. Applications of this framework are described via examples of two software tools. These tools exploit the facilities provided by fuzzy parsers for different purposes and also for different languages
This paper presents a method to locate geospatial data sets  over the Web that are needed to create land use plans. Land  use planning is a critical issue for many communities, and  current methods of finding data sets are quite cumbersome.
The paper introduces a software architecture to support a user from the image processing community in the development of time-constrained image processing applications on parallel computers. The architecture is based on abstract data types with a well defined interface. The interface separates an application from the actual hardware used. On the application side of the interface the programmer is presented with a familiar (sequential) programming model. On the hardware side of the interface detailed knowledge of a parallel machine may be employed to arrive at efficient implementations of basic functionality. Knowledge of both suitable data distributions for images and performance characteristics of operations on those image allows for automated selection of an appropriate data distribution scheme throughout the application. Experiments show that with little effort reasonable levels of efficiency and scalability are achieved on a 32-node MIMD architecture.  Keywords: software developmen...
Research on ubiquitous computing (UC) infrastructures at the Telecooperation Group emphasizes ears-and-mouth devices for nomadic users. In this context, the following document describes the prototype of such a device. This voicecentric headset features a Bluetooth radio for wireless network connectivity and a combination of sensors to gather context information, such as the location of the mobile user in space and the head orientation. The Talking Assistant headset targets tour, exhibition, and museum guidance as an initial application domain. A corresponding demonstrator testbed has been built in order to evaluate the functionality of the device, context model, and software architecture. The Talking Assistant is also a key element of the MUNDO ubiquitous computing infrastructure which will be briefly sketched at the end.
During the last few years, many eorts have been done in integrating dierent informations in a variational framework to segment images. Recent works on curve propagation were able to incorporate stochastic informations [14, 10] and prior knowledge on shapes [3, 11]. The information inserted in these studies is most of the time extracted oine. Meanwhile, other approaches have proposed to extract region information during the segmentation process itself [2, 4, 13].
this paper, the author proposes a new document image analysis method with cooperative interaction between layout analysis and logical structure analysis to resolve the above problems. The proposed method has the following three advantages compared with other methods
... This paper offers a comprehensive description of the problems arising from typing binary methods, and collects and contrasts diverse views and solutions. It summarizes the current debate on the problem of binary methods for a wide audience.
After a short introduction to the BOS project, we discuss the &quot;why, what and how&quot; of  the use of formal methods in the project, some recent experience using Promela/Spin and  reflections on the pragmatics of validation.
FantasyA is a computer game where two characters face each other in a duel and emotions are used as the driving elements in the action decision of the characters. In playing the game, the user influences the emotional state of his or her semi-autonomous avatar using a tangible interface for affective input, the SenToy. In this paper we show how we approached the problem of modelling the emotional states of the synthetic characters, and how to combine them with the perception of the emotions of the opponents in the game. This is done by simulating the opponents action tendencies in order to predict their possible actions. For the user to play, he or she must understand the emotional state of his opponent which is achieved through animations (featuring affective body expressions) of the character. FantasyA was evaluated with 30 subjects from different ages and the preliminary results showed that the users liked the game and were able to influence the emotional states of their characters, in particular the young users.
Trust and reputation are central to effective interactions in open multi-agent systems in which agents, that are owned by a variety of stakeholders, can enter and leave the system at any time. This openness means existing trust and reputation models cannot readily be used. To this end, we present FIRE, a trust and reputation model that integrates a number of information sources to produce a comprehensive assessment of an agent&apos;s likely performance. Specifically, FIRE incorporates interaction trust, role-based trust, witness reputation, and certified reputation to provide a trust metric in virtually all circumstances. FIRE is empirically benchmarked and is shown to help agents effectively select appropriate interaction partners.
this paper we show how the notion of paraconsistent negation can be thought from a modal viewpoint
We introduce a novel method to diagnose pronunciation errors that are most critical to the intelligibility of L2 learners. A preliminary study showed that error rates computed by a speech recognition-based system can be used to characterize intelligibility. We deduce a probabilistic algorithm to derive intelligibility from error rates. We also define an error priority function that indicates which errors are most critical to intelligibility. Experimental results proved the validity of the approach.
When patterns occur in large groups generated by a single source (style consistent test data), the statistics of the test data differ from those of the training data which consists of patterns from all sources. We present a Gaussian model for continuously distributed sources under which we develop  adaptive classifiers that specialize to the statistics of styleconsistent test data. On NIST handwritten digit data, the adaptive classifiers reduce the error rate by more than 50% operating  on one writer (ff10 samples/class) at a time.
Intelligent problem solving requires the ability to select actions autonomously from a specific state to reach objectives. Planning algorithms provide approaches to look ahead and select a complete sequence of actions. Given a domain description consisting of preconditions and effects of the actions the planner can take, an initial state, and a goal, a planning program returns a sequence of actions to transform the initial state into a state in which the goal is statisfied. Classical planning research has addressed this problem in a domain-independent manner -- the same algorithm generates a complete plan for any domain specification. This feature comes at a cost which domain-independent planners incur either in high search efforts or in tedious hand-coded domain knowledge. Previous approaches...
We couple artificial ant and computer graphics techniques  to create an approach to Non-Photorealistic Rendering (NPR). A user  interactively takes turns with an artificial ant colony to transform a photograph  into a stylized picture. In turn with the user specifying its control  parameters, members of a colony of artificial ants scan the source image  locally, following strong edges or wandering around flat zones, and draw  marks on the canvas depending on their discoveries. Among a variety of  obtained effects, two are painterly rendering and pencil sketching.
A dual half-wave patch antenna for channel measurements is being developed. The polarization isolation and bandwidth are suitable for an antenna array in channel sounding.
This research asked the following question: is there a correlation between types of organizational culture and factors influencing knowledge transfer? It hypothesized that organizations scoring high on the cultural factors of openness to change/innovation, and task-oriented organizational growth would tend to be fertile to knowledge transfer. Second, it hypothesized that organizations scoring high on the factors of bureaucratic and competition/confrontation would tend to be infertile to knowledge transfer.
Microarray technology is becoming an essential tool in functional genomics. The possibility of monitoring the expression level of thousands of genes simultaneously, as the response to a particular biological condition, gives to the biologists the chance to widen the aims of their experiments and opens a door to the understanding of cellular transcription processes. In order to extract valuable information from the big amount of data that microarrays experiments generate, suitable and powerful statistical and computational methods are required. An example of the effort of statisticians and computer scientists is the release of the first Bioconductor software and the increasing number of functions for microarray data analysis implemented in several programming languages (e.g. R, MATLAB, Java) by different research teams all around the world.
MPEG-7 is an emerging standard for representing information carried by multimedia data. Such a standard is considered crucial for the oncoming integration of broadcast (TV) and Internet technologies and applications. This paper reports on the development of methods for searching multimedia data using MPEG-7 in the context of the SAMBITS application, and, in particular, in the development of a SAMBITS terminal. SAMBITS, a European broadcast application project, developed a multimedia studio and terminal technology for new interactive broadcast services based on the MPEG standards. This paper describes, after an introduction to MPEG7 and a description of SAMBITS, a retrieval model for MPEG-7 in a broadcast terminal. The retrieval model was developed and implemented using the HySpirit software, which is a flexible framework for representing complex data and describing retrieval functions effectively. A user interface was also implementing to provide insights in integrating a search functionality in a broadcast terminal.
The aim of the paper is showing, how projection methods can be used for  computing contact-problems in elasticity for different classes of obstacles.
In this paper we propose a new numerically reliable computational approach to determine the inner-outer factorization of a rational transfer matrix G of a linear descriptor system. In contrast to existing computationally involved &quot;one-shot&quot; methods which require the solution of Riccati or generalized Riccati equations, the new approach relies on an effcient recursive zeros dislocation technique. The resulting inner and outer factors have always minimal order descriptor representations. The proposed approach is completely general being applicable whenever  G is proper/strictly proper or not, or of full column/row rank or not.
Information integration systems, also knows as mediators, information brokers, or information gathering agents, provide uniform user interfaces to varieties of different information sources. With corporate databases getting connected by intranets, and vast amounts of information becoming available over the Internet, the need for information integration systems is increasing steadily. Our work focusses on query planning in such systems...
We show that a graph has an orientation under which every circuit of even length  is clockwise odd if and only if the graph contains no subgraph which is, after the  contraction of at most one circuit of odd length, an even subdivision of K 2;3 . In fact  we give a more general characterisation of graphs that have an orientation under  which every even circuit has a prescribed clockwise parity. Moreover we show that  this characterisation has an equivalent analogue for signed graphs.
Infinite contexts and their corresponding lattices are of theoretical  and practical interest since they may offer connections with and  insights from other mathematical structures which are normally not restricted  to the finite cases. In this paper we establish a systematic connection  between formal concept analysis and domain theory as a categorical  equivalence, enriching the link between the two areas as outlined in [25].
In this paper, we propose a robust wavelet domain method for noise filtering in medical images. The proposed method adapts itself to various types of image noise as well as to the preference of the medical expert: a single parameter can be used to balance the preservation of (expert-dependent) relevant details against the degree of noise reduction.
Over the last decade, a new idea challenging the  classical self-non-self viewpoint has become  popular amongst immunologists. It is called the  Danger Theory. In this conceptual paper, we  look at this theory from the perspective of  Artificial Immune System practitioners. An  overview of the Danger Theory is presented with  particular emphasis on analogies in the Artificial  Immune Systems world. A number of potential  application areas are then used to provide a  framing for a critical assessment of the concept,  and its relevance for Artificial Immune Systems.
In Thailand, acid sulphate soils are found mainly in the Central Plains. They cover approximately 1.5 million ha. They are dominantly soft clay soils. Sulphur occurs mainly in the form of sulphide in iron sulphide layer below the plow layer. The major limitations of these soils for agricultural production are their strong acidity, phosphorus deficiency, and abundance of aluminium (Al), iron (Fe) and manganese (Mn) causing toxicities. Most of the acid sulphate soils in Thailand are used for rice cultivation. Without soil improvement measures, yields are relatively low, ranging from 1 to 1.5 t ha    . Traditionally, the cultivation of glutinous and non-glutinous rice during the wet season has been identified the most sustainable use of these soils. There is no single method for the reclamation and management of acid sulphate soils. But reducing soil acidity by liming and leaching (e.g., by flooding) can improve soil productivity. Adequate drainage, frequent applications of water and moderate applications of lime can improve the soil conditions significantly. Drainage is essential for the leaching, and hence efficient drainage systems are required to drain the excess water applied. Changes in the soil physical and chemical properties of acid sulphate soils affect local land-use systems and cropping practices. There are options for the diversification of rice-based cropping systems to include other cash crops, such as fruits and vegetable crops. Pineapple and sugarcane have shown to grow well on acid sulphate soils. The raised-bed system (i.e., ridge and ditch system) is also an appropriate technology for the use of acid sulphate soils. In some areas, these systems are used successfully for mangoes, citrus, bananas and watermelon. Acid sulphate soils exhibit considerable li...
Traditional decision theory has assumed that agents havecomplete,  consistent and readily available beliefs and preferences. Obviously,even if  an expert system has complete and consistent beliefs, it cannot have them readily  available. Moreover, some beliefs about beliefs are not even approximately computable.
We consider relevant aspects of evaluating creativity to be input,  output and the process by which the output is achieved. These issues
fic web site and enter their current location and destination. Although this is a powerful capability, it nonetheless requires the user to remember to check in a timely manner to adjust their route or leave earlier than forecast and to enter mundane data into the user interface. In ubiquitous computing, an autonomous agent would constantly be checking the user&apos;s current location (available from GPS or in-building wireless infrastructure) and their calendar to determine if an adjustment in schedule should be made. If traffic conditions warrant it, the user can be signaled and their PDA can show a brief reminder with the minutes left before they should head off. Another example uses a biologist in a laboratory that would like to record all the steps in their experimental protocol so that others can faithfully reproduce them. Traditional computing would provide the biologist with a PDA or laptop in which to enter each step but requiring the biologist to interrupt the flow of their work to
This paper presents some general lessons in institution building that has relevance for reform of the judiciary. The paper emphasizes the value of simplicity in design commensurate with country capacity, the importance of innovation/experimentation, and of economic openness in effective institution building. It underscores how the incentives of individuals depend on both the details of institutional design within the judiciary itself but also some critical institutions external to the judiciary. Finally the paper argues for the need to ground reform initiatives on a solid empirical and comparative approach. It illustrates some of these issues by drawing on a recent project conducted by the World Bank and others. World Bank Policy Research Working Paper 3134, September 2003  The Policy Research Working Paper Series disseminates the findings of work in progress to encourage the exchange of ideas about development issues. An objective of the series is to get the findings out quickly, even if the presentations are less than fully polished. The papers carry the names of the authors and should be cited accordingly. The findings, interpretations, and conclusions expressed in this paper are entirely those of the authors. They do not necessarily represent the view of the World Bank, its Executive Directors, or the countries they represent. Policy Research Working Papers are available online at http://econ.worldbank.org
Recent results from a multiwavelength survey of spatially resolved outows in nearby active galaxies are presented. Optical Fabry-Perot and long-slit spectroscopic data are combined with VLA and ROSAT images, when available, to probe the warm, relativistic and hot gas components involved in the outow. The emphasis is placed on objects which harbor wide-angle galactic-scale outows but also show evidence at radio or optical wavelengths for collimated jet-like phenomena (e.g., Circinus, NGC 4388, and to a lesser extent NGC 2992). Our results are compared with the predictions from published jet-driven thermal wind models.
The primary objective in cutting and packing problems is trim loss or material  input minimization (in stock cutting) or value maximization (when  packing into a knapsack). However, in real-life production we usually have  many other objectives (costs) and constraints, for example, the number of  different patterns. We propose a new simple model for setup minimization  (in fact, an extension of the Gilmore-Gomory model for trim loss minimization)  and develop a branch-and-price algorithm on its basis. The algorithm  is tested on problems with industrially relevant sizes of up to 150 product  types. The behavior is investigated on a broad range of problem classes  and significant differences between instances of a class are found. Allowing  even 0.2% more material input than the minimum significantly improves the  results, this tradeoff has not been investigated in the earlier literature. Comparison  to a state-of-the art heuristic KOMBI shows mostly better results; to  a previous exact approach of Vanderbeck, slightly worse solutions and much  worse LP bound, which is a consequence of the simplicity of the model.
Let b_1 .. b_m be an arbitrary basis of lattice L that is a block Korkin--Zolotarev basis with block size B and let l_i denote the successive minima of lattice L. We prove that for i = 1 .. m [4/(i + 3)] z^[-2(i-1)/(B-1)] &lt; |b_i|^2/l_i(L)^2 &lt; [(i+3)/4] z^[2(m-i)/(B-1)] where z is the Hermite constant. For B = 3 we establish the optimal upper bound |b_1|^2/l_i^2 &lt; (3/2)^[(m-3)/2] and we present block Korkin--Zolotarev lattice bases for which this bound is tight. We improve the Nearest Plane Algorithm of Babai (1986) using block Korkin--Zolotarev bases. Given a block Korkin--Zolotarev basis b_1 .. b_m with block size B and x in L(b_1 .. b_m ) a lattice point v can be found in time B^O(B) satisfying |x - v|^2 &lt; m z^[2m/(B-1)] min[u in L] |x-u|^2.
Graph has become increasingly important in modelling complicated structures and schemaless data such as proteins, chemical compounds, and XML documents. Given a graph query, it is desirable to retrieve graphs quickly from a large database via graph-based indices. In this paper, we investigate the issues of indexing graphs and propose a novel solution by applying a graph mining technique. Different from the existing path-based methods, our approach, called gIndex, makes use of frequent substructure as the basic indexing feature. Frequent substructures are ideal candidates since they explore the intrinsic characteristics of the data and are relatively stable to database updates. To reduce the size of index structure, two techniques, size-increasing support constraint  and discriminative fragments, are introduced. Our performance study shows that gIndex has 10 times smaller index size, but achieves 3-10 times better performance in comparison with a typical path-based method, GraphGrep. The gIndex approach not only provides an elegant solution to the graph indexing problem, but also demonstrates how database indexing and query processing can benefit from data mining, especially frequent pattern mining. Furthermore, the concepts developed here can be applied to indexing sequences, trees, and other complicated structures as well.
Shape as well as force is required to represent the cloth in virtual environment. A model, based on the cost functions, realizes the shape together with force by using physical data obtained from Kawabata Evaluation System. However, cost/ energy minimization technique is very slow that restricts the increase of mesh density. Some of previous work has employed the adaptive refinement in this regard. However, mesh simplification is not considered. We propose a method, which proceeds in the reverse direction of refinement. Simulation starts with denser mesh of cloth and mesh density is reduced adaptively during simulation, which we say the `Adaptive Coarsening&apos;. This paper describes the comparison between adaptive refinement and adaptive coarsening. We found that adaptive coarsening is comparatively better in terms of speed and quality for cloth simulation.
There are many shape recognition algorithms. Their Achilles heel usually is the control of the number of false positive, or false alarms. A match between two shapes F and F    being proposed with a distance d, we compute the inumber of false alarms&quot; of this match. This number is computed as an upper bound of the expectation of the number of shapes which could have casually a distance lower than d to F in the database. It turns out that a simple encoding of shape elements as pieces of level lines leads to compute numbers of false alarms for the good matches as small as 10    . As an application, one can decide with a parameterless method whether any two digital images share some shapes or not.
The Winnow family of learning algorithms can cope well  with large numbers of features and is tolerant to variations in document  length, which makes it suitable for classifying large collections of large  documents, like patent applications.
Although 1995 was an important year in the deregulation of the Finnish electricity industry, with the Electricity Market Act (EMA), the market in Finland has always been very liberalized. This is probably one of the most distinct features of the Finnish situation. To put the Finnish reform process in perspective, a four-dimension restructuring framework is introduced. Its dimensions are market type, ownership and horizontal and vertical integration. Some important aspects of reforms in other countries are also presented, showing the originality of the Finnish case. We give here a description of the market before and after 1995, with a highlight on the changes. Such an account of the Finnish electricity reform is not otherwise documented in the energy literature. ff 2000 Elsevier Science Ltd. All rights reserved.
Egecioglu and Remmel [2] gave an interpretation for the entries of the  inverse Kostka matrix K -1 in terms of special rim-hook tableaux. They were able  to use this interpretation to give a combinatorial proof that KK -1 = I but were  unable to do the same for the equation K -1 K = I. We define an algorithmic signreversing  involution on rooted special rim-hook tableaux which can be used to prove  that the last column of this second product is correct. In addition, following a suggestion  of Chow [1] we combine our involution with a result of Gasharov [5] to give  a combinatorial proof of a special case of the (3+1)-free Conjecture of Stanley and  Stembridge [14].
Invariance and representation learning are  important precursors to modeling and classi-  cation tools particularly for non-Euclidean  spaces such as images, strings and nonvectorial  data. This article proposes a  method for learning invariances in data while  jointly estimating a model. The technique results  in a convex programming problem with  a consistent and unique solution. Representation  variables are considered as ane  transformations con  ned by multiple equality  and inequality constraints. These interact  individually with each datum yet maintain  the overall solvability of the model estimation  process while uniquely solving for the  representational variables themselves. The  method is applicable to various types of modeling,  including maximum likelihood estimation,  principal components analysis, and discriminative  methods. Starting from ane invariance,  several types of invariances are proposed  and implemented as convex programs  including clustering, permutation, selection,  rotation, and translation. Experiments on  non-vectorial data such as images and collections  of tuples provide promising results.
Recent work in machine translation and information extraction has demonstrated the utility of a level that  represents the predicate-argument structure. It would be especially useful for machine translation to have  two such Proposition Banks, one for each language under consideration. A Proposition Bank for English  has been developed over the last few years, and we describe here our development of a tool for facilitating  the development of a Chinese Proposition Bank. We also discuss some issues specific to the Chinese  Treebank that complicate the matter of mapping syntactic representation to a predicate-argument level, and  report on some preliminary evaluation of the accuracy of the semantic tagging tool.
Programming security vulnerabilities are the most common cause of software security breaches in current day computing. While these can easily be avoided by an attentive programmer, many programs still contain these kinds of vulnerabilities. This document will describe what the most commonly occuring ones are and will then explain how these can be abused to make a program do something it did not intend to do. We will then take a look at how a recent vulnerability in popular piece of software was exploited to allow an attacker to take control of the execution flow of that program. Several solutions exist to detect and prevent many, though not all, of the vulnerabilities described in this document in existing programs without requiring source code modifications, and in some cases without even requiring access to the source code to the applications. We will take an indepth look at how these solutions are implemented and what their effects are on legitimate programs, how they attempt to mitigate the restrictions they impose and what their impact is on the performance of the programs they attempt to protect. We will also describe if and how these solutions can be bypassed.
Genesis and ATIS are being developed under the umbrella of the Intelligent Vehicle Highway  Systems to facilitate various kinds of travel, including daily commuting to work via  private/public transportation and visiting new places and points of attraction. Travelers are  naturally mobile, and the most effective way to aid travelers is via mobile computing, which is  being explored in the Genesis project at Minnesota. The travelers can use personal communication  devices including pagers and portable computers (e.g. Apple Newton) to avail themselves of  the ATIS services provided by Genesis. This extended abstract presents an overview of the goals  and preliminary design of Genesis. We believe that ATIS provides a very important commercial  application of mobile computing and can be the killer application to bring mobile computing to  mass markets. This paper focuses on describing the application domain rather than evaluating  candidate solutions.
Independent component analysis (ICA) has proven useful for modeling brain and electroencephalographic (EEG) data. Here, we present a new, generalized method to better capture the dynamics of brain signals than previous ICA algorithms. We regard EEG sources as eliciting spatio-temporal activity patterns, corresponding to, e.g., trajectories of activation propagating across cortex. This leads to a model of convolutive signal superposition, in contrast with the commonly used instantaneous mixing model. In the frequency-domain, convolutive mixing is equivalent to multiplicative mixing of complex signal sources within distinct spectral bands. We decompose...
This paper describes the motivations and the design of a communication architecture...
This paper provides a sample of a LaTeX document which conforms, somewhat loosely, to the formatting guidelines for ACM SIG Proceedings. It is an alternate style which produces a tighterlooking  paper and was designed in response to concerns expressed, by authors, over page-budgets. It complements the document Author &apos;s (Alternate) Guide to Preparing ACM SIG Proceedings Using   T E X2 ff and BibT E X. This source file has been written with the intention of being compiled under L    T E X2 ff and BibTeX. The developers have tried to include every imaginable sort of &quot;bells and whistles&quot;, such as a subtitle, footnotes on title, subtitle and authors, as well as in the text, and every optional component (e.g. Acknowledgments, Additional Authors, Appendices), not to mention examples of equations, theorems, tables and figures. To make best use of this sample document, run it through L    T E X and BibTeX, and compare this source code with the printed output produced by the dvi file. A compiled PDF version is available on the web page to help you with the `look and feel&apos;.
Previous studies have demonstrated that designing special  purpose constraint propagators can significantly improve the effciency  of a constraint programming approach. In this paper we present an efficient  algorithm for bounds consistency propagation of the generalized  cardinality constraint (gcc). Using a variety of benchmark and random  problems, we show that on some problems our bounds consistency algorithm  can dramatically outperform existing state-of-the-art commercial  implementations of constraint propagators for the gcc. We also present  a new algorithm for domain consistency propagation of the gcc which  improves on the worst-case performance of the best previous algorithm  for problems that occur often in applications.
Discovering communities from a graph structure such as the Web has become an interesting research problem recently. In this paper, comparing with the state-of-the-art authority detecting and graph partitioning methods, we propose a concentric-circle model to more accurately define communities. With this model, a community could be described as a set of concentric-circles. The most important objects representing the concept of a whole community lie in the center and are called core objects. Affiliated objects, which are related to the core objects, surround the core with different ranks. Base on the concentric-circle model, a novel algorithm is developed to discover communities conforming to this model. We also conducted a case study to automatically discover research interest groups in the computer science domain from the Web. Experiments show that our method is very effective to generate high-quality communities with more clear structure and more tunable granularity.
We describe the ICSI-SRI-UW team&apos;s entry in the Spring 2004 NIST Meeting Recognition Evaluation. The system was derived from SRI&apos;s 5xRT Conversational Telephone Speech (CTS) recognizer by adapting CTS acoustic and language models to the Meeting domain, adding noise reduction and delay-sum array processing for far-field recognition, and postprocessing for cross-talk suppression. A modified MAP adaptation procedure was developed to make best use of discriminatively trained (MMIE) prior models. These meeting-specific changes yielded an overall 9% and 22% relative improvement as compared to the original CTS system, and 16% and 29% relative improvement as compared to our 2002 Meeting Evaluation system, for the individual-headset and multiple-distant microphones conditions, respectively.
Problem parameters are ubiquitous. In every area of computer  science, we find all kinds of &quot;special aspects&quot; to the problems encountered.
The problem of inferring 3D orientation of a camera from video sequences has been mostly addressed by first computing correspondences of image features. This intermediate step is now seen as the main bottleneck of those approaches. In this paper, we propose a new 3D orientation estimation method for urban (indoor and outdoor) environments, which avoids correspondences between frames. The basic scene property exploited by our method is that many edges are oriented along three orthogonal directions; this is the recently introduced Manhattan world (MW) assumption. In addition to the novel adoption of the MW assumption for video analysis, we introduce the small rotation (SR) assumption, that expresses the fact that the video camera undergoes a smooth 3D motion. Using these two assumptions, we build a probabilistic estimation approach. We demonstrate the performance of our method using real video sequences.
Poker is an imperfect information game that requires decision-making under conditions of uncertainty, much like many real-world applications. Strong poker players have to skillfully deal with multiple opponents, risk management, opponent modeling, deception and unreliable information. These features make poker an interesting area for Arti  cial Intelligence research. This thesis describes work done on improving the knowledge representation, betting strategy, and opponent modeling of Loki, a pokerplaying program at the University of Alberta. First, a randomized betting strategy that returns a probability triple is introduced. A probability triple is a probabilistic representation of betting decisions that indicates the likelihood of each betting action occurring in a given situation. Second, real-time simulations are used to compute the expected values of betting decisions. These simulations use selective sampling to maximize the information obtained with each simulation trial. Experimental results show that each of these enhancements represents a major advance in the strength of Loki.
In 1996, the College of Engineering at Southern Illinois University Carbondale began offering a  freshmen orientation program, appropriately named SUCCESS Week. In this voluntary orientation program, new freshmen engineering students pay a small fee to attend a three-day program during the week before the start of fall classes. About 60% of the freshmen participate in this program. Fifteen to twenty-five upper class mentors are hired to host the new freshmen. As with other orientation programs, activities include guided tours for class locations, a group photo, a pizza party, a session on establishing campus computer and internet access, and a movie night. A unique aspect of this freshmen orientation is the Team&apos;s Initiative Course at the Touch of Nature Environmental Center. Professional group facilitators conduct icebreakers and team games so that freshmen and mentors get to know each other. After five years, an assessment of the program&apos;s impact on retention is presented.
This paper presents a formal technique for incremental construction of system specifications, algorithm descriptions, and simulation proofs showing that algorithms meet their specifications
Although the use of computers to create otherwise impossible  characters has long been a staple of film, corresponding use in live stage  performance is uncommon, and such characters have typically been only  electronic puppets. Computer theatre experiments over the last decade have  ranged from digital scenography and virtual reality, to semi-autonomous  computer, or synthetic, actors. Improvisation is an essential part of rehearsals  for a live stage (scripted) performance, providing development of character and  plot; in addition, the rehearsal process provides a training ground for the actors  involved with the performance. The author aims to develop synthetic  characters capable of taking full part in this process. Initial experiments  (dynamically adding an affect component to scripted speech) have been  promising, and form the beginnings of the larger system, which will  autonomously build up a model of the character that the synthetic actor is  portraying, with the aim of presenting continually improved performances.
This paper develops a new I/O automaton model called the Clock General Timed Automaton (Clock GTA) model. The Clock GTA is based on the General Timed Automaton (GTA) of Lynch and Vaandrager. The Clock GTA provides a systematic way of describing timing-based systems in which there is a notion of &quot;normal&quot; timing behavior, but that do not necessarily always exhibit this &quot;normal&quot; behavior. It can be used for practical time performance analysis based on the stabilization of the physical system. We use the Clock GTA automaton to model, verify and analyze the paxos algorithm. The paxos algorithm is an efficient and highly fault-tolerant algorithm, devised by Lamport, for reaching consensus in a distributed system. Although it appears to be practical, it is not widely known or understood. This paper contains a new presentation of the paxos algorithm, based on a formal decomposition into several interacting components. It also contains a correctness proof and a time performance and fault-tole...
The critical question challenging information systems educators in the new millennium is: How can university information systems courses add enough value to students that they will choose to study with us for three years rather than opt for a one-year certification course leading to similar economic and status outcomes in the short term? This paper describes  research in progress aimed at assessing the feasibility and desirability of achieving a better  match between delivery of information systems education and the professional workplace. A  simple working model for educating a &quot;Compleat Information Systems Graduate&quot; is presented. A brief introduction to cognitive and learning principles is followed by a  discussion of Problem Based Learning and its potential to help to achieve a better fit between student aspirations and employer requirements. The paper concludes with a description of an  experimental trial of the Problem Based Learning approach in an information systems subject  conducted in the second half of 1999.
In future digital libraries, even &quot;perfect&quot; retrieval will typically return too much material for a user to cope with. One way to deal with this problem is to produce automated summaries tailored to the user&apos;s requirements. One of the prime purposes of a summary of a collection of documents is to collapse together all of the important information elements that are common to the collection. This requires some method of discovering classes of similar items, e.g., word classes. This paper describes automated techniques for placing words in similarity classes. To do this, each target word is described by a composite vector that records the occurrence of words positioned near any occurrence of the target. Target words with similar contexts are grouped together by a clustering algorithm. We describe how such classifications can be used in information retrieval and for the summarization of biological literature.
This work explores the framework of Conceptual Knowledge Processing using Formal Concept Analysis (FCA) and Conceptual Graphs (CGs). We hypothesize that the theory of Power Context Families can be used to model data structures for Conceptual Graphs, which can in turn be used to extend Conceptual Knowledge Processing (in particular the FCA framework) to accommodate the more expressive knowledge representation of Conceptual Graphs. Furthermore, we hypothesize that Conceptual Graphs can be mapped onto a relational database model, using the modelled data structures.
A major issue with large dynamic datasets is the processing of small changes in the input through correspondingly small rearrangements of the output. This was the motivation behind the design of incremental or on-line algorithms for lattice maintenance, whose work amounts to a gradual construction of the final lattice by repeatedly adding rows/columns to the data table. As an attempt to put the incremental trend on strong theoretical grounds, we present a generic algorithmic scheme that is based on a detailed analysis of the lattice transformation triggered by a row/column addition and of the underlying sub-structure. For each task from the scheme we suggest an efficient implementation strategy and put a lower bound on its worst-case complexity. Moreover, an...
This paper considers several filtering methods of stochastic nature, based on Monte Carlo drawing, for the  sequential data assimilation in nonlinear models. They include some known methods such as the particle filter  and the ensemble Kalman filter and some others introduced by the author: the second-order ensemble Kalman  filter and the singular extended interpolated filter. The aim is to study their behavior in the simple nonlinear  chaotic Lorenz system, in the hope of getting some insight into more complex models. It is seen that these filters  perform satisfactory, but the new filters introduced have the advantage of being less costly. This is achieved  through the concept of second-order-exact drawing and the selective error correction, parallel to the tangent  space of the attractor of the system (which is of low dimension). Also introduced is the use of a forgetting  factor, which could enhance significantly the filter stability in this nonlinear context.
The global identifiability is a structural property of models, which associates a unique set of parameters with given input/output response. The translation of this property into bond graph modelling language allows the combination of the physically meaningful language of bond graph methodology and the numerical accuracy of identified transfer function models. Based on the building mechanisms of a transfer function from a bond graph model, the paper develops and explains why a bond graph can be not identifiable. Both internal and input/output dynamics can be written with the Mason&apos;s rule, using causal loops and action chains. Then the way the combination of causal loops and action chains influences the identifiability of models is discussed. As a result a criterion is given, which decides whether a bond graph model is structurally globally identifiable or not. This is a crucial issue in order to guarantee the reliability of identification processes. 
In this paper two-sided estimate of the distribution of the exit time is shown together with off diagonal heat kernel lower bound for random walks on weighted graphs.  
We present a simple, one-pass word alignment algorithm for parallel text. Our algorithm utilizes synchronous parsing and takes advantage of existing syntactic annotations. In our experiments the performance of this model is comparable to more complicated iterative methods. We discuss the challenges and potential benefits of using this model to train syntactic parsers for new languages.
Boosting is well known to increase the accuracy of propositional  and multi-relational classification learners. However, the base  learner&apos;s efficiency vitally determines boosting&apos;s efficiency since the complexity  of the underlying learner is amplified by iterated calls of the  learner in the boosting framework. The idea of restricting the learner  to smaller feature subsets in order to increase efficiency is widely used. Surprisingly,
This paper describes a project that extends the multiple objective decision support system (MODSS) by offering knowledge-based guidance to an intelligent multiple objective decision support system (IMODSS). This IMODSS integrates expert system (ES), multiple objective decision-making (MODM) methodologies, graphical user interface (GUI) and decision support systems (DSS) technologies. This IMODSS uses an expert system shell CLIPS to build  a knowledge base to guide the decision-makers (DMs) to select the most suitable MODM method(s) from the MODM methodology base in order to solve their particular decision problems. This IMODSS has been implemented and tested. This paper mainly discusses the  design and implementation of the knowledge-based intelligent guidance subsystem in IMODSS.
Given a set of strings, the subsequence automaton accepts  all subsequences of these strings. We will derive a lower bound for the  maximum number of states of this automaton. We will prove that the  size of the subsequence automaton for a set of k strings of length n is      ) for any k  1. It solves an open problem posed by Crochemore  and Troncek [2] in 1999, in which only the case k  2 was shown.
HP Labs has developed a software platform, called MUMS, for moderating economics games between human and/or robot participants. The primary feature of this platform is a flexible scripting language that allows a researcher to implement any economics games in a relative short time. This scripting language eliminates the need to program low-level functions such as networking, databases and interface components. The scripts are description of games including definitions of roles, timing rules, the game tree (in a stage format), input and output (with respect to a role, not client software).
this paper we show how Bayesian models can be used advantageously for various adaptation  tasks in open WWW-based hypermedia systems. We discuss how to model user knowledge  about different topics and learning dependencies between these topics, how to make  inferences for calculating the system&apos;s belief on a user&apos;s knowledge based on observations  from exercises/observations. We illustrate this discussion by examples from an educational  hypermedia system we developed for our course &quot;
This paper describes FLAVERS, a finite-state verification approach that analyzes whether concurrent systems satisfy user-defined, behavioral properties. FLAVERS automatically creates a compact, event-based model of the system that supports efficient data-flow analysis. FLAVERS achieves this efficiency at the cost of precision. Analysts, however, can improve the precision of analysis results by selectively and judiciously incorporating additional semantic information into an analysis.
We present a method for extracting verbal (diathesis) alternations from a valency dictionary, based on comparison  of selectional restrictions. The quality of match between selectional restrictions is evaluated according  to an entropy-based measure with backing-off facility. We use the proposed method to derive a provisional  listing of the range and distribution of verbal alternations in Japanese.
To model combinatorial decision problems involving  uncertainty and probability, we extend the  stochastic constraint programming framework proposed  in [Walsh, 2002] along a number of important  dimensions (e.g. to multiple chance constraints  and to a range of new objectives). We also provide a  new (but equivalent) semantics based on scenarios.
The category-partition method and the classification-tree method help construct test cases from specifications. In both methods, an early step is to identify a set of categories (or classifications) and choices (or classes). This is often performed in an ad hoc manner due to the absence of systematic techniques. In this paper, we report and discuss three empirical studies to investigate the common mistakes made by software testers in such an ad hoc approach. The empirical studies serve three purposes: (a) to make the knowledge of common mistakes known to other testers so that they can avoid repeating the same mistakes, (b) to facilitate researchers and practitioners develop systematic identification techniques, and (c) to provide a means of measuring the effectiveness of newly developed identification techniques. Based on the results of our studies, we also formulate a checklist to help testers detect such mistakes.
We consider the problem of error control for receiver-driven layered multicast of audio and  video over the Internet. The sender injects into the network multiple source layers and multiple  channel coding (parity) layers, some of which are delayed relative to the source. Each receiver  subscribes to the number of source layers and the number of parity layers that optimizes the  receiver&apos;s quality for its available bandwidth and packet loss probability. We augment this  layered FEC system with layered pseudo-ARQ. Although feedback is normally problematic in  broadcast situations, ARQ can be simulated by having the receivers subscribe and unsubscribe  to the delayed parity layers to receive missing information. This pseudo-ARQ scheme avoids an  implosion of repeat requests at the sender and is scalable to an unlimited number of receivers.
this article:  1. Solar Array. The solar array converts solar energy to electrical energy and delivers it to the loads and battery. The solar array consists of two wings, each comprising four solar panels supported by a boom and attached to a solar array drive (SAD). The SAD rotates the wings and electrically connects their solar cell circuitry, via a junction box, through slip rings to the main bus. The redundant control electronics unit (CEU) controls the drives so they can track the Sun. The solar panels are populated with silicon solar cells electrically connected into strings
This paper uses laboratory methods to evaluate determinants of supra-competitive pricing. The experiment involves three treatments, each with the same market supply, demand, and competitive price. In the baseline treatment, capacity is divided among five sellers so that the competitive price is a Nash equilibrium. Market power is created in a second treatment by reallocating capacity among the sellers. This market power raises observed prices in all sessions. In a third treatment, the three smallest sellers are merged in a way that holds market power constant. The consolidation has little residual effect on prices.
This paper presents the results of a perception experiment designed to investigate the contribution of a pre-focal hesitation pause, delayed focal peak and a raised F0 range for signaling interrogative mode in Swedish. The results are consistent with a previous study which demonstrated that a delayed peak combined with a raised F0 range can effectively signal interrogative mode in Swedish echo questions. In addition, the present study supports the hypothesis that the presence of a pre-focal hesitation pause strengthens the interpretation of a focal peak delay as signaling question intonation. The results are discussed in terms of a framework of biological codes for universal meanings of intonation proposed by Gussenhoven. An additional biological code is proposed on a behavioral level where cognitive loading results in a hesitation pause or dysfluency which is perceived as non-assertiveness.
We first show on a number of examples that positive polar questions, negative polar questions and alternative questions (containing a proposition and its negation) are not interchangeable in context. We will account for the differences pragmatically, using decision theory. We offer a simple classification of three types of use, which covers a number of phenomena hitherto not systematically dealt with. Finally, we do away with Ladd’s typology of negative polar questions and give a more systematic interpretation of the data.  
We developed a general method for global conformal parameterizations based on the structure of the cohomology group of holomorphic one-forms with or without boundaries [1], [2]. For genus zero surfaces, our algorithm can find a unique mapping between any two genus zero manifolds by minimizing the harmonic energy of the map. In this paper, we apply the algorithm to the cortical surface matching problem. We use a mesh structure to represent the brain surface. Further constraints are added to ensure that the conformal map is unique. Empirical tests on MRI data show that the mappings preserve angular relationships, are stable in MRIs acquired at different times, and are robust to differences in data triangulation, and resolution. Compared with other brain surface conformal mapping algorithms, our algorithm is more stable and has good extensibility. Index Terms--- Conformal Map, Brain Mapping, Landmark Matching, Spherical Harmonic Transformation.
Traditionally, neural networks such as multi-layer perceptrons handle acoustic context by increasing the dimensionality of the observation vector, in order to include information of the neighbouring acoustic vectors, on either side of the current frame. As a result the monolithic network is trained on a high multi-dimensional space. The trend is to use the same fixed-size observation vector across the one network that estimates the posterior probabilities for all phones, simultaneously. We propose a decomposition of the network into modular components, where each component estimates a phone posterior. The size of the observation vector we use, is not fixed across the modularised networks, but rather accounts for the phone that each network is trained to classify. For each observation vector, we estimate very large acoustic context through broad-class posteriors. The use of the broad-class posteriors along with the phone posteriors greatly enhance acoustic modelling. We report significant improvements in phone classification and word recognition on the TIMIT corpus. Our results are also better than the best context-dependent system in the literature.
Numerical function approximation over a Boolean domain is a classical  problem with wide application to data modeling tasks and various forms of learning.
Most of the popular interconnection networks can be represented as Cayley graphs. Star  graph is one of the extensively studied undirected Cayley graphs, which is considered to be  an attractive alternative to the popular binary n-cube. The n-rotator graph and the cycle  prefix digraph are a set of directed Cayley graphs introduced recently. Since the recently  introduced directed Cayley graphs have some interesting properties, a comparative study of  star and directed Cayley graphs is worthy of study. In this paper we compare the structural  and algorithmic aspects of star graphs with that of directed Cayley graphs. In the process we  present some new results for star graphs and directed Cayley graphs. We present a formula  to calculate the number of nodes at any distance from the identity permutation in star  graphs. The minimum bisection width of star and rotator graphs is obtained. Partitioning  and fault tolerant parameters for both star and directed Cayley graphs are analyzed. The  node disjoint parallel paths and hence the upper bound on the fault diameter of rotator  graphs are presented. We compare the minimal path routing in star and rotator graphs  using simulation results. Broadcasting and embedding in star and directed Cayley graphs  are also compared.
A new algorithm is proposed for removing large objects from digital images. The challenge is to fill in the hole that is left behind in a visually plausible way.
Recently we developed a diagonal homotopy method to compute a  numerical representation of all positive dimensional components in the  intersection of two irreducible algebraic sets. In this paper, we rewrite  this diagonal homotopy in intrinsic coordinates, which reduces the  number of variables, typically in half. This has the potential to save a  significant amount of computation, especially in the iterative solving  portion of the homotopy path tracker. Three numerical experiments  all show a speedup of about a factor two.
this paper it is shown using the &quot;baking up a truck&quot; controller example from Kong and Kosko [1]
this paper examines the market efficiency hypothesis by estimating time-varying coefficients using Kalman Filter techniques for an Autoregressive Moving Average model (ARMA) with Generalized Autoregressive Conditional Heteroskedasticity (GARCH) errors. The estimation technique utilized was maximum likelihood for sugar future returns . The higher lag order variable coefficients were the ones which showed greater fall in absolute value over time, which may suggest that variables with higher lags may lose weight as markets get more efficient. Keywords: Time Series Analysis, Market Efficiency and Kalman Filter  Professor Adjunto do Departamento de Economia/PIMES -- UFPE (rlima@npd.ufpe.br)   Departamento de Economia - UFPE, PET/CAPES ( a am mf fo o@ @d de ec co on n. . u uf fp pe e . . b br r) ) 1. 
A computer consists of multiple components such as functional  units, cache and main memory. At each moment of execution, a  program may have a varied amount of work for each component. Recent  development has exploited this imbalance to save energy by slowing the  components that have a lower load. Example techniques include dynamic  scaling and clock gating used in processors from Transmeta and Intel.
In this paper, we present an unbiased adaptive modelling approach to feedback cancellation in hearing aids. The approach is based on a closed loop identification of the feedback path as well as the (linear prediction) model of the near-end input signal. In general, both models are not simultaneously identifiable in the closed loop system at hand. We show that-under certain conditions e.g. if a delay is inserted in the forward path- identification of both models is indeed possible. Simulation results demonstrate that-under these conditions- the unbiased modelling approach outperforms the biased continuous adaptation algorithm.
Spoken dialogue includes an abundance of speech repairs and abandoned utterances. We built in a special display style to our dialogue annotation tool (DialogueView), which  cleans up these phenomena. In this paper, we report the results of a human-subject experiment, in which subjects do a comprehension test on dialogues with the cleanedup version and the original full version. We found that the cleaned-up version improves speed without harming the accuracy of comprehension. This result suggests that this special display style of DialogueView, allowing annotators to view the cleaned-up version, should enable them to annotate dialogues in less time without compromising accuracy. Quick comprehension implies quick annotation since annotating discourse structure typically requires fully understanding what is going on in the dialogues.
We introduce a kinetic approach to approximation of generalised  curvature ows. Basing on that a convolution-thresholding approximation  scheme for the generalised mean curvature evolution is constructed.
A cooperative project between the International Rice Research Institute in Los Baiios, Philippines, and the U.S. EPA Environmental Research Laboratory in Corvallis, Oregon, was initiated to estimate how rice yield in Asia might be affected by future climate change and enhanced UV-B irradiance following stratospheric ozone depletion. A radiative transfer model was used to estimate daily UV-B irradiance levels using remotely sensed ozone and cloud cover data for 1274meteorological stations. A rice yield model using daily climatic data and cultivar-specific coefficients was used to predict changes in yield under given climate change scenarios. This paper gives an overview of the data required to run these two models and describes how a geographical information system (GIS)was used as a data or postprocessor. Problems in finding reliable such as cloud cover data needed for the UV-B radiation model and radiation data needed for the rice yield model are discussed. Issues of spatial and scales are also addressed. Using simulation models at large spatial scales helped identify weaknesses of GIS data overlay and interpolation capabilities. Even though we focussed our efforts on paddy rice, the database is not intended to be system specific and could also be used to analyze the response of other natural systems to climatic change. 1. 
Most programming languages adopt static binding, but for distributed programming an exclusive reliance on static binding is too restrictive: dynamic binding is required in various guises, for example when a marshalled value is received from the network, containing identifiers that must be rebound to local resources. Typically it is provided only by ad-hoc mechanisms that lack clean semantics. In this
t, le systme vrie, sans ncessiter d&apos;annotations du code source, que chaque ot d&apos;information ralis par le programme analys est correct vis vis de la politique de scurit dnie par le programmeur.  ffffffffffffffffff ff Caml, Analyse de ots d&apos;information, Infrence de type, Scurit des languages de programmation ffffffffffffffff I An introduction to Flow Caml 7  ff ffffffffffffffff ff  1.1 Language-based Information Flow Analysis . . . . . . . . . . . . . . . . . . . . . . 9 1.2 Relating Flow Caml to Objective Caml . . . . . . . . . . . . . . . . . . . . . . . . 10 1.3 How to get the Flow Caml system ? . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.4 Theoretical background and related work . . . . . . . . . . . . . . . . . . . . . . . 10  ff ffffffffffffffffff ffff  2.1 Security levels and data structures . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.1.1 Simple types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.1.2 Strings . . . . . . . . . . . . . . . . . . . . . . . . . . 
Capstone design courses in engineering that provide students the opportunity to tackle open-ended realworld projects provide excellent learning experiences for the  students. Reported here are experiments with several models that have students work in teams on industry sponsored projects requiring a tested prototype as the final  deliverable. The first quarter deliverable is a written design proposal to the client with a poster presentation. The proposal is a complete specification of the design and a plan describing how it will brought to prototype stage in the next quarter. The proposal includes the design and economic analysis, drawings, schedule for completion and  specification of the resource requirements in support of the proposal. The second quarter is devoted to detailed design, prototype construction and evaluation. A web-based system was employed to provide collaboration between students, clients, and the instructional staff as well as providing management structure and full documentation of project activities.
In a recent study a series of model checkers, among which Spin [5],  SMV [9], and a newer system called XMC [10], were compared on  performance. The measurements used for this comparison focused on a model  of the i-protocol from GNU uucp version 1.04. Eight versions of this iprotocol  model were obtained by varying window size, assumptions about the  transmission channel, and the presence or absence of a patch for a known  livelock error. The results as published in [1] show the XMC system to  outperform the other model checking systems on most of the tests. It also  contains a challenge to the builders of the other model checkers to match the  results. This paper answers that challenge for the Spin model checker. We  show that with either default Spin verification runs, or a reasonable choice of  parameter settings, the version of Spin that was used for the tests in [1] (Spin  2.9.7) can outperform the results obtained with XMC in six out of eight tests.
Similarity-based and rule-based accounts of cognition are often portrayed as opposing  accounts. In this paper we suggest that in learning and development, the process of comparison   can act as a bridge between similarity-based and rule-based processing. We suggest that  comparison involves a process of structural alignment and mapping between two representations. This kind
Automated negotiation is gaining interest, but issues relating to the  construction of negotiating agent architectures have not been addressed sufficiently.
Classical floorplanning minimizes a linear combination of area and wirelength. When Simulated Annealing is used, e.g., with the Sequence Pair representation, the typical choice of moves is fairly straightforward. In this work
Structural learning with forgetting is an established method of using Laplace regularization to generate skeletal artificial neural networks. In this paper we develop a continuous dynamical system model of regularization in which the associated regularization parameter is generalized to be a time-varying function. Analytic results are obtained for a Laplace regularizer and a quadratic error surface by solving a different linear system in each region of the weight space. This model also enables a comparison of Laplace and Gaussian regularization. Both of these regularizers have a greater effect in weight space directions which are less important for minimization of a quadratic error function. However, for the Gaussian regularizer, the regularization parameter modifies the associated linear system eigenvalues, in contrast to its function as a control input in the Laplace case. This difference provides additional evidence for the superiority of the Laplace over the Gaussian regularizer.
We introduce a second-order system V 1 -Horn of bounded arithmetic formalizing polynomial-time reasoning, based on Grädel&apos;s (Grädel, 1992) second-order Horn characterization of P. Our system has comprehension over P predicates (defined by Grädel&apos;s second-order Horn formulas) , and only finitely many function symbols. Other systems of polynomial-time reasoning either allow induction on NP predicates (such as Buss&apos;s S  2 or the second-order V  1 ), and hence are more powerful than our system (assuming the polynomial hierarchy does not collapse), or use Cobham&apos;s theorem to introduce function symbols for all polynomial-time functions (such as Cook&apos;s PV and Zambella&apos;s P-def). We prove that our system is equivalent to QPV  and Zambella&apos;s P-def. Using our techniques, we also show that V 1 -Horn is finitely axiomatizable, and, as a corollary, that the class of 8    2 is finitely axiomatizable as well, thus answering an open question.
conversation. They make use of various techniques such as pattern matching, indexing, sentence reconstruction, and even natural language processing. In this paper we present an approach to chatterbots that mixes pattern matching with indexing and query matching methods inspired by information retrieval. We propose a model in which new sentences can be produced from existing ones using an evolutionary algorithm adapted to the structure of the natural language.
Disjunctive logic programs are a powerful tool in knowledge representation  and commonsense reasoning. The recent development of an efficient  disjunctive logic programming engine, named DLV, allows to exploit disjunctive  logic programs for solving complex problems. However, disjunctive logic programming  systems are currently missing any interface supporting the integration  between commonly used software development languages (like Java or C++) and  disjunctive logic programs. This paper focuses on the DLV Wrapper, a library,  actually implemented in Java, that &quot;wraps&quot; the DLV system inside an external application,  allowing to embed disjunctive logic programs inside Object-Oriented  source code.
We calculate equilibrium solutions for Ising spin models on `small world&apos; lattices, which are constructed by super-imposing random and sparse Poissonian graphs with finite average connectivity c onto a one-dimensional ring. The nearest neighbour bonds along the ring are ferromagnetic, whereas those corresponding to the Poisonnian graph are allowed to be random. Our models thus generally contain quenched connectivity and bond disorder. Within the replica formalism, calculating the disorder-averaged free energy requires the diagonalization of replicated transfer matrices. In addition to developing the general replica symmetric theory, we derive phase diagrams and calculate effective field distributions for two specific cases: that of uniform sparse long-range bonds (i.e. `small world&apos; magnets), and that of    random sparse long-range bonds (i.e. `small world&apos; spin-glasses).
Video Streaming refers to the real-time transmission of stored video. In order to control network congestion, rate control schemes adjust the output rate of the video stream to the estimated available bandwidth in the network. Loss based rate control schemes like AIMD, TFRC employ the packet loss rate reported by the receiver as the principal feedback parameter to estimate the state of the network.
The predicate control problem involves synchronizing a distributed computation  to maintain a given global predicate. In contrast with many popular  distributed synchronization problems such as mutual exclusion, readers writers,  and dining philosophers, predicate control assumes a look-ahead, so that the  computation is an off-line rather than an on-line input. Predicate control is targeted  towards applications such as rollback recovery, debugging, and optimistic  computing, in which such computation look-ahead is natural.
We use linear algebra to show that an algebraic privacy homomorphism  proposed by Domingo-Ferrer is insecure for some parameter  settings.
We present a computational model of planning, scheduling, and execution that is capable of supporting the interactions of a self-interested agent with other agents in a contracting environment over an extended period of time. The model is appropriate for situations in which a customer agent, in order to fulfill its goals, must contract with other supplier agents for all or part of the necessary tasks. The agents are assumed to be self-interested and with limited rationality. Supplier agents attempt to gain the greatest possible benefit, and customer agents will attempt to pay the lowest price.
The research work on sweet sorghum carried out at the Nimbkar Agricultural Research Institute (NARI) during last twenty-five years has been summarized. American lines were crossed with a local Indian fodder/grain variety to produce varieties with a juicy stalk and good quality grain. Further breeding was carried out to produce varieties and hybrids giving high yield of good quality grain while retaining the characteristic of juicy stalks high in sugar. Complete development of indigenous technology for fermentation of sweet sorghum juice, solar distillation of ethanol and finally its use as a cooking and lighting fuel in new and improved stoves and lanterns was carried out. The technology of producing jaggery (unrefined sugar) and syrup from sweet sorghum was also developed. Consumer response to these products was assessed by marketing them in limited quantities. A completely automated multifuel gasification system capable of producing thermal output between 120-500 kW was developed for direct heat applications such as those in jaggery and syrup making units. Sweet sorghum bagasse was also tested in an existing paper mill to assess its suitability for paper manufacture. Areas of possible research for better exploitation of sweet sorghum have been suggested.
This paper describes how environmental complexity measures can be employed in the process of validating experimental robotics work. We advocate the use of metrics that attempt to quantify the `difficulty&apos; of motion for a given environment. Space syntax methods (from the urban and building design literature) and fluid-flow models (used in crowd modeling) are described and proposed as relevant measures for mobile robotics domains. We show experimentally that these two metrics give very different expressions of complexity. We then discuss how, given their properties, these different metrics may be applied to robotics controller design and evaluation.
Large enterprises connect their locations together by building a corporate network out of private communication channels such as leased lines, Frame Relay and ATM links, called physical private networks or PPNs. PPNs provide good quality of service, but they are expensive. On the other hand, Internet-based Virtual Private Networks (VPNs) can provide speedy deployment of multisite corporate networks at a small fraction of the cost of private lines. However, Internet-based VPNs do not offer the same accountability and predictability as PPNs do, since the Internet is not admistrated by a single provider.
One of the problems of middleware for shared state is that they are designed, explicitly or implicitly, for symmetric networks. However, since the Internet is not symmetric, end-to-end process connectivity cannot be guaranteed. Our solution to this is to provide the middleware with a network abstraction layer that masks the asymmetry of the network and provides the illusion of a symmetric network. We describe the communication service of our middleware, the Distribution Subsystem~(DSS), which carefully separates connections to remote processes from the protocols that communicate over them. This separation is used to plug-in a peer-to-peer module to provide symmetric and persistent connectivity. The P2P module can provide both up-to-date addresses for mobile processes as well as route discovery to overcome asymmetric links.
We present the first constant-factor approximation algorithm for network design with multiple commodities and economies of scale. We consider the rent-or-buy problem, a type of multicommodity buy-at-bulk network design in which there are two bought, which allows unlimited use after payment of a large fixed cost. Given a graph and a set of source-sink pairs, we seek a minimum-cost way of installing sufficient capacity on edges so that a prescribed amount of flow can be sent simultaneously from each source to the corresponding sink.
List of Figures ....................................................................................................9 6. List of Tables....................................................................................................11 7. List of supporting papers .................................................................................. 13 8. Introduction......................................................................................................15 9. Factors affecting N 2 O production and emission from soil - literature review ...... 17 9.1 Processes ...................................................................................................... 17 9.1.1 Nitrogen fertiliser .................................................................................. 19 9.1.2 Carbon containing fertiliser....................................................................19 9.1.3 Oxygen and water content ..............................................................
Niyogi&apos;s book entitled &quot;The Informational Complexity of Learning&quot; [6] addresses the problem of learning from examples. He considers two distant fields: artificial neural networks and natural language. In both areas he gives a theoretical analysis of informational complexity, i.e. the effects of the size of the leaning set and the number of model parameters on the accuracy of learning depending on the target function class. After outlining the main ideas, this work discusses the usability of such results in practice and the relevance of the book in linguistic research, and also raises a philosophical question about the possibility of error prediction.
The asymptotic optimal performance of variable-rate vector  quantizers of fixed dimension and large rate was first developed in a rigorous fashion by Paul Zador. Subsequent design algorithms for such compression codes used a Lagrangian formulation in order to generalize Lloyd&apos;s classic quantizer optimization algorithm to variable rate codes. This formulation has been subsequently adopted in a variety of practical systems including rate-optimized streaming video. We describe a Lagrangian formulation of Zador&apos;s variable-rate quantization results and apply it to estimate Zador&apos;s constant using the generalized Lloyd algorithm.
On the evening of 2 November 1988, someone &quot;infected&quot; the Internet with a worm program. That program exploited flaws in utility programs in systems based on BSD-derived versions of UNIX. The flaws allowed the program to break into those machines and copy itself, thus infecting those systems. This program eventually spread to thousands of machines, and disrupted normal activities and Internet connectivity for many days. It was the first major network-wide attack on computer systems, and thus was a matter of considerable interest.
The high resolution infrared spectrometer AIRS was launched on the NASA AQUA platform in May 2002. This paper documents briefly some of the characteristics, potential and initial experience with AIRS data in the ECMWF forecasting system.
Interpretation is the process whereby a hearer reasons to an interpretation  of a speaker&apos;s discourse. The hearer normally adopts a  credulous attitude to the discourse, at least for the purposes of interpreting  it. That is to say the hearer tries to accommodate the truth  of all the speaker&apos;s utterances in deriving an intended model. We  present a non-monotonic logical model of this process which defines  unique minimal preferred models and so effciently simulates a kind of  closed-world reasoning of particular interest for human cognition.
This article presents work related to event driven B that has been undertaken by  ClearSy, in close collaboration with J.R. Abrial and Louis Mussat (DCSSI)
Many digital signal processing algorithms are first developed in floating point and later mapped into fixed point for digital hardware implementation. During this mapping, wordlengths are searched to minimize total hardware cost and maximize system performance. Complexity and distortion measures have been separately researched for optimum wordlength selection. This paper proposes a complexityand -distortion measure (CDM) method that combines these two measures. The CDM method trades off these two measures using a weighting factor. The proposed method is applied to wordlength design of a fixed broadband wireless demodulator. For this case study, the proposed method finds the optimal solution in one-third the time that exhaustive search takes. The contributions of this paper are (1) a generalization of search methods based on complexity or distortion measures, (2) a framework of automatic wordlength optimization, and (3) a wireless demodulator case study.
this document is published in / Une version de ce document se trouve dans :  Journal of Materials Science Letters, v. 21, no. 22, Nov. 15, 2002, pp. 1773-1775  www.nrc.ca/irc/ircpubs  NRCC-45653  JOURNAL OF MATERIALS SCIENCE LETTERS 21,2002,1773 -- 1775  L. D. MITCHELL,P. S. WHITFIELD    , J. MARGESON, J. J. BEAUDOIN  Institute for Research in Construction,    Institute for Chemical Process and Environmental Technology,  National Research Council Canada, Montreal Road, Ottawa ON K1A 0R6, Canada  E-mail: Lyndon.mitchell@nrc.ca  The synthesis of nanoparticulate ff-Al 2 O 3 at 600 ff C using a sucrose-based polymer dispersion technique was reported recently by Das et al. [1]. This is of interest, as ff -Al 2 O 3 is frequently an unwanted intermediate phase in the conversion of nanoparticulate amorphous alumina into nanoparticulate ff-Al 2 O 3 . ff -Al 2 O 3 has a relatively disordered structure, but usually requires temperatures in the order of 1100 ff C to transform completely to the stable ff-Al 2 O 3 phase [2]
this paper is to summarize the general transfer results for  E- connections that have recently been obtained in [7]. The generality of the results is due to the fact that    are defined and investigated using the framework of so-called abstract description systems (ADSs), a common generalization of description logics, modal logics, logics of time and space, and many other logical formalisms [2]. Thus, we can connect not only DLs with DLs, but also, say, description logics with spatial logics [8]. A natural interpretation of link relations in this context would then be, for instance, to describe the spatial extension of abstract (DL) objects
We present a graph theoretical methodology that reduces the implementation complexity of a vector multiplied by a scalar. The proposed approach is called MRP (minimally redundant parallel) optimization and is presented in FIR filtering framework to obtain a low-complexity multiplierless implementation. The key idea is to expand the design space using shift inclusive differential coefficients together with computation reordering using a graph theoretic approach to obtain maximal computation sharing. The transformed architecture of a filter is obtained by solving a set cover problem of the graph. A simple algorithm based on a greedy approach is presented. The proposed approach is merged with common sub-expression elimination. The simulation results show that 70% and 16% improvement in terms of computational complexity over simple implementation (transposed direct form) and common sub-expression, respectively, when using carry lookahead adder synthesized from synopsys designware library in .25  technology.
of our ideas concerning the generation of personalized recommendations within a configurator system are discussed (Section 3). Section 3.1 describes our configurator system architecture under the aspect of recommendation generation. Section 3.2 outlines recommendation granularities and the functioning of the filtering component. In Section 3.3, we describe our framework for combining different filtering algorithms in our configurator system.
The assumption/commitment (also called rely/guarantee) style has been advocated for the specification of interactive components of distributed systems. One of its motivations is to achievemodularity for state transition specifications of system components. It suggests the structuring of specifications into assumptions about the behavior of the component&apos;s environment and into commitments that are fulfilled by the component provided the environment fulfills these assumptions. We define the assumption/commitment formats for functional system specifications. In particular, we work out a canonical decomposition of system specifications following the assumption/commitment format into safety and liveness aspects. We demonstrate the format of assumption/commitment specifications byanumber of examples. In particular, we discuss the methodological significance of the assumption/commitment format in the stepwise development of specifications.
In this paper we present compiler-assisted checkpointing, a new technique which uses static program analysis to optimize the performance of checkpointing. We achieve this performance gain using libckpt, a checkpointing library which implements memory exclusion in the context of user-directed checkpointing. The correctness of user-directed checkpointing is dependent on program analysis and insertion of memory exclusion calls by the programmer. With compiler-assisted checkpointing, this analysis is automated by a compiler or preprocessor. The resulting memory exclusion calls will optimize the performance of checkpointing, and are guaranteed to be correct. We provide a full description of our program analysis techniques and present detailed examples of analyzing three fortran programs. The results of these analyses have been implemented in libckpt, and we present the performance improvements that they yield.
In this paper we present a novel interpretive approach for accurate and cost-effective performance prediction in a high performance computing environment, and describe the design of a source-driven HPF/Fortran 90D performance prediction framework based on this approach. The performance prediction framework has been implemented as part of a HPF/Fortran 90D application development environment. A set of benchmarking kernels and application codes are used to validate the accuracy, utility, usability, and cost-effectiveness of the performance prediction framework. The use of the framework for selecting appropriate compiler directives and for application performance debugging is demonstrated.  Keywords: Performance prediction, HPF/Fortran 90D application development, System &amp; Application characterization.  1 Introduction  Although currently available High Performance Computing (HPC) systems possess large computing capabilities, few existing applications are able to fully exploit this potenti...
A Bluetooth ad hoc network can be formed by interconnecting piconets into scatternets. The constraints and properties of Bluetooth scatternets present special challenges in forming an ad hoc network efficiently. In this paper, we present and analyze a new randomized distributed protocol for Bluetooth scatternet formation. We prove that our protocol achieves O(log n) time complexity and O(n) message complexity. The scatternets formed by our protocol have the following properties: (1) any device is a member of at most two piconets, and (2) the number of piconets is close to be optimal. These properties can help prevent overloading of any single device and lead to low interference between piconets. We validate the theoretical results by simulations, which also show that the scatternets formed have O(log n) diameter. As an essential part of the scatternet formation protocol, we study the problem of device discovery: establishing multiple connections simultaneously with many Bluetooth devices. We investigate the collision rate and time requirement of the inquiry and page processes. Our simulation results indicate that the total number of packets sent is O(n) and that the maximum number of packets sent by any single device is O(log n).
This paper illustrates the importance of shopping and marketing strategies when the Bertrand price-setting institution often used to model retail exchange is modified to allow sellers to offer buyer-specific discounts from the list price. Unlike the simple Bertrand game, a variety of shopping and marketing strategies can be rational in this more complex setting. Moreover, different strategy combinations yield distinct predictions: Equilibrium prices may either essentially match those predicted in the absence of buyer-specific discounts, or they may be at the collusive level. Data from laboratory markets with discount opportunities similarly indicate two distinct strategy-dependent behavioral outcomes.
this paper are entirely those of the authors and do not necessarily represent the views of the World Bank. 1. See, for example, Calvo and Mendoza (2000). This argument has provided ammunition to those who have supported the reintroduction of capital controls, including Krugman (1998) and Stiglitz (2000)
We describe an approach to tagging a monolingual  dictionary with linguistic features. In particular,  we annotate the dictionary entries with  parts of speech, number, and tense information.
We address the problem of permutation independent comparison of two pseudo Boolean functions given  by multiplicative binary moment diagrams (ffBmds), i. e., the problem of deciding whether there exists a  permutation of the input variables such that the two    are equal.
We introduce a new cryptanalytic technique based on Wagner  &apos;s boomerang and inside-out attacks. We first describe this new attack  in terms of the original boomerang attack, and then demonstrate its use  on reduced-round variants of the MARS core and Serpent. Our attack  breaks eleven rounds of the MARS core with 2        partial decryptions. Our attack breaks eight rounds  of Serpent with 2        decryptions.
this paper the diffusion particle is studied in the framework of the continuous Brownian model and some kind of transport equation, in general a Fokker-Planck equation (FPE) in two-vector variables (position and velocity of the diffusing particle) must be solved. The stochastic theory based on the FPE is able to describe different diffusion mechanisms. Quasi-continuous diffusion and hopping mechanisms (by single or multiple jumps) correspond to different ranges of the friction and of the potential barrier or, more physically to different ratios between some typical time scales. Long- and short- time dynamics can be quantitatively investigated by calculating the diffusion coefficient and the relevant correlation functions respectively. Thus, the most complete statistical information about the diffusing particles is contained in the dynamic structure factor ()ffq, S , which is proportional to the quasi-elastic scattering intensity both in neutron and in atom scattering experiments. The full width at half maximum (fwhm) ()q ff of the quasi-elastic peak of ff) S(q, , at small momentum transfer q , is proportional to the diffusion constant, while its behaviour at larger q  depends on the diffusion mechanism
In this paper, the supervisory control of a class of Discrete Event Systems is investigated. Discrete event systems are modeled either by a collection of Finite State Machines that behave asynchronously or by a Hierarchical Finite State Machine. The basic problem of interest is to ensure the invariance of a set of particular configurations in the system. When the system is modeled as asynchronous FSMs, we provide algorithms that, based on a particular decomposition of the set of forbidden configurations, solve the control problem locally (i.e. on each component without computing the whole system) and produce a global supervisor ensuring the desired property. We then provide sufficient conditions under which the obtained controlled system is non-blocking. This kind of objectives may be useful to perform dynamic interactions between different parts of a system. Finally, we apply these results to the case of Hierarchical Finite State Machines.
We prove that for q   1, there exists r(q) &lt; 1 such that for  p &gt; r(q), the number of points in large boxes which belongs to the infinite cluster has a normal central limit behaviour under the random cluster measure  ff p,q on Z   , d   2. Particularly, we can take r(q) = p ff g for d = 2, which is commonly conjectured to be equal to pc . These results are used to prove a q-  dimensional central limit theorems relative to the fluctuation of the empirical measures for the ground Gibbs measures of the q-state Potts model at very low temperature and the Gibbs measures which reside in the convex hull of them. A similar central limit theorem is also given in the high temperature regime. Some particular properties of the Ising model are also discussed. 1. 
This paper describes the DB2    Configuration Advisor, an expert tool for the  configuration of DB2 databases. This advisor has shown dramatic results for  tuning and configuring DB2 servers on UNIX    and Windows    platforms. The  recognition of the essential need for administration and design tools has  spurred renewed interest among leading relational database management  system (RDBMS) vendors. The DB2 Configuration Advisor is a key feature in  DB2&apos;s Autonomic Computing self-managing technology portfolio. This paper  discusses the purpose and features of this expert advisor. Experimental  results are presented with a description of the advisor&apos;s interfaces.
We have worked on the measurement protocol and related image reconstruction of 4D data sets in the field of time-resolved Contrast-Enhanced Magnetic Resonance Angiography (CE-MRA). The method aims at improving the interpolation of sparsely sampled    data. It is based on exploiting prior knowledge on the time courses (TCs) of the image pixels, when monitoring the uptake of contrast agents in the blood vessels. The result is compared with an existing approach called 3D-TRICKS.
We propose modeling images and related visual objects as bags of pixels or sets of vectors. For instance, gray scale images are modeled as a collection or bag of (X; Y; I) pixel vectors. This representation implies a permutational invariance over the bag of pixels which is naturally handled by endowing each image with a permutation matrix. Each matrix permits the image to span a manifold of multiple con- gurations, capturing the vector set&apos;s invariance to orderings or permutation transformations. Permutation congurations are optimized while jointly modeling many images via maximum likelihood. The solution is a uniquely solvable convex program which computes correspondence simultaneously for all images (as opposed to traditional pairwise correspondence solutions). Maximum likelihood performs a nonlinear dimensionality reduction, choosing permutations that compact the permuted image vectors into a volumetrically minimal subspace. This is highly suitable for principal components analysis which, when applied to the permutationally invariant bag of pixels representation, outperforms PCA on appearance-based vectorization by orders of magnitude. Furthermore, the bag of pixels subspace bene- ts from automatic correspondence estimation, giving rise to meaningful linear variations such as morphings, translations, and jointly spatio-textural image transformations. Results are shown for several datasets.
We introduce in [10] the formal foundations to make a generic combination of one process algebra and one algebraic specification language possible. Furthermore, to strengthen the contribution of this work, a concrete illustration about an orders invoicing case study is detailed in [9]. In this paper, we especially focus on the addition of other languages; indeed in the initial work, we only consider a restricted number of process algebras: CCS, CSP, ACP, basic LOTOS. Therefore, we aim at formalizing the way to extend the previous combination. To achieve this goal, we present a method to enhance the syntax and semantics of the formal kernel introduced in [10]. These guidelines are illustrated with the -Calculus.
A near-regular texture deviates geometrically and photometrically from a regular congruent tiling. Although near-regular textures are ubiquitous in the man-made and natural world, they present computational challenges for state of the art texture analysis and synthesis algorithms. Using regular tiling as our anchor point, and with user-assisted lattice extraction, we can explicitly model the deformation of a near-regular texture with respect to geometry, lighting and color. We treat a deformation field both as a function that acts on a texture and as a texture that is acted upon, and develop a multimodal framework where each deformation field is subject to analysis, synthesis and manipulation. Using this formalization, we are able to construct simple parametric models to faithfully synthesize the appearance of a near-regular texture and purposefully control its regularity.
Inspired by the abstractions provided in desktop user interface software toolkits [7], a variety of research projects are exploring abstractions of physical sensors and physical input devices. For example, Papier-Mch can provide high-level events related to the appearance of an RFID near a reader, the scanning of a barcode, or a computer vision system, without requiring the application programmer to know exactly what model of RFID reader or what type of barcode is involved [6]. The Context Toolkit offers similar abstractions, such as informing an application that a person is present at a given location, without requiring the application to understand whether the notification was prompted by the detection of an RFID, the docking of an iButton, or the entry of a username/password combination [1].  AmIBusy seeks to provide a different type of abstraction. Rather than abstracting how information was collected, AmIBusy seeks to abstract what information was collected. To clarify, the depl
For differential equations on a matrix Lie group it is known that most traditional methods do not keep the numerical solutions in the Lie group. To obtain numerical solutions with the correct qualitative behaviour we solve overdetermined systems of differential-algebraic equations, including differential equations on the corresponding Lie algebra, by standard Runge-Kutta methods with projection in the Lie algebra. Theoretical analysis and numerical experiment demonstrate that this new class of Lie-group invariant methods is more efficient than previous ones.
New conjectures are proposed on the numbers of FPL con gurations pertaining to certain types of link patterns. Making use of the Razumov and Stroganov Ansatz, these conjectures are based on the analysis of the ground state of the Temperley-Lieb chain, for periodic boundary conditions and so-called \identi- ed connectivities&quot;, up to size 2n = 22. 1. 
In image deconvolution or restoration using Kalman filter, the image and blur models are required to be known for the restoration process. Generally, the accuracy of the restoration depends on the accuracy of the given models. Unfortunately, the image and blur models are normally unknown in practice. To solve the problem, an identification stage is employed to estimate the image and blur models. However, the estimated models are seldom accurate especially with the presence of noise in the image. This paper presents a robust Kalman filter design for image deconvolution that can accommodate the inaccuracy in the estimated image and blur models. If the inaccuracy can be modelled as addictive white Gaussian noise with a known variance, it can be stochastically account for in the robust filter design. In the simulation tests performed, the robust design achieved improved accuracy in the image restoration even though inaccurate image and blur models were used.
this paper, a new semi-parametric  hybrid neural network is proposed to separate the post nonlinearly mixed blind signals where crosschannel  disturbance is included. This hybrid network consists of two cascading modules, which are a  neural nonlinear module for approximating the post nonlinearity and a linear module for separating  the predicted linear blind mixtures. The nonlinear module is a semi-parametric expansion made up  of two sub-networks, one of which is a linear model and the other of which is a three-layer perceptron
This work deals with the application of Memetic Algorithms to the Microarray Gene Ordering problem, a NP-hard problem with strong implications in Medicine and Biology. It consists in ordering a set of genes, grouping together the ones with similar behavior. We propose a MA, and evaluate the influence of several features, such as the intensity of local searches and the utilization of multiple populations, in the performance of the MA. We also analyze the impact of different objective functions on the general aspect of the solutions. The instances used for experimentation are extracted from the literature and represent real biological systems.
The recent advances in database technology have enabled the development of a new  generation of spatial databases, where the DBMS is able to manage spatial and non-spatial  data types together. Most spatial databases can deal with vector geometries (e.g., polygons,  lines and points), but have limited facilities for handling image data. However, the widespread  availability of high-resolution remote sensing images has improved considerably the  application of images to environmental monitoring and urban management. Therefore, it is  increasingly important to build databases capable of dealing with images together with other  spatial and non-spatial data types. With this motivation, this paper describes a solution for  efficient handling of large image data sets in a standard object-relational database management  system. By means of adequate indexing, compression and retrieval techniques, satisfactory  performances can be achieved using a standard DBMS, even for very large satellite images.
We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern
Pickup and delivery problems discussed in the literature are often constrained to particularly simple solutions in terms of the sequence of visited locations. We study the very simplest pickup and delivery paths which are concatenations of short patterns visiting one or two requests. This restricted variant, still N P -hard, is close to the traveling salesman problem with the additional choice of what patterns to visit. We compare the number of restricted and unrestricted paths, and evaluate their respective path lengths. We conclude with two polynomially solvable cases.
Architecture Description Languages (ADLs) such as Acme (a mainstream second generation ADL which contains the most common ADL constructs) provide formality in the description of software architectures, but are not easily reconciled with dayto -day development concerns, thus hampering their adoption by a larger community. UML, on the other hand, has become the de facto standard notation for design modeling, both in industry and in academia. In this paper we map Acme modeling abstractions into UML 2.0, using its new component modeling constructs, its lightweight extension mechanisms and OCL wellformedness rules. The feasibility of this mapping is demonstrated through several examples. This mapping bridges the gap between architectural specification with Acme and UML, namely allowing the transition from architecture to implementation, using UML design models as a middle tier abstraction.
The maturing domain of ad hoc networking still holds some major challenges on its way to commercial exploitation. The dynamic and decentralized nature of ad hoc networks calls for new paradigms in the design of distributed applications. Most notably, there is a need for self-organization mechanisms in the middleware layer to enable medium-scale and large-scale distributed applications. In this paper we present MESHMdl, a middleware platform for ad hoc networks that is built on mobile agents and tuple space technology. MESHMdl provides basic building blocks for the implementation of selforganizing distributed applications for ad hoc networks. Unlike other middleware platforms, MESHMdl puts a high emphasis on mobility awareness, decoupling and bio-inspired coordination primitives. We discuss the aspects of ad hoc networking and self-organization and derive the basic architecture of MESHMdl before we present our prototype implementation.
By ca. A.D. 1100 the nearly level 3    terrace of Monks Mound had been constructed by prehistoric Cahokians who used this surface for approximately two centuries. After a 400-year hiatus, during which the mound was probably vegetated by grass and/or trees,   terrace has been periodically occupied by aboriginal groups and EuroAmerican settlers. The primary objectives of this study were: (1) to determine the extent of soil formation during the past 900 years; and, (2) to determine if any mixing of the surficial soil horizons occurred during the historic period. The soil profile shows that during the last 900 years an unusually thick mollic epipedon and an A horizon were developed to depths of 0.9 to 1.2 m, respectively. Magnetic susceptibility, magnetic minerals, fly-ash, and organic C amounts and distribution within the surficial soil horizons suggest that the soil was disturbed after 1864, but not within the last 70 years.
well as metaphysics.  The emphasis in Indian philosophy on knowing the outside through an analysis of cognitive categories was far in advance of the concepts used by historians of science until the rise of modern physics. As a result, the six darsanas were often misrepresented in the commentaries that were written with the rise of Indian studies in the nineteenth century. These mistakes have been repeated in more recent works because this commentatorial tradition still operates within the framework of reductionist physics and analysis. With the rise of relativity and quantum mechanics, the subject has become central in the understanding of the physical universe. The outer world exists because there is someone to perceive it; likewise the mind is characterized by the associations between various objects and processes of the outer world.    An examination of the physical world in terms of categories of the mind or of &quot;being&quot; constitutes a perfectly legitimate way of approaching the outer
In this paper, we model fresh names in the ff-calculus using abstractions with respect to a new binding operator ff. Both the theory and the metatheory of the ff-calculus benefit from this simple extension. The operational semantics of this new calculus is finitely branching. Bisimulation can be given without mentioning any constraint on names, thus allowing for a straightforward definition of a coalgebraic semantics, within a category of coalgebras over permutation algebras. Following previous work by Montanari and Pistore, we present also a finite representation for finitary processes and a finite state verification procedure for bisimilarity, based on the new notion of ff-automaton.
In this paper, we describe an algorithm to measure the shape complexity for discrete approximations of planar curves in 2D images and manifold surfaces for 3D triangle meshes. We base our algorithm on shape curvature, and thus we compute shape information as the entropy of curvature. We present definitions to estimate curvature for both discrete curves and surfaces and then formulate our theory of shape information from these definitions. We demonstrate our algorithm with experimental results.
In this work, we propose a priority buffer management scheme for MPEG-2 video traffic called MPFD. The MPFD algorithm is based on constructing a virtual buffer that represents the future buffer occupancy using information about video frames within a future lookahead period. The virtual buffer is then used to determine whether incoming low priority frames need to be dropped in order to protect future high priority frames. MPFD is compared to existing buffer management schemes that has been used for video traffic. MPFD shows a significant frame loss reduction as compared to other buffer management schemes.
In this paper, we describe implementation aspects and performance results of an innovative and publicly available RSVP implementation. Much debate exists about the applicability of RSVP as a signalling protocol in the Internet, particularly for a large number of unicast flows. While there has been a significant amount of work published on the theoretical concepts of RSVP signalling and conjectures about its presumed shortcomings, rather little attention has been paid to the implementation details of the core protocol engine. With our work, in spite of being still far from a final judgement, we try to shed light on this issue by presenting certain design details of a new implementation and a study about its performance. One particular result is given by the observation that a relatively cheap router based on PC hardware can sustain the signalling for more than 50,000 unicast flows.
Software testing is an important and costly part of the software development life cycle. More than 50% of software development budgets are used in validation and verification. This leads to the question -- How much testing do we teach in our computer science curriculum? Over the past five years I have incorporating software testing into my courses at the University of Northern Iowa by: 1) introducing testing concepts in 
This paper addresses the commonly asked question &quot;When should I combine quantitative and qualitative research methods in my research design?&quot; Often the question intended is &quot;when should nomothetic and ideographic methods be combined?&quot; This paper will address both these questions. An analysis of the issues leads to the conclusion that researchers should focus on the nature of the phenomenon to be investigated, rather than specific research methods.
The true potential of three dimensional System-OnPackage (SOP) technology lies in its capability to integrate both active and passive components into a single high speed/density multi-layer packaging substrate. We propose a new interconnect-centric SOP global routing algorithm that handles arbitrary routing topologies and produces near optimal results. The contribution of this work is threefold: (i) modeling of the SOP routing resource,  (ii) formulation of the new SOP global routing problem, and (iii) development of a fast and novel algorithm that considers the various design constraints unique to SOP. Our related experimental results demonstrate the effectiveness of our algorithm.
Tuberculosis (TB) has came back to be, today a serious problem of public health. The new cases of this illness in developing countries and its resurgence in developed countries, brought up the necessity of thinking about more efficient interventions for its combat and control. One estimates that around 1/3 of the world-wide population (about 1.7 billion individuals) is infected by the Mycobacterium tuberculosis. Latin America and Caribbean countries are responsible for 400.000 cases and 50% of those had occurred in Brazil, Peru and Mexico with a number of deaths, caused by this illness in Brazil, between to 4 to 5 thousands a year.
This paper describes classification of typed  student utterances within AutoTutor, an intelligent  tutoring system. Utterances are classified  to one of 18 categories, including 16  question categories. The classifier presented  uses part of speech tagging, cascaded finite  state transducers, and simple disambiguation  rules. Shallow NLP is well suited to the task:  session log file analysis reveals significant  classification of eleven question categories,  frozen expressions, and assertions.
In this paper we present a theory for reasoning about actions which is based on Dynamic Linear Time Temporal Logic (DLTL). DLTL is a simple extension of propositional temporal logic of linear time in which regular programs of propositional dynamic logic can be used for indexing temporal modalities. The action theory we define allows to reason with incomplete initial states, to do postdiction and to deal with ramifications and with nondeterministic actions, which are captured by possibly alternative extensions (temporal models). The expressiveness of temporal logic is exploited to enhance the action language by allowing the definition of general temporal constraints as well as complex actions in the specification of the domain description. We show that the temporal projection problem and the planning problem can be modelled as satisfiability problems in DLTL.
Over the last decade or so there has been a phenomenal growth in the use and diversity of information and communications technologies (ICTs), with the rise of Internet being of particular note. Current estimates, as of autumn 2001, are that 513 million people from around the world use the Internet for all manner of personal and business communications (Nua 2001). Concomitant to this growth, there has been a multi-billion dollar investment in vast assemblages of powerful computer servers and the infrastructure necessary to support current and projected demand in information processing and exchange, including long haul fibre-optic backbones networks to link countries and metropolitan cores, high-speed routers and switches, and `last-mile&apos; DSL and cable connections (see OECD 2001, TeleGeography 2001 for current statistics). This strategic investment is designed to garner market share in the rapidly expanding information economy (worth a reported $775.6 billion in the US alone...
Financial crises affect income distribution via different channels. In this paper, we argue that financial transfers are an important channel, which has been overlooked by the literature. We study the role of financial transfers by analyzing some of the most severe Latin American crises during the last decades (Chile 1981-1983, Mexico 1994-1995, Ecuador 1998-2000, Argentina 2001-2002, and Uruguay 2002). First, we investigate transfers to the financial sector, which are those from non-participants to participants of the financial sector. Second, we explore who receives these financial transfers by identifying the winners and losers within the financial sector. Our analysis suggests that financial transfers during crises are large and expected to increase income inequality.
In this paper we show how to assist the development for problems fitting the Commanded Behaviour frame, using UML with a well-founded approach, and we illustrate our ideas with a lift case study.
APS algorithms use the basic idea of distance vector routing to find positions in an ad hoc network using only a fraction of landmarks, for example GPS enabled nodes. All the nodes in the network are assumed to have the possibility of measuring: range, angle of arrival (AOA), orientation, or a combination of them. We give a lower bound for positioning error in a multihop network for a range/angle free algorithm, and examine the error characteristics of four classes of multihop APS algorithms under various conditions, using theoretical analysis and simulations. Analysis of range/angle free, range based, angle based, and multimodal algorithms shows a complex tradeoff between the capabilities used, the density of the network, ratio of landmarks, and the quality of the positions obtained.
The DES encryption standard resisted rather well to some 20 years of  massive worldwide cryptanalysis effort. DES S-boxes also haven&apos;t an obvious algebraic  structure that could lead to algebraic attacks. For all these reasons, DES is not only  very widely implemented and used today, but triple DES and other derived schemes  will probably still be around in ten or twenty years from now.
We present in this article a method for detecting similarity links between documents&apos; content and speech recordings&apos; content. This process, further called thematic alignment, is a novel research area that combines both document and speech analysis. This alignment will a) provide temporal indexes to documents, which are non-temporal data, and b) help discovering hidden thematic structures. This article first introduces a multi-layered document structure and quickly introduces the traditional speech structure. Further, it presents a simple similarity measure and various multilevel simple alignments between those two structures. Later, the meeting corpus is presented, as well as an evaluation of the implemented alignments. Finally, we present our future works on multi-alignments and thematic structure discovery.
An intelligent user interface sometimes needs to present a sequence of related recommendations to a user, in spite of being uncertain in advance as to whether (and with what success) the user will follow each recommendation. There are potential advantages to the use of decision-theoretic planning methods which yield an optimal policy for the situationdependent presentation of recommendations. This approach is discussed with reference to an example involving route instructions given by an airport assistance system.
We present a modular approach to defining logics for a wide variety of state-based systems. We use coalgebras to model the behaviour of systems, and modal logics to specify behavioural properties of systems. We show that the  syntax, semantics and proof systems associated to such logics can all be derived in a modular way. Moreover, we show that the logics thus obtained inherit soundness, completeness and expressiveness properties from their building blocks. We  apply these techniques to derive sound, complete and expressive logics for a wide variety of probabilistic systems.
Both proof planning and schema-guided program synthesis can be  seen as a recursive problem decomposition process followed by a recursive solution  composition process, with problems being either conjectures or specifications,  and solutions being either tactics or programs. We thus develop a unified  view of these two activities. This approach reveals an opportunity for identifying  and integrating useful heuristics of when and how to apply what program schema,  which dimension had hitherto been much neglected for program schemas, but not  for proof methods, and allows us to encode program schemas as proof planning  methods, so as to be able to use a proof planner (in particular Clam) as an implementation  platform for developing the first schema-guided synthesiser of (standard  or constraint) logic programs. This approach has the pleasant side-effect that  any proof obligations --- such as verification conditions, matchings, or simplifications  --- that arise during schema-guided program synthesis can also be handled  within Clam.
Online handwriting recognition is gaining renewed interest owing to the increase of pen computing applications and new  pen input devices. The recognition of Chinese characters is different from western handwriting recognition and poses a special  challenge. To provide an overview of the technical status and inspire future research, this paper reviews the advances in online  Chinese character recognition (OLCCR), with emphasis on the research works from the 1990s. Compared to the research in the  1980s, the research efforts in the 1990s aimed to further relax the constraints of handwriting, namely, the adherence to standard stroke  orders and stroke numbers and the restriction of recognition to isolated characters only. The target of recognition has shifted from  regular script to fluent script in order to better meet the requirements of practical applications. The research works are reviewed in  terms of pattern representation, character classification, learning/adaptation, and contextual processing. We compare important  results and discuss possible directions of future research.
Double R Grammar is a linguistic theory of the grammatical encoding and integration of referential and relational meaning in English. Double R Grammar is fundamentally a Cognitive Linguistic theory (Langacker, 1987, 1991; Lakoff, 1988; Talmy, 2003) and the use of the term &quot; grammar&quot; encompasses both meaning and structure as it does in Cognitive Grammar (also known as Cognitive Semantics). Grammar is the symbolization of meaning. Discussion of grammar goes hand in hand with discussion of the meaningful consequences of grammatical variation. The linear stream of English text encodes multiple dimensions of meaning. The encoding of multiple dimensions of meaning in a single linear dimension results in trade-offs in encoding across dimensions and variation within a given dimension across different grammatical contexts. A primary goal of Double R Grammar is to provide integrated representations of referential and relational meaning that reflect these trade-offs.
Replication is one of the prominent approaches for obtaining fault tolerance. Implementing replication on commodity hardware and in a transparent fashion, i.e., without changing the programming model, has many challenges. Deciding at what level to implement the replication has ramifications on development costs and portability of the programs. Other difficulties lie in the coordination of the copies in the face of non-determinism.
We consider a problem intimately related to the creation of maxima under Gaussian blurring: the number of  modes of a Gaussian mixture in D dimensions. To our knowledge, a general answer to this question is not known. We conjecture
In a preliminary design environment, the designer needs to have freedom to quickly evaluate different configurations and come up with the most promising configuration. In the supersonic regime, most linearized codes that are available today can only handle specific shapes and configurations. These codes only aid in optimizing conventional configurations and do not span the entire space of possible shapes, which include revolutionary and unconventional configurations. This paper proposes using a set of GNU libraries and analyses codes to overcome the shortcomings of the legacy codes. It is known that any surface can be discretized into triangles using effcient Delaunay triangulation algorithms. The proposed method involves creating a triangulated aircraft from a generic CAD environment, using the set of geometric libraries and then performing necessary surface operations for the desired result, which in our case is the calculation of the wave drag. Linearized methods for wave drag estimation call for the calculation of the intercepted areas of the aircraft with a Mach cone and the GNU libraries help us in obtaining these areas. Finally, in order to validate the code, the new code is used to compute the wave-drag of a Sears-Haack body and F-16 and the results are compared to the results from AWAVE, the Harris Wave Drag program.
We propose an architecture for large scale, multi-user, distributed multimedia based on cooperating agents communicating by means of an observable communication channel. In addition to the traditional protocols based on point-to-point communication, coordination and cooperation should be supported via social awareness and overhearing. Overhearing also allows the collection of contextual information without interfering with already deployed systems. Our domain of application is interactive museums, which are typical examples of so-called &quot;active environments&quot; or &quot;ambient intelligence&quot;. Purpose of this paper is to present the core concepts, and outline the future lines of research.
Most image coding systems rely on signal processing concepts such as transforms, VQ, and motion compensation. In order to achieve significantly lower bit rates, it will be necessary to devise encoding schemes that involve mid-level and high-level computer vision. Model-based systems have been described, but these are usually restricted to some special class of images such as head-and-shoulders sequences. We propose to use mid-level vision concepts to achieve a decomposition that can be applied to a wider domain of image material. In particular, we describe a coding scheme based on a set of overlapping layers. The layers, which are ordered in depth and move over one another, are composited in a manner similar to traditional &quot;cel&quot; animation. The decomposition (the vision problem) is challenging, but we have attained promising results on simple sequences. Once the decomposition has been achieved, the synthesis is straightforward.
vi 1 
For a family F (k) = fF    2 ; : : : ; F    t g of k-uniform  hypergraphs let ex(n; F (k)) denote the maximum number of k-tuples  which a k-uniform hypergraph on n vertices may have, while not containing  any member of F (k). Let rk (n) denote the maximum cardinality  of a set of integers Z  [n], where Z contains no arithmetic progression  of length k.
In this paper, we investigate the optimisation of the Random Access CHannel (RACH) of the UMTS FDD system in terms of generated interference. Conditional on open loop measurements, we search the best initial energy of the RACH in order to reduce the multiple access interference while keeping a reasonable mean access time.
The traditional approach to fault-tolerant computation has been via modular hardware redundancy.
In the survivable network design problem (SNDP), the goal is to find a minimum-cost spanning  subgraph satisfying certain connectivity requirements. We study the vertex-connectivity  variant of SNDP in which the input specifies, for each pair of vertices, a required number of  vertex-disjoint paths connecting them.
This paper reviews the Maximum Likelihood estimation problem and its solution  via the Expectation-Maximization algorithm. Emphasis is made on the  description of finite mixtures of multi-variate Bernoulli distributions for modeling  0-1 data. General ideas about convergence and non-identifiability are  presented. We discuss improvements to the algorithm and describe thoroughly  what we believe are novel ideas in the treatment of the topic: 1) identification  of unique data points and recycling of that information 2) parallelization of  the algorithm in a multi-threaded fashion 3) cluster assignment options. Experiments  demonstrate that most of our approaches produce good results and  encourage further research on the topic.
The demonstration of a limitation on the abilities of a computational model is a central theme in complexity theory. Hardness results, which demonstrate such limitations, are crucial for complexity theory&apos;s efforts to classify problems into classes based on their inherent difficulty. In this work, we discuss three...
We define notions of stability for learning algorithms and show how to use these notions to derive generalization error bounds based on the empirical error and the leave-one-out error. The methods we use can be applied in the regression framework as well as in the classification one when  the classifier is obtained by thresholding a real-valued function. We study the stability properties  of large classes of learning algorithms such as regularization based algorithms. In particular we  focus on Hilbert space regularization and Kullback-Leibler regularization. We demonstrate how  to apply the results to SVM for regression and classification.
Interest in the influence of landscape features on animal movement has been widespread; however, few field  studies of the emigration of small mammals from patches of habitat directly consider the effects of the  scale landscape features. The simulation models of Stamps et al. b) and Buechner b) suggest  that the size of a dispersal sink relative to the size of the source patch, the average distance traveled by dispers-  ers in the sink, the ease with which dispersers cross the edge between the sink and a source patch, and source  patch ratio may all be important influences on emigration rates. A review of field studies of  small mammal dispersal into sinks suggests that in a substantial fraction of such studies the values of these  factors fall within the ranges that the simulation models indicate have the greatest potential effect on emigra-  tion rates. New field studies of dispersal sinks that include a consideration of these factors are necessary in  order to evaluate the magnitude of the impact of these factors on natural populations.
This paper discusses building complex classifiers from a single labeled example and vast number of unlabeled observation sets, each derived from observation of a single process or object. When data can be measured by observation, it is often plentiful and it is often possible to make more than one observation of the state of a process or object. This paper discusses how to exploit the variability across such sets of observations of the same object to estimate class labels for unlabeled examples given a minimal number of labeled examples. In contrast to similar semi-supervised classification procedures that define the likelihood that two observations share a label as a function of the embedded distance between the two observations, this method uses the Naive Bayes estimate of how often the two observations did result from the same observed process. Exploiting this additional source of information in an iterative estimation procedure can generalize complex classification models from single labeled observations. Some examples involving classification of tracked objects in a low-dimensional feature space given thousands of unlabeled observation sets are used to illustrate the effectiveness of this method.
Process Landscaping is a method, which supports modelling of  related processes. These processes can be modelled on different levels of  abstraction. Interfaces between processes are considered as first class process  entities. It prevents loosing an overview of the whole framework of processes  and ensures that decisions about processes are not burdened out by an  overwhelming amount of details. In this article we discuss the approach of  Process Landscaping by developing a real-world software process.
Freeform surfaces are commonly used in Computer Aided Geometric Design,  so accurate analysis of surface properties is becoming increasingly important. In  this paper we define surface slope and surfacespeed,develop visualization tools,  and demonstrate that they can be useful in the design process. Generally, surface  properties such as curvature and twist are evaluated at a finite set of predetermined  samples on the surface. This paper takes a different approach. A small set of tools  is used to symbolically compute surfaces representing curvature, twist and other  properties. These surfaces are then analyzed using numeric techniques.
This paper proposes a new evolutionary algorithm for subspace clustering in very large and high dimensional databases. The design includes task-specific coding and genetic  operators, along with a non-random initialization procedure. Reported experimental results show the algorithm scales almost linearly with the size and dimensionality of the database as well as the dimensionality of the hidden clusters.
Leveraging the explicit formula for European swaptions and coupon-bond options in HJM one-factor model we develop a semi-explicit formula for 2-Bermudan options (also called Canary options). For this we first extend the European swaption formula to future times. Then we are able to reduce the valuation of a 2-Bermudan swaption to a single numerical integration. In that integration the most complex part of the valuation of the embedded European swaptions has been simplified in such a way that it has to be performed only once and not for every point of the numerical integration. 1. 
 We consider parallel machine scheduling problems where the jobs are  subject to precedence constraints, and the processing times of jobs are governed  by independent probability distributions. The objective is to minimize  the weighted sum of job completion times    j w j C j in expectation,  where w j  0. Building upon an LP-relaxation by Mohring, Schulz, and  Uetz (J. ACM 46 (1999), pp. 924-942) and an idle time charging scheme  by Chekuri, Motwani, Natarajan, and Stein (SIAM J. Comp., to appear)  we derive the  rst approximation algorithms for this model.
Enterprise Resource Planning (ERP) systems like SAP R/3    are business software tools that help managing important parts of business, including e.g. product planning, parts purchasing, maintaining inventories, interacting with vendors, customer service, and tracking orders. These systems provide a complete infrastructure for enterprise corporate information processing and the capability of integrating and summarizing detailed data to produce relevant management information. Current ERP systems do not support automatic discovery and extraction of knowledge from data. Due to increasing complexity of transactions and short resources conventional reporting is often not enough to maintain a sufficient competitive advantage. The application of Business Intelligence for the efficient solution of problems in different economic areas is mandatory.
A speaker normalization scheme that uses explicit knowledge of acoustic phonetics is presented. The scheme warps the frequency axis linearly in critical band rate with respect to the fundamental frequency F_0. It thus allows an immediate adaption to a new speaker which is an advantage over commonly used schemes. Variants with different values of F_0 and different parameters have been evaluated on several tasks of SpeechDat(II). The results show significant performance improvements on three tasks with monophone models, the most prominent result is a reduction in WER of 44.5 % for an isolated digit task. However, the results achieved with tied triphone models are very modest. It is argued that the normalization scheme may still be correct but that the MFCC feature extraction erases its effect. Evidence for the need of a new feature extraction method that locates spectral peaks and ignores irrelevant portions of the spectrum is given.
Preliminary results from an experimental study of readers&apos; perceptions of lexical cohesion and lexical semantic relations in text are presented. Readers agree on a common &quot;core&quot; of groups of related words and exhibit individual differences. The majority of relations reported are &quot;non-classical&quot; (not hyponymy, meronymy, synonymy, or antonymy). A group of commonly used relations is presented. These preliminary results indicate potential for improving both relations existing in lexical resources, and methods dependent on lexical cohesion analysis. 1. 
We seek the scene interpretation that best explains image data.
Usage patterns discovered through Web usage mining are effective in capturing item-to-item and user-to-user relationships and similarities at the level of user sessions Without the benefit of deeper domain knowledge, such patterns provide little insight into the underlying reasons for which such items or users are grouped together This can lead to a number of important shortcomings in personalization systems based on Web usage mining or collaborative filtering. For example, if a new item is recently added to the Web site, it is not likely that the pages associated with the item would be a part of any of the discovered patterns, and thus these pages cannot be recommended. Keyword-based content-filtering approaches have been used to enhance the effectiveness of collaborative filtering systems by focusing on content similarity among items or pages. These approaches, however, are incapable of capturing more complex relationships at a deeper semantic level based on different types of attributes associated with structured objects. This paper represents work-in-progress towards creating a general framework for using domain ontologies to automatically characterize usage profiles containing a set of structured Web objects. Our motivation is to use this framework in the context of Web personalization, going beyond page- or item-level constructs, and using the full semantic power of the underlying ontology.
The Iterative Closest Point (ICP) algorithm is widely used to register two roughly aligned surfaces. Its most expensive step is the search for the closest point. Many efficient variants have been proposed to speed up this step, however they do not guarantee that the closest point on a triangulated surface is found. Instead they might result in an approximation of the closest point. In this paper we present a method for the closest point search that is fast and exact. The method was implemented and used to evaluate the accuracy of head motion estimation using dense 3D data obtained from a stereo camera system. The results show that the accuracy of the estimated head motion is better than 1 mm for translational movements and better than 1 degree for rotations.
We present an expectation-maximization (EM) algorithm that yields topology preserving maps of data based on probabilistic mixture models. Our approach is applicable to any mixture model for which we have a normal EM algorithm. Compared to other mixture model approaches to self-organizing maps, the function our algorithm maximizes has a clear interpretation: it sums data log-likelihood and a penalty term that enforces self-organization. Our approach allows principled handling of missing data and learning of mixtures of self-organizing maps. We present example applications illustrating our approach for continuous, discrete, and mixed discrete and continuous data.
Software Visualization is, despite the many publications and advances in this research field, still not being considered by mainstream software industry: currently very few integrated development environments offer (if at all) only limited visualization support, and in general it can be said that software visualization is being ignored at a professional level by the average software developer. Moreover, even relatively successful software visualization tools (such as Rigi, Shrimp, JInsight, etc.) are seldom being used except by their developers themselves. In this position paper, based on our own experience and an analysis of the current state and possible future trends of integrated development environments, we put up a non-exhaustive list of features that software visualization tools should possess in the future to have more consideration by mainstream development.
In real world information systems, data analysis and processing are usually needed to be done  in an on-line, self-adaptive way. In this respect, neural algorithms of incremental learning and  constructive network models are of increased interest. In this paper we present a new algorithm  of evolving self-organizing map (ESOM), which features fast one-pass learning, dynamic network  structure, and good visualisation ability. Simulations have been carried out on some benchmark  data sets for classification and prediction tasks, as well as on some macroeconomic data for  data analysis. Compared with other methods, ESOM achieved better classification with much  shorter learning time. Its performance for time series modelling is also comparable, requiring  more hidden units but with only one-pass learning. Our results demonstrate that ESOM is an  effective computational model for on-line learning, data analysis and modelling.
Learning by demonstration and learning by instruction offers a potentially more powerful paradigm than programming robots directly for specific tasks. Learning in humans or primates substantially benefits from demonstration of actions or instruction by language in the appropriate context and there is initial neurocognitive cortical evidence for such processes. Cortical assemblies have been identified in the cortex that activate in response to the performance of motor tasks at a semantic level. This evidence supports that such mirror neuron assemblies are involved in actions, observing actions and communicating actions. Furthermore, neurocognitive evidence supports that cell assemblies are activated in different regions of the brain dependent on the action type being processed. Based on this neurocognitive evidence we have begun to design a neural robot in the MirrorBot project that is based on multimodal integration and topological organisation of actions using associative memory. As part of these studies in this paper we describe a self-organising model that clusters actions into different locations dependent on the body part they are associated with. In particular, we use actual sensor readings from the MIRA robot to represent semantic features of the action verbs. Furthermore, ongoing work focuses on integration of motor, vision and language representations for learning from demonstration and language instruction.
This paper investigates the idea of knowledge work appropriate to the context of organization transformation. Specifically, we describe an actionable framework of knowledge synthesis, which accommodates the shift of information system (IS) support from automating to informating to knowledging. Our discussion intends to clarify the ideal of a learning organization which is designed to help transfer learning from individuals to a group, provide for organizational renewal, keep an open attitude to the outside world, and support a commitment to knowledge. The paper deals with the classification of knowledge tasks and its relation to organizational design. We elaborate the issue of knowledge characterizations that help structure and facilitate knowledge interconnectivity, through the exposition of the information continuum. We also describe the spiral approach of knowledge creation in terms of different modes of knowledge conversion, realizable in any of the contemporary organizations. Finally, we conclude by reiterating the various challenges of creating a communal knowledge space within the working of a learning organization.
We examine the optimization of a system where the servers in a cluster  may be switched dynamically and preemptively from one kind of work to  another. The demand consists of two job types joining separate queues,  with different arrival and service characteristics, and also different relative  importance represented by appropriate holding costs. The switching of a  server from queue 1 to queue 2, or vice versa, incurs a cost which may be  monetary or may involve a period of unavailability. The optimal switching  policy in the case of bounded queues is obtained numerically by solving  a dynamic programming equation. Two simple heuristic policies -- one  static and one dynamic -- are evaluated by simulation and are compared  to the optimal policy. The dynamic heuristic is shown to perform well  over a range of parameters, including changes in demand.
this document 2 WP1.4 partners 2 Document history 2 1. 
It is generally believed that question answering can benefit from natural language processing methods. So far, however, there have been few systematic studies of this conjecture. We report on ongoing work that is aimed at understanding the contribution of linguistically informed modules and resources to the overall performance of a generic question answering system. Specifically, we describe the ways in which currently we use linguistically motivated techniques, and demonstrate the impact of integrating, or not integrating, these techniques on the overall performance of our question answering system. Evaluation results are based on the TREC 2002 and TREC 2003 question sets.
The Natural Element Method (NEM) is a mesh-free numerical method for the  solution of partial differential equations. In the natural element method, natural  neighbor coordinates, which are based on the Voronoi tesselation of a set  of nodes, are used to construct the interpolant. The performance of NEM in  two-dimensional linear elastodynamics is investigated. A standard Galerkin  formulation is used to obtain the weak form and a central-difference time integration  scheme is chosen for time history analyses. Two different applications  are considered: vibration of a cantilever beam and dispersion analysis of the  wave equations. The NEM results are compared to finite element and analytical  solutions. Excellent dispersive properties of NEM are observed and good  agreement with analytical solutions is obtained.
Web search engines work well for finding crawlable pages, but not for finding datasets hidden behind Web search forms. We describe a novel technique for detecting search forms, which could be the basis for a next-generation distributed search application. We use automatic feature generation to describe candidate forms and C4.5 decision trees to classify them. In two testbeds, we get an accuracy of more than 85% and a precision of more than 87%. One of our decision trees is effective on both testbeds, suggesting that it is a useful general-purpose tree.
Grid presents a continuously changing environment. It also introduces a new set of failures. The data grid initiative has made it possible to run data-intensive applications on the grid. Data-intensive grid applications consist of two parts: a data placement part and a computation part. The data placement part is responsible for transferring the input data to the compute node and the result of the computation to the appropriate storage system. While work has been done on making computation adapt to changing conditions, little work has been done on making the data placement adapt to changing conditions. In this work, we have developed an infrastructure which observes the environment and enables run-time adaptation of data placement jobs. We have enabled Stork, a scheduler for data placement jobs in heterogeneous environments like the grid, to use this infrastructure and adapt the data placement job to the environment just before execution. We have also added dynamic protocol selection and alternate protocol fall-back capability to Stork to provide superior performance and fault tolerance.
The peer-to-peer (p2p) paradigm is attracting increasing attention from both the research community and software engineers, due to potential performance, reliability and scalability improvements. This paper emphasizes that JXTA can help teachers to teach p2p with Java. This paper also presents an approach for performance analysis of JXTA pipes - one of the key abstractions in JXTA, which has not yet been fully evaluated. It explains how to assess a pipe and demonstrates performance results of the JXTA-Java implementation. In doing so, this paper assists software developers in estimating the overall performance and scalability of JXTA, and the suitability of choosing JXTA for their specific application.
The problem of  nding good preconditioners for the numerical solution of inde  nite linear systems is considered. Special emphasis is put on preconditioners that have a 2  2 block structure and which incorporate the (1; 2) and (2; 1) blocks of the original matrix. Results concerning the spectrum and form of the eigenvectors of the preconditioned matrix and its minimum polynomial are given. The consequences of these results are considered for a variety of Krylov subspace methods. Numerical experiments validate these conclusions.
This paper analyzes the issue of money superneutrality through an intertemporal optimizing model of capital accumulation with endogenous fertility, i.e. endogenous population growth. Two elements of this setup invalidate money superneutrality: i) a demand for fertility that depends on real money balances, and ii) an inverse relation between capital-labor ratio and population growth. Higher monetary growth increases fertility, since it reduces its opportunity cost, and hence diminishes capital intensity, and per capita output. This reverse Tobin eect is matched by an increase in aggregate capital and output growth rates. In this framework, the optimal monetary growth rule is a &quot;distorted Friedman rule&quot;. Keywords: Money superneutrality; Inffation; Fertility; Capital accumulation  JEL classi...cation: O42, O11, J13  1. 
Genome databases store data about molecular biological entities such as genes,  proteins, diseases, etc. The main purpose of creating and maintaining such databases in  commercial organizations is their importance in the process of drug discovery. Genome  data is analyzed and interpreted to gain so-called leads, i.e., promising structures for new  drugs. Following a lead through the process of drug development, testing, and finally several  stages of clinical trials is extremely expensive. Thus, an underlying high quality database  is of utmost importance. Due to the exploratory nature of genome databases, commercial  and public, they are inaccurate, incomplete, outdated and in an overall poor state.
We present video summarization and indexing techniques using the MPEG-7 motion activity  descriptor. The descriptor can be extracted in the compressed domain and is compact, and hence  is easy to extract and match. We establish that the intensity of motion activity of a video shot is  a direct indication of its summarizability. We describe video summarization techniques based on  sampling in the cumulative motion activity space. We then describe combinations of the motion  activity based techniques with generalized sound recognition that enable completely automatic  generation of news and sports video summaries. Our summarization is computationally simple  and flexible, which allows rapid generation of a summary of any desired length.
We propose a spiking neural network model that is inspired from an oversimplified general structure of layer IV of the cortex. The neuron model is an integrate-and-fire model whose threshold changes depending on its firing activity. Firing activity of neurons in combination with interactivity between them creates a highly dynamical self-organized process. The spiking activity, the neuron&apos;s threshold that changes depending on the firing activity and the time of stabilization of the proposed network can be used to represent its complex behavior. We experimentally show that this representation of the complex behavior of the network can be used to characterize spatio-temporal information of input patterns. Also, a new paradigm for novelty detection by the spiking neural network is proposed. The stabilization time of the neural network is used as a criterion for novelty detection.
Recently numerous proposals for modelling and querying Multidimensional Databases (MDDB) are proposed. Among the still open problems there is a rigorous classification of the different types of hierarchies. In this paper we propose and discuss some different types of hierarchies within a single dimension of a cube. These hierarchies divide in different levels of aggregation a single dimension. Depending on them, we discuss the characterization of some OLAP operators which refer to hierarchies in order to maintain the data cube consistency. Moreover, we propose a set of operators for changing the hierarchy structure. The issues discussed provides modelling flexibility during the scheme design phase and correct data analysis.
Let f = (f 1 ; : : : ; fm ) be a holomorphic mapping in a neighborhood of the origin in C   . We find sufficient condition, in terms of residue currents, for a smooth function to belong to the ideal in C   generated by f . If f is a complete intersection the condition is essentially necessary. More generally we give sufficient condition for an element of class C   in the Koszul complex induced by f to be exact. For the proofs we introduce explicit homotopy formulas for the Koszul complex induced by f . 1. 
XML documents generated dynamically by programs are typically represented as text strings or DOM trees. This is a low-level approach for several reasons: 1) traversing and modifying such structures can be tedious and error prone; 2) although schema languages, e.g. DTD, allow classes of XML documents to be defined, there are generally no automatic mechanisms for statically checking that a program transforms from one class to another as intended. We introduce...
Global software development projects use virtual teams, which are primarily linked through computer and telecommunications technologies across national boundaries. Global Virtual Teams rarely meet in a face-to-face context and thus face challenging problems not associated with traditional co-located teams. To understand the complex issues in a virtual project environment during the requirements definition phase of the software development cycle, we conducted an exploratory research study, involving 24 virtual teams based in Canada and India, working on defining business requirements for software projects, over a period of 5 weeks. The study indicates that ease of use of technology, trust between the teams and well-defined task structure influence positively the efficiency, effectiveness, and satisfaction level of global virtual teams.
The ubiquity of Windows-based desktop environments has not been matched by a corresponding emergence of tools supporting the Unix tool composition paradigm. Outwit is a suite of tools based on the Unix tool design principles allowing the processing of Windows application data with sophisticated data manipulation pipelines. The outwit
Here I reanalyze the well-known steady-state supergalactic wind solution. I show that superwinds driven by compact and powerful starbursts undergo a catastrophic cooling close to the star cluster surface and establish temperature distributions that are radically different from the adiabatic solution. The observational implications of the cold wind model both in X-ray and visible line regimes are discussed.
This report examines the requirements for management skills in Electronic Commerce. It includes a summary of definitions of Electronic Commerce, sample e-commerce business models, and a comprehensive worldwide analysis of university courses and programs in Electronic Commerce. It also includes a selective analysis of Electronic Commerce management training initiatives outside the university sphere. The problem of generating business value from investments in Information Technology has a long history. The question of management skills for Electronic Commerce is a recent manifestation of a fundamental management challenge for many firms: the challenge of managing technology-related business innovation. Firms need managers who can work in crossover business and technology environments. From a job design and work organization perspective, firms need to find individual managers with hybrid business/technology skills, and they need to establish and properly use multiskilled, rapid-learner teams. Workers and entry-level management personnel also require specific learning opportunities to function effectively in Electronic Commerce environments. Universities provide a variety of learning opportunities in Electronic Commerce. The report describes the varieties of programs and the contents of the core Electronic Commerce syllabus as it is emerging in faculties of business around the world. The section that examines three components of Electronic Commerce (ecommerce uptake by SMEs, business process reengineering/workflow skills, and the transformation of the Purchasing function) provides an overview of the management training needs in these areas, and suggests the range of curricula and delivery mechanisms required to promote management skill development. The report describes view...
We investigate the performance of the  Structured Language Model (SLM) in  terms of perplexity (PPL) when its components  are modeled by connectionist models.
Worst-Case Execution Time (WCET) analysis means to compute a safe upper bound to the execution time of a piece of code. Parametric WCET analysis yields symbolic upper bounds: expressions that may contain parameters. These parameters may represent, for instance, values of input parameters to the program, or maximal iteration counts for loops. We describe a technique for fully automatic parametric WCET analysis, which is based on known mathematical methods: an abstract interpretation to calculate parametric constraints on program flow, a symbolic method to count integer points in polyhedra, and a symbolic ILP technique to solve the subsequent IPET calculation of WCET bound. The technique is capable of handling unstructured code, and it can find upper bounds to loop iteration counts automatically. 1 
In this paper we consider a set of metrics for comparing different authentication  schema. We describe a model for an authentication system; unlike other models,  this one is oriented towards implementation and not cryptographic properties. We  compare several authentication systems within the framework of this model using  as metrics the size and probability distribution of authenticators provided by the  system, the ease of use, and the ease of abuse by attackers.
Lie algebras of dim]3M:S  n   5 have been classified by G.M. Mubarakzyanov in [2]. There are twelve algebras L 4,j which are not directsum of algebras of lower dimMG3SR55 Let us consider the algebra L 4,6 with non-zerocom utation relations  L 4,6 :[Q 1 ,Q 4 ]=aQ 1 , [Q 2 ,Q 4 ]=b 2  -Q  3 , [Q 3 ,Q 4 ]=Q 2 + b 3 , (a  ff=0,b  0). From [3] we know that any one of the operators Q i (i =1,...4) m y be equal to ff x or ff(x). Let  Q 1 = ff x and other of operators have theform (1):  Q i = ff i (x)ff x + ff i (x),i=2, 3, 4. As [Q 1 ,Q 2 ]=[Q 1 ,Q 3 ] = 0, [Q 1 ,Q 4 ]=aQ 1 then ff i = ff i , ff i = A i , i =2, 3, ff 4 = ax, ff 4 = A 4 ,  where ff i   R, A i are arbitrary constantm5:]A2S     r. Substituting Q 1 = ff x , Q 2 = ff 2 ff x +A 2 ,  Q 3 = ff 3 ff x +A 3 ,Q 4 = axff x +A 4 into the com utation relations we obtain (a      -1.
This paper considers the design of an algorithm that maximizes explicitly its own stability. The  stability criterion - as often used for the construction of bounds on the generalization error of a learning  algorithm - is proposed to compensate for overfitting. The primal-dual formulation characterizing  Least Squares Support Vector Machines (LS-SVMs) and the additive regularization framework [13] are  employed to derive a computational and practical approach combined with convex optimization. The  method is elaborated for non-linear regression as well as classification. The proposed stable kernel machines  also lead to a new notion of Lff and Lff curves instead of the traditional L-curves defined on  training data.
This paper presents M-Planning, a mobile tool to support collaborative planning, and its conjunction use with the IX Process Panels. M-Planning works like an intelligent mobile panel for agents on the move. The tool permits agents&apos; interaction, visualisation and manipulation of information related to collaborative processes of planning. The architecture of the M-Planning tool is integrated with the I-X System, and based on some of its concepts. The relevant I-X concepts for this work are discussed in the paper. M-Planning is intended to be used in real world domains and applications. To motivate and illustrate M-Planning use, a mixed initiative planning application is described in a fictitious scenario of disaster relief operations.
The beautiful, branching structure of ice is one of the most striking visual phenomena of the winter landscape. Yet  there is little study about modeling this effect in computer graphics. In this paper, we present a novel approach  for visual simulation of ice growth. We use a numerical simulation technique from computational physics, the  &quot;phase field method,&quot; and modify it to allow aesthetic manipulation of ice crystal growth. We present acceleration  techniques to achieve interactive simulation performance, as well as a novel geometric sharpening algorithm  that removes some of the smoothing artifacts from the implicit representation. We have successfully applied this  approach to generate ice crystal growth on 3D object surfaces in several scenes.
As our economy and critical infrastructure increasingly rely on the Internet, securing routing protocols becomes of critical importance. In this paper, we present four new mechanisms as tools for securing distance vector and path vector routing protocols. For securing distance vector protocols, our hash tree chain mechanism forces a router to increase the distance (metric) when forwarding a routing table entry. To provide authentication of a received routing update in bounded time, we present a new mechanism, similar to hash chains, that we call tree-authenticated oneway  chains. For cases in which the maximum metric is large, we present skiplists, which provides more efficient initial computation cost and more efficient element verification; this mechanism is based on a new cryptographic mechanism, called MW-chains, which we also present. For securing path vector protocols, our cumulative authentication mechanism authenticates the list of routers on the path in a routing update, preventing removal or reordering of the router addresses in the list; the mechanism uses only a single authenticator in the routing update rather than one per router address. We also present a simple mechanism to securely switch one-way chains, by authenticating the next one-way chain using the previous one. These mechanisms are all based on efficient symmetric cryptographic techniques and can be used as building blocks for securing routing protocols.
Introduction  We apply a memory-based learner to the CoNLL-2001 shared task: clause identification (Tjong Kim Sang and D&apos;ejean, 2001). The task is divided in three parts. The first two parts are classification tasks: identifying the positions of clause starts and clause ends given a word, its part-of-speech tag and the syntactic base chunk it belongs to. Our memory-based learner can be applied to these tasks in a straightforward way. The third part of the shared task is identifying complete embedded clauses. We will perform this task by first identifying clause starts and clause ends and then combining these to clauses with a set of conversion rules.  2 Approach  The first two parts of the CoNLL-2001 shared task are similar to the CoNLL-2000 shared task: classify words in context according to some tagging scheme. We have participated in the latter shared task (Tjong Kim Sang, 2000) and we will use a similar approach for the first two parts of the 2001 shared task. The goal of these par
In this paper we explore the potential of using a general class of functional representation techniques, kernel-based regression, in the nonlinear model reduction problem. The kernel-based viewpoint provides a convenient computational framework for regression, unifying and extending the previously proposed polynomial and piecewise-linear reduction methods. Furthermore, as many familiar methods for linear system manipulation can be leveraged in a nonlinear context, kernels provide insight into how new, more powerful, nonlinear modeling strategies can be constructed. We present an SVD-like technique for automatic compression of nonlinear models that allows systematic identification of model redundancies and rigorous control of approximation error.
Almost all the work in Average-reward Reinforcement Learning (ARL) so far has focused on table-based methods which do not scale to domains with large state spaces. In this paper, we propose two extensions to a model-based ARL method called H-learning to address the scale-up problem. We extend H-learning to learn action models and reward functions in the form of Bayesian networks, and approximate its value function using local linear regression. We test our algorithms on several scheduling tasks for a simulated Automatic Guided Vehicle (AGV) and show that they are effective in significantly reducing the space requirement of H-learning and making it converge faster. To the best of our knowledge, our results are the first in applying function approximation to ARL. 1 Introduction  Most Reinforcement Learning (RL) methods optimize the discounted total reward received by an agent (Barto, Bradtke, &amp; Singh, 1995; Watkins &amp; Dayan, 1992). However, in many real-world domains, the natural criterio...
We recalibrate the abundance sequence for extragalactic H II regions and  compare photoionization and shock models for a luminous sub-sample of southern  starburst galaxies. We  nd H II regions are young. Luminous IR galaxies are excited  by continuous star formation and/or AGN and have higher than solar metallicity.
We present a new transformation method by which a given Horn theory is transformed in such a way  that resolution derivations can be carried out which are both linear (in the sense of Prologs SLDresolution)   and unit-resulting (i.e. the resolvents are unit clauses). This is not trivial since although  both strategies alone are complete, their na ve combination is not. Completeness is recovered by  our method through a completion procedure in the spirit of Knuth-Bendix completion, however with  different ordering criteria. A powerful redundancy criterion helps to find a finite system quite often.
We consider the problem of constructing a constant-round  zero-knowledge proof system for all languages in NP. This problem has  been previously addressed by Goldreich and Kahan (Jour. of Cryptology,  1996). Following recent works on concurrent zero-knowledge, we propose  an alternative solution that admits a considerably simpler analysis.
Traditionally, dynamic voltage scaling (DVS) techniques have focused on minimizing the processor power consumption as opposed to the entire system energy. However, the slowdown resulting from DVS can increase the energy consumption of components like memory and network interfaces. Furthermore, leakage power consumption, which is increasing with the scaling device technology, must also be considered. In this paper, we present an algorithm to compute task slowdown factors based on the contribution of the processor leakage and standby energy consumption of the resources in the system. We combine slowdown with procrastination of task executions to extend sleep intervals which significantly reduces the leakage energy consumption. We show that the scheduling approach minimizes the total static and dynamic energy consumption of the systemwide resources. In this work, we consider a real-time system model using the Earliest Deadline First (EDF) policy. Our simulation experiments using randomly generated task sets show on an average 10% energy gains over traditional dynamic voltage scaling. Our procrastination scheme increases the average energy savings to 15%.
Large-scale enterprise data is increasingly organized in the form of distributed storage. The three important issues that arise when moving to distributed storage are reliability, security and concurrency. We introduce a collection of methods drawn from the theory of erasure-correcting codes to design reliable and secure distributed storage that can perform correctly and efficiently even when concurrently accessed by multiple uncoordinated clients. Our methods are general and are capable of wide usage in a variety of distributed storage applications.
In a paper of20 Kaas, Dhaene and Goovaerts investigate the present value of a rather general cash flow, as a special case of sums of dependent risks. Making use of comonotonic risks, they derive upper and lower bounds for the distribution of the present value, in the sense of convex ordering. These bounds are very close to the real distribution in case all payments have the same sign; however, if there are both positive and negative payments, the upper bounds perform rather badly. In the present contribution we show what happens when solving this problem by means of copulas. The idea consists of splitting up the total present value in the difference of two present values with positive payments. Making use of a copula as an approximation for the joint distribution of the two sums, an approximation for the distribution of the original present value can be derived. Keywords  cash flow,prq5ff t value, convex orff7ff copula,distr)4ffff6S)  1 Description of the problem  Consider a gener7 serHS ofdeterff)Gff7&quot;S payments ff 1 , ff 2 , ..., ff n due at times 1, 2, ..., n, which can be positive as well as negative. The pr46ff t valuefor this cash flow can be wrq55S as        , (1) wher the stochastic varffqqS7  Y (i)ar defined as Y (i)=Y 1 +Y 2 + ... +Y i and wher the varq4qq7   irqS5S&quot;)  t the stochastic continuous compoundedr ate ofr55ffS over the perff d [i   1,i].
Many propagation and search algorithms have been developed for constraint  satisfaction problems (CSPs). In a standard CSP all variables are existentially  quantified. The CSP formalism can be extended to allow universally quantified  variables, in which case the complexity of the basic reasoning tasks rises from  NP-complete to PSPACE-complete. Such problems have been studied in the context  of quantified boolean formulae and CSPs with continuous domains. Little  work has been done on problems with discrete non-boolean domains. We try to  address this by extending propagation and search algorithms from standard CSPs  to the quantified case. We first describe an arc consistency algorithm that can deal  with arbitrary binary constraints. We then extend standard search algorithms so  that they can handle quantification. We also show how the notion of value interchangeability  can be exploited to break symmetries and speed up search by orders  of magnitude. We test experimentally the algorithms and methods proposed and  make a preliminary analysis of the phase transition.
Organizations specify a set of guidelines to be followed by a programmer while coding. These coding standards help the organizations maintain the code easily. The speci  ed standards are normally veri  ed by the Quality Assurance personnel manually. We present one way of automating this veri  cation process. The approach uses a Logic Language to specify the standards. We discuss the implementation details of the Compliance Veri  er that has been developed to enforce coding standards on programs written in C++. We also provide a design that can be used to develop a Generic Compliance Enforcement Tool, a tool that enforces coding standards on programs written in any programming language.
We show that for a graph G it is NP -hard to decide whether its independence number  (G) equals its clique partition number (G) even when some minimum clique partition of  G is given. This implies that any (G)-upper bound provably better than (G) is NP -hard  to compute.
fertility on an acid soil of the subtropical area in Sao Paulo State, Brazil. A case study of soil chemical properties (pH, organic matter content, Ca, Mg, K, Al, CEC and Base Saturation (V)) in a field (160 m x 120 m) is used to explore sampling issues. Soil samples were taken, first, in a recently tilled soil, just after the land had been abandoned for a long time, and later on in the same soil managed according to minimum tillage practices for successive years. A regular sampling grid scheme (20 m x 20 m) with 63 sites was used to identify the magnitude and scale of spatial variation of selected nutrients and soil properties. Data sets were collected from two depths within this single field and for three different times, 1985, 1987 and 1988 in a long-term experience. The statistical results indicate that the initial soil condition was found to be more diverse, providing evidence that in the studied Ferralsol, clearing and long-term cultivation tend to increase soil heterogeneity at both studied depths. Experimental variograms of individual variables were computed and modelled by a nugget component plus a spherical structure. The nugget effect were important, generally higher than 50% for most of the studied variables, at the first sampling date and tended to decrease at successive sampling dates. Thus, the effect of agricultural soil use was to improve the continuity of the studied attributes at close distances. The autocorrelation ranges for most of the studied soil properties varied between 40 and 80 m. Semivariograms provided a description of nutrients spatial structure and some insight into possible processes affecting their distribution. Kriging maps allowed the identification of small regions with distinct nutrient concentrations and illustrate the diversity of...
We survey the problem of comparing labeled trees based on simple local operations of deleting,  inserting and relabeling nodes. These operations lead to the tree edit distance, alignment distance  and inclusion problem. For each problem we review the results available and present, in detail,  one or more of the central algorithms for solving the problem.
Though the utility of domain  Ontologies is now widely  acknowledged in the IT (Information  Technology) community, several  barriers must be overcome before  Ontologies become practical and  useful tools. One important  achievement would be to reduce the  cost of identifying and manually  entering several thousand-concept  descriptions. This paper describes a  text mining technique to aid an  Ontology Engineer to identify the  important concepts in a Domain  Ontology.
The (axis-parallel) stabbing number of a given set of line segments is the maximum number of segments that can be intersected by any one (axis-parallel) line. We investigate problems of finding perfect matchings, spanning trees, or triangulations of minimum stabbing number for a given set of points. The complexity of these problems has been a long-standing open problem; in fact, it is one of the original 30 outstanding open problems in computational geometry on the list by Demaine, Mitchell, and O&apos;Rourke. We show that minimum stabbing problems are NPcomplete. We also show that an iterated rounding technique is applicable for matchings and spanning trees of minimum stabbing number by showing that there is a polynomially solvable LP-relaxation that has fractional solutions with at least one heavy edge. This suggests constant-factor approximations. Our approach uses polyhedral methods that are related to another open problem (from a combinatorial optimization list), in combination with geometric properties. We also demonstrate that the resulting techniques are practical for actually solving problems with up to several hundred points optimally or near-optimally.
The results of a production study of two speakers provide evidence against models of French intonation in which there is a one-to-one correspondence between a syllable and each of the L (low) and H (high) tones of a rise. The results also fail to confirm claims about the influence of the consonant or vowel status of an accentable syllable on the placement of the H of the early rise. The L tone of the early rise consistently straddled the boundary between the last function word and first content word of the phrase (e.g., for un ENfant  SAGE, the L tone would be realized very late in un or very early in EN), with a low leading plateau extending leftward to the beginning of the phrase. This timing is accounted for in the current proposal by a double association of the L tone of a LH- edge tone to the left edge of the phrase and to the left edge of the first content word of a phrase, similar to the double association of phrase accents discussed in Grice et al. (2000) for a number of languages.
On information markets, many suppliers and buyers of information goods exchange values. Some of these goods are data, whose value is created in buyer interactions with data sources. These interactions are enabled by data market services (DMS). DMS give access to one or several data sources. The major problems with the creation of information value in these contexts are (1) the quality of information retrievals and related queries, and (2) the complexity of matching information needs and supplies when different semantics are used by source systems and information buyers. This study reports about a prototype DMS (called CIRBA), which employs an ontology-based information retrieval system to solve semantic problems for a DMS. The DMS quality is tested in an experiment to assess its quality from a user perspective against a traditional data warehouse (with SQL) solution. The CIRBA solution gave substantially higher user satisfaction than the data warehouse alternative. 
The present study compares an adaptive simulated cellular-phone based recommender system to a non-adaptive one, in order to evaluate user preferences with respect to system adaptivity. The results show that users prefer the adaptive system over the non adaptive one even after minimal interaction with the system.
With the advent of web services standards and a service-oriented Grid architecture, it is foreseeable  that competing as well as complimenting computational services will proliferate. Current efforts in standardising  service interface focuses on how one can execute these services in terms of their syntactic  descriptions. Their capabilities and relations with other service types are only articulated through natural  language in the form of documentation. In this paper, we present the ICENI semantic service adaptation  framework. We seek to capture the capability of services by annotating their programmatic interface using  the Web Ontology Language (OWL)[2] in relation to some domain concepts thereby allowing services to  be semantically matched based on their ontological annotation. By inferences on this metadata, syntactically  different but semantically equivalent service implementations may be autonomously adapted and  substituted. Combining it with familiar high-level programming language, we demonstrate a practical  service-oriented programming model.
We have developed an infrastructure for end-to-end run-time monitoring, behavior / performance analysis, and dynamic adaptation of distributed software applications. This feedback-loop infrastructure is primarily targeted to pre-existing systems and thus operates outside the application itself without making assumptions about the target system&apos;s internal communication/computation mechanisms, implementation language/framework, availability of source code, etc. This paper assumes the existence of the monitoring and analysis components, presented elsewhere, and focuses on the mechanisms used to control and coordinate possibly complex repairs/reconfigurations to the target system. These mechanisms require lower-level actuators or effectors somehow attached to the target system, so we briefly sketch one such facility (elaborated elsewhere). The core of the paper is the model, architecture, and implementation of Workflakes, the decentralized process engine we use to tailor, control, coordinate, respond to contingencies, etc. regarding a cohort of such actuators. We have validated our approach and the Workflakes prototype in several case studies, related to different application domains. Due to space restrictions we concentrate primarily on one case study, elaborate with some detais a second, and only sketch others.
Spurred by the development of Universidade de vora&apos;s Integrated  Information System (SIIUE) and on the authors&apos; present research interests  came the idea of a Natural Language Analysis System, which  would provide a simple interface for getting access to existent information.
Designer-centred design methods build on the knowledge and skills of the  designers to improve user interface design. Unlike users, designers are trained  to handle complexity and to make design trade-offs. They are the people  centrally in control of the design process. Designer-centred design complements  and refocuses user-centred design methods. A particular designer-centred  design approach, &quot;Design Aloud&quot; is introduced and justified.
This paper presents a method for automatically deriving a time- and content-dependent  information value function for probabilistic information. This function describes  analytically what real world value an agent can obtain by using a certain piece  of information at a certain time. The general form of this function is formulated and a  specific example with two-valued outputs is presented in the factory scheduling domain. The
The increase in maintenance of software and the increased amounts of reuse are having major positive impacts on the quality of software, but are also introducing some rather subtle negative impacts on the quality. Instead of talking about existing problems (faults), developers now discuss &quot;potential problems,&quot; that is, aspects of the program that do not affect the quality initially, but could have deleterious consequences when the software goes through some maintenance or reuse. One type of potential problem is that of common coupling, which unlike other types of coupling can be clandestine. That is, the number of instances of common coupling between a module M and the other modules can be changed without any explicit change to M. This paper presents results from a study of clandestine common coupling in 391 versions of Linux. Specifically, the common coupling between each of 5332 kernel modules and the rest of the product as a whole was measured. In more than half of the new versions, a change in common coupling was observed, even though none of the modules themselves was changed. In most cases where this clandestine common coupling was observed, the number of instances of common coupling increased. These results provide yet another reason for discouraging the use of common coupling in software products.
To determine the value of perfect information  in an influence diagram, one needs first  to modify the diagram to reflect the change  in information availability, and then to compute  the optimal expected values of both the  original diagram and the modified diagram. The value
The previous video track results demonstrated that it is far from trivial to take advantage of multiple modalities for the video retrieval search task. For almost any query, results based on ASR transcripts have been better than any other run. This year&apos;s main success in our runs is that a combination of ASR and visual performs better than either alone! In addition we experimented with dynamic shot models, combining topic examples, feature extraction and interactive search.
We present an inverse halftoning algorithm for error diffused halftones. At each pixel, the algorithm applies a separable 7 ff 7 FIR fflter parameterized by the horizontal and vertical edge strengths computed from the local gradients. The algorithm requires entirely local operations, storage of 7 rows, and fewer than 300 arithmetic operationsffpixel. The algorithm can be easily implemented in embedded software or hardware. We compare our algorithm with previously reported approaches to show that it delivers comparable PSNR and subjective quality at a fraction of the computation and memory requirements. A C implementation of the algorithm is available at http:ffffwww.ece.utexas.eduff~bevansff projectsffinverseHalftoning.html.
In previous work, we developed a framework for expressing general  preference information in default logic and logic programming. Here we show  that the approach of Brewka and Eiter can be captured within this framework.
We present a scheme for allocating unsolicited grants to the end hosts of synchronous applications of a wireless access network, in accordance with the condition of the channel, the importance of each packet and the specific loss recovery mechanism employed in the channel. The proposed scheme is generic in the sense that it maximizes the effectiveness of the channel under various conditions and it can be used along with every FEC-based or retransmission-based error recovery strategy.
Using formal information retrieval methods, International News Connection ([4]) provides a centralized location on the Web that allows users to access constantly updated international news, through dynamic links to news stories from 14 different news sources. The links are updated every 15-20 minutes. The news stories are classified into seven regional categories: Africa, Asia (excluding China), China, Europe (excluding Russia), Middle East, Russia, and South America. The system also contains a clustering function allowing users to retrieve news stories similar to a particular one.
The development of formal specifications may benefit from prototyping activities. The production of an executable model for a given description helps bridging the gap between this specification and the corresponding reality. The KIDS/VDM system, based on the KIDS environment, provides these prototyping facilities for the model-based specification language of VDM. This paper illustrates its use in the specification of a bank transfer operation. It shows how animation may be helpful at several stages of a specification process based on a series of refinements of an initial abstract specification.
Behaviour co-ordination is one of the major problems in behaviour-based robotics. This paper presents a teaching method for mobile robots to learn the behaviour coordination. In this method, the sensory information is abstracted into a limited number of feature states that correspond to physical events in the interactive process between a robot and its environment. The continuous motor actions are abstracted into a limited number of behaviours. Then, the goal of the behaviour co-ordination is to map the feature states into the behaviours in the light of environment rewards. The teaching process consists of an imitation stage and an autonomous learning stage. Both stages employ Q-learning algorithms to implement the mapping. The imitation stage serves as a preliminary stage for the teaching method. The learning result will be used to bootstrap the autonomous learning stage. Experiments are conducted in the domain of soccer playing of Sony legged robots. Experiment results show that the robot can acquire the behaviour coordination ability.
theory of mental models with the notion of context. A context is defined as a collection of local mental models, namely mental models which represent only part of a situation. We argue that most reasoning tasks essentially involve a stage at which the information represented in a set of initial &quot;input&quot; contexts is integrated in a more comprehensive &quot;target&quot; context, where a solution can be searched for. Our claim is that this more structured account of mental representations can provide insights in the constructive process through which mental models are generated and assembled. The preliminary experimental evidence reported in this paper shows that the organization of contexts matters in explaining how individuals perform disjunctive reasoning tasks, and that the difficulty in information integration complement the number of mental models as an explanation for deductive failures.
The concept of neutral evolutionary networks being a significant factor in evolutionary dynamics was first proposed by Huynen et al. about 7 years ago. In one sense, the principle  is easy to state -- because most mutations to an organism are deleterious, one would expect that neutral mutations that don&apos;t affect the phenotype will have disproportionately greater representation amongst successor organisms than one would expect if each mutation was equally likely. So it was
The bootstrap is a numerical technique, with solid theoretical  foundations, to obtain statistical measures about the quality of an estimate  by using only the available data. Performance assessment through  bootstrap provides the same or better accuracy than the traditional error  propagation approach, most often without requiring complex analytical  derivations. In many computer vision tasks a regression problem in which  the measurement errors are point dependent has to be solved. Such regression  problems are called heteroscedastic and appear in the linearization  of quadratic forms in ellipse fitting and epipolar geometry, in camera  calibration, or in 3D rigid motion estimation. The performance of these  complex vision tasks is difficult to evaluate analytically, therefore we propose  in this paper the use of bootstrap. The technique is illustrated for  3D rigid motion and fundamental matrix estimation. Experiments with  real and synthetic data show the validity of bootstrap as an evaluation  tool and the importance of taking the heteroscedasticity into account.
For the past few years Cooper Union has been working on the implementation of assessment processes in engineering projects, courses, and programs, as required in the new criteria for accreditation by the Accreditation Board for Engineering and Technology (ABET). The assessment program has been successful and is now fully institutionalized. Faculty participation in assessment has contributed to student learning progress, as measured by competency development. Assessment, in addition, has contributed to make students&apos; participation in the educational process more active, to raise their confidence in their own skills and abilities, and to make their learning a more successful one. Index Terms --- Formative and summative assessment, oneon -one interaction, implementation processes and strategies  I. 
In this paper we thoroughly analyze a distributed procedure for the problem of local database update in a network of database peers, useful for data exchange scenarios. The algorithm supports dynamic networks: even if nodes and coordination rules appear or disappear during the computation, the proposed algorithm will eventually terminate with a sound and complete result.
Very sparse bitmaps are used in a wide variety of applications, ranging from adjacency  matrices in representation of large sparse graphs, representation of sparse space  occupancy to book-keeping in databases. In this paper, we propose a method based  on pruning of binary space partition (BSP) tree in minimal description length (MDL)  principle for coding very sparse bitmaps. This new method for coding of sparse bitmaps  meets seemingly competing objectives of good compression, the ability of conducting  queries directly in compression domain, and simple and fast decoding.
A new interpretation of transform coding is developed that downplays quantization and emphasizes entropy coding, allowing a comparison of entropy coding methods with different memory requirements. With conventional transform coding, based on computing Karhunen--Love transform coefficients and then quantizing them, vector entropy coding can be replaced by scalar entropy coding without an increase in rate. Thus the transform coding advantage is a reduction in memory requirements for entropy coding. This paper develops a transform coding technique where the source samples are first scalar-quantized and then transformed with an integer-to-integer approximation to a nonorthogonal linear transform. Among the possible advantages is to reduce the memory requirement further than conventional transform coding by using a single common scalar entropy codebook for all components. The analysis shows that for high-rate coding of a Gaussian source, this reduction in memory requirements comes without any degradation of rate-distortion performance.
Based on queuing theory we develop analytical approximations for the average packet transfer time of a store-and-forward and a cutthrough buffer insertion ring with two client traffic priorities. These types of rings are architectural alternatives for resilient packet rings (RPR) which transport data (e.g., IP) packets over optical media. We use the approximations for a delay comparison of both ring architectures. It turns out that high priority traffic is more delayed in the cut-through architecture than in the store-and-forward architecture whereas low priority traffic performs similarly in both architectures.
Using an embedded, interpreted language to control a complicated application can have significant software-engineering benefits. But existing interpreters are designed for embedding into C code. To embed an interpreter into a different language requires a suitable API. Lua-ML is a new API that uses higher-order functions and types to simplify the use of an embedded interpreter. A typical application-program function can be added to a Lua-ML interpreter simply by describing the function&apos;s type.
The quantity f(n, r), defined as the number of permutations of the set [n] =  f1; 2; : : : ng whose fixed points sum to r, shows a sharp discontinuity in the neighborhood of r = n. We explain this discontinuity and study the possible existence of other discontinuities in f(n; r) for permutations. We generalize our results to other families of structures that exhibit the same kind of discontinuities, by studying  f(n; r) when &quot;fixed points&quot; is replaced by &quot;components of size 1&quot; in a suitable graph of the structure. Among the objects considered are permutations, all functions and set partitions. 
In our research we explore the benefits resulting from the application of  Semantic Web technologies in the recruitment domain. We use currently available  standards and classifications to develop a human resource ontology which gives us  means for semantic annotation of job postings and applications. Furthermore, we  outline the process of semantic matching which improves the quality of query results.
This paper develops a new method for hierarchical clustering. Unlike other existing clustering schemes, our method is based on a generative, tree-structured model that represents relationships between the objects to be clustered, rather than directly modeling properties of objects themselves. In certain problems, this generative model naturally captures the physical mechanisms responsible for relationships among objects, for example, in certain evolutionary tree problems in genetics and communication network topology identification. The paper examines the networking problem in some detail, to illustrate the new clustering method. More broadly, the generative model may not reflect actual physical mechanisms, but it nonetheless provides a means for dealing with errors in the similarity matrix, simultaneously promoting two desirable features in clustering: intra-class similarity and inter-class dissimilarity.
The traditional goal of numerical analysis is to devise and  understand algorithms that approximate fast, robustly and within given  error tolerance the solution of an underlying mathematical problem. In  this paper we advance a philosophy that, while not at variance with this  goal, calls for an important change of emphasis. In many problems of  interest, mathematical analysis provides us with a partial knowledge of the  solution, its properties and possibly its invariants. We suggest that such  problems be computed by numerical methods that share their qualitative  attributes. Thus, the recovery of the correct qualitative behaviour of a  mathematical problem should be seen as a major criterion in devising and  analysing numerical algorithms. We specialise
In this report we study the problem of determining three-dimensional orientations for noisy projections of randomly oriented identical particles. The problem is of central importance in the tomographic reconstruction of the density map of macromolecular complexes from electron microscope images and it has been studied intensively for more than 30 years.
We discuss using the semi-regenerative method, importance sampling, and stratification to estimate the expected cumulative reward until hitting a fixed set of states for a discrete-time Markov chain on a countable state space. We develop a general theory for this problem and present several central limit theorems for our estimators. We also present some empirical results from applying these techniques to simulate a reliability model.
We describe the deductive and proof presentation capabilities of a rule-based system implemented in Mathematica. The system can compute proof objects, which are internal representations of deduction derivations which respect a specification given by the user. It can also visualize such deductions in human readable format, at various levels of detail. The presentation of the computed proof objects is done in a natural-language style which is derived and simplified for our needs from the proof presentation styles of Theorema.
Introduction  To accommodate the new Argos 500 MEG instrument [1] a general upgrade of the existing 2+1layer magnetically shielded room at the neurological department was necessary.  Due to the innovative technology and the ambitious goal more stringent requirements where set for shielding performances and shielding anisotropy.  The upgrade process of the existing MSR was carried out in cooperation with Advanced Technologies Biomagnetics and consisted in:  1. Adding a third layer of soft magnetic material.  2. Redesigning the structure supporting the magnetic layers of the door to improve the matching with the walls to reduce the door effects.  3. Redesigning completely the operation of the door by removing all hidden mechanisms and leverages and replacing them with external pneumatic locks.  4. Adding extra features like automatic emergency release of the door in case of power and or airpressure failure and manual locks disengage in case of mechanical failure.  2 Methods  The Magnetic
In this paper, we investigate the condition numbers for the generalized matrix inversion and the rank de cient linear least squares problem: min x kAx bk 2 , where A is an  m-by-n (m  n) rank deficient matrix. We rst derive an explicit expression for the condition number in the weighted Frobenius norm k [AT ; b] kF of the data A and b, where  T is a positive diagonal matrix and  is a positive scalar. We then discuss the sensitivity of the standard 2-norm condition numbers for the generalized matrix inversion and rank de cient least squares and establish relations between the condition numbers and their condition numbers called level-2 condition numbers.
We present the Pulse protocol which is designed for multi-hop wireless infrastructure access. While similar to the more traditional access point model, it is extended to operate across multiple hops. This is particularly useful for conference, airport, or large corporate deployments. In these types of environments where users are highly mobile, energy efficiency becomes of great importance. The Pulse protocol utilizes a periodic flood initiated at the network gateways which provides both routing and synchronization to the network. This synchronization is used to allow idle nodes to power off their radios for a large percent of the time when they are not needed for packet forwarding. This results in substantial energy savings. Through simulation we validate the performance of the routing protocol with respect to both packet delivery and energy savings.
In this work, we present a fundamentally new approach to visual servoing using lines. It is based on a theoretical and geometrical study of the main line representations which allowed us to define a new representation, the so-called binormalized Plücker coordinates. They are particularly well suited to visual servoing. Indeed, they allow the de  nition of a proper image line alignment notion. The control law which realizes such an alignment has moreover several properties: partial decoupling between rotation and translation, analytical inversion of the motion equations and global asymptotic stability conditions. This control lawwas validated both in simulation and experimentally in the specific case of an orthogonal trihedron.
Distributed Shared Memory (DSM) systems usually employ a number of hardware pages as management units. The gap between the size of application objects and coherence units leads to the undesirable effect of false sharing, resulting in  a significant performance degradation for a wide range of applications. To prevent false sharing and reduce the scope of consistency actions some systems introduce objects as the unit of sharing. The size of shared objects is deduced from application objects and may not change during program execution which has usually a negative influence on DSM performance. We present a distributed memory management which is neither oriented to application objects nor to page size. Object granularity may vary during program execution to adapt the unit of sharing to application requirements which usually change over the time. This leads to a prefetching of the working set of activities and thereby improved performance by reducing the number of messages sent in the distributed system. Our memory management is integrated into a language-based approach to construct structured object-based distributed systems taking advantage of the implicit structural relationships between passive and active objects to further improve the performance.
CONTENTS OERSTED&apos;S DISCOVERY OF ELECTROMAGNETISM .......................................................... 1 THE DYNAMO.............................................................................................................................. 2 THE TELEGRAPH ........................................................................................................................ 2 THE ELECTROMECHANICAL RELAY..................................................................................... 3 THE ELECTRIC MOTOR ............................................................................................................. 3 INGREDIENTS OF DISCOVERY ................................................................................................ 4 iii  TABLE OF FIGURES Figure 1. Timeline of Electromagnetic Discovery......................................................................... 4 THE BIRTH OF ELECTROMAGNETIC ENGINEERING By the 18   century scientists knew that there was some so
this paper:  ASSUMPTION G1. F is a globally Lipschitz continuous vector field
We study a general equilibrium model with endogenous humancmanMS formation inwhic  ex anteidenticI  groups may be treated asymmetricHMx in equilibrium. TheinteracMx&apos; between an informational externality and general equilibriumeffeci cfeci incibriu for groups to specSMx&apos;8H DiscMx&apos;8HI`Mc may arise even if thecM&apos;HI&apos;jMx&apos;8H model with a single group has a unique equilibrium. The dominant group gains fromdiscj`IUMx&apos;UII rationalizing why a majority may bereluc@IM to eliminatediscateM&quot;U8HM The model is alsocoMS8S`&apos;M with &quot;reverse discrseMC8&apos;UMx&apos; as a remedy against disctM&apos;SU&apos;@Mx sinc it may benec8@@8M to decU`&apos;U the welfare of the dominant group to achieve parity.
This paper demonstrates the advantage of combining a ranked output from a fulltext index and another from an index generated using generic summary texts. Our main findings are as follows: (1) The combination of a fulltext-based run and a summary-based run can significantly outperform either alone, especially when the fulltext evidence is emphasized. (2) The primary role of the summary index in the above combination is to promote some of the relevant documents already retrieved by the fulltext index, rather than to discover new relevant documents. (3) Moreover, though the summary quality can make a difference for this combination approach, even simple extracts such as 5% &quot;lead&quot; may give significant improvements.
The algorithmic analysis of timed automata is fundamentally  limited by the undecidability of the universality problem. For this reason  and others, there has been considerable interest in restricted classes of  timed automata. In this paper we study the universality problem for  two prominent such subclasses: open and closed timed automata. This  problem is described as open in [6, 8] in the case of open timed automata.
Constraint programming uses enumeration and search tree pruning to solve combinatorial optimization problems. In order to speed up this solution process, we investigate the use of semidefinite relaxations within constraint programming. In principle, we use the solution of a semidefinite relaxation to guide the traversal of the search tree, using a limited discrepancy search strategy. Furthermore, a semidefinite relaxation generally produces a tight bound for the solution value, which improves the pruning behaviour. Experimental results on stable set problem instances and maximum clique problem instances show that constraint programming can indeed greatly benefit from semidefinite relaxations.
Network security and authentication are very important for all kinds of communication networks to assure network stability and to avoid subscription fraud. In the last years -- even for wireless local area networks -- mechanisms have been found to support both in a cellular network. In multi--hop networks based on IEEE802.11, security and authentication are still open issues. Mainly the low price of the network infrastructure makes this kind of networks vulnerable. Within the scope of this paper we describe the application of IEEE802.1x in combination with UMTS Authentication and Key Agreement (AKA) to enable authentication and security in multi--hop networks.
VHLD Mixed Signal Modeling and Signal Modeling and Simulation To facilitate simulation of mixed signal HDL models within a DEVS simulator, generic DEVS models and HDL to DEVS conversion procedures are required. These models and conversion procedures are designed for a subset of VHDL created for this project named sAMS-VHDL, and are targeted toward the CD++ DEVS simulation toolkit. Hierarchical models written in sAMS-VHDL that utilize Processes, Signals and Simultaneous Statements may be simulated in CD++ by elaborating the model, and converting the model hierarchy into an equivalent CD++ coupled model composed of Process, Signal and Integrator models. These Process, Signal and Integrator models and their associated conversion procedures were designed and then tested in CD++ using a number of characteristic sAMS-VHDL models. Acknowledgments  Special thanks to Professor G. Wainer for all of his help, Akara Corporation for all of their tutelage and Dhanu and Bobby for their support. . . . . . . . . . .
Survivability is defined as the ability of a system to fulfill its mission, in a timely manner, in the presence of attacks, accidents, and failures. The mission of the U.S. electric power industry is to reliably and profitably generate and supply electricity wherever and whenever it is needed in North America. However, the U.S. electric power industry is becoming increasingly dependent on information systems, including highly distributed information systems that operate in unbounded networks, such as the Internet. As a result, the vulnerabilities of these information systems can undermine the industry&apos;s ability to supply electricity reliably to its customers. In addition, the current restructuring (deregulation) of the electric power industry creates problems that can threaten the ability of the competitive market to provide reliable electricity service to its customers. We will show that traditional computer security is not adequate to protect the mission of the industry and that a survivability approach is required. In order to study the survivability of the electric power system, simulating the system would be beneficial because we cannot test the real electric power system to evaluate survivability problems and solutions. The Easel simulation language developed at the Software Engineering Institute (SEI) in Carnegie Mellon University (CMU) is appropriate for investigating survivability issues (such as choosing which one of several alternate policy implementations would be the most survivable). In particular, the Easel simulator is based on emergent algorithms, which achieve global effects through local actions and neighbor interactions. Easel is beneficial to simulate the electric power system because the industry has emergent behavior and survivability is a key eme...
Alternating-time Temporal Logic (ATL) is a logic developed by Alur, Henzinger, and Kupferman  for reasoning about coalitional powers in multi-agent systems. Since what agents can achieve  in a specific state will in general depend on the knowledge they have in that state, in an earlier  paper we proposed ATEL, an epistemic extension to ATL which is interpreted over Alternating-time  Temporal Epistemic Transition Systems (AETS). The logic ATEL shares with its ancestor ATL the  property that its model checking problem is tractable. However, while model checkers have been  implemented for ATL, no model checker yet exists for ATEL, or indeed for epistemic logics in general.
Notation and Abbreviations 3 1 
this paper, then the realist and constructivist accounts have the appearance of contradictories. It might be assumed that arguments against one would count in favour of the other. However, if the realist/constructivist distinction is combined with the pure/applied distinction then there are four permutations and in two of these realism and constructivism are not even contraries let alone contradictories. These are:  First: A realist account of pure and applied arithmetic
We propose a transcoding-enabled streaming media caching system (TeC) along with a new set of caching strategies. Our system is designed for efficient delivery of rich media web contents to heterogeneous network environments and client capabilities. The proxies perform transcoding as well as caching in our system. This design choice allows content adaptation to be performed at the edges of the networks. Depending on the connection speed and processing capability of an end user, the proxy transcodes the requested (and possibly cached) video into an appropriate format and delivers it to the user. By serving the transcoded video directly from the proxy, we improve the cache performance. Performance evaluation via simulation is presented. Specifically, simulations using both synthesized traces and real traces derived from enterprise media server logs are conducted. Simulation results indicate that by incorporating transcoding capability at the network edges, the traffic to the content origin server is further reduced.
Many electrical engineering (EE) students have difficulty in learning technical subjects because they lack sufficient competence in mathematical modeling and in  algebra. Maple is a powerful program for doing symbolic algebra, numerical calculation, and plotting of graphs, so using this program allows students to spend more time on  modeling and interpreting results. Maple also has a text editor, which makes it feasible to require students to explain their results in writing. The design of Maple documents suitable for EE teaching is discussed; a standard format, including bibliographical information, is recommended for easier use.
A collection of benchmark examples is presented for the numerical solution of continuous -time algebraic Riccati equations. This collection may serve for testing purposes in the construction of new numerical methods, but may also be used as a reference set for the comparison of methods. 1 
This paper reports an examination of the acquisition of tense-aspect markers in three children acquiring Mandarin Chinese as their first language. Focusing on the earliest markers acquired---ne and le---we argue that ne is primarily used as a default tense-aspect marker in early spontaneous speech production
We present experimentally-acquired MR image sequence reconstruction results and a review of the recently proposed doubly adaptive temporal update method (DATUM) for the acquisition of dynamic MRI sequences. The DATUM algorithm is novel in providing an estimation and tracking framework for both image reconstruction and the image acquisition inputs.
This paper discusses auspicious methods for the implementation of intelligent solutions for embedded systems. An embedded system is a computer system designed to perform a dedicated or narrow range of functions with a minimal user intervention. An intelligent system is a system that is able to react appropriately to changing situations without user input. Main challenges for intelligent solutions in embedded systems come from dependability and real-time requirements and from constraints on cost, size, and power consumption.
In many distributed applications, pairs of queries and values  are evaluated by participating nodes. This includes keyword search  for documents, selection queries on tuples, and publish-subscribe. These  applications require that all values accepted by the query be evaluated.
Assigning semantics to logic programs via selector generated  models (Schwarz 2002/2003) extends several semantics, like the stable,  the inationary, and the stable generated semantics, to programs with  arbitrary formulae in rule heads and bodies. We study this approach by  means of a unifying framework for characterizing dierent logic programming  semantics using level mappings (Hitzler and Wendt 200x, Hitzler  2003), thereby supporting the claim that this framework is very exible  and applicable to very diversely de  ned semantics.
INTRODUCTION  We describe an extension of Bunched Typing, or the ffff- calculus, as described by O&apos;Hearn [2], intended to increase the flexibility of the system and to suggest further possible routes for investigation of bunched type systems.  The system, called ffsep extends the two binary context formers of the ffff-calculus to n-place context formers with arbitrary separation relations between their members. These relations express pairwise separation constraints on the resources used by the members of the context. The system then allows a slight distinction between the combination of resources and the expression of constraints between resources. This system can express certain constraints that the ffff-calculus cannot, and can express other constraints in a clearer way. This system is described more fully, with categorical and functor category semantics, in [1].  This research was supported by the MRG project (IST2001 -33149) which is funded by the EC under the FET proactive initiative on
This paper presents an approach to animate elastic deformable materials at interactive rates using space-time adaptive resolution. We propose a new computational model, based on the conventional Hooke&apos;s law, that uses a discrete approximation of differential operators on irregular grid. It allows local refinement or simplification of the computational model based on local error measurement. We in effect minimize calculations while ensuring a realistic and scaleindependent behavior within a given accuracy threshold. We demonstrate this technique on a real-time virtual liver surgery application.
We firstly survey several forms of Herbrand&apos;s theorem. What  is commonly called &quot;Herbrand&apos;s theorem&quot; in many textbooks is actually  a very simple form of Herbrand&apos;s theorem which applies only to  ffff-formulas;  but the original statement of Herbrand&apos;s theorem applied  to arbitrary first-order formulas. We give a direct proof, based on cutelimination,  of what is essentially Herbrand&apos;s original theorem. The &quot;nocounterexample  theorems&quot; recently used in bounded and Peano arithmetic  are immediate corollaries of this form of Herbrand&apos;s theorem.
One key element in understanding the molecular machinery of the cell is to understand the meaning, or function, of each protein encoded in the genome. A very successful means of inferring the function of a previously unannotated protein is via sequence similarity with one or more proteins whose functions are already known. Currently, one of the most powerful such homology detection methods is the SVM-Fisher method of Jaakkola, Diekhans and Haussler (ISMB 2000). This method combines a generative, profile hidden Markov model (HMM) with a discriminative classification algorithm known as a support vector machine (SVM). The current work presents an alternative method for SVMbased protein classification. The method, SVM-pairwise, uses a pairwise sequence similarity algorithm such as SmithWaterman in place of the HMM in the SVM-Fisher method. The resulting algorithm, when tested on its ability to recognize previously unseen families from the SCOP database, yields significantly better remote protein homology detection than SVM-Fisher, profile HMMs and PSI-BLAST.
A scheme for pose-independent face recognition is presented. An &quot;unwrapped&quot;  texture map is constructed from a video sequence using a texture-from-motion  approach, which is shown to be quite accurate. Simple lighting normalization  methods improve robustness to directional and/or varying lighting conditions.
This paper studies how to bring flexibility to fault-tolerant systems. Firstly, multi-agent systems are identified as a very valuable basis for reaching this goal, and reliability is also shown to be a rare and attractive feature for such systems. We then propose a framework for building applications that provide adaptive fault tolerance, and put forward the promising results obtained when testing the implementation of this framework. We conclude with drawing some perspectives of evolution of our work.
etwork of interoperable shared virtual worlds. To increase the HLA&apos;s ease-of-use, the XRTI includes a proxy compiler that converts the FOMs stored in arbitrary FOM Document Data (FDDs) into sets of fully documented Java^TM proxy classes with consistent and intuitive type-safe interfaces. To ease the process of defining a common RTI message protocol, the XRTI employs a novel bootstrapping technique to define its messages in terms of HLA constructs and encodings. To allow applications to introduce new kinds of objects and interactions into active worlds, the XRTI uses its Reflection Object Model (ROM) to represent each world&apos;s FOM as a set of manipulable shared objects, and introduces the mergeFDD method as a means of dynamically extending the FOM. This extension capability is particularly important for applications such as those based on NPSNET-V, a component-based platform for dynamically extensible virtual environments. A set of XRTI controller modules allows NPSNET-V applications to 
We describe a multi-link multi-source model of the TCP Vegas congestion control mechanism  as a distributed optimization algorithm. The model provides a fundamental understanding of  delay, fairness and loss properties of TCP Vegas. It implies that Vegas stabilizes at a weighted  proportionally fair allocation of network capacity when there is sufficient buffering in the network.
We present a novel output-sensitive algorithm for sound rendering of  complex scenes, i.e. scenes that contain a large amount of sound sources.
This paper examines the CORBA Naming, Event, Notification, Trading, Time and Security Services, with the objective of identifying the issues that must be addressed in order make these Services fault-tolerant. The reliability considerations for each of these Services involves strategies for replicating the Service objects, and for keeping the states of the replicas consistent. Of particular interest are the sources of non-determinism in each of these Services, along with the means for addressing the non-deterministic behavior in the interests of ensuring strong fault tolerance.
This paper describes an estimation and representation method for object structure  in 3D image data. A windowed Fourier transform is used to estimate the  parameters of 3D local structures (lines, planes and spheres) across a number  of window sizes or scales. We present the estimation method and describe  a decision process which aims to find the simplest object description. Results  of the method on test data and blood vessels from magnetic resonance  imaging are presented.
this paper, we propose a concept of a biomagnetic inverse analysis with care for uncorrelated signal decomposition and for signal source localization simultaneously
this paper that the combination represents a particular challenge to the study and regulation of communication networks
Linear representations and linear dimension reduction techniques are very common in signal and image processing. Many such applications reduce to solving problems of stochastic optimizations or statistical inferences on the set of all subspaces, i.e. a Grassmann manifold. Central to solving them is the computation of an &quot;exponential&quot; map (for constructing geodesics) and its inverse on a Grassmannian. Here we suggest efficient techniques for these two steps and illustrate two applications: (i) For image-based object recognition, we define and seek an optimal linear representation using a Metropolis-Hastings type, stochastic search algorithm on a Grassmann manifold. (ii) For statistical inferences, we illustrate computation of sample statistics, such as mean and variances, on a Grassmann manifold.
The description of public resources such as web site contents, web services  or data files in open peer-to-peer networks using some formal framework  like RDF usually reflects solely the subjective requirements, opinion and preferences  of the resource provider. In some sense, such resource descriptions appear  &quot;antisocial&quot; as they do not reflect the social impact of the respective resource  and therefore might not provide impartial, reliable assessments. E.g., commercial  web sites do not contain any relationship to the information, service and product  offers of competing sites, and the assessment of the site by customers, experts or  competitors is unknown to users and information agents also. We introduce an  open multiagent system framework which derives multidimensional resource descriptions  from the possibly conflicting opinions of interacting description agents,  which act as representatives for individual, organizational or institutional clients,  and compete in the assertion of individual opinions against others to provide a  &quot;socially enhanced&quot; solution for this problem. In contrast to the results of majority  voting based recommendation systems, the obtained social resource descriptions  reflect social structures such as norms and roles which emerge from  communication processes.
Steel production is a complex process and finding coherent schedules for the wide variety of production steps in a dynamic environment where disturbances frequently occur is a challenging task. Steel production involves a range of processes: continuous caster, hot strip mill, furnaces. The scheduling systems of the caster and the hot strip mill have very different objectives and constraints. It is important that the schedules for these two processes are able to react to the presence of real time information concerning production failures and customer requests. In this paper
Some concepts of information theory, as entropy, conditional entropy and mutual information may be very useful to analyse several financial time series, especially statistical dependences. Trough the similarity between those measures and variance analysis is possible to get a new approach to study the level of statistical dependences in financial time series.
Visibility determination is the most expensive task in cluster hierarchical radiosity.
Recen t gen ration s of high-den ity an high-speed FPGAs provide a suffcien t capacity for implemen tin g complete conq urable systemson  a chip (CSoCs). Hybrid CPUs that combin e stanZ rd CPU cores withrecon figurable coprocessors are an importan t subclass of CSoCs. With partial y reconfigurable FPGAs, coprocessorscan be loadedon deman while the CPU remain s runD n . However, the lack of high-level design tools for partial recon figuration makes practical implemen tation s a challen gin g task. In this paper, we in troduce a design flow to implemen t hybrid processors on Xilin x Virtex. The design flow is based on two techn iques, virtual socketsan  d feed-through components, an can effcien tly gen rate partial conx uration from inVM try-quality cores. We discuss the design flow an d presen t a fully operation al audio streamin g prototype to demon strate its feasibility. 1 
We evaluate the dynamic aperture for the CERN muon storage ring, and, in particular, study the effect of magnet fringe fields. The detuning with amplitude is computed via normal-form analysis. Particle tracking reveals the dependence of the dynamic aperture on betatron tune and momentum offset, and demonstrates satisfactory performance. The depolarisation in transverse phase space is estimated from a spin normal form. All calculations are performed with the computer codes COSY INFINITY and MAD. Geneva, Switzerland May 4, 2000 1 
This research tests an explanatory model of IT outsourcing behaviour. It relies on transactions cost theory. A survey was conducted to test the hypotheses. As anticipated, the results suggest that uncertainty and the origin of the investments required to conduct a transaction have a significant influence on the outsourcing decision. However, asset specificity shows an inconsistent effect on outsourcing. This suggests that asset specificity might not be a key driver when evaluating IT outsourcing.
In this paper we present a possible approach to the application of digital rights languages in the field of learning media, and indicate how it facilitates the establishment of new learning media services (specifically rights enforcement). Digital rights languages are used to specify usage rights to learning resources (LR) in electronic contracts. Important issues for rights enforcement are to identify those parts of  contracts which can reliably be enforced electronically,  as well as suitable means of translating the enforceable parts into concrete access control information. We addressed these two problems by identifying criteria for  the enforceability of electronic contracts, and by designing a flexible strategy for translating the expression of rights into access control information.
Network operators must have control over the flow of traffic into, out of, and across their networks. However, the Border Gateway Protocol (BGP) does not facilitate common traffic engineering tasks, such as balancing load across multiple links to a neighboring AS or directing traffic to a different neighbor. Solving these problems is difficult because the number of possible changes to routing policies is too large to exhaustively test all possibilities, some changes in routing policy can have an unpredictable effect on the flow of traffic, and the BGP decision process implemented by router vendors limits an operator&apos;s control over path selection. We propose...
bot vehicle that is capable of navigating successfully within the confines of a building may be completely unable to function outdoors. An intelligent vehicle that is able to drive on well marked roads may be unable to drive successfully through the woods, or in tall grass or weeds. An intelligent vehicle that is capable of driving on the freeway may be unable to drive on two lane roads with oncoming traffic, or on city streets with intersections and traffic signals. This suggests that Performance Measures for Intelligent Systems must not only measure the behavior of the system, but the characteristics of the environment in which the system must perform.  The U.S. Army has launched a major initiative to field a Future Combat System that will consist of light-weight, airtransportable vehicles that include both manned and unmanned vehicles. A major effort is directed at measuring the technology readiness of technology for unmanned driving, both on-road and off-road in all kinds of weathe
Over the past few years, technology drivers for processor designs have changed significantly. Media data delivery and processing -- such as telecommunications, networking, video processing, speech recognition and 3D graphics -- is increasing in importance and will soon dominate the processing cycles consumed in computer-based systems. This paper describes a processo, called Linedancer, that provides high media performance with low energy consumption by integrating associative SIMD parallel processing with embedded microprocessor technology. The major innovations in the Linedancer is the integration of thousands of processing units in a single chip that are capable to support software programmable high-performance mathematical functions as well as abstract data processing. In addition to 4096 processing units, Linedancer integrates on a single chip a RISC controller that is an implementation of the SPARC architecture, 128 Kbytes of Data Memory, and I/O interfaces. The SIMD processing in Linedancer implements the ASProCore architecture, which is a proprietary implementation of SIMD processing, operates at 266 MHz with program instructions issued by the RISC controller. The device also integrates a 64-bit synchronous main memory interface operating at 133 MHZ (double-data rate, DDR), and a 64-bit 66 MHz PCI interface.
The analysis of linked faults has proven to be a source for new memory tests, characterized by an increased fault coverage. The paper gives a set of five new tests to target all possible linked faults. The tests are merged into a single test, March SL, detecting all faults in the linked fault space. The preliminary test results of an experiment done at Intel will be reported; they show that March SL scores high and detects some unique faults.
The set points of supervisory control strategy are optimized with respect to energy use and thermal comfort for existing HVAC systems. The set point values of zone temperatures, supply duct static pressure, and supply air temperature are the problem variables, while energy use and thermal comfort are the objective functions. The HVAC system model includes all the individual component models developed and validated against the monitored data of an existing VAV system. It serves to calculate energy use during the optimization process, whereas the actual energy use is determined by using monitoring data and the appropriate validated component models. A comparison, done for one summer week, of actual and optimal energy use  shows that the on-line implementation of a genetic algorithm optimization program to determine the  optimal set points of supervisory control strategy could save energy by 19.5%, while satisfying the minimum zone airflow rates and the thermal comfort. The results also indicate that the application of the two-objective optimization problem can help control daily energy use or daily building thermal comfort, thus saving more energy than the application of the one-objective optimization problem.
This study describes the demographic features of a population of Sigmodon hispidus utilizing the habitat mosaic provided by a Carolina Bay on the Atlantic coastal plain of South Carolina. A total of 71 cotton rats were captured 160 times on a 4 ha grid during a winter decline from to less that Body weights of adults declined until early February and then increased; those of subadults grew very slowly until February followed by a spurt in growth. Weight gain did not differ between survivors and non-survivors for males, but female survivors gained 1.5 g per week more than non-survivors. Female subadults exhibited higher mor-  tality early in the decline and males later. Adult females were randomly distributed across 8 microhabitats, whereas adult males were almost exclu-  sively confined to heavy cover. males used wet sites more than any other cohort; females were widely distributed using drier sites most frequently. By the end of the decline, all survivors were localized inRubus-dominated patches. No statistically significant changes in electromorphgenotypes or allele frequencies were detected, but survivors had a higher frequency of the F-allele at the adenylate kinase locus than did non-survivors (42.3% vs. 16.7%). Our findings affirm the importance of a landscape perspective in understanding the population dynamics of cotton rats, and show how a habitat mosaic influences survival differentially among sex-age cohorts. 1. 
We introduce a new approach for the computation of viewindependent solutions to the diffuse global illumination problem in polyhedral environments. The approach combines ideas from hierarchical radiosity and discontinuity meshing to yield solutions that are accurate both numerically and visually. First, we describe a modified hierarchical radiosity algorithm that uses a discontinuitydriven subdivision strategy to achieve better numerical accuracy and faster convergence. Second, we present a new algorithm based on discontinuity meshing that uses the hierarchical solution to reconstruct an object-space approximation to the radiance function that is visually accurate. Our results show significant improvements over both hierarchical radiosity and discontinuity meshing algorithms.
Embodied Agents are still looking for their own body language. Our eorts aim at analyzing human speakers to extract individual repertoires of gesture. We argue that TV personalities or actors are better suited for this purpose than ordinary subjects. For analysis, we propose an annotation scheme and present the tool ffffffffff for transcoding gesture and posture. For transfer to synthetic agents, we suggest to think of gestures as categorizable in equivalence classes, a subset of which can make up an agent&apos;s nonverbal repertoire. Results of investigating dierent levels of gesture organisation and posture shifts are presented. The concept of GGroups is introduced whichishypothesized to correspond to rhetorical devices. Also, we give a brief sketchonhow  these results are planned to be integrated in a multimodal generation system for presentation teams.
We present PROTON, a policy-based solution for 4G mobile devices -- it allows users to seamlessly connect to highly integrated heterogeneous wireless networks. The key motivation behind PROTON stems from the statement that handover process complexity will increase in 4G systems, creating the need for augmented knowledge about context, as well as more flexibility. This paper demonstrates (1) how a flexible policy-based approach is suitable for 4G scenarios, and (2) how to incorporate richer context into policies and still maintain a light weight solution appropriate for mobile devices.
We report on the application of the HR program (Colton,  Bundy, &amp; Walsh 1999) to the problem of automatically inventing  integer sequences. Seventeen sequences invented by  HR are interesting enough to have been accepted into the Encyclopedia  of Integer Sequences (Sloane 2000) and all were  supplied with interesting conjectures about their nature, also  discovered by HR. By extending HR, we have enabled it to  perform a two stage process of invention and investigation.
In order to monitor a region for target intrusion, sensors can be deployed to perform collaborative target detection. To adequately deploy sensors, metrics need to be define to measure the ability of a given deployment to successfully complete this task. Recently, the measure of exposure was defined to evaluate the coverage of the region by a sensor network. Exposure is a versatile metric that is expected to provide a pertinent measure of the region coverage for various scenarios. This paper develops a method to find the minimum exposure of a target traversing the region at variable speed. It also introduces target activities different from region traversal for which the coverage by the sensors remains not well defined. Finally, the problem of coverage in the presence of obstacles in the region is identified. Exposure is expected to provide a good coverage measure for these various problems.
Array programming shines in its ability to express computations at a high-level of abstraction, allowing one to manipulate and query whole sets of data at once. This paper presents the OOPAL model that enhances object-oriented programming with array programming features. The goal of OOPAL is to determine a minimum set of modifications that must be made to the traditional object model in order to take advantage of the possibilities of array programming. It is based on a minimal extension of method invocation and the definition of a kernel of methods implementing the fundamental array programming operations. The model is validated in F-SCRIPT, a new scripting language.
This research report is a compilation of two articles describing new results concerning the 3D Marching Lines algorithm. The Marching Lines extracts, with sub-voxel accuracy, characteristic 3D lines out of 3D images, such as the Crest Lines. Those feature lines can then be used to perform higher level Image Processing tasks, such as 3D image registration, or automatic labeling of anatomical structures, when Medical Images are processed. The first paper concentrates on the computation of the differential characteristics of iso-intensity surfaces, and shows how to characterize Crest Lines points directly from the differentials of the 3D Image. The second paper brings the proof of the good topological properties of the reconstructed surfaces and 3D curves obtained with the Marching Lines algorithm. New experiments on real and synthetic data are also presented, showing the high precision and stability of the extracted feature lines.
This paper reports on a study whose goal was to investigate how people make use of gestures and spoken utterances while playing a videogame without the support of standard input devices. We deploy a Wizard of Oz technique...
We consider the problem of optimal rate-distortion streaming of packetized multimedia data in the context of sender-driven transmission over a single-QoS network using feedback and retransmissions. For a single data unit, we prove that the problem is NP-hard and provide efficient branch and bound algorithms that are much faster than the previously best solution based on dynamic programming. For a group of data units, we show how to compute optimal solutions with branch and bound algorithms. The branch and bound algorithms for a group of data units are much slower than the current state of the art, a heuristic technique known as sensitivity adaptation. However, in many real-world situations, they provide a significantly better rate-distortion performance.
Hypothesis generation, a crucial initial step for making scientific discoveries, relies on prior knowledge, experience and intuition. Chance connections made between seemingly unrelated concepts sometimes turn out to be fruitful. A key goal in text mining is to assist in this process by automatically discovering a small set of interesting hypotheses from a suitable text collection. We focus on text mining in the biomedical domain using MEDLINE, the database produced by the National Library of Medicine with more than 12 million citations. Our overall goal is to build applications that mine MEDLINE for novel concept connections and thereby support scientists in hypothesis discovery. In this paper we first present concept profiles as a mechanism for generating concept representations from text collections. There are several advantages offered by concept profiles. They can be as current as the text database or they can be generated from temporal subsets. Profiles may be restricted to particular views and also they may be generated for concepts that are as complex as needed. We then show how concept profiles may be used to identify similar concepts. In particular, we present experiments where concept profiles are used to identify genes that are associated with the same disease and drugs that are functionally similar.
OF THE DISSERTATION Parallel Discrete Event Simulation of Large Scale Wireless Ad-hoc Networks by Jay Matthew Martin Doctor of Philosophy in Computer Science University of California, Los Angeles, 2002 Professor Rajive L. Bagrodia, Chair Discrete-event simulation is among the most commonly used techniques to evaluate protocols and architectures for mobile ad hoc networks. However, detailed simulation of large wireless networks is computationally intensive due to the necessity of simulating the interference among multiple simultaneous transmissions in the radio channel. The first result of this work is establishing necessity of simulating physical layer factors such as signal reception, path loss, fading, interfe rence and noise computation, and preamble xiv length. It is shown that such factors are relevant to the performance evaluations of higher layer protocols. The second part of this work investigates a number of techniques to optimize the performance of such simulations. The specific techniques that have been used include geographic partitioning using the effects of signal propagation on basic simulator performance as well as the parallel optimizations needed to execute the simulation using a synchronous conservative parallel simulation algorithm. The parallel optimizations primarily consist of extracting lookahead necessary for the conservative simulation algorithm from the IEEE 802.11 MAC protocol and radio models. The optimizations were implemented in GloMoSim, a parallel simulator for mobile ad hoc networks, and a set of experiments were performed to measure their impact on the execution time of the simulation. Chapter 1 
The Virginia Board of Education mandated that, by the Fall of 1989, all public elementary schools in the  Commonwealth should establish elementary school guidance programs and employ an elementary school guidance counselor for every 500 students.
We generalize the notion of pattern avoidance to arbitrary functions on ordered sets, and consider specifically three scenarios for permutations: linear, cyclic and hybrid, the first one corresponding to classical permutation avoidance. The cyclic modification allows for circular shifts in the entries. Using two bijections, both ascribable to both Deutsch and Krattenthaler independently, we single out two geometrically significant classes of Dyck paths that correspond to two instances of simultaneous avoidance in the purely linear case, and to two distinct patterns in the hybrid case: non-decreasing Dyck paths (first considered by Barcucci et al.), and Dyck paths with at most one long vertical or horizontal edge. We derive a generating function counting Dyck paths by their number of low and high peaks, long horizontal and vertical edges, and what we call sinking steps. This translates into the joint distribution of fixed points, excedances, deficiencies, descents and inverse descents over 321-avoiding permutations. In particular we give an explicit formula for the number of 321-avoiding permutations with precisely k descents, a problem recently brought up by Reifegerste. In both the hybrid and purely cyclic scenarios, we deal with the avoidance enumeration problem for all patterns of length up to 4. Simple Dyck paths also have a connection to the purely cyclic case; here the orbit-counting lemma gives a formula involving the Euler totient function and leads us to consider an interesting subgroup of the symmetric group.
Speech transmission over packet networks has to cope with packet delays and packet losses. When a packet loss occurs the missing information must be estimated. In this contribution we focus on restoring the spectral parameters of a speech coder. A novel approach to estimating missing Line Spectral Frequency (LSF) parameters using Gaussian Mixture Models (GMM) is proposed. We present the estimation algorithm and study its performance when one or several LSF parameters are lost. We show that a GMM of a relatively low order is sufficient to achieve a substantial improvement in parameter SNR. Therefore, the new estimation procedure requires much less memory than histogram based estimation methods.
This report intends to summarize some of the degradation modes and capabilities of typical LEDs and laser diodes currently used in many communication and sensing systems
A class C of graphs is said to be H-bounded if each graph in the  class C admits a homomorphism to H. We give a general necessary  and sucient condition for the existence of bounds with special local  properties. This gives a new proof of Haggkvist-Hell theorem [5] and  implies several cases of the existence of triangle free bounds for planar  graphs.
Wireless OFDM systems have attractive means for adapting wireless transmission to a given situation: one possibility is to assign a varying number of subcarriers to wireless terminals for downlink communication. Deciding how many and which subcarriers to assign to a given terminal is a difficult problem. This paper concentrates on deciding how many: we use the relative length of a terminal&apos;s queue in an access point to determine this number. Applying this scheme to the transmission of homogeneous MPEG-4 videos, we obtain a significant capacity increase compared to nonadaptive subcarrier allocation schemes.
Engineering competence is a major competitive parameter in knowledge society, especially in the industry  subject to very fast innovation processes. Continuing Professional Development of engineering staff is therefore very  important. Developing a new engineering educational setup, which integrates formal education and productive engineering is a new challenge. The best way to learn and understand a theory is trying to see whether you can apply the theory in engineering problem solving. Therefore it is obvious to try to combine the academic learning process and engineering problem solving. This has been one of the fundamental reasons for the existing Problem Based Learning concept (PBL). In the light of the experiences from  this PBL concept some further development makes it possible to integrate work based learning in academic engineering education. This presentation will give some examples on how this is being implemented now and discuss further improvements by combining engineering education, Continuing Professional Development and productive engineering.
this paper a methodology for feature selection for the handwritten digit string recognition  is proposed. Its novelty lies in the use of a multiobjective genetic algorithm where  sensitivity analysis and neural network are employed to allow the use of a representative  database to evaluate fitness and the use of a validation database to identify the subsets of  selected features that provide a good generalization. Some advantages of this approach  include the ability to accommodate multiple criteria such as number of features and  accuracy of the classifier, as well as the capacity to deal with huge databases in order to  adequately represent the pattern recognition problem. Comprehensive experiments on  the NIST SD19 demonstrate the feasibility of the proposed methodology
was described in the calibration section and measures the minimum capacity, as limited by the antenna arrays. Measurements in the office-building environment predict the capacity available to a system with similar antenna arrays and in similar propagation conditions as those found in the measurements. Data from these environments are presented in the following section.  7.1.1 Free Space Calibration  The ideal free space environment, measured with an ideal system, produces an all-ones H-matrix and results in the minimum theoretical MEA capacity. Non-ideal characteristics of the measurement system increase the minimum measurable capacity by introducing noise or amplitude variations in the received signal tones. The result of this calibration measurement represents the minimum capacity that can be measured by the system when implemented with the antenna arrays specified in Chapter 4.  The non-ideal characteristics of this measurement system were addressed in Chapter 5 and will be reiterat
This survey article deals, instead, with the  synchronization of telecommunications networks  from a historical point of view. After  a short introduction to the main network  synchronization strategies that have found  the widest application, we show how the network  synchronization issues evolved with  telephone networks in the last 30 years,  beginning with old frequency-division multiplexing  (FDM) networks up to the latest  technologies through PDH, SDH/SONET,  and ATM. For each case, the different synchronization  needs and peculiar techniques of timing transfer and distribution among the network nodes are pointed out, thus providing a comprehensive overview of the evolution steps of telecommunications network synchronization.
In mobile ad-hoc networks, nodes act both as terminals and information  relays, and participate in a common routing protocol, such as Dynamic  Source Routing (DSR). The network is vulnerable to routing misbehavior, due to  faulty or malicious nodes. Misbehavior detection systems aim at removing this  vulnerability. In this paper we investigate the use of an Artificial Immune System  (AIS) to detect node misbehavior in a mobile ad-hoc network using DSR. The  system is inspired by the natural immune system of vertebrates. Our goal is to  build a system that, like its natural counterpart, automatically learns and detects  new misbehavior. We describe the first step of our design; it employs negative  selection, an algorithm used by the natural immune system. We define how we  map the natural immune system concepts such as self, antigen and antibody to a  mobile ad-hoc network, and give the resulting algorithm for misbehavior detection.
Current computational simulation are capable of producing enromous amounts of data. Complete understanding of their features presents a challenge even if very sophisticated visualization techniques are deployed. Computational analysis of conformational behaviour of biologically active compounds represents such simulation. We investigate methods how haptic rendering may contribute to better and faster understanding of the simulation results. This paper presents current progress in our research. I. Molecular Flexibility and Conformational Behaviour  Many molecules exhibit an important chemical property --- flexibility. Biological activity of large molecules is directly related to their flexibility. The flexibility can be described in terms of conformational behaviour of the molecule. Undergoing the behaviour, the molecule changes its shape (config- uration) only, no chemical bonds are either created or broken, as well as the absolute configuration on atoms (or other chiral centres) does not change. Roughly speaking, internal potential energy of the molecule is a function of the configuration, therefore, in general, not all the configurations are favoured equally. By conformations we mean local potential energy minima, i. e. the configurations that are more stable than the others. Then the conformational behaviour is the process of traversing among conformations via transition states.
Along with &quot;distributiveness&quot;, convergence speed of power control is one of the most important criteria bywhichwe can determine the practical applicability of a given power control algorithm. Agoodpower control algorithm should quickly and distributively converge to the state where the system supports as many users as possible. This paper proposes a fast and distributed power control algorithm based on the well-known PI-controller. As in the paper byFoschini and Miljanic, we start with differential equation form of the controller and analyze its convergence properties in the case of feasible systems. The actual power control algorithm is then derived by discretization of the continuous time version. Using the distributed constrainedpower control (DCPC) as a reference algorithm, we carried out computational experiments on a CDMA system. The results indicate that our algorithm significantly enhances the convergence speed of power control.
Peer-to-peer (P2P) technology has undergone rapid growth, producing new protocols and applications, many of which enjoy considerable commercial success and academic interest. Yet, P2P applications are often based on complex protocols, whose behavior is not completely understood. We believe that in order to enable an even more widespread adoption of P2P systems in commercial and scientific applications, what is needed is a modular paradigm in which well-understood, predictable components with clean interfaces can be combined to implement arbitrarily complex functions. The goal of this paper is to promote this idea by describing our initial experiences in this direction. Our recent work has resulted in a collection of simple and robust components, which include aggregation and membership management. This paper shows how to combine them to obtain a novel load-balancing algorithm that is close to optimal with respect to load transfer. We also describe briefly our simulation environment, explicitly designed to efficiently support our modular approach to P2P protocol design.
A common stylistic element of Western tonal music is the change of key within a musical sequence (known as modulation in musical terms). The aimof the present study was to investigate neural correlates of the cognitive processing of modulations with event-related brain potentials. Participants listened to sequences of chords that were infrequently modulating. Modulating chords elicited distinct effects in the event-related brain potentials: an early right anterior negativity reflecting the processing of a violation of musical regularities and a late frontal negativity taken to reflect processes of harmonic integration. Additionally, modulations elicited a tonic negative potential suggested to reflect cognitive processes characteristic for the processing of tonal modulations, namely, the restructuring of the &quot;hierarchy of harmonic stability&quot; (which specifies musical expectations), presumably entailing working memory operations. Participants were &quot;nonmusicians&quot;; results thus support the hypothesis that nonmusicians have a sophisticated (implicit) knowledge about musical regularities. &amp;  
Automatic classification of music files is a key problem in multimedia  information retrieval. In this paper we present a solution to this problem that  addresses the issues of feature extraction, feature selection and design of classifier. We outline
this paper, we give an exact analysis of online learning in a simple model system. Our aim is twofold: (1) to assess how the combination of non-infinitesimal learning rates j and finite training sets (containing ff examples per weight) affects online learning, and (2) to compare the generalization performance of online and offline learning. A priori, one Online learning can also be used to learn teacher rules that vary in time. The assumption of an infinite set (or `stream&apos;) of training examples is then much more plausible, and in fact necessary for continued adaptation of the student. We do not consider this case in the following
In this position paper, a product derivation process is described, which is based on specifications of known customer requirements, features, artifacts in a knowledge base. In such a knowledge base a model about all kinds of variability of a combined software/hardware systems are represented by using a logical-based representation language. Having such a language, a machinery which interprets the model is defined and actively supports the product derivation process e.g. by handling dependencies between features, customer requirements, and artifacts. Because the adaptation and new development of artifacts is a basic task during the derivation process where a product for a specific customer is developed, the evolution task is integrated in the proposed knowledge-based derivation process.
The area of analysis and control of linear parameter-varying ffLPVff systems has received much recent attention because of its importance in developing systematic techniques for gain-scheduling. An LPV system resembles a linear system that nonlinearly depends on one or more time-varying parameters. Nonlinear systems are often modeled in the LPV system via the parameterized Jacobian linearization. Typical approaches for the analysis and control of LPV systems are the scaled small-gain approach and the dissipative systems framework using smooth parameter-dependent Lyapunov functions ffPDLFsff. The dissipative systems framework is the more desirable of the twotechniques because it can directly treat time-varying parameters and yield an LPV-type controller. Furthermore, the dissipative systems framework attractively formulates analysis and synthesis problems as convex optimization problems involving linear matrix inequalities ffLMIsff, which are now very efficiently solved by computer. However, the current dissipative systems framework has two major potential drawbacks: ff1ff difficulty in selecting an optimal PDLF in order to reduce conservatism of the dissipative systems approachff ff2ff difficulty in solving exactly convex optimization problems involving an infinite number of LMIs. The thesis presents new analysis and control design techniques to avoid these potential drawbacks of the smooth dissipative systems framework. The thesis focuses on a piecewise-affine parameter-dependent linear parameter-varying ffPALPVff system which is a new class of LPV systems. Associated with the PALPV system is a piecewise-affine parameter-dependent Lyapunov function ffPALff. To address the non-differential nature of both the PALPV system and the PAL, the thesis develops a nonsmooth dissipative systems framework. Then, t...
effective surveillance in the digitized battlefield and for environmental monitoring. In this paper, we present the first systematic theory that leads to novel sensor deployment strategies for effective surveillance and target location. We represent the sensor field as a grid (two- or threedimensional) of points (coordinates), and use the term target location to refer to the problem of pin-pointing a target at a grid point at any instant in time. We use the framework of identifying codes to determine sensor placement for unique target location. We provide coding-theoretic bounds on the number of sensors and present methods for determtheir placement in the sensor field. We also show that sensor placement for single targets provides asymptotically complete (unambiguous) location of multiple targets.
autonomous data sources in an environment  where constraints cannot be placed  on the shared contents of sources. Our solutions  rely on the use of mapping tables  which define how data from different sources  are associated. In this setting, the answer  to a local query, that is, a query posed  against the schema of a single source, is  augmented by retrieving related data from  associated sources. This retrieval of data  is achieved by translating, through mapping  tables, the local query into a set of  queries that are executed against the associated  sources. We consider both sound  translations (which only retrieve correct answers)  and complete translations (which retrieve  all correct answers, and no incorrect  answers) and we present algorithms to compute  such translations. Our solutions are  implemented and tested experimentally and  we describe here our key findings.
This paper describes a method for learning  the countability preferences of English  nouns from raw text corpora. The method  maps the corpus-attested lexico-syntactic  properties of each noun onto a feature  vector, and uses a suite of memory-based  classifiers to predict membership in 4  countability classes. We were able to assign  countability to English nouns with a  precision of 94.6%.
Data[Semantics int]  dt int exists : Axiom Exists (x: (pod data type?[Semantics int])): True  dt int : (pod data type?[Semantics int])  End Cxx Int  The identifiers with sshort refer to the corresponding items from the semantics of signed short. First we declare the size of the value representation, this becomes important for the unsigned integer types, see below. We define the value type Semantics int as a predicate subtype of the PVS integer type int. The axioms int longer and int contains sshort formalise the requirement that &quot;[short int] provides at least as much storage as [int]&quot; (3.9.1 (2)).
This paper examines and analyzes results from empirical studies that begin to outline some of the that situate how F/OSS systems are developed in different communities. In particular, examples drawn from different F/OSS project communities reveal how processes and practices for the development and propagation of F/OSS technology are intertwined and mutually situated to the benefit of those motivated to use and contribute to it. 11 The future of research in the development and use of STINs as a conceptual lens for observing and analyzing F/OSSD processes and practices seems likely to focus attention to the following topics. First, the focus of software process research is evolving to include attention to sociotechnical processes of people, resources, organizational forms, and institutional rules that embed and surround an F/OSS system, as well as how they interact and interface with one another. Such a focus draws attention to the web of socio-technical relations that interlink people in particular settings to a situated configuration of globally available Web-based artifacts and locally available resources (skills, time, effort, computing) that must collectively be mobilized or brought into alignment in order for a useful F/OSS system to be continuously (re)designed to meet evolving user needs. Second, participation in F/OSS system design, assertion of system requirements, or design decision-making is determined by effort, willingness, and prior public experience in similar situations, rather than by assignment by management or some other administrative authority. Similarly, the openness of the source code/content of a F/OSS system encourages and enables many forms of transparency, access, and ability to customize/localize a system&apos;s design to best address user/develope...
problem under consideration, when N is the smallest odd integer satisfying (1), has been addressed by many authors ([3, 4, 5], and other papers). However, implementation of the known coding algorithms seems to be difficult if n is big enough. We develop the approach, proposed for solving the data transmission problem in asynchronous mode [6], and include a small redundancy in the representation, which allows us to avoid the difficulties. The inequality      (3/2) log n  which follows from (1) and Stirling&apos;s approximation formula for the factorial, can be helpful in the evaluation of the number of additional redundant bits.  TABLE 1. Some parameters of the code for regular binary trees.  nffNn/N  7 9 23 0.3043 15 9 31 0.4839 31 11 51 0.6078 63 11 83 0.7590 127 13 151 0.8411 255 15 283 0.9011 511 15 539 0.9480 1023 17 1055 0.9697 2095 17 2127 0.9850 4095 19 4131 0.9913 8191 19 8227 0.9956 16,383 21 16,423 0.9976 32,767 23 32,811 0.9987 THEOREM 1. Let n be a given odd integer and let ff be 
The desorption of phosphate from aluminum (Al) hydroxides by 0.01 mol L    in the presence of different organic acids and from Al-OH, Al-P and Al-Oxalate, AlCitrate complexes by organic acids was studied. When phosphate adsorbed on Al(OH)x in the presence and absence of oxalate was desorbed sequentially twice using 0.01 mol   KCl, total desorption rates of phosphate adsorbed in the presence of 0.5-2.0 mmol L    oxalate were 4.10%-4.75%. However, in the absence of organic acid, the desorption rate was only 2.05%. The amount of P desorbed by 0.01 mol L    KCl increased with increasing concentration of organic acids in adsorption solution. Some phosphate, which could not be desorbed by oxalate, would be desorbed by citrate at the same concentration. When phosphate was desorbed using a series of oxalate and citrate, the total desorption rate of phosphate from Al-oxalate complex was less than that from Al hydroxide, while the rate from Al-citrate complex was the highest. Citrate could release the phosphate from Al-P complex, and the amount of phosphate desorption increased with the increment of citrate concentration used. All these indicated that the mechanism of phosphate desorption involved ligand exchange and dissolution by organic acid. Organic acids increase the desorption of phosphate and improve the efficiency of phosphate in acid soil.
Object discovery is one of the steps to effective content  -based image retrieval. If a vision system has the  ability to identify potential objects in an image, it  enables the user to phrase queries in terms of  objects, and it enables the computer to search the  image library for similar objects. This paper  describes a physics-based approach to object discovery.
As MPEG-7 gets extended, DVB systems find the need to transport  metadata over MPEG-2. This paper gives a view of a real implementation  in an MHP application framework of the amendment ISO/IEC 138181:  2000/FDAM 1, which standardizes the carriage of MPEG-7 metadata  over MPEG2. The paper focuses mainly on the transport of metadata and  how to synchronize metadata with the video content by means of Normal  Play Time (NPT). The paper also presents some ways of broadcasting a  Dynamic Metadata Service in a MHP application framework.
The allocation of router resources among competing multicast flows is an open question. In this paper, we introduce a hierarchical multicast scheduling protocol in which each router independently prioritizes multicast packets based upon the number of downstream receivers interested in listening to the multicast stream. We describe our protocol design and evaluate the protocol&apos;s scalability, optimality, and security.
Peer-to-peer systems are often vulnerable to disruption by minorities. There are several
With recent work in the field of workflows it is possible to define  more flexible business models. With the standardization of the Business Process  Execution Language (BPEL4WS) a new implementation method is available.
A finite sequence u = a 1 a 2 : : : a p of some symbols is contained in another sequence v = b 1 b 2 : : : b q if  there is a subsequence b i 1 b i 2 : : : b i p of v which can be identified, after an injective renaming of symbols,  with u. We say that u = a 1 a 2 : : : a p is k-regular if i \Gamma j  k whenever a i = a j ; i ? j. We denote  further by juj the length p of u and by kuk the number of different symbols in u. In this expository  paper we give a survey of combinatorial results concerning the containment relation. Many of them  are from the author&apos;s PhD thesis with the same title. Extremal results concern the growth rate of  the function Ex(u; n) = max jvj, the maximum is taken over all kuk-regular sequences v, kvk  n,  not containing u. This is a generalization of the case u = ababa : : : which leads to Davenport-Schinzel  sequences. Enumerative results deal with the numbers of abab-free and abba-free sequences. We  mention a well quasiordering result and a tree generalization of our extremal function from sequences  (=colored paths) to colored trees.
Diffusion processes observed partially, typically at discrete timepoints and possibly with  observation error, arise when constructing stochastic models in continuous time. This paper  introduces a novel Sequential Monte Carlo approach to inference for partially observed diffusion  processes. The method of Sequential Monte Carlo provides an alternative to Markov Chain  Monte Carlo methods, and has proven to be effective in complex models at the cutting edge  of scientific research. The new methodology enables filtering, prediction, smoothing and parameter  estimation in certain nonlinear models for which these are diffcult or impossible using  existing Monte Carlo methods. As a byproduct, a new method is presented for inference from  discretely observed diffusion processes. A novel measure of filter accuracy is proposed, and used  to highlight strengths and weaknesses of the methods.
Several unsupervised learning algorithms based on an eigendecomposition provide  either an embedding or a clustering only for given training points, with no  straightforward extension for out-of-sample examples short of recomputing eigenvectors.
Object-oriented coding in the MPEG-4 standard enables the separate processing of foreground objects and the scene background (sprite). Since the background sprite only has to be sent once, transmission bandwidth can be saved. This paper shows that the concept of merging several views of a non-changing scene background into a single background sprite is usually not the most efficient way to transmit the background image. We have found that the counter-intuitive approach of splitting the background into several independent parts can reduce the overall amount of data. For this reason, we propose an algorithm that provides an optimal partitioning of a video sequence into independent background sprites (a multi-sprite), resulting in a significant reduction of the involved coding cost. Additionally, our algorithm results in background sprites with better quality by ensuring that the sprite resolution has at least the final display resolution throughout the sequence. Even though our sprite generation algorithm creates multiple sprites instead of a single background sprite, it is fully compatible with the existing MPEG-4 standard. The algorithm has been evaluated with several test-sequences, including the well-known Table-tennis and Stefan sequences. The total coding cost could be reduced by factors of about 2.7 or even higher.
FUSE is a lightweight failure notification service for building distributed systems. Distributed systems built with FUSE are guaranteed that failure notifications never fail. Whenever a failure notification is triggered, all live members of the FUSE group will hear a notification within a bounded period of time, irrespective of node or communication failures. In contrast to previous work on failure detection, the responsibility for deciding that a failure has occurred is shared between the FUSE service and the distributed application. This allows applications to implement their own definitions of failure. Our experience building a scalable distributed event delivery system on an overlay network has convinced us of the usefulness of this service. Our results demonstrate that the network costs of each FUSE group can be small; in particular, our overlay network implementation requires no additional liveness verifying ping traffic beyond that already needed to maintain the overlay, making the steady state network load independent of the number of active FUSE groups.
In a previous paper we have investigated subsumption in the presence of terminological  cycles for the description logic    which allows conjunctions, existential  restrictions, and the top concept, and have shown that the subsumption problem  remains polynomial for all three types of semantics usually considered for cyclic  definitions in description logics.
A study of electroencephalographic brain activity in behaving animals has guided the development of a model for the self-organization of goal-directed behavior. Synthesis of a dynamical representation of brain function is based in the concept of intentionality as the organizing principle of animal and human behavior. The constructions of patterns of brain activity constitute meaning and not information or representations. The three accepted meanings of intention: &apos;aboutness&apos;, goal-seeking, and wound healing, can be incorporated into the dynamics of meaningful behavior, centered in the limbic system interacting with the sensory and motor systems. Evidence is noted for the maintenance in cortical neuropil of a felt work of synaptic connections, that have incorporated past experience by changes with learning, and that act as a unified whole in shaping each intentional action at each moment. This constitutes the intentional structure of a brain. Meaning is a focus having a place without edges in this structure, which continually moves through it along a chaotic trajectory. In this view, consciousness is the active state of an intentional structure, and awareness is the subjective aspect of the shifting focus.
As part of the Synthetic Actors project, a vision system has been developed to provide synthetic actors with the information necessary to interact with their human counterparts. Rather than providing complete information about the environment that the actor is in, the purpose of the vision system is to clarify existing knowledge. As a result, the system does not need to be feature-rich, extremely fast, or perfectly accurate, but rather affordable and easily modified. The paper
The design of artificial agents that are meant to model behavioral,  cognitive, economic or social structures asks for tools that aid in layout and  implementation of agent architectures. To implement agents based on Drner&apos;s  Psi theory of emotion and cognition, our group has introduced a toolkit that  assists in designing modular architectures, as well as representational structures,  such as semantic networks, control scripts and connectionist structures by  means of a graphical editor. At the same time, the framework supports the  inclusion of functionality written in a native programming language. This paper  gives an overview over the implementation of agents according to Drner&apos;s  theory, and while it also aims at giving an insight into the functioning of these  agents (which we call &quot;MicroPsi&quot; agents), its main purpose is the explanation of  the use of the toolkit.
We develop a new low-dimensional video frame feature that is more insensitive to lighting change, motivated by color constancy work in physics-based vision, and apply the feature to keyframe production using hierarchical clustering. The new feature has the further advantage of more expressively capturing image information and as a result produces a very succinct set of keyframes for any video. Because we effectively reduce any video to the same lighting conditions, we can produce a universal basis on which to project video frame features. We carry out clustering efficiently by adapting the hierarchical clustering data structure to temporallyordered clusters. Using a new multi-stage hierarchical clustering method, we merge clusters based on the ratio of cluster variance to variance of the parent node, merging only adjacent clusters, and then follow with a second round of clustering. The second stage merges clusters incorrectly split in the first round by the greedy hierarchical algorithm, and as well merges non-adjacent clusters to fuse near-repeat shots. The new summarization method produces a very succinct set of keyframes for videos, and results are excellent. 1. 
Knowledge Management (KM) is an important issue in organizations. However there are several barriers to successful KM. In particular, knowledge hoarding, difficulties in identifying organizational knowledge, not understanding KM requirements, and technical difficulties of knowledge representation. In this work we focus on a connection between the managerial and technical aspects of knowledge management. We study the nature of organizational knowledge in order to derive knowledge management requirements to support the design of computerized Knowledge Management Systems.
This paper begins with consideration on push technology in Java ORB environment, and investigates interoperability of event channels of the event service of CORBA, which should be technically addressed to build a true push service in a Java ORB environment. We present three different methods that enable one event channel to be connected with other event channels. Pros and cons of each method are described. All mentioned approaches have been implemented in Java. An experimental performance evaluation has been carried out and evaluation results are also given.
Howard Karloff    Uri Zwick  April 28, 1997  Modified: August 17, 1997  Abstract  We describe a randomized approximation algorithm which takes an instance of MAX 3SAT as input.
The convolved Fibonacci numbers F    j are defined by (1 x x        j0      x    .
The use of flight simulation tools to reduce the schedule, risk, and required amount of flight-testing for complex aerospace systems is a well-recognized benefit of these approaches. However, some special challenges arise when one attempts to obtain these benefits for the development and operation of a research Uninhabited Aerial Vehicle (UAV) system. Research UAV systems are characterized by the need for continual checkout of experimental software and hardware. Also, flight-testing can be  further leveraged by complementing experimental results with flight-test validated simulation results for the same vehicle system. In this paper, flight simulation architectures for system design, integration, and operation of an experimental helicopterbased UAV, are described. The chosen helicopter-based UAV platform (a Yamaha RMax)  is well instrumented: differential GPS, an inertial measurement unit, sonar  altimetry, and a 3-axis magnetometer. One or two general-purpose flight processors can be utilized. Research flight test results obtained to date, including those completed in conjunction with the DARPA Software Enabled Control program, are summarized.
Approach: Technologies like micro- or macroarrays are the source of various problems concerning analysis and evaluation of the datasets obtained. In the present thesis a functional classification provided by the Gene Ontology is used to group large-scale gene-expression. The problem was: How can the expression levels of all genes in a functional group be combined to a single number (score), and how can this score be tested for differential gene expression of the GO-node? Two different statistical tests are discussed to find an accumulation of induced genes in a GO-node on the one hand. On the other hand, we test for a contamination of GO-node with genes that display moderately increased or decreased expression levels. The tests are applied on GOscores measuring the level of differential gene-expression in GO-nodes. Two different methods are proposed to calculate a GO-score.
The nature of an open distributed environment provides a resoundingly diverse yet potentially chaotic environment for users. A great deal of research has focused on the management of resources in such an environment and policy-based management has emerged as one such promising solution.
this document, we present the progress made in workpackage 2 (WP 2), devoted to distributed algorithms and services in the project PEPITO. The period covered by the report is from 2003.01.01 to 2003.12.31
We apply machine learning techniques  to classify automatically a set of verbs  into lexical semantic classes, based on  distributional approximations of diatheses,  extracted from a very large annotated  corpus. Distributions of four grammatical  features are sufficient to reduce  error rate by 50% over chance. We conclude  that corpus data is a usable repository  of verb class information, and that  corpus-driven extraction of grammatical  features is a promising methodology for  automatic lexical acquisition.
As part of our charge from the Virtual Sockets Interface Alliance we search for a notation in which standards documents can be precisely specified. We approach the specification for standard problem in the context of the Virtual Component Interface Standard. We propose six orthogonal axes of specification as guides to creating a cohesive, well-rounded requirements specification. We then specify the Virtual Component Interface Standard in the Unified Modeling Language and evaluate that specification based on our six axes.
This paper introduces a two view tracking method which uses the homography relation between the two views to handle occlusions. An adaptive appearance-based model is incorporated in a particle filter to realize robust visual tracking. Occlusion is detected using robust statistics. When there is occlusion in one view, the homography from this view to other views is estimated from previous tracking results and used to infer the correct transformation for the occluded view. Experimental results show the robustness of the two view tracker.
FSP (Finite State Processes) is an established process algebra with an accompanying Labelled Transition System Analyser (LTSA) tool for analysing behavioural properties of concurrent systems. Here we introduce a stochastic extension to FSP (SFSP) that augments the language with probabilistic choice and arbitrary time delays modelled by clocks. The system supports compositional performance measures. SFSP models are analysed by discrete-event simulation.
Design techniques for sigma-delta modulators from communications are applied and adapted to improve the spectral characteristics of high frequency power electronic applications. A high frequency power electronic circuit can be regarded as a quantizer in an interpolative &amp;Sigma;&amp;Delta; modulator. We review one dimensional sigma-delta modulators and then generalize to the hexagonal sigma-delta modulators that are appropriate to three-phase converters. A range of interpolative modulator designs from communications can then be generalized and applied to power electronic circuits. White noise spectral analysis of sigma-delta modulators is generalized and applied to analyze the designs so that the noise can be shaped to design requirements. Simulation results for an inverter show significant improvements in spectral performance.
Software architecture descriptions can play a wide variety of roles in the software lifecycle, from requirements specification, to logical design, to implementation architectures. In addition, execution architectures can be used both to constrain and enhance the functionality of running systems, e.g. security architectures and debugging architectures. Along with others from DARPA&apos;s DASADA program we proposed an execution infrastructure for so-called self-healing, self-adaptive systems -- systems that maintain a particular level of healthiness or quality of service (QoS). This externalized infrastructure does not entail any modification of the target system -- whose health is to be maintained. It is driven by a reflective model of the target system&apos;s operation to determine what aspects can be changed to effect repair. Herein we present that infrastructure along with an example implemented in accord with it.
The Embedded Machine is a virtual machine that mediates in real time the interaction between software processes and physical processes. It separates the compilation of embedded programs into two phases. The first, platform-independent compiler phase generates E code (code executed by the Embedded Machine), which supervises the timing --- not the scheduling--- of application tasks relative to external events, such as clock ticks and sensor interrupts. E code is portable and exhibits, given an input behavior, predictable (i.e., deterministic) timing and output behavior. The second, platform-dependent compiler phase checks the time safety of the E code, that is, whether platform performance (determined by the hardware) and platform utilization (determined by the scheduler of the operating system) enable its timely execution. We have used the Embedded Machine to compile and execute high-performance control applications written in Giotto, such as the flight control system of an autonomous model helicopter.
The quantum periodic XXZ chain with alternating spins is studied. The properties of the  related R-matrix and Hamiltonians are discussed. A compact expression for the ground state  energy is obtained. The corresponding conformal anomaly is found via the finite-size computations  and also by means of the Bethe ansatz method. In the presence of an external magnetic field, the  magnetic susceptibility is derived. The results are also generalized to the case of a chain containing  l different spins.
Though acknowledged as being very closely related, requirements engineering and architecture modeling have been pursued largely independently of one another in the past years. The inter-dependencies and constraints between architectural elements and requirements elements are thus not well-understood and subsequently only little guidance is available in bridging requirements and architectures. This paper identifies a number of relevant relationships we have identified in the process of trying to relate a requirements engineering approach with an architecture-centered approach. Our approach, called CBSP (Component-Bus-System, and Properties) provides an intermediate language for representing requirements in an architectural fashion. In this paper, we will present the basics of our CBSP approach but also emphasize the challenges that still need to be resolved.
In practical applications, the scheduling problem should enable taking into account dynamic changes of tasks and resources. Very often, the task must be considered as a complex object possesing its own resources. Thus, we obtain a hierarchy of cooperating units, e.g., computer network, multiprocesor computer unit, matrix/vector processor, and scalar processor, all together making possible to distribute scheduling procedure into the hierarchy of levels and into several parallel processes at each level. This way we have an opportunity to increase the scheduling system performance. But from the point of view of a single unit, the higher its position in the hierarchy, the larger search space for its optimal schedule and, what it follows, more difficoult or practically impossible exact (analytic) solution of the problem.
Laboratory tests were conducted to investigate the effect of latent heat on frost penetration in backfill specimens. Each specimen was prepared and placed in PVC moulds, which represent a rock trench. Rigid polystyrene insulation boards and expanded polystyrene beads were used in some specimens. The specimens were then subjected to a sudden temperature drop from 22 C to --15 C. The preliminary study showed that high moisture content backfill materials had a lower cooling rate and a longer time plateau around the freezing point than low moisture content ones because of the low thermal diffusivity and the release of latent heat. The use of polystyrene insulation actually promoted a faster frost penetration than without the insulation. The study shows that the rigid insulation was not effective in delaying frost advance to the centre of the specimens.
Computer networks are crucial for providing access to cadastral information. This paper focuses on two projects which concretize the possible role of internet technology for searching, delivery and visualization of products of the Dutch Cadastre in the future. The first project is the development of a National Clearinghouse Geo-information (NCGI), in which the Cadastre participates. It aims to provide a national entrance to geographically referenced datasets by means of their metadata. The user searches through the internet by specifying text-based or in a map the criteria of the needed geo-dataset. These criteria are matched with the descriptions of geo-datasets in a central metadatabase. Internet GIS-functionality is developed for manipulation and analysis of selected datasets, possibly from decentral servers of various data suppliers. Conditions are a secure communication, support of both raster and vector data and realization of a real open solution.
Trace analysis can be a useful way to discover problems in a program under test.
This paper overviews an ongoing project aimed at developing  an automatic generator of Java Card applets from higher-level  spec(i  cation)s written in a domain-speci  c language called \SmartSlang  &quot;. The generator is based on Specware, a system for the formal  speci  cation and re  nement of software. The applet generator translates  a SmartSlang spec into the logical language of Specware, re-expresses the  translated spec in terms of Java Card concepts via a series of re  nement  steps using Specware&apos;s machinery, and generates Java Card code from  the re  ned spec. The Java Card concepts used for re  nement and code  generation are captured as a shallow embedding of the Java Card language  and API in the logic of Specware. Since proofs are associated to  re  nement steps, the applet generator produces a machine-processable  proof tree along with the code, enabling the correctness of the generated  code (with respect to the spec) to be checked independently from the  applet generator, via a smaller and simpler applet checker to be also  developed in this project.
This paper proposes a new approach to providing fault tolerance in MPLS networks based on the concept of &quot;domain protection&quot; where protection paths for all working paths that terminate in an egress router are calculated simultaneously. The proposed scheme guarantees that every protected node is connected to two protection paths placed in a way that no single link failure would cause simultaneous loss of connectivity between a node and the egress router on both protection paths. The use of dual protection paths permits decoupling the protection path placement from the working path placement thus allowing much greater flexibility than other recently proposed schemes. Several heuristics to improve the quality and reduce the cost of the protection path placement are proposed and evaluated. The simulation results show that the algorithm together with the heuristic extensions achieves protection which is less costly or comparable to two recently proposed MPLS protection schemes -- RSVP Backup Tunnels and Fast Reroute -- while exhibiting comparatively lower algorithmic complexity.
perception has blossomed into an active interdisciplinary endeavor, including the fields of psychophysics, neurophysiology, sensory perception, psycholinguistics, linguistics, artificial intelligence, and sociolinguistics.  1. Psychophysics of Speech Perception  In any domain of perception, one goal is to determine the stimulus properties responsible for perception and recognition of the objects in that domain (see Psychophysics) .Thestudyofspeechperceptionpromisesto be even more challenging than other domains of perception because there appears to be a discrepancy between the stimulus and the perceiver&apos;s experience of it. For speech, we perceive mostly a discrete auditory message composed of words, phrases, and sentences. The stimulus input for this experience, however, is a continuous stream of sound (and facial and gestural movements in face-to-face communication) produced by the speech production process. Somehow, this continuous input is transformed into more or less a meaningful 
A Hierarchical Extended Kohonen Map (HEKM) learns to  associate actions to perceptions under the supervision of a planner: they  cooperate to solve path finding problems. We argue for the utility of  using the hierarchical version of the KM instead of the &quot;flat&quot; KM. We  measure the benefits of cooperative learning due to the interaction of  neighboring neurons in the HEKM. We highlight a beneficial side-effect  obtained by transferring motion skill from the planner to the HEKM,  namely, smoothness of motion.
Block-DCT based information embedding methods introduce distinctive non-stationarities into the stego-image. In particular, the difference between neighboring pixel intensities in one block and across two blocks have different distributions. A universal binary hypothesis test is then proposed to discriminate between stego-images and unmarked images.
This paper presents and evaluates a number of techniques to improve the execution time of interprocedural pointer analysis in the context of large C programs. The analysis is formulated as a graph of set constraints and solved using a worklist algorithm. Indirections lead to new constraints being added during this process. In this work
The development of efficient application software capable of exploiting available High Performance Computing (HPC) systems is non-trivial and is largely governed by the availability of sufficiently high-level languages, tools, and application development environments. In this paper we describe the design and operation of a toolkit for HPF/Fortran 90D application development. The toolkit incorporates the following systems: (1) ESP: An Interpretive Framework for HPF/Fortran 90D Performance Prediction; (2) ESP-i: A HPF/Fortran 90D Functional Interpreter; and (3) ESPial: An Integrated Environment for HPF/Fortran 90D Application Development &amp; Execution.  The toolkit has been implemented on the iPSC/860 hypercube system, and is supported by an interactive, graphical user interface (ESPView) which provides application developers with the following functionality: design evaluation capability, functional verification capability, performance visualization support, experimentation capability, com...
References  
Computer supported collaborative work (CSCW) is currently of  growing interest for application industrial context. Integrated workflow systems  are available to allow the integration of globally distributed teams. However, in  many areas (i.e. in electronic design automation), tool integration remains to  be a critical issue since existing legacy tools have to be integrated into  distributed workflows. By integrating tools based on the operational semantics,  dynamic assignment and replacement of tools in the workflows becomes  possible. We present a middleware for secure collaborative engineering that  enables flexible tool management and overcomes existing networking problems  through a peer-to-peer based network infrastructure.
Our personal conversation memory agent is a wearable  `experience collection&apos; system, which unobtrusively records  the wearer&apos;s conversation, recognizes the face of the dialog  partner and remembers his/her voice. When the system sees  the same person&apos;s face or hears the same voice it uses a  summary of the last conversation with this person to remind  the wearer. To correctly identify a person and help  remember the earlier conversation, the system must be  aware of the current situation, as analyzed from audio and  video streams, and classify the situation by combining  these modalities. Multimodal classifiers, however, are  relatively unstable in the uncontrolled real word  environments, and a simple linear interpolation of multiple  classification judgments cannot effectively combine  multimodal classifiers. We propose a meta-classification  strategy using a Support Vector Machine as a new  combination strategy. Experimental results show that  combining face recognition and speaker identification by  meta-classification is dramatically more effective than a  linear combination. This meta-classification approach is  general enough to be applied to any situation-aware  application that needs to combine multiple classifiers.
One main purpose for the use of formal description techniques (FDTs) is formal reasoning and verification. This requires a formal calculus and a suitable formal semantics of the FDT. In this paper, we discuss the basic verification requirements for Estelle, and how they can be supported by existing calculi. This leads us to the redefinition of the standard Estelle semantics using Lamport&apos;s temporal logic of actions and Dijkstra&apos;s predicate transformers. Keyword Codes: F.3.2; D.2.1; C.2.4 Keywords: Semantics of Programming Languages; Requirements/Specifications; Distributed Systems 1 Introduction  Formal description techniques (FDTs) serve two main purposes (see, e.g., [ISO88, ISO89]). Firstly, specifications written in an FDT shall be precise and unambiguous. This requires the semantics of the FDT to be defined in a mathematical way. Secondly, an FDT shall support formal reasoning and, in particular, the formal verification (i.e., exhaustive proof) that a specification meets its (more ...
We have developed SWAP, a system that automatically detects process dependencies and accounts for such dependencies in scheduling. SWAP uses system call history to determine possible resource dependencies among processes in an automatic and fully transparent fashion. Because some dependencies cannot be precisely determined, SWAP associates confidence levels with dependency information that are dynamically adjusted using feedback from process blocking behavior. SWAP can schedule processes using this imprecise dependency information in a manner that is compatible with existing scheduling mechanisms and ensures that actual scheduling behavior corresponds to the desired scheduling policy in the presence of process dependencies. We have implemented SWAP in Linux and measured its effectiveness on microbenchmarks and real applications. Our results show that SWAP has low overhead, effectively solves the priority inversion problem and can provide substantial improvements in system performance in scheduling processes with dependencies.
this memorandum it was noted that for  both the original pollock CDQ final rule in November 1992 and the halibut/sablefish CDQ final  rule in November 1993, commercial or subsistence harvests from Federal or State waters may  have been used to determine community eligibility. See discussion infra on pages 4-5. In order to determine whether all appropriate Federal and State waters commercial or subsistence harvests  were considered in a community&apos;s eligibility evaluation, NMFS should re-examine the information submitted for currently eligible communities for consistency with this MSA criterion
Recent studies have shown that the surface of the brain is deformed by up to 20mm after the skull is opened during neurosurgery, which could lead to substantial error in commercial image guided surgery systems. We quantitatively analyse the intraoperative brain deformation of 24 subjects to investigate whether simple rules can describe or predict the deformation. Interventional MR images acquired at the start and end of the procedure are registered non-rigidly to obtain deformation values throughout the brain. Deformation patterns are investigated quantitatively with respect to the location and magnitude of deformation, and to the distribution and principal direction of the displacements. We also measure the volume change of the lateral ventricles by manual segmentation. Our study indicates that brain...
We investigate the structure of the  I) of all quasi-orderings  on a set I. We describe a natural set of the so called fundamental inequalities  defining all minimal inequalities in \Omega\Gamma I) and develop an axiomatic characterization  of \Omega\Gamma I). We further describe the automorphism group of \Omega\Gamma I).
This article is a summary of the scienti  c contributions of the participants to this  colloquium, as well as a description of the general atmosphere of the colloquium
During the last decade, increased activity around the semantics-pragmatics boundary  has re-emphasised the need to address the role of context in the interpretation process of  human language. This trend has been driven mainly by attempts to develop formal  approaches to deal with typically pragmatic phenomena. In particular for presupposition,  the pragmatic phenomenon par excellence, several accounts (e.g., Stalnaker, 1974;  Karttunen , 1974; Gazdar, 1979; Heim, 1983; Van der Sandt, 1988, 1992; Beaver, 1993)  have given firm evidence of the impact of context.
In this article we address a number of issues related to motion planning and analysis of rectangular metamorphic robotic systems. We first present a distributed algorithm for reconfiguration that applies to a relatively large subclass of configurations, called horizontally convex configurations. We then discuss several fundamental questions in the analysis of metamorphic systems. In particular the following two questions are shown to be decidable: (i) whether a given set of motion rules maintains connectivity; (ii) whether a goal configuration is reachable from a given initial configuration (at specified locations). In the general case in which each module has an internal state, the following is shown to be undecidable: given a set of motion rules, whether there exists a certain type of configuration called a uniform straight-chain configuration that yields a disconnected configuration.
On the road for the future success of mobile agents, we believe that inter-agent  communication is an issue that has not been adequately addressed by the mobile  agents community. Supplementing mobile agents with the ability to interact with  other mobile or static agents, or agentified information sources is a necessity in  the vastly heterogeneous arena where mobile agents are called to compete. Thus,  an agent communication language should be interpreted as a tool with the capacity  to integrate disparate sources of information. In the first segment, we argue  that mobile agents can benefit from current standards efforts on agent communication  since the focus of such work is to address heterogeneity by defining a  &quot;common language&quot; for communicating agents. In the second part, we discuss  ongoing research on agent to agent communication and we present current standards  efforts relevant to agent communication.
this paper, we define an access control model with the following properties: (1) system administrators can define system access control requirements on applications and (2) application developers can use the same model to enforce application access control requirements without the need for ad hoc security mechanisms. This access control model uses features of role-based access control models to enable (1) specification of a single role that applies to multiple application instances; (2) selection of a content&apos;s access rights based on the content&apos;s application and role in the application; (3) consistency maintained between application state and content access rights; and (4) control of role administration. We detail a system architecture that uses this access control model to implement secure collaborative applications. Lastly, we describe an implementation of this architecture, called the Lava security architecture
Extremely important to the climate in any region are the radiation balance and the exchange processes of heat, water vapour and momentum. Most climatological parameters temperature, humidity, wind speed, cloudiness and precipitation) are the direct or indirect result of the radiation balance and these ex-  change processes. The weather of the West European coast from Tarifa (Spain) to Skagen (Denmark) is especially suitable for the formation of dunes. Often a wind is blowing, varying widely in force and direction. The conditions are optimal for the formation of high and wide dune complexes, given a large supply of sand by the sea. The annual precipitation surplus is considerable for most of this coast. This favours the establishment of vegeta-  tion, and thereby it enhances dune formation. The short distance to the land-sea border causes strong gra-  dients in severalclimatological parameters. These gradients lead to mesoscaleeffects, such as land-sea breezes and coastal fronts. The varying vegetation cover and the presence of slopes in all directions induce a strongly varying microclimate. However, this microclimate is not unique to the coastal dunes. Unique is the interac-  tion with the wide range of ambient weather, which is inherent to the coast. It is not possible to be conclusive about the effects of climatic change on coastal dunes because climate models are not yet able to predict the changes adequately and because these models supply information on  the expected mean climate, but not on the actual weather. 
In dense integrated circuit designs, management of routing congestion is essential; an over congested design may be unroutable. Many factors influence congestion: placement, routing, and routing architecture all contribute. Previous work has shown that different placement tools can have substantially different demands for each routing layer; our objective is to develop methods that allow &quot;tuning &quot; of interconnect topologies to match routing resources. We focus on
The so called Visual Query Systems try to support unskilled  users in formulating complex queries to highly structured databases,  by providing graphic paradigms for visualizing the database conceptual  models, and by allowing to build queries through visual interactions.
Over the past years, significant developments in mobile technologies and associated economies of scale via mature manufacturing processes have made the construction of ubiquitous computing applications possible in specific domains. This paper presents the business rationale and the architectural framework of an innovative ubiquitous computing application for the grocery sector (MyGROCER). MyGROCER exploits the opportunities provided by emerging wireless and mobile commerce technologies, coupled with automatic product identification technologies (RF-Id), in order to enable an efficient home replenishment schema, enhance the quality of service  provided by retailers, and ultimately add value to the end-consumer. We present the architectural elements of the  application by identifying the design challenges and the way they were dealt with. The paper concludes with a  critical appraisal of ubiquitous computing applications in supermarket environments and identifies future research  challenges.
Network dimensioning is an important issue to provide stable and QoS--rich communication services. A reliable estimation of bandwidths of links between the end--to--end path is a first step towards the network dimensioning. Pathchar is one of such tools for the bandwidth estimation for every link between two end hosts. However, pathchar still has several problems. If unexpectedly large errors are included or if route alternation is present during the measurement, the obtained estimation is much far from the correct one. We investigate the method to eliminate those errors in estimating the bandwidth. To increase the reliability on the estimation, the confidence interval for the estimated bandwidth is important. For this purpose, two approaches, parametric and nonparametric approaches, are investigated to add the confidence intervals. Another important issue is the method for controlling the measurement period. If the link is stable, small measurement data is sufficient. On the other hand, if the data is not sufficient, many measurements is necessary to obtain an accurate and reliable estimation. In this paper, we propose a measurement method to adaptively control the number of measurement data sets.
We define and evaluate methods to perform robust network monitoring using trajectory sampling in the presence of report loss. The first challenge is to reconstruct an unambiguous set of packet trajectories from the reports on sampled packets received at a collector. In this paper we extend the reporting paradigm of trajectory sampling to enable the elimination of ambiguous groups of reports, but without introducing bias into any characterization of traffic based on the surviving reports.
The intensive care unit is a challenging environment to both patient and caregiver. Continued shortages in staffing, principally in nursing, increase risk to patient and healthcare workers. To evaluate the use of intelligent systems in the improvement of patient care, an agent was developed to regulate ICU patient sedation. A temporal differencing form of reinforcement learning was used to train the agent in the administration of intravenous propofol in simulated ICU patients. The agent utilized the well-studied Marsh-Schnider pharmacokinetic model to estimate the distribution of drug within the patient. A pharmacodynamic model then estimated drug effect. A processed form of electroencephalogram, the bispectral index, served as the system control variable. The agent demonstrated satisfactory control of the simulated patients consciousness level in static and dynamic setpoint conditions. The agent demonstrated superior stability and responsiveness when compared to a well-tuned PID controller, the control method of choice in closed-loop sedation control literature.
INTRODUCTION  Optical fibers are used in a wide field of applications, for instance in communication networks. Their degradation by ionizing radiation is often an unwanted effect. The main radiation effect is a wavelength-dependent increase of attenuation by absorption and scattering processes. Despite the fact that the different types of modern telecommunication fibers show strongly increased radiation hardness (1), compared with the early ones, there still exist some single-mode (SM) and multi-mode graded index (MM GI) fibers with high and only slowly annealing loss increase. Such fibers can be used for in situ local radiation dosimetry also at particle accelerators, to observe the emission of radiation along the beam line where radiation-sensitive equipment might have to be installed (2, 3). The attenuation of the optical fiber can be measured with an Optical Time Domain Reflectometer OTDR (figure1). A fraction of the incoming laser pulse will be reflected along the fiber. The scatt
Systems that extract structured information from natural language passages have been highly successful in specialized domains. The time is opportune for developing analogous applications for molecular biology and genomics. We present a system, GENIES, that extracts and structures information about cellular pathways from the biological literature in accordance with a knowledge model that we developed earlier.
elative   ff. vs Relative Bounds Absolute Bounds Best guarantee about &quot;worst case&quot; distortion. Guarantee on distortion is independent of input metric. Relative Bound Given, as input, a finite metric, embed it into the host metric to (approximately) minimize distortion. [cf. Ravi&apos;s Talk] Comparing against the best possible distortion for the given input metric. Note: Absolute  ffffRelative   ff. Bounds: ExistingWork  [LLR95] minimizing maximum distortion of embedding arbitrary finite metrics    via Semi-Definite Programming.  ff1-approximation  maximum distortion problem. [WLB   98] PTAS for minimum routing cost spanning tree.  ff(1+ff)-approximation  average distortion of embedding arbitrary (graph) metrics into spanning tree metrics. Open: Can one give an algorithm  o(log  relative (average) distortion for embeddings results Given a finite metric, embed it into a line in non-contracting fashion.   average distortion of embedding a general metric into line. Better bounds for when the input is
The Secure Remote Password (SRP) protocol is an authentication and key-exchange protocol suitable for secure password verification and session key generation over insecure communication channels. The modular exponentiations involved, however, are very time-consuming, causing slow logon procedures. This work presents the design of a hardware accelerator that performs modular exponentiation of very wide integers. The experimental platform is tutwlan, a Wireless Local Area Network (wlan) being developed at Tampere University of Technology. It runs on the Altera Excalibur development board that contains a microprocessor and a chip with programmable hardware. The results show that a full modular exponentiation with 1023-bit inputs can be performed in less than 40 ms using less than 10,000 logic elements, each consisting of a 4-input lookup table and a register. By using the implemented hardware accelerator in the authentication protocol, the execution time is reduced by a factor of 4. In addition, proposals to improve the implemented modular exponentiation architecture are presented. An additional factor of 5 improvement (totaling a factor of 20) can be achieved by implementing the fastest design.
An algorithm for Josephus Problem has been proposed only the time complexity of O(m+  log m=(m 1) (n=m)). A pre-algorithm was used to obtain the best algorithm. By increasing m  the algorithm becomes more ecient than the algorithms suggested before.
In the first part, an iterative receiver for joint channel estimation and cochannel interference cancellation suitable for time-division multiple-access (TDMA) cellular radio systems is proposed. The receiver is based on joint maximum-likelihood sequence estimation (JMLSE). It is blind with respect to the data of the interfering users but training-based with respect to the data of the desired user and is therefore referred to as semi-blind. As opposed to concurrent receiver structures where the number of receiving antennas must exceed the number of cochannel interferers, the proposed receiver is suitable for just one receive antenna. No knowledge on the burst structure of the interferer is used. Hence, the proposed receiver is suitable for (but not restricted to) the downlink in asynchronous networks. The proposed receiver is able to operate at a signal/interference ratio of 0 dB in asynchronous GSM/GPRS networks given a single dominating interferer. Furthermore, the results indicate that with proper interference cancellation it is not necessary to synchronize GSM/GPRS networks.
this report in order to adequately specify the experiment procedure. In no case does such identification imply recommendations or endorsement by the National Research Council, nor does it imply that the product or material identified is the best available for the purpose. 13 Figure 4: Data Acquisition in Stairwells  The picture on the left shows the configuration used in the stairwells (the stairwell depicted was not one measured in this study). The illuminance meters are mounted on a pillar such that the plane of the detectors is 1 m above the stair tread. The operator manually directs the software and data acquisition hardware mounted on the laptop computer to poll the illuminance meter, and store the measured values on the computer hard disk. Once the polling operation has been completed, the operator moves the apparatus to the next measurement location, and repeats the operation. The picture on the right shows the configuration used to collect measurements on the floors. The operator pushes the device along the centreline of the measurement path. As the calibrated wheel (circumference 1 m) turns, the data acquisition system is directed to poll the illuminance meters and store the resulting value in the computer once every 25 cm of travel. For the stairwells, a total of 12 readings were collected between each floor as follows: three readings were collected at the door landing and the mid-landing, and one each at the top, middle and last stair of each staircase. The data acquisition system software was configured so each reading was polled by the operator manually once the apparatus was correctly placed. All stairwells were windowless, so no daylight supplemented the electric lighting. On Floors 9, 10 and 11, the operator pushed the measuring system along the centreli...
y claim is a viable alternative with &quot;three functional components.&quot; Numbers being the issue, it is interesting to observe that their Figure 1 actually reveals a total of 6 main boxes, some comprising up to 6 sub-components, linked by no less than 13 arrows.  To keep this reply short, we shall not dwell here on the avowed lack of &quot;fine specification&quot; of P&amp;P&apos;s model. Rather, we briefly show why the explanations that they sketch for the dissociations observed in patient VOL do not work.  READING DEFICIT AND  PRESERVATION OF OTHER  NUMERICAL TASKS  1. P&amp;P claim that the reading deficit resulted from an impaired access to Core Numeral Meaning, all subsequent processing being spared. However, their model includes an alternative route for semantic access, from the Arabic Numeral Recognition System  to the Stored Arithmetic Knowledge, and thence to the Core Numeral Meaning. This intact pathway should allow for a successful reading, while the patient actually showed a severe reading deficit.  I
Traditional database systems have been successful in handling large amounts of data but lack mechanisms for schema derivations and schema verification. In this paper, we propose (i) structures that carry expressive and useful information on the database schema, (ii) a set of inference rules for schema derivations, and (iii) a mechanism for discovering contradictory schema declarations. To this end, in addition to the usual Isa relation, we define a stronger form of specialization for properties, that we call restriction isa, or Risa for short. The Risa relation expresses property value refinement. A distinctive feature of our model is that it supports the interaction between (explicit or derived) Isa and Risa relations, in different contexts. We demonstrate that the combination of  Isa and Risa provides a powerful conceptual modeling mechanism. The Risa relation allows to express participation constraints on properties. Specifically, properties of a class are characterized as necessary...
this paper shows that the combination  Eclipse/Asf+Sdf Meta-Environment creates a versatile experimentation platform for programming language research
Despite large caches, main-memory access latencies still cause significant performance losses in many applications. Numerous hardware and software prefetching schemes have been proposed to tolerate these latencies. Software prefetching typically provides better prefetch accuracy than hardware, but is limited by prefetch instruction overheads and the compiler&apos;s limited ability to schedule prefetches sufficiently far in advance to cover level-two cache miss latencies. Hardware prefetching can be effective at hiding these large latencies, but generates many useless prefetches and consumes considerable memory bandwidth. In this paper, we propose a cooperative hardware-software prefetching scheme called Guided Region Prefetching (GRP), which uses compiler-generated hints encoded in load instructions to regulate an aggressive hardware prefetching engine. We compare GRP against a sophisticated pure hardware stride prefetcher and a scheduled region prefetching (SRP) engine. SRP and GRP show the best performance, with respective 22% and 21% gains over no prefetching, but SRP incurs 180% extra memory traffic---nearly tripling bandwidth requirements. GRP achieves performance close to SRP, but with a mere eighth of the extra prefetching traffic, a 23% increase over no prefetching. The GRP hardware-software collaboration thus combines the accuracy of compilerbased program analysis with the performance potential of aggressive hardware prefetching, bringing the performance gap versus a perfect L2 cache under 20%.
this document we would like to propose some relatively minor additions to Java&apos;s syntax and semantics [GJS96] in order to increase the expressiveness of the language with little cost in semantic complexity. We avoided making suggestions that would invalidate or change the semantics of programs written in the current version of Java. While the main point of the language extensions are to support parametric polymorphism, we also added features which provide better support for binary methods, a kind of method that is often difficult to support in a statically typed language. (See [BCC
We present a large and homogeneous sample of optical and UV observations of PNe and derive their abundances and the implications for the Intermediate Mass Stars (IMS) and Galactic chemical evolution. We observe the \hot bottom burning&quot; (HBB) reaction, oxygen depletion (ON cycle) and/or production (likewise carbon) during the third dredge-up. All these eects are more ecient at lower metallicity. A strong warning is that oxygen can not be used to derive the initial composition of the progenitor star, but we have to use other elements like sulfur or argon.
Emerging data stream processing systems rely on windowing to enable on-the-fly processing of continuous queries over unbounded streams. As a result, several recent efforts have developed window-aware implementations of query operators such as joins and aggregates. This focus on individual operators, however, ignores the larger issue of how to coordinate the pipelined execution of such operators when combined into a full windowed query plan. In this paper, we first show how the straightforward application of traditional pipelined query processing techniques to sliding window queries can result in ineffcient and incorrect behavior. We then present three alternative execution techniques that guarantee correct behavior for pipelined sliding window queries and develop new algorithms for correctly evaluating window-based duplicateelimination, Group-By and Set operators in this context. We implemented all of these techniques in a prototype data stream system and report the results of a detailed performance study of the system.
XPath is the W3C--standard node addressing language for XML documents. XPath is still under development and its technical aspects are intensively studied. What is missing at present is a clear characterization of the expressive power of XPath, be it either semantical or with reference to some well established existing (logical) formalism. Core XPath (the logical core of XPath 1.0 defined by Gottlob et al.) cannot express queries with conditional paths as exemplified by &quot;do a child step, while test is true at the resulting node.&quot; In a first-order complete extension of Core XPath, such queries are expressible. We add conditional axis relations to Core XPath and show that the resulting language, called conditional XPath, is equally expressive as first-order logic when interpreted on ordered trees. Both the result, the extended XPath language, and the proof are closely related to temporal logic. Specifically, while Core XPath may be viewed as a simple temporal logic, conditional XPath extends this with (counterparts of) the since and until operators.
This paper investigates the education of undergraduate chemical engineers with respect to their development of interpersonal skills, such as teamworking and leadership for which there is an increased employer demand. The main objective of this work is to compare the impact of different learning frameworks, which are in place at four academic institutions actively involved with teaching skills, and relate the results to a constructivist theory. Constructivism relates to a learning theory whereby the teacher or educator starts from the students&apos; initial conceptions and understanding of a particular subject area and then `builds&apos; or constructs further knowledge on that initial foundation. Data has been collated through the  observation of specific courses, obtaining feedback from  both students and course providers. The results are of  particular interest in establishing a link between how skills are taught and what impact that teaching has on student learning. Findings so far indicate that, for those institutions with established skills programmes, this learning process  has been reflected in their students. They become articulate about their learning, and can identify the transition from being taught to actually learning, understanding and subsequently applying their transferable skills to their academic and work-related activities.
Introduction  The neighbour joining method is a distance based method for constructing evolutionary trees [2]. Conceptually, it starts out with a star-formed graph where each leaf corresponds to a species, and iteratively picks two nodes adjacent to the root and joins them, by inserting a new node between the root and the two selected nodes. When joining nodes, the method selects the node pair i; j that minimises  Q ij = (r - 2)d ij - (R i + R j ) (1) where d ij is the distance between node i and j (assumed symmetric, i.e., d ij = d ji ), R k is the row sum over row k: R k =  P    d ik (where i ranges over all nodes adjacent to the root node), and r is the remaining number of nodes adjacent to the root. When nodes i and j are joined, they are replaced with a new node A with distance to the remaining nodes given by  d Ak = (d ik + d jk - d ij )=2: (2) In the simple implementation of neighbour joining, the method performs a search for argmin i;j Q ij , taking time O(r    ), and joins i a
Resonant tunneling diodes (RTDs) have intriguing properties which make them a primary nanoelectronic device for both analog and digital applications. We propose a bistable RTD-based cell for the cellular neural network (CNN) which exhibits superior performance in terms of circuit complexity, and processing speed compared to standard cells.
... assess the energy performance of new and innovative energy efficient materials and components for houses. The two research houses are identical energy efficient houses typical of tract-built models available on the local housing market. They also feature identical simulated occupancies based on home-automation technologies and are monitored for energy performance and thermal comfort. The simulated occupancy controls turn on and off major appliances, lighting and equipment. The houses were commissioned in the winter and spring of 1999, and benchmarked in the next heating season. This paper records the energy features of the houses and commissioning results. With the benefit of detailed monitoring of energy systems in both houses, many of the anomalies in component operation and controls were found and fixed. These anomalies could easily go undetected in regular houses
We describe a detailed computational model of skilled behavior and  learning in a reactive task domain. We model not only the problem solving behavior,  but also the skill acquisition process: our model can improve its performance over  time, acquire new knowledge, and recover from incorrect knowledge. This model  allowed us to predict the effectiveness of various tutoring system and curriculum  design choices, which has provided guidance for the development of an intelligent  tutoring system.
In our past work, we have attempted to use a mid-level feature namely the state population  histogram obtained from the Hidden Markov Model (HMM) of a general sound class, for speaker  change detection so as to extract semantic boundaries in broadcast news. In this paper, we  compare the performance of our previous approach with another approach based on video shot  detection and speaker change detection using the Bayesian Information Criterion (BIC). Our  experiments show that the latter approach performs significantly better than the former. This  motivated us to examine the mid-level feature closely. We found that the component population  histogram enabled discovery of broad phonetic categories such as vowels, nasals, fricatives etc,  regardless of the number of distinct speakers in the test utterance. In order for it to be useful  for speaker change detection, the individual components should model the phonetic sounds of  each speaker separately. From our experiments, we conclude that state/component population  histograms can only be useful for further clustering or semantic class discovery if the features  are chosen carefully so that the individual states represent the semantic categories of interest.
In this paper, we present a new deconvolution method, able to deal with noninvertible blurring functions. To avoid noise amplification, a prior model of the image to be reconstructed is used within a Bayesian framework. We use a spatially adaptive prior, defined with a complex wavelet transform in order to preserve shift invariance and to better restore variously oriented features. The unknown image is estimated by an EM technique, whose E step is a Landweber update iteration, and the M step consists of denoising the image, which is achieved by wavelet coefficient thresholding. The new algorithm has been applied to high resolution satellite and aerial data, showing better performance than existing techniques when the blurring process is not invertible, like motion blur for instance.
One of the biggest challenges of the Semantic Web is to make its tools usable by ordinary users for grass-roots production and integration of semantic information. This paper introduces the ongoing research on this issue in our research group at the Information Sciences Institute.
This paper describes symbolic techniques for the construction,  representation and analysis of large, probabilistic systems. Symbolic  approaches derive their eciency by exploiting high-level structure and  regularity in the models to which they are applied, increasing the size of  the state spaces which can be tackled. In general, this is done by using  data structures which provide compact storage but which are still e-  cient to manipulate, usually based on binary decision diagrams (BDDs)  or their extensions. In this paper we focus on BDDs, multi-valued decision  diagrams (MDDs), multi-terminal binary decision diagrams (MTBDDs)  and matrix diagrams.
In this article we provide a review of the literature with respect to  the efficient markets hypothesis and chaos. In doing so, we contrast  the martingale behavior of asset prices to nonlinear chaotic dynamics,  discuss some recent techniques used in distinguishing between probabilistic  and deterministic behavior in asset prices, and report some  evidence. Moreover, we look at the controversies that have arisen  about the available tests and results, and raise the issue of whether  dynamical systems theory is practical in finance.
Real-time applications are becoming more and more popular and important. To address the specific needs, such as data loss and transmission delay, of these applications, FEC (Forward Error Correction) techniques based on error correcting codes are widely used. The choice of codes and their parameters are crucial in determining the efficiency of FEC, in terms of loss recovery capability, bandwidth use and computation complexity. The parity
of the thorniest problems faced by cognitive scientists (Allport, 1989; Kahneman, 1973; Posner and Boies, 1971; Schneider and Shiffrin, 1977; Wickens, 1980). When considering attention at the cognitive level, we must consider the following functional issues. First, an individual operating in an environment is bombarded by a vast array of perceptual inputs simultaneously and must, in order to function effectively, somehow select certain things for enhanced processing while ignoring others (Allport, 1989; Posner, 1991). (Selective attention may, of course, be further subdivided into operations such as disengagement from a current focus, engagement of a new focus, and sustained focal attention over time (Posner and Peterson, 1990; Posner, 1991).) Second, there appear to be limits on the number of things that can be processed simultaneously; that is, a bottleneck or capacity limitation exists on the ability to divide attention between multiple stimuli or mental events.  Much early laborato
EUNITE network recently organized a world-wide competition  on electricity load forecasting. This paper details our approaches  and results where the main machine learning technique used is support  vector machine.
It seems to be a generally acknowledged fact that you should never trust a computer and that you should trust the person operating the computer even less. This in particular becomes a problem when the party that you do not trust is one which is separated from you and is one on which you depend, e.g. because that party is the holder of some data or some capability that you need in order to operate correctly. How do you perform a given task involving interaction with other parties while allowing these parties a minimal influence on you and, if privacy is an issue, allowing them to learn as little about you as possible. This is the general problem of secure multiparty computation. The usual way of formalizing the problem is to say that a number of parties who do not trust each other wish to compute some function of their local inputs, while keeping their inputs as secret as possible and guaranteeing the correctness of the output. Both goals should be obtained even if some parties stop participating or some malevolent coalition of the parties start deviating arbitrarily from the agreed protocol. The task is further complicated by the fact that besides their mutual distrust, nor do the parties trust the channels by which they communicate. A general solution to the secure multiparty computation problem is a compiler which given any feasible function describes an effcient protocol which allows the parties to compute the function securely on their local inputs over an open network.
Many NP-complete problems can be encoded in the answer set semantics  of logic programs in a very concise way, where the encoding reflects  the typical &quot;guess and check&quot; nature of NP problems: The property is encoded  in a way such that polynomial size certificates for it correspond to stable models  of a program. However, the problem-solving capacity of full disjunctive logic  programs (DLPs) is beyond NP at the second level of the polynomial hierarchy.
Resilient optical networks are predominately designed to protect against single failures of fiber links.
Beitrag der Arbeit / Achievement  Universitt Stuttgart  INSTITUT FR NACHRICHTENVERMITTLUNG UND DATENVERARBEITUNG  Prof. Dr.-Ing. Dr. h. c. P. J. Khn N 37  QoS Metrics for Elastic IP Traffic Stefan Bodamer and Klaus Dolzer June 12, 2001 24 Seiten / Pages This work was done as part of the Internet traffic modelling project sponsored by Siemens AG, Munich. QoS, Internet, TCP/IP, performance measure, performance evaluation, traffic model, processor sharing Definition and comparison of QoS metrics for TCP-based IP traffic (elastic traffic) with short TCP connections In this report, we introduce a variety of possible performance measures for elastic traffic and discuss how these measures simplify when they are applied to a processor sharing model and a packet-level model using TCP, respectively. Furthermore, case studies are presented in which we compare the impact of various parameters on some of the introduced measures for the processor sharing model and the TCP model. -- 1 --  1 
The purpose of this study was to assess job satisfaction of middle school principals in Virginia as measured by the Minnesota Satisfaction Questionnaire (MSQ). The primary question addressed by the study was: What is the general satisfaction level of middle school principals in Virginia as measured by the Minnesota Satisfaction Questionnaire? In addition to the primary question, three sub-questions were addressed by the study. They were: a) What is the general satisfaction level according to the demographic variables gender, age, degree, experience, school location, and school size? b) what is the satisfaction level for each of the 20 dimensions of the job measured by the MSQ? and c) what is the satisfaction level for the 20 dimensions of the job according to the demographic variables gender, age, degree, experience, school location, and school size? One hundred eighty-eight middle school principals in Virginia selected from the 1997-98 Virginia Educational Directory were surveyed with the Individual Data Sheet and the Minnesota Satisfaction Questionnaire. Demographic data pertaining to gender, age, experience, degree, school location, and school size were collected through use of the Individual Data Sheet. The 1967 Long-Form Minnesota Satisfaction Questionnaire was used to measure job satisfaction. Using this instrument, the general satisfaction score for the respondents resulted in a mean of 3.65 (SD= .57) indicating that these principals are &quot;Satisfied&quot; (3.00-3.99) with their jobs. According to the demographic variables, all general satisfaction scores were within the &quot;Satisfied&quot; range. The mean scores for the 20 dimensions ranged from &quot;Slightly Satisfied&quot; (2.00-2.99) to &quot;Very Satisfied&quot; (4.00-4.99). Compensation ranked the lowest in the hierarchy ( M=2.83, SD=.94)...
Understanding the data gathered thus far in a criminal investigation is  of great importance, particularly in terms of guiding its future course. One established  method of comprehending such data is to incrementally visualize it as  a network to ascertain if, and how, objects of interest are connected. In this paper  we describe this form of visual data analysis, discuss why current database  support for the method is inadequate, present an experimental database query  system for exploring data in this manner, and outline areas of future research  and development.
As a contribution to the gait generation and motion control of quadrupeds a concept has been formulated that in its modular structure can be easily adapted and extended. It is generally suitable for quadrupedal walking machines (as shown for Alduro and Bisam) and only a few modules need to be adapted according to the machine specific topology.
In this paper we examine three interfaces for secure method invocation in single-address-space operating systems. We examine the advantages and drawbacks of each model, and how these models relate to linking and loading in the single address space. A model is chosen based on its ability to securely interface multiple languages with low overhead.
Although it seems obvious that decision making is a contextual task,  papers dealing with decision making tackle rarely the problem of contextual  information management. After a brief presentation of our view on context, we  examine the contextual dimension of decision making. Then we explain our  views about the acquisition of contextual data and the construction of a  reasoning framework appropriate for decision making. We call this process  proceduralization and we refer to a rational construction for action (rca).
Many visual matching algorithms can be described in terms of the features and the inter-feature distance or metric. The most commonly used metric is the sum of squared differences (SSD), which is valid from a maximum likelihood perspective when the real noise distribution is Gaussian. However, we have found experimentally that the Gaussian noise distribution assumption is often invalid. This implies that other metrics, which have distributions closer to the real noise distribution, should be used. In this paper we considered a shape matching application. We implemented two algorithms from the research literature and for each algorithm we compared the efficacy of the SSD metric, the SAD (sum of the absolute differences) metric, and the Cauchy metric. Furthermore, in the case where sufficient training data is available, we discussed and experimentally tested a new metric based directly on the real noise distribution, which we denoted the maximum likelihood metric.
The growing prevalence of electronic commerce and the widespread use of mobile devices has made mobile payments an interesting alternative method of payment for customers and merchants. One major issue to be resolved is the integration of a real-time wireless means of payment within their current payment system. In this paper, we propose a new architecture of mobile payment system to improve business processes and increase customer loyalty. An all-in-one device that enables mobile payments and also integrates a membership scheme would simplify and significantly accelerate the payment process at the point of sale (POS). Moreover, the deployment of an effective customer relationship management (CRM) system and an adapted data mining tool would allow retailers to propose a dynamic-generated website to their customers. This would follow a one-to-one e-marketing strategy and would improve companies&apos; ability to suggest customized offers and coupons.
this paper and results of these simulations can be provided on request. Comments and suggestions from several anonymous referees considerably improved earlier versions of this paper
A computer worm is a program that propagates itself over a  network, reproducing itself as it goes. Because of the recursive structure of  this propagation, the spread rate of worms is very fast and poses a big  threat on the Internet infrastructure as a whole. Modern worms are capable  of gaining control over a substantial portion of the Internet hosts within  several minutes. No human mediated response is possible to stop an attack  that is so rapid. Possible devastating effects on the Internet operation are  hard to underestimate given that even traffic generated for the victim  probes and worm propagation is so huge that it can be considered as a  DDoS attack on the whole Internet and already used to bring down the  Internet infrastructure of the whole countries. In this paper we present a  survey of the existing worms and proposed methods of their containment.
Contents  1 Finite Fields 5 2 Projective geometries 13 3 The link to codes 21 4 An application: resilient functions 29 5 Arcs in projective planes 33 6 Symmetric bilinear forms 37 7 Symplectic bilinear forms 51 8 Quadratic forms in characteristic 2 53 9 Unitary bilinear forms 61 10 Quadrics in PG(2, q) and in PG(3, q) 65 11 Designs, projective planes and GQ 69 12 The small Witt designs 79 13 Symmetry groups 85 14 Generators and Spreads 91 15 Reed-Muller codes and Kerdock codes 97 4 CONTENTS 16 Projective planes 107 17 Generalized polygons 113 18 Diagram geometries 121 19 The sporadic A 7 -geometry 125 Finite Fields  Finite fields and the principles of linear algebra will be fundamental for everything we do in this class. Recall the definition of a field:  1.1 Definition. A field F is a set with two binary operations + (addition)   (multiplication) such that the following hold:  .  (F, +) is an abelian group (the additive group of F ). Denote the neutral element by 0.  .  (F  \ {0}, )  
this paper, Lee found that important words in speech tended to be stressed. The stressed words tended to be verbs, adjectives, adverbs, and nouns while the more predictable words were reduced. These grammatically predictable words included articles and prepositions. The first category contains the &quot;content&quot; words while the second contain the &quot;function&quot; words
this article the author offers an analysis of the implications of the new agile methods in the field  of software development
This paper proposes example guided inverse kinematics (EGIK) which extends and enhances existing inverse kinematics technique. In conventional inverse kinematics, redundancy in the model produces an infinite number of solutions. The motion could be jerky depending on the choice of solutions at each frame. EGIK exploits the redundancy for imitating an example motion (a pre-measured motion data) so that a unique solution is chosen. To minimize the gap between the goal and current end-effector position and imitate the original motion at the same time, nonlinear optimization technique is employed. So, the resulting motion resembles the original one in an optimal sense. Experiments prove that the method is a robust and effective technique to animate high DOF articulated models from an example motion.
A method is presented for refining the Canny edge detection  algorithm, and subsequently ranking the resulting edges relative  to their distance (in a topological sense) from the perimeter.
This paper introduces a new area of advice that is specific to  advising a multiagent team: Coordination Advice. Coordination  Advice differs from traditional advice because it pertains  to coordinated tasks and interactions between agents.
A subset S =  {s  1 , . . . , s k    of an Abelian group G is called an S t -set of size k if all sums of t different elements in S are distinct. Let s(G) denote the cardinality of the largest S 2 -set in G. Let v(k) denote the order of the smallest Abelian group for which s(G)    k. We develop bounds for s(G), and we determine v(k) for k    groups of order up to 183 using exhaustive backtrack search with isomorph rejection.
Consider an investor in the Black-Scholes model during the period [0; T ],  where T ? 0 and where the mean rate of return on the stock is assumed to be strictly greater than the rate of return on the bond. The investor strives to maximize the expected utility from terminal wealth. Suppose the investor has the initial wealth x ? 0 and let  ff x be the amount invested in the stock at time zero. Set   x =  ff x =x: It will be proved that the function  x !   x decreases if the investor possesses an increasing relative risk aversion and that the function x !  ff x increases if the investor possesses a decreasing absolute risk aversion. Similar results are obtained by Arrow in a so called one period market model with time consisting of two points [1] : 1. 
IPv6 is used for a variety of tasks, such as autoconfiguration, neighbor detection, and router discovery.
The ability to control the transport of individual electrons in SET technology introduces a broad range of new possibilities and challenges for implementing computer arithmetic circuits. In this paper, we first briefly discuss the concept of electron counting based arithmetic. Second, we introduce the types of building blocks that are required in order to implement this concept in SET technology. These blocks can be divided in three function categories: encoding binary operands as quantities of charge, controlling charge transport, and re-converting quantities of charge to binary results. Finally, we propose possible SET based implementations of these building blocks, and demonstrate the designs by means of simulation.
We study the throughput and delay as a function of load, of data traffic in an  IEEE 802.11 Wireless LAN using the public domain network simulator ns-2. We  evaluate the Distributed Coordination Function (DCF) as well as the Point  Coordination Function (PCF) modes of operation of IEEE 802.11 in detail.
An optimal Y-U-V transformation based on Karhunen-Loeve transformation for image compression proposed in this paper is considered as a spectral redundancy reduction. The PSNR is gained for optimal Y-U-V in comparison with traditional fixed Y-U-V transformation, because the variances are most separately after K-L transformation and the down sampling is taken on the coordinates with smallest variances in optimal Y-U-V transformation. The K-L transformation in optimal Y-U-V is an image-dependent transform that is to de-correlate the data in color spectral domain by using eigenvector matrix of covariance of the colors of the image. A normalization matrix follows the optimal Y-U-V transformation is used for ranging the data of Y-U-V within 0-255. The procedure is used in encoding only and the Y-U-V transformation can be transmitted with compressed data and de-coding easily.
Evolutionary robotics is a biologically inspired approach to  robotics that is advantageous to studying the evolution of  language. A new model for the evolution of language is  presented. This model is used to investigate the  interrelationships between communication abilities, namely  linguistic production and comprehension, and other  behavioral skills. For example, the model supports the  hypothesis that the ability to form categories from direct  interaction with an environment constitutes the ground for  subsequent evolution of communication and language. A  variety of experiments, based on the role of social and  evolutionary variables in the emergence of communication,  are described.
Service consumers are increasingly becoming aware of QoS and  service subscriptions. Service providers (SPs) are also recognizing the  opportunity to generate revenue by bundling services in packages and providing  them to customers. Standardization bodies have begun to address requirements  of charging and usage accounting management as SPs are increasingly relying  on them. Federated accounting management concept stems from these  developments. It specifically supports the requirement of SPs to co-operate in a  federated manner and share the revenue. It also supports the requirements of  charging and billing being subcontracted to a third-party. This paper proposes  that standard-based interfaces and shared information model, and a wellaccepted  development methodology are keys to enabling co-operation between  SPs. It presents a requirement analysis and a federated accounting management  system architecture, which are based on the recommendations laid down by  TMForum, IPDR, OMG and IETF organisations. The architecture supports  mediation, rating, charges aggregation, settlement, and online billing.
We introduce regular graph constraints and explore their decidability properties. The motivation for regular graph constraints is 1) type checking of changing types of objects in the presence of linked data structures, 2) shape analysis techniques, and 3) generalization of similar constraints over trees and grids. Typestate checking
By further generalizing the concept of Hermitian (or normal) and skew-Hermitian splitting  for a non-Hermitian and positive-definite matrix, we introduce a new splitting, called  positive-definite and skew-Hermitian (PS) splitting, and then establish a class of positivedefinite  and skew-Hermitian splitting (PSS) methods similar to the Hermitian (or normal)  and skew-Hermitian splitting (HSS or NSS) method for iteratively solving the positive definite  systems of linear equations. Theoretical analysis shows that the PSS method converges  ff Subsidized by The Special Funds For Major State Basic Research Projects G1999032803.
In this paper we apply the theory of averaging and motion control on Lie groups [1] to the problem of controlling energy transfers between dynamic storage elements in switched electrical networks. The switched networks of interest have bilinear state-space models in which the control u, representing the position of the switch, takes value in the set f0; 1g. The corresponding state transition matrix can be described by a right-invariant system evolving on a matrix Lie group [2], and as such we can use our theory to derive high-order average approximations to the evolution of the state transition matrix. We show how to use these average solutions to control energy transfers for a simple network that models the conversion portion of a dc-dc converter. Our approach provides an alternative to the feedback approach of Sira-Ramirez [3] which is based on variable structure systems with sliding regimes. Our methodology is based on open-loop control (combined with feedback if desired) and thus ensures that prescribed energy transfers can be accomplished with a finite number of switchings. This avoids chattering problems sometimes associated with sustaining sliding motions.
Early estimation of the size of a software product is extremely important. In this paper we analyze two software packages developed by a CMM level 3 software firm. We study if any property of analysis objects can be used to infer the size of the final code in an object-oriented environment. In both cases we find the number of methods well correlated with software size, in the sense that the correlation with the final size is high (r &gt; 0.77) and significant at the level .05. Inferential statistics guarantee that the results of this study are also applicable outside the scope of the two projects. 1 
checking are limited when it comes to the verification of complex distributed embedded real-time systems. Our approach addresses this problem and in particular the state explosion problem for the software controlling mechatronic systems, as we provide a domain specific formal semantic definition for a subset of the UML 2.0 component model and an integrated sequence of design steps. These steps prescribe how to compose complex software systems from domain-specific patterns which model a particular part of the system behavior in a well-defined context. The correctness of these patterns can be verified individually because they have only simple communication behavior and have only a fixed number of participating roles. The composition of these patterns to describe the complete component behavior and the overall system behavior is prescribed by a rigorous syntactic definition which guarantees that the verification of component and system behavior can exploit the results of the verification of individual patterns.
New methods were devised to improve the discrimination of EEG spatial amplitude patterns recorded from arrays of 64 electrodes placed on visual, auditory or somatic cortex. The 64 traces shared a spatially coherent, aperiodic carrier wave with a spatial pattern of amplitude modulation (AM). Previous observations on AM patterns from rabbits trained to discriminate conditioned stimuli with reinforcement (CS+ ) and without (CS- ) had revealed epochs between the CS and the CR in which AM patterns on CS+ trials could be distinguished from AM patterns on CS- trials. The AM patterns were expressed by points in 64-space that formed clusters. Levels of CS-/CS+ pattern separation were quantified by a pair-wise Euclidean distance method with cross-validation. The present study documents use of the technique for nonlinear mapping (NLM) to project the 64-dimensional structure onto a plane while preserving the relative distances between all points. The goodness of classification by the Euclidean distance measure was the same or improved after projection. Whereas the Euclidean distance measure only gave pair-wise classifications, the planar displays showed the patterns for multiple clusters simultaneously. These NLM-based methods revealed previously unrecognized structures within distributions of AM patterns in sensory cortices in the time period between the CS and CR. 1999 Elsevier Science B.V. All rights reserved.
Interactive fisheye views use distortion to show both local detail and global context in the same display space. Although fisheyes allow the presentation and inspection of large data sets, the distortion effects can cause problems for users. One such problem is memorability -- the ability to find and go back to objects and features in the data. In this paper we investigate the issue of how people remember object locations in distorted spaces, using a Sarkar-Brown fisheye lens that drastically affects the space. We carried out two studies. The first gathered information about what memory strategies people choose at increasing levels of distortion, without presupposing any particular strategy. The second looked more closely at how two particular strategies (maintaining a mental map, and using landmarks in the data) affected memory performance. We found that as distortion increases, people do use different memory strategies and that at higher levels of distortion, landmarks become increasingly important as memory aids.
this paper we  investigate the role of the periodic distribution of ryanodine-sensitive channels in  determining whether a spark can trigger a wave, using a modification of the kinetic  model proposed by Tang and Othmer, 1994b, Biophys. J. 67, 2223--2235, for  calcium-induced calcium release. We show that the spatial localization of these  channels near the T-tubules has a significant effect on both wave propagation and  the onset of oscillations in this system. Spatial localization provides a possible explanation  for the differing effects of various experimental protocols on the system&apos;s  ability to propagate a traveling wave
In this paper we present a method of data decomposition  to avoid the necessity of reasoning on data with missing attribute values.
In the future, webs of unmanned air and space vehicles  will act together to robustly perform elaborate  missions in uncertain environments. We coordinate  these systems by introducing a reactive  model-based programming language (RMPL) that  combines within a single unified representation the  flexibility of embedded programming and reactive  execution languages, and the deliberative reasoning  power of temporal planners. The KIRK planning  system takes as input a problem expressed as  a RMPL program, and compiles it into a temporal  plan network (TPN), similar to those used by  temporal planners, but extended for symbolic constraints  and decisions. This intermediate representation  clarifies the relation between temporal planning  and causal-link planning, and permits a single  task model to be used for planning and execution. Such a 
Reasoning in Bayesian networks is exponential in a graph parameter w    known  as induced width (also known as tree-width and max-clique size). In this paper, we  investigate the potential of causal independence (CI) for improving this performance.
In this paper we inspect the possibilities of using Bluetooth for building Ad-Hoc networks suitable for transmitting audio and esp. voice data using synchronous SCO links. We analyze the features or problems that Bluetooth offers for transmitting audio data in a multihop network. As the existing MANET routing protocols that emerged out of the work of the IETF MANET WG (like AODV, DSR etc.) can not be directly used to work with Bluetooth, we present a new routing protocol called Bluetooth Scatternet Routing (BSR) that is influenced by other MANET routing protocols but pays special attention to the restrictions of Bluetooth (like number of connections, connection setup times etc.). The protocol is also inspired by the channel switching concept of ATM. Some initial results of simulations and real-life tests give an impression of the performance and efficiency this protocol can reach in an application scenario.
this paper is the Boolean formalism as a building block for modeling complex, large-scale, and dynamical networks of genetic interactions. We discuss the goals of modeling genetic networks as well as the data requirements. The Boolean formalism is justified from several points of view. We then introduce Boolean networks and discuss their relationships to nonlinear digital filters. The role of Boolean networks in understanding cell differentiation and cellular functional states is discussed. The inference of Boolean networks from real gene expression data is considered from the viewpoints of computational learning theory and nonlinear signal processing, touching on computational complexity of learning and robustness. Then, a discussion of the need to handle uncertainty in a probabilistic framework is presented, leading to an introduction of probabilistic Boolean networks and their relationships to Markov chains. Methods for quantifying the influence of genes on other genes are presented. The general question of the potential effect of individual genes on the global dynamical network behavior is considered using stochastic perturbation analysis. This discussion then leads into the problem of target identification for therapeutic intervention via the development of several computational tools based on first-passage times in Markov chains. Examples from biology are presented throughout the paper
Integrated modelling of global environmental change impacts faces the challenge that knowledge from the domains of Natural and Social Science must be integrated. This is complicated by often incompatible terminology and the fact that the interactions between subsystems are usually not fully understood at the start of the project. While a modular modelling approach is necessary to address these challenges, it is not sufficient. The remaining question is how the modelled system shall be cut down into modules. While no generic answer can be given to this question, communication tools can be provided to support the process of modularisation and integration. Along those lines of thought a method for building modular integrated models was developed within the EU project DINAS-COAST and applied to construct a first model, which assesses the vulnerability of the world&apos;s coasts to climate change and sea-level-rise. The method focuses on the development of a common language and provides domain experts with an intuitive interface to code their knowledge in form of modules. However, instead of rigorously defining interfaces between the subsystems at the project&apos;s beginning, an iterative model development process is defined and tools to facilitate communication and collaboration are provided. This flexible approach has the advantage that increased understanding about subsystem interactions, gained during the project&apos;s lifetime, can immediately be reflected in the model.
This paper presents a novel use of a chemical experiments&apos; framework as a control layer and sound source in a concert situation. Signal fluctuations from electrolytic batteries made out of household chemicals, and acoustic samples obtained from an acid/base reaction are used for musical purposes beyond the standard data sonification role. The batteries are controlled in handy ways such as warming, stirring and pouring that are also visually engaging. Audio mappings include synthetic and sampled sounds completing arecipethatconcocts a live performance of computer music. Keywords  Chemical music, Applied chemistry, Battery Controller.
Table soccer (also called &quot;foosball&quot;) is much simpler than real soccer. Nevertheless, one faces the same challenges as in all other robotics domains. Sensors are noisy, actions must be selected under time pressure and the execution of actions is often less than perfect. One approach to solve the action selection problem in such a context is decision-theoretic planning, i.e., identifying the action that gives the maximum expected utility. In this paper we present a decision-theoretic planning system suited for controlling the behavior of a table soccer robot. The system employs forward-simulation for estimating the expected utility of alternative action sequences. As demonstrated in experiments, this system outperforms a purely reactive approach in simulation. However, this superiority of the approach did not extend to the the real soccer table.
This paper presents an fuzzy-based approach to increase movement possibilities of prostheses using algorithms that can be implemented in portable environments (microcontroller). The aims of this paper are  . to present a concept for movement detection (section 3) and  . to demonstrate an implementation strategy for fuzzy classifiers on microcontrollers (section  ??)
availability of the right medical knowledge at the right time. This concept paper presents a knowledge management research program to (a) identify, capture and organize the tacit knowledge inherent within on-line problem-solving discussions between pediatric pain practitioners; (b) establish linkages between topic-specific pediatric pain discussions and corresponding published medical literature on children&apos;s pain available at PubMed---i.e. linking tacit expert knowledge to explicit medical literature; and (c) make these knowledge resources available to pediatric pain practitioners via the WWW for timely access to various modalities of clinical knowledge.
INTRODUCTION  Recently, we witness the rapid growth and thus the prevalence of databases on the Web. Our recent survey [2] in December 2002 estimated between 127,000 to 330,000 deep Web sources. On this deep Web, myriad online databases provide dynamic query-based data access through their query interfaces, instead of static URL links. As the &quot;door&quot; to the deep Web, it is essential to integrate these query interfaces for integrating the deep Web.  The overall goal of the MetaQuerier project (http://metaquerier.- cs.uiuc.edu) aims at opening up the deep Web to users, by building a system to help users exploring and integrating deep Web sources. In particular, to start with, we focus on the integration of deep Web sources in the same domain (e.g., Books, Airfares), which is itself an important integration task. The typical scenarios include purchasing a book with lowest price among book sources and a flight ticket with the best trade-off between price and number of connections among airl
One-phase commit protocols substantially reduce the overheads of transaction commit processing, making them attractive for incorporation in distributed real-time databases. A major obstacle, however, is that these protocols significantly increase the occurrence of priority inversions. This arises from the cohorts of each distributed transaction being in a prepared state for extended periods of time, during which their data locks cannot be pre-empted. We present here a new one-phase real-time commit protocol, called PEP, that addresses the above-mentioned problem by suitably utilizing our previously proposed prepared data borrowing technique. Simulation-based evaluation of PEP for real-time applications with firm deadlines demonstrates that, for a variety of environments, it substantially reduces the number of killed transactions as compared to its multi-phase counterparts. In fact, PEP often provides better performance than even an equivalent centralized system.
Tele-immersion is a technology that augments your space with real-time 3D projections of remote spaces thus facilitating the interaction of people from different places in virtually the same environment. Tele-immersion combines 3D scene recovery from computer vision, and rendering and interaction from computer graphics. We describe the realtime 3D scene acquisition using a new algorithm for trinocular stereo. We extend this method in time by combining motion and stereo in order to increase speed and robustness. 1.
We prove a natural bijection between the polytopal tilings of a zonotope Z by zonotopes, and the one-element-liftings of the oriented matroid M(Z) associated with Z. This yields a simple proof and a strengthening of the Bohne-Dress Theorem on zonotopal tilings. Furthermore we prove that not every oriented matroid can be represented by a zonotopal tiling.
We present a novel methodology for predicting future  outcomes that uses small numbers of individuals participating in  an imperfect information market. By determining their risk attitudes  and performing a nonlinear aggregation of their predictions,  we are able to assess the probability of the future outcome  of an uncertain event and compare it to both the objective probability  of its occurrence and the performance of the market as a  whole. Experiments show that this nonlinear aggregation mechanism  vastly outperforms both the imperfect market and the best  of the participants. We then extend the mechanism to prove robust  in the presence of public information.
This paper presents a unified probabilistic framework for denoising and  dereverberation of speech signals. The framework transforms the denoising  and dereverberation problems into Bayes-optimal signal estimation.
We describe a data structure for effciently maintaining views of dynamic graphs.
Operators of high-speed networks are interested in implementing simple charging schemes with which they can fairly recover costs from their customers and effectively allocate network resources. This paper describes an approach for computing such charges from simple measurements (the duration and transferred volume of a connection), and relating these to bounds of the effective bandwidth. A requirement for usage-based charging schemes is that they capture the relative amount of resources used by connections. Based on this criteria, we evaluate our approach for Internet Wide Area Network traffic. Furthermore, its incentive compatibility is displayed with an example involving deterministic multiplexing, and the effect of pricing on a network&apos;s equilibrium is investigated for deterministic and statistical multiplexing.  Keywords  Usage-based charging, effective bandwidths, incentive compatibility, ATM, Internet 1 INTRODUCTION  A method for charging and pricing is an essential requirement i...
etamorphism of the Central Western Carpathians is   given in the excursion guide.   The manuscript of this work was improved by helpful   suggestions of S. Jacko, D. Plaienka and M. Jank. This work was   supported by Slovak Academic Agency, project WEGA-ff/5003/98     ffffffffffffffffffffffffffffffffff    ff50     Contents     ff. Outline on structure and metamorphism of the Western Carpathians  ff5ff  ff.ff External Western Carpathians  ff52  ff.2 Central Western Carpathians  ff52  ff.2.ff Pieniny Klippen Belt  ff52  ff.2.2 The Tatra-Fatra Belt  ff53  ff.2.2.ff. Pre-Alpine metamorphism in the Tatra-Fatra Belt  ff53  ff.2.2.2. Alpine metamorphism in the Tatra-Fatra Belt  ff55  ff.2.3 The Vepor Belt  ff55  ff.2.3.ff. Pre-Alpine metamorphism in the Vepor Belt  ff56  ff.2.3.2. Alpine metamorphism in the Vepor Belt  ff59  ff.2.4. The Gemer Belt  ff60  ff.2.4.ff. Pre-Alpine metamorphism in the Gemer Belt  ff60  ff.2.4.2 Alpine metamorphism in the Gemer Belt  ff62  ff.3 Inner Western Carpathians  ff62  2. References  ff64  3. Description of Loc
The increased importance of XML as a universal data representation format has led to several proposals for enabling the development of applications that operate on XML data. These proposals range from runtime API-based interfaces to XML-based programming languages. The subject of this paper is XJ, a research language that proposes novel mechanisms for the integration of XML as a first-class construct into Java. The design goals of XJ distinguish it from past work on integrating XML support into programming languages --- specifically, the XJ design adheres to the XML Schema and XPath standards, and supports in-place updates of XML data thereby keeping with the imperative nature of Java. We have also built a prototype compiler for XJ, and our preliminary experimental results demonstrate that the performance of XJ programs can approach that of traditional low level API-based interfaces, while providing a higher level of abstraction.
Existing association rule mining algorithms suffer from many problems when mining massive transactional datasets. Some of these major problems are: (1) the repetitive I/O disk scans, (2) the huge computation involved during the candidacy generation, and (3) the high memory dependency. This paper presents the implementation of our frequent itemset mining algorithm, COFI, which achieves its efficiency by applying four new ideas. First, it can mine using a compact memory based data structures. Second, for each frequent item assigned, a relatively small independent tree is built summarizing co-occurrences. Third, clever pruning reduces the search space drastically. Finally, a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed to generate all relevant frequent patterns.
Disjunctive Logic Programming (DLP) under the consistent answer  set semantics is an advanced formalism for knowledge representation and reasoning.
One of the main challenges in computer graphics is still the realistic rendering of complex materials such as  fabric or skin. The difficulty arises from the complex meso structure and reflectance behavior defining the unique  look-and-feel of a material. A wide class of such realistic materials can be described as 2D-texture under varying  light- and view direction namely the Bidirectional Texture Function (BTF). Since an easy and general method for  modeling BTFs is not available, current research concentrates on image-based methods which rely on measured  BTFs (acquired real-world data) in combination with appropriate synthesis methods. Recent results have shown  that this approach greatly improves the visual quality of rendered surfaces and therefore the quality of applications  such as virtual prototyping. This STAR will present in detail the state-of-the-art techniques for the main tasks  involved in producing photo-realistic renderings using measured BTFs  
There is currently need for an up-to-date and thorough survey of the research in the eld of computer and network intrusion detection. This paper presents such a survey, with a taxonomy of intrusion detection system features, and a classi- cation of the surveyed systems according to the taxonomy. The conclusion is reached that current research interest should lie in the study of the e ectiveness of intrusion detection and how to handle attacks against the intrusion detection system itself.
Visualization exists from the first known maps to today. Developments in computer science and technologies have resulted in arising new techniques besides maps for visualization of geographic data. Combining technologies such as image processing, computer graphics, animation, simulation, multimedia, and virtual reality, computers can help us present information in a new way so that patterns can be found, greater understanding can be developed, and problems can be solved. Therefore, maps have new missions. Nowadays maps are not only communication means but also visual thinking means for the users. New techniques mentioned above for visualization of geographic information have begun to use commonly in cartography and GIS. These enable us more comprehensively understand our living world. Scientific visualization supports visualization processes. It is using of the computer technology for creating visual supplies to make thinking and problem solving easy. In this paper, we present a case study for 3D visualization of YTU&apos;s new campus area. First, geographic and cartographic visualization techniques are shortly given. Then, some applications are given, implemented in MapInfo Vertical Mapper and Autodesk 3D Studio VIZ. YTU Davutpasa Campus is in a big and sloping area, in which there is only a faculty building. There are many construction works in this area. Construction plans include making new roads and new buildings. University needs 3D visual geo-products of the campus for better planning, fast and economic working and easy campus management. In case study, YTU Davutpasa Campus is first 3D visualized in Vertical Mapper by grid method, using the points (x,y,z) derived from large scale topographic map of the campus. Then, draping process is made using georeferenced map o...
Globally, water resources are being fast depleted or deteriorated, while the demands are increasing  manifold. Water resources management is becoming a greater challenge in the developing countries with  changing environmental, political, social and economic conditions. Politics and shadow economy are among  major hurdles in resource management, making implementation of projects and policy guidelines often a failure.
Energy consumption of cryptographic algorithms and security protocols is a crucial factor in wireless ad-hoc networks. This work explores the energy cost of a key agreement process between two parties of an ad-hoc network using public-key encryption techniques and compares the results with regular networks which use secret-key based key-exchange protocols. Elliptic Curve public-key and Rijndael AES secret-key algorithms are chosen to explore the energy cost of Diffie-Hellman and Kerberos key agreement protocols on a WINS sensor network. The results show that the total energy cost of the Diffie-Hellman key agreement process using Elliptic Curve point-multiplication in an ad-hoc network is between one to two orders of magnitude larger than the key exchange process based on the AES secret-key algorithm in a regular non ad-hoc network.
Observation equivalence is a well-known technique for proving that  a concurrent system satisfies its specification. We report on our experience in the mechanization of observation equivalence proofs with the help of a general-purpose theorem prover. Several case-studies are considered, including a sliding window and a cache-coherence protocol. In all cases the system has an infinite number of states, and sometimes also an arbitrarily large number of components. We show how compositionality and bisimulation-up-to techniques can be applied to reduce the size of the proofs. 
The field of Genetic Algorithms has grown into a huge area over the last few years. Genetic Algorithms are adaptive methods, which can be used to solve search and optimisation problems over a period of generations, based upon the principles of natural selection and survival of the fittest. This paper describes an innovative tool to introduce the basics of the subject of Genetic Algorithms called GAVIn (Genetic Algorithms Visual Interface). It focuses on the domain of numerical function optimisation problems as these form a good basis for learning and operator comparison. The other problem domains are too varied and problem dependent to form a general, robust learning tool.  1 Introduction to Genetic Algorithms  Genetic Algorithms (GA&apos;s) are adaptive methods, which can be used to solve search and optimisation problems. The principles behind the power of GA&apos;s are based upon the genetic processes of biological organisms which over many generations, evolve according to the principles of na...
Programmers of parallel processes that communicate through shared globally distributed data  structures (DDS) face a difficult choice. Either they must explicitly program DDS management,  by partitioning or replicating it over multiple distributed memory modules, or be content with  a high latency coherent (sequentially consistent) memory abstraction that hides the DDS&apos; distribution.
We discuss different methods of reducing symmetry in CSPs  where the variables correspond to the cells of a matrix, and both the rows  and the columns of the matrix are interchangeable. Two basic approaches are  considered: adding symmetry-breaking constraints to the model, and breaking  symmetry during search. We also show how the two approaches can be  combined in this case. We empirically compare several different approximations  to eliminating the symmetry and an exact method that eliminates the  symmetry completely, but is only feasible for small matrices.  1 
Simulation modeling of complex systems is receiving increasing research attention over the past years. In this paper, we discuss the basic concepts involved in multi-resolution simulation modeling of complex stochastic systems. We argue that, in many cases, using the average over all available high-resolution simulation results as the input to subsequent low-resolution modules is inappropriate and may lead to erroneous final results. Instead high-resolution output data should be classified into groups that match underlying patterns or features of the system behavior before sending group averages to the low-resolution modules. We propose high-dimensional data clustering as a key interfacing component between simulation modules with different resolutions and use unsupervised learning schemes to recover the patterns for the high-resolution simulation results. We give some examples to demonstrate our proposed scheme.
Wide-bandwidth measurements are necessary when testing electronically switched are addressed the measurement of motor phase input power at low speed. with a high modulation frequency and the measurement of small torque ripple on a large mean an  ment. Extensive use Is made of signal processing techniques to reduce noise and ensure high accuracy. mental results are presented to illustrate each technique. Signal processing. reluctance motors. The authors have been the potential of high perform-  ance reluctance motors. for application in servo systems. Such mo-  tors must develop smooth hence the need for accurate measurement on the prototypes. They have also needed to measure input power to the motor in order to quantify the and measure the a range of speeds. signal tech-  niques have been successfully applied to both measurements. The torque measurement method provides a means of measuring a rela-  tively small of narrow bandwidth. whereas the power measure-  ment method allows the measurement of a relatively large signal of bandwidth. The accurate measurement of and power in switched and Turner. converter-fed reluc-  tance drives at low speed (and hence low fundamental frequency presents particular addition to the well known  bauer and 1989) of making accurate measurements near power converters high-speed switches. Both of the tech-  niques described In the paper provide good immunity to switching The measurement and equipment and used to develop both techniques Is shown in 1. The controls the data capture unit via a software package. The data  files then to the workstation, where they can be proc-  essed a of tools and  1978:NAG,  ure the motor the load must be smooth and a magnetic powder brake was selected. The motor fitted with a position resolver. which a  digit...
This paper, extends previous work [1-3] in equation-based congestion control for unicast traffic. Most best effort traffic on the internet is appropriately served by TCP which is the dominant transport protocol on the internet. However, there is a growing number of multimedia applications for which TCP is not well suited. For those applications, several congestion control mechanisms have been proposed [1] in order to avoid congestion collapse on the internet [4]. One of them is the recently proposed TCP Friendly Rate Control Protocol (TFRC) [1-3]. It can be only used by flows that have a constant packet size. In this paper, we propose an extension to the TFRC protocol in order to support variable packet size flows. Variable packet size has been used for the transmission of video over the internet [5] and is also used in voice applications. So it is important for a congestion control protocol to support variable packet size flows.
This paper presents a new model for simulating Spiking Neural Networks using discrete event simulation which might possibly offer advantages concerning simulation speed and scalability. Spiking Neural Networks are considered as a new computation paradigm, representing an enhancement of Artificial Neural Networks by offering more flexibility and degree of freedom for modeling computational elements. Although this type of Neural Networks is rather new and there is not very much known about its features, it is clearly more powerful than its predecessor, being able to simulate Artificial Neural Networks in real time but also offering new computational elements that were not available previously. Unfortunately, the simulation of Spiking Neural Networks currently involves the use of continuous simulation techniques which do not scale easily to large networks with many neurons. Within the scope of the present paper, we discuss a new model for Spiking Neural Networks, which allows the use of discrete event simulation techniques, possibly offering enormous advantages in terms of simulation 
flexibility and scalability without restricting the qualitative computational power.
One of the most important aspects in asset allocation problems is the assumed probability distribution of asset returns. Financial managers generally suppose normal distribution, even if extreme realizations usually have an higher frequency than results in the case of normally distributed returns. Using Monte Carlo simulation, we propose and solve an asset allocation problem with shortfall constraint, evaluating the exact risk-level for managers in the case of misspecification of tails behaviour. In particular, in the optimisation problem, we assume that returns are generated by a multivariate Student-t, when in reality returns come from a multivariate distribution where each marginal is a Student-t with different degrees of freedom; this method permits us to value the effective risk for managers. In the case analysed, it is also interesting to observe that a multivariate density with different marginal distributions produces a shortfall probability and a shortfall return level that can be approximated adequately by assuming a multivariate Student-t in the optimisation problem. The present approach could be an important instrument for investors who require a qualitative assessment of the reliability and sensitivity of their investment strategies when their models are potentially misspecified.
Using micro-level data from the mortgage market, we examine yield spreads as a funnction of both credit risk and prepayment risk using a two-stage instrumental variable approach. Our results are consistent with findings in the finance literature that leverageinduced financial risk reflected in capital structure results in higher yield spreads on debt.
Let X be a simply connected space and k a commutative ring. Goodwillie
We construct an invariant of the bi-Lipschitz equivalence of analytic  function germs (R      (R,0) that varies continuously in many analytic  families. This shows that the bi-Lipschitz equivalence of analytic function  germs admit continuous moduli. For a germ f the invariant is given in terms of  the leading coefficients of the asymptotic expansions of f along the sets where  the size of |x||grad f(x)| is comparable to the size of |f(x)|.
  A constraint satisfaction problem (CSP) consists of a triple, (X; D;C), where X is a set of variables, D is a set of corresponding domains of values and C is a set of constraints which specify allowed combinations of assignments of values to variables. Constructing an eective model of a CSP is, however, a challenging task. Carefully chosen transformations of a basic model can greatly reduce the amount of eort required to solve the problem by systematic search (e.g. [ 5 ] ). These include adding constraints that are implied by other constraints in the problem, adding constraints that remove symmetrical solutions to the problem, removing redundant constraints and replacing constraints with their logical equivalents.  
In this paper, we address multiple motion analysis from the standpoint of orientation analysis. Using the fact that multiple motions are equivalent to multiple planes in the derivative space or in the spectral domain, we apply a new kind of 3D steerable filter in motion estimation. This new method is based on the decomposition of the sphere with a set of overlapping basis filters in the feature space. It is superior to principal axis analysis based approaches and current 3D steerability approaches in achieving high orientation resolution. Our approach is similar to the 3D Hough transform, but more efficient and robust. It also improves the performance of the expectation-maximization algorithm.
Most studies of the vocal signs of emotion depend on acted data. This paper reports the development of a vocal coding system to describe the signs of emotion in naturally occurring emotion. The system has been driven by empirical observation, not by a priori assumptions based on acted or laboratory data. The data used to develop it is the Belfast Naturalistic Database. The system takes a multi-level approach to coding, starting broad brush and moving through progressive layers to finer resolution. The first level uses broad categories which apply to each clip as a whole. Thereafter it uses a tiered approach, starting with an outer tier of relatively coarse descriptors and progressing through successive tiers to more detailed descriptors associated more precisely with locations in a clip. The coding system shows that there are vocal signs of naturally occurring emotion which have not been picked up before in acted data.
Having learners construct computer-based simulations is  becoming increasingly important as an approach to induce  learning. Qualitative simulations incorporate a rich  vocabulary for articulating insights about systems and their  behaviours, including notions such as structural  constituents, identifying qualitatively distinct behaviours,  and making explicit the causal dependencies that govern a  system&apos;s behaviour. When building a qualitative model all  these details have to be made explicit (they have to be  represented in the model). As a result, building a qualitative  model induces a deep understanding of the system and its  behaviour. In our research we want to exploit this  phenomenon, i.e. have learners learn by building qualitative  simulations. However, building qualitative models is  generally seen as a difficult task, and no easy-to-use tools  exist to support a learner in performing such a task. In this  paper, we present MOBUM, a domain independent model  building environment aimed at supporting learners in  building qualitative simulations. MOBUM is a fully  implemented prototype in JAVA. The main goal of this  prototype is to generate and clarify ideas with respect to  how such a model building tool should be constructed.
In a database of emotional speech, dimensional descriptions of emotional states have been correlated with acoustic variables. Many stable correlations have been found. The predictions made by linear regression widely agree with the literature. The numerical form of the description and the choice of acoustic variables studied are particularly well suited for future implementation in a speech synthesis system, possibly allowing for the expression of gradual emotional states.
Our motivation is powered by the rapid changes in the mobile telecommunication society. The influence to integrate fixed and mobile networks; the developments in service engineering, network management and intelligent networks; the desire to have multi-application hand-held terminals; and above all the increasing scope of the multimedia services expected by customers; all these demands lean to performance requirements beyond the capability of Second Generation (2G) technology. For this purpose Third Generation (3G) systems and technologies are being actively researched world wide, such systems are commonly referred in Europe under the name UMTS (Universal Mobile Telecommunications Systems).
This paper presents a brief comparison of two operating systems, Windows  NT and UNIX. The comparison covers two different aspects. First, we  compare the main security features of the two operating systems and then  we make a comparison of a selection of vulnerabilities most of which we  know have been used for making real intrusions. We found that Windows  NT has slightly more rigorous security features than &quot;standard&quot; UNIX but  the two systems display similar vulnerabilities. The conclusion is that there  are no significant differences in the &quot;real&quot; level of security between these  systems.  1  Dept of Computer Engineering  Chalmers University of Technology  S-412 96 Gteborg, SWEDEN  {sax, Erland.Jonsson}@ce.chalmers.se  2  Dept of Computer Science  University of Karlstad  S-651 88 Karlstad, SWEDEN  {Hans.Hedbom, Stefan.Lindskog}@hks.se  +  Presented at the Third Nordic Workshop on Secure IT Systems, NORDSEC  &apos;98, 5-6 November, 1998, Trondheim, Norway.  A Comparison of the Security ...
In recent years, researchers in graph mining have been exploring linear paths as well as subgraphs as pattern languages. In this paper, we are investigating the middle ground between these two extremes: mining free (that is, unrooted) trees in graph data. The motivation for this is the need to upgrade linear path patterns, while avoiding complexity issues with subgraph patterns. Starting from such complexity considerations, we are defining free trees and their canonical form, before we present FreeTreeMiner, an algorithm making efficient use of this canonical form during search. Experiments with two datasets from the National Cancer Institute&apos;s Developmental Therapeutics Program (DTP), anti-HIV and anti-cancer screening data, are reported.
. The paper gives a brief overview of the interdisciplinary DFG priority program on spatial cognition and presents one specific theme which was the topic of a recent workshop in Göttingen in some more detail. A taxonomy of landmark, route, and survey knowledge for navigation tasks proposed at the workshop is presented. Different ways of acquiring route knowledge are discussed. The importance of employing different spatial reference systems for carrying out navigation tasks is emphasized. Basic mechanisms of spatial memory in human and animal navigation are presented. After outlining the fundamental representational issues, methodological issues in robot and human navigation are discussed. Three applications of spatial cognition research in navigation tasks are given to exemplify both technological relevance and human impact of basic research in cognition. The German Priority Program on Spatial Cognition  Spatial cognition includes acquisition, organization, use, and revision of knowled...
The practice of medicine, with its wide range of environmental conditions and complex dependencies, has long been used as a testbed for various advanced technologies. Telemedicine as conceptualised within the Multimedia Super Corridor (MSC) context is seen as the application of several relatively mature technologies--- artificial intelligence (AI), multimedia communication and information systems (IS) amongst others. We will discuss in general terms the Malaysian vision on the comprehensive MSC telemedicine solution, its functionality and associated operational conditions. In particular, this paper focuses on the conceptualisation of one key telemedicine component i.e. the Lifetime Health Plan (LHP) system, which is eventually intended to be a distributed multi-module application for the periodic monitoring and generation of health-care advisories for upwards of 20 million Malaysians.
We present two new branch and bound algorithms for solving  Max-SAT, and provide experimental evidence that outperform the  algorithm of Borchers &amp;Furman on randomly generated instances. Our  approach decreases the time needed to solve an instance, as well as the  number of backtracks, up to two orders of magnitude.
Knowledge-based systems have been severely restricted in areas where the speed of processing is a key factor. This is especially evident in large systems where the speed of knowledge-base searches is important. This paper proposes a fuzzy pattern recognition technique which identifies data patterns using possibility distributions and documents a fuzzy algorithm which is implemented. The technique is based on the theory of possibility. The results obtained using sensor data in manufacturing are encouraging: the fuzzy technique outperforms non-fuzzy techniques convincingly. The results for comparison with non-fuzzy techniques include shell-sort and quick-sort with binary search. The fuzzy technique identifies the correct pattern in the sensor database with nearly 99% accuracy. These results highlight the role of new fuzzy technologies for making knowledge-based systems more attractive in areas where they are currently limited by speed considerations.  1  S. Singh and M. Steinl. Fuzzy Sea...
This paper describes features of the active logic formalism as a tool for implementing mixed initiative intelligent systems. A framework is provided for assessing systems as &quot;mixed-initiative&quot;, which is used to explore the relevant features both in the active logic formalism and associated implementations based upon and using that formalism. Active logics were developed as a means of combining the best of two worlds -- inference and reactivity -- without giving up much of either. This requires a special evolvingduring -inference model of time. Active logics are able to react to incoming information (including dialogue utterances by a collaborative partner) while reasoning is ongoing, blending new inputs into its inferences without having to start up a new theorem-proving effort. An implementation of active logic is described, which also includes special features for reasoning about and performing actions. This implementation is also used as the backbone of a dialogue system. 1 Introduc...
Shape optimization is applied to time-dependent trailing-edge flow in order to minimize aerodynamic  noise. Optimization is performed using the surrogate management framework (SMF),  a non-gradient based pattern search method chosen for its effciency and rigorous convergence  properties. Using SMF, design space exploration is performed not with the expensive actual  function but with an inexpensive surrogate function. The use of a polling step in the SMF  guarantees that the algorithm generates a convergent subsequence of mesh points, each iterate  of which is a local minimizer of the cost function on a mesh in the parameter space. Results  are presented for an unsteady laminar flow past an acoustically compact airfoil. Constraints on  lift and drag are handled within SMF by applying the filter pattern search method of Audet  and Dennis, within which a penalty function is used to form and optimize a surrogate function.
Free-choice Signal Transition Graphs (STG) are a class of interpreted Petri nets with applications to the verification and synthesis of speed-independet circuits. Several synthesis techniques for free-choice STGs have been proposed which require to know the concurrency relation of the net, i.e., the pairs of transitions that can become concurrently enabled at some reachable marking. We use some results about freechoice nets to derive an efficient polynomial algorithm for the computation of the concurrency relation. 1 Introduction  Signal Transition Graphs (STGs) have become a popular and much studied formalism for the specification and verification of speed independent circuits [3, 9, 10]. STGs are bounded Petri nets whose transitions carry labels of the form y  +  , y  \Gamma  , where y is a circuit signal.  1  The occurrence of a transition with label y  +  raises y, i.e., sets its value to 1, while the occurrence of a transition with label y  \Gamma  lowers y, i.e., sets its value t...
Many potential applications for artificial intelligence (AI) in manufacturing extend classical AI in two ways. First, they go beyond modeling a single decision-maker to incorporate the interactions of multiple &quot;agents.&quot; Second, instead of fitting a &quot;problem solving&quot; rubric that seeks to reach achievement goals, they are more aptly cast as &quot;going concerns&quot; that support maintenance goals in the face of continual disruption. The first extension is the focus of active research in multi-agent systems (MAS), with which the IMS community has established effective ties. This paper proposes that the second extension can benefit from the use of concepts from dynamical systems theory (informally, &quot;chaos theory&quot;). This field provides useful metaphors for discussing going concerns. Its promise goes further than metaphor, for multi-agent systems are actually dynamical systems, whose effective design, monitoring, and control requires application of techniques developed in dynamical systems. Thus rese...
A new approach to representing qualitative spatial knowledge  and to spatial reasoning is presented. This approach is motivated by  cognitive considerations and is based on relative orientation information  about spatial environments. The approach aims at exploiting properties of  physical space which surface when the spatial knowledge is structured  according to conceptual neighborhood of spatial relations. The paper  introduces the notion of conceptual neighborhood and its relevance for  qualitative temporal reasoning. The extension of the benefits to spatial  reasoning is suggested. Several approaches to qualitative spatial reasoning  are briefly reviewed. Differences between the temporal and the spatial  domain are outlined. A way of transferring a qualitative temporal reasoning  method to the spatial domain is proposed. The resulting neighborhood-oriented  representation and reasoning approach is presented and illustrated.
In all areas of the e-era, personalization plays an important role. Particularly in elearning a main issue is student modeling, that is the analysis of student behavior and prediction of his/her future behavior and learning performance. In fact, nowadays, the most prevailing issue in the e-learning environment is that it is not easy to monitor students&apos; learning behaviors. In this paper we have focused our attention on the system (the Profile Extractor) based on Machine Learning techniques, which allows for the discovery of preferences, needs and interests of users that have access to an e-learning system. The automatic generation and the discovery of the user profile, to agree as simple student model based on the learning performance and the communication preferences, allow creating a personalized education environment. Moreover, we presented an evaluation of the accuracy of the Profile Extractor system using the classical Information Retrieval metrics.
We present a new domain for unsupervised learning: automatically  customizing the computer to a specific melodic performer  by merely listening to them improvise. We also describe  BoB, a system that trades customized real-time solos  with a specific musician. We develop a probabilistic mixture  model, derived from the multinomial distribution, for  the clustering and generation of variable sample-sized histograms. With this model
Implementing personalisation in Web Directories depends not only on  developing appropriate architectures and equipping Web Directories with adaptation  techniques, but also on incorporating human factors considerations at an early  design stage. Among a range of human factors this paper explores cognitive styles  and their influence on users&apos; preferences. Preferences with respect to the  organisation and presentation of the content, and the navigation paths are identified  through a small-scale study. The findings are analysed and used to implement a  prototype Web Directory Browser, gearing interface features to cognitive stylerelated  preferences.
Production networks require the transport of high-quality multimedia traffic between outside broadcast vans and the main studio. This is typically done through dedicated terrestrial or satellite links, with bandwidth purchased from third party network providers, which is expensive and lacks flexibility. Given the emergence of IP networks and the Internet as the multi-service network of choice, it is plausible to consider their use for transporting production network traffic with high bandwidth and low delay and packet loss requirements. Emerging technologies for quality of service such as Differentiated Services and MPLS can be used for premium quality traffic. In this paper we try to use the emerging IP technologies to support services like production network traffic. We present a Traffic Engineering and Control System that starts from agreed services with customers and provisions the network according to the expected traffic demand so as to meet the requirements of contracted services while optimising the use of network resources. We devise a non-linear programming formulation of the problem and show through extensive simulations that we can achieve the objectives and meet the requirements of demanding production network traffic. Our solution is generic enough and not only tuned to production networks, so it can be used in other contexts for supporting services with stringent quality of service requirements.
Nodal network approaches are a common framework for considering water allocation problems. In this type of model framework, a river basin is represented as a series of nodes, where nodes generally represent key points of extraction along the stream. Agricultural production and other water use decisions generally interact with the stream system in two ways: they can affect the generation of runoff and thus the volume of water reaching the stream; or, they may involve direct extraction or use of streamflow once it has reached the stream. This paper provides a generalised conceptual framework for considering these types of interactions and their representation in integrated water allocation models.
Statistical offces are faced with the problem of multiple-database data mining at least for two reasons. On one side, there is a trend to avoid direct collection of data from respondents and use instead administrative data sources to build statistical data; such administrative sources are typically diverses and scattered across several administration level. On the other side, intruders may attempt disclosure of confidential statistical data by using the same approach, i.e. by linking whatever databases they can obtain. This paper discusses issues related to multipledatabase data mining, with a special focus on a method for linking records across databases which do not share any variables.
The design of secure buffer managers for  database systems supporting real-time applications  with firm deadlines is studied here. We first  identify the design challenges and then present  SABRE, a new buffer manager that aims to address  these challenges. SABRE guarantees covert  channel-free security, employs a fully dynamic  one-copy allocation policy for efficient usage of  buffer resources, and incorporates several optimizations  for reducing the number of killed transactions  and for decreasing the unfairness in the  distribution of killed transactions across security  levels. Using a detailed simulation model,  the real-time performance of SABRE is evaluated  against unsecure conventional and real-time  buffer management policies. Our experiments  show that SABRE provides security with only a  modest drop in real-time performance. Finally, we  present FSABRE, an adaptive admission controlaugmented  version of SABRE, which efficiently  ensures close to ideal fairness across tran...
This paper deals with the various changes that can be made to the basic PageRank model. We  document the recent findings and add a few new contributions. These contributions concern (1) the  sensitivity of the PageRank vector, (2) another method of forcing the Markov chain to be irreducible,  and (3) a proof of the full spectrum of the PageRank matrix.
Data interoperability across heterogeneous systems can be hampered by differences in terminology, particularly when multiple scientific communities are involved. To reconcile differences in semantics, a common semantic framework was created through the development of Earth science ontologies. Such a shared understanding of concepts enables ontology-aware software tools to understand the meaning of terms in documents and web pages.
Audio data carries much more information than text and are widely used in many applications like broadcast news. But there is a lack of good techniques for retrieving some information from such data. We are trying to address this problem, called the Spoken Document Retrieval (SDR) for Indian languages, by matching speech-segments without actual recognition. The objective is to index the audio data in suitable format so that we can search in this indexed database for a given query. The output can be some audio file(s) or segment(s). At present, the experiments are done on labeled data and text query. The system is interactive and works based on probabilistic searching. Further work is being done on partially labeled as well as unlabeled speech data and speech query. The techniques being explored involve hypothesizing the word boundaries, developing phoneme-level Hidden Markov Models (HMMs), phoneme-boundary marking using segmental k-means algorithm, deriving syllable-level models, and Vector Quantization. We provide the outline and various issues related to both these scenarios: labeled and transcribed data, and unlabeled and nontranscribed data. The implementation details of the former case is given and the latter case is analyzed. We show that we could achieve reasonably well precision and recall for speech documents, too.
A method for the analysis of nonstationary time series with multiple operating modes is presented. In particular, it is possible to detect and to model a switching of the dynamics and also a less abrupt, time consuming drift from one mode to another. This is achieved by an unsupervised algorithm that segments the data according to inherent modes, and a subsequent search through the space of possible drifts. An application to physiological wake/sleep data demonstrates that analysis and modeling of real-world time series can be improved when the drift paradigm is taken into account. In the case of wake/sleep data, we hope to gain more insight into the physiological processes that are involved in the transition from wake to sleep.
Many of the challenges faced by the field of Computational Intelligence in building intelligent agents, involve determining mappings between numerous and varied sensor inputs and complex and flexible action sequences. In applying nonparametric learning techniques to such problems we must therefore ask: &quot;Is nonparametric learning practical in very high dimensional spaces?&quot; Contemporary wisdom states that variable selection and a &quot;greedy&quot; choice of appropriate functional structures are essential ingredients for nonparametric learning algorithms. However, neither of these strategies is practical when learning problems have thousands of input variables, and tens of thousands of learning examples. We conclude that such nonparametric learning is practical by using a methodology which does not use either of these techniques. We propose a simple nonparametric...
Set Partitioning in Hierarchical Trees (SPIHT), proposed by Said and Pearlman [1], is generally regarded as a very efficient wavelet-based still image compression scheme. The algorithm uses an efficient, joint scanning and bit-allocation mechanism for quantizing the scalar wavelet coefficients, and produces a perfectly embedded bitstream. This work extends set partitioning to scan vectors of wavelet coefficients, and use successive refinement VQ techniques such as multistage and tree-structured VQ, to quantize several wavelet coefficients at once. The new scheme is named VSPIHT (Vector SPIHT). Coding results are presented to demonstrate that the vector-based approach (without arithmetic coding) surpasses the scalar counterpart (also without arithmetic coding), in the mean-squared-error sense, for most images at low bitrates. The superiority of the vector-based approach is more pronounced for images that are generally regarded as difficult to code (such as Barbara) because of a large am...
Practical parallelizations of multi-phased low-level image-processing algorithms may  require working in batch mode. The features of a new processing model, employing  a pipeline of processor farms, are described. A simple exemplar, the Karhunen-Lo`eve  transform, is prototyped on a network of processors running a real-time operating system.  The design trade-offs for this and similar algorithms are indicated. In the manner of codesign,  eventual implementation on large- and fine-grained hardware is considered. The  chosen exemplar is shown to have some features, such as strict sequencing and unbalanced  processing phases, which militate against a comfortable implementation.  Keywords: Karhunen-Lo`eve transform, parallel pipeline, multi-spectral images, co-design  A shortened version of this paper appeared in the Third International Euro-Par Conference, Passau, Germany, August 1997  1  1 Introduction  Many low-level image-processing (IP) algorithms, such as spatial filters, are complet...
Future parallel computers must efficiently execute not only hand-coded applications but also programs written in high-level, parallel programming languages. Today&apos;s machines limit these programs to a single communication paradigm, either message-passing or shared-memory, which results in uneven performance. This paper addresses this problem by defining an interface, Tempest, that exposes low-level communication and memory-system mechanisms so programmers and compilers can customize policies for a given application. Typhoon is a proposed hardware platform that implements these mechanisms with a fully-programmable, user-level processor in the network interface. We demonstrate the utility of Tempest with two examples. First, the Stache protocol uses Tempest&apos;s finegrain access control mechanisms to manage part of a processor &apos;s local memory as a large, fully-associative cache for remote data. We simulated Typhoon on the Wisconsin Wind Tunnel and found that Stache running on Typhoon perform...
Checkpointing is a technique used for many purposes, including, but not limited to assistance in fault-tolerant applications, rollback transactions and migration. Many tools have been proposed in the past to help solve these problems. But in the field of migration there is still a lack, because either: (a) the tool is limited to some kind of parallel programming library (PVM, MPI), (b) the size of the checkpoint image is too big to be worth migrating, (c) the checkpoint is limited to some well-behaved applications or (d) the checkpoint image must be saved to file or sent to centralized servers instead of going directly to the target machine&apos;s memory. We developed a new tool called Epckpt that can solve this lack in process migration. Epckpt can: (a) checkpoint almost all kinds of applications independent of their behavior, (b) limit the size of the applications image to its minimum, (c) checkpoint fork-parallel applications, (d) checkpoint an application that was not meant for being ch...
We propose a method for finding a glo al optimal solution of programs with linear complementarity constraints. The program arises for instance from the ilevel programs. The main idea of the method is to generate a sequence of points either ending at a glo al optimal solution within a finite num er of iterations or converging to a glo al optimal solution. The construction of such a sequence is ased on the techniques such as ranch and ound technique, which are used successfully in glo al optimization. Some results on a numerical test of the algorithm are reported. 1. 
This paper describes the construction of an English valency dictionary which lists a wide range of alternations  for each verb sense. Information is automatically extracted from the on-line version of GoiTaikei --- a Japanese  Lexicon, and an extended COMLEX, incorporating information from EVCA and WordNet, as well as additional  features such as argument status, and an augmented case-role representation. Senses are distinguished  on semantic grounds, depending on the core lexical meaning of the verb. Each sense may have one or more  alternations, thus keeping the number of senses manageable, while allowing for systematic variation in the  lexical realization.
Several user studies have shown that in many cases electronic planning and scheduling tools are perceived by the users as disruptive with respect to their actual work practices often based on physical tools. This can result in rejecting such electronic tools without fully appreciating the long-term benefits of their adoption. As a part of a larger effort for providing support along scheduling and negotiation processes across distributed organizations, we have designed a solution to allow manual and computer-supported mixed initiative that aims at letting the users continue working according to their work practices leveraging the benefit of an automated support.
This article describes an algorithm for factorizing an ambiguous finite-state transducer (FST) into two FSTs,  T 1 and T 2 , such that T 1 is functional and T 2 retains the ambiguity of the original FST. The application of T 2 to  the output of T 1 never leads to a state that does not provide a transition for the next input symbol, and always  terminates in a final state. In other words, T 2 contains no &quot;failing paths&quot; whereas T 1 in general does. Since  T 1 is functional, it can be factorized into a left-sequential and a right-sequential FST that jointly constitute a  bimachine. The described factorization can accelerate the processing because no failing paths are ever followed.
controlled artificial creatures that can be considered either as biological models or as simulated robots. Environments and creatures can be built and modified during simulation time using a large set of graphical editors and viewers. The simulator also includes an extensive package for generation of graphs and statistics. To let the neural network controllers improve automatically, it possible to invoke an evolution module that uses the genetic algorithm to create new neural networks using a large selection of evolution schemes. The simulator is in the public domain and suitable both as a research tool and in education.
The development of information and communication technologies and the expansion of the Internet means that, nowadays, there are huge amounts of information available via these emergent media. A number of content management systems have appeared which aim to support the management of these large amounts of content. Most of these systems do not support collaboration among several, distributed sources of managed content. In this paper we present a proposal for an architecture, Infoflex, for the efficient and flexible management of distributed content using Next Generation Web Technologies: Web Services and Semantic Web facilities.
Due to the limitations of current operating systems in supporting multimedia applications, much work has been done to provide resource management mechanisms to address this problem. As processor cycles are often the most oversubscribed and critical resource, most of this work has focused on uniprocessor scheduling. However, hardware platforms are moving to multiprocessor systems, and little work has been done to address the problem of supporting multimedia applications in a multiprocessor context. This paper proposes a new multiprocessor scheduler designed to meet the requirements of multimedia applications. We present an overview of the scheduling algorithm, describe its implementation in a commercial operating system, and discuss directions for future work.
this article were defrayed in part by the payment of page charges. The article must therefore be hereby marked &quot;advertisement&quot; in accordance with 18 U.S.C. Section 1734 solely to indicate this fact
Aggregation of data flows has two major advantages. One is the reduction of state complexity within the network, the other is the saving of resources by statistical multiplexing between the aggregated flows. In this paper, we show a simple, yet effective scheme to aggregate real-time flows which require a statistical guarantee on experienced loss and a deterministic guarantee on the maximum delay on an individual basis. The focus of our aggregation scheme is on the reduction of state complexity. Therefore, we try to maximize the number of flows to be aggregated by the consideration of heterogeneous flows at the cost of maximally saving resources which would require homogenous flows to be aggregated. Our approach is to first gain insight on the buffer occupancy distribution of a single flow. In practice, the buffer occupancy distribution function of a real-time flow can be considered as monotonic decreasing. We show that the uniform distribution, which is analytically very tractable, is always more pessimistic than a monotonic decreasing distribution. This allows us to aggregate heterogeneous flows by taking the uniform distribution as a worst-case bound for the individual flows&apos; buffer distributions and exploiting its statistical properties to save buffer resources by statistical multiplexing between the individual flows of the aggregate. Finally, we discuss at which rate such an aggregate of heterogeneous flows has to be served while maintaining the statistical guarantees given to individual flows.
Adaptation to changes in organizational procedures and business rules is a sine qua non for workflow management systems, if they have to be useful to organizations. This paper describes an approach based on workflow agents capable of managing dynamic changes in business policies. The two key aspects of our approach are (i) the agentification of the process engine, in the sense that this is viewed as controlled and executed by autonomous workflow agents capable of reacting and adapting to external changes, and (ii) the fact that these agents are reflective, in the sense that they can observe and thus modify their own behavior. The model is described in the context of an agent-based framework for coordination with reflective capabilities, and is evaluated against a well-known case of dynamic change.
Terrestrial laser scanners provide a three-dimensional sampled representation of the surfaces of objects resulting in a very large number of points. The spatial resolution of the data is much higher than that of conventional surveying methods. Since laser scanners have a limited field of view, in order to obtain a complete representation of an object it is necessary to collect data from several different locations that must be transformed into a common coordinate system. Existing registration methods, such as the Iterative Closest Point (ICP) or Chen and Medioni&apos;s method, work well if good a priori alignment is provided. However, in the case of the registration of partially overlapping and unorganised point clouds without good initial alignment, these methods are not appropriate since it become very difficult to find correspondence. A method based on geometric primitives and neighbourhood search is proposed. The change of geometric curvature and approximate normal vector of the surface formed by a point and its neighbourhood are used to determine the possible correspondence of point clouds. Our method is tested with a simulated point cloud with various levels of noise and two real point clouds.
Recen t l y, a l ase r scanne r has been r ece i v i ng mo r e a t t en t i on as a use fu l t oo l fo r r ea l - t i me 3D da t a  acqu i s i t i on , and va r i ous app l i ca t i ons such as c i t y mode l i ng , DTM gene r a t i on and 3D mode l i ng o f  cu l t u r a l he r i t age we r e p r oposed .
We consider the k-server problem [23] in a distributed setting.
We present a high performance implementation of a belief propagation decoder for decoding low-density parity-check (LDPC) codes on a fixed point digital signal processor. A simplified decoding algorithm was used and a stopping criteria for the iterative decoder was implemented to reduce the average number of required iterations. This leads to an implementation with increased throughput compared to other implementations of LDPC codes or Turbo codes. This decoder is able to decode at 5.4Mbps on a Texas Instruments TMS320C64xx DSP running at 600MHz.
Multi-step quasi-Newton methods for optimisation (using data from more than one previous step to revise the current approximate Hessian) were introduced by Ford and Moghrabi in [4], where they showed how to construct such methods by means of interpolating curves. These methods also utilise standard quasi-Newton formulae, but with the vectors normally employed in the formulae replaced by others determined from a multi-step version of the Secant Equation. Some methods (the `Accumulative&apos; and `Fixed-Point&apos; approaches) for de  ning the parameter values which correspond to the iterates on the interpolating curve were presented in [5]. Both the Accumulative and the Fixed-Point methods measure the distances required to parameterise the interpolating polynomials via a norm de  ned by a positive-de  nite matrix M . The Fixed-Point algorithm which takes M to be the current approximate Hessian was found, experimentally, to be the best of the six multi-step methods studied in [5] (all of which exhibited improved numerical performance by comparison with the standard single-step BFGS method).
. The Golomb ruler problem has been proposed as a challenging constraint satisfaction problem. We consider a large number of different models of this problem, both binary and non-binary. The problem can be modelled using quaternary constraints, but in practice using a set of auxiliary variables and ternary constraints gives better results. A binary encoding of the problem gives a smaller search tree, but is impractical because it takes far longer to run. We compare variable ordering heuristics and consider the use of implied constraints to improve propagation. We believe that more case studies such as this are essential to reduce the skill currently required for successful modelling. 1 Introduction  In his AAAI-98 invited talk, Gene Freuder identified modelling as one of the major hurdles preventing the uptake of constraint satisfaction technology. The availability of non-binary constraints can increase the number of possible models of a problem amnd so makes modelling still more diffi...
A scalable approach to trust negotiation is required in Web service environments that have large and dynamic requester populations. We introduce Trust-Serv, a model-driven trust negotiation framework for Web services. The framework employs a model for trust negotiation that is based on state machines, extended with security abstractions. Our policy model supports lifecycle management, an important trait in the dynamic environments that characterize Web services. In particular, we provide a set of change operations to modify policies, and migration strategies that permit ongoing negotiations to be migrated to new policies without being disrupted. Experimental results show the performance benefit of these strategies. The proposed approach has been implemented as a containercentric mechanism that is transparent to the Web services and to the developers of Web services, simplifying Web service development and management as well as enabling scalable deployments.
Histograms and Wavelet synopses provide useful  tools in query optimization and approximate  query answering. Traditional histogram  construction algorithms, such as V-Optimal,  optimize absolute error measures for which  the error in estimating a true value of 10 by  20 has the same effect of estimating a true  value of 1000 by 1010. However, several researchers  have recently pointed out the drawbacks  of such schemes and proposed wavelet  based schemes to minimize relative error measures.
this paper we investigate the performance implications, with respect to meeting transaction deadlines, of providing security in a real-time database system. Database Security
The computation of consistency-based diagnoses is addressed by using OBDDs (Ordered Binary Decision Diagrams) for encoding both the system model and the resulting diagnoses.
The past few years have dramatically changed the view of high performance applications  and computing. While traditionally such applications have been targeted towards dedicated  parallel machines, we see the emerging trend of building &quot;meta-applications&quot; composed of  several modules that exploit heterogeneous platforms and employ hybrid forms of parallelism.  In particular, Java has been recognized as modern programming language for heterogeneous  distributed computing. In this paper we present OpusJava, a Java based framework for  Distributed High Performance Computing (DHPC) that provides a high level component  infrastructure and facilitates a seamless integration of high performance Opus (i.e., HPF)  modules into larger distributed environments. OpusJava offers a comprehensive programming  model that supports the exploitation of hybrid parallelism and provides high level coordination  means.  
In this paper, we study texture metamorphosis, or how to generate texture samples that smoothly transform from a source texture image to a target. We propose a patternbased approach to specify the feature correspondence between two textures, based on the observation that many texture images have stochastically distributed patterns which are similar to each other. First, the user selects a pattern in the source and target textures, and establishes the &quot;local feature correspondence&quot; between these two patterns by specifying landmarks. Then, repeated patterns are automatically detected and localized in the source and target textures. The &quot;pattern correspondence&quot; between two textures is formulated as an integer programming problem and solved using the Hungarian algorithm. Finally, we obtain a warp function between two textures by combining &quot;local feature correspondence&quot; and &quot;pattern correspondence&quot;. Experiments demonstrate that our technique produces visually appealing morphing sequences, with moderate amount of user interaction.
An assessment of neonatal outcome may be obtained from analysis of blood in the umbilical cord of an infant immediately after delivery. This can provide information on the health of the newborn infant and guide requirements for neonatal care, but there are problems with the technique. Samples frequently contain errors in one or more of the important parameters, preventing accurate interpretation and many clinical staff lack the expert knowledge required to interpret results. The development and validation of an expert system to overcome these difficulties is described. The initial development utilised conventional `crisp&apos; logic within the rule base and this system was evaluated to commercial release. This expert system validates the raw data, provides an interpretation of the results for clinicians and archives all the results, including the quality control and calibration data, for permanent storage. Subsequent development went on to incorporate fuzzy logic into part of the expert system knowledge base, but tests of this preliminary fuzzy system showed that it performed worse than the original crisp expert system. A tuning algorithm was then employed to modify the fuzzy model and this process resulted in improved performance to a level comparable to clinicians and superior to the crisp system. Finally, the entire knowledge base was converted to utilise fuzzy logic and this `integrated&apos; fuzzy expert system was validated against international expert opinion.
This paper focuses on large-scale distributed systems that can be modeled as workflows of activities sharing resources, knowledge, know-how and services. We propose a programming environment for such global activity-based applications the execution of which relies on the agent-technology. Methods and emerging technologies will be discussed from the user level applications to the run-time support. The programming environment we propose is the result of our experience in designing and implementing applications within specific application domains such as controlling industrial platforms and bioinformatics.
In this paper, the previous results that the performance of iterative  learning control(ILC) algorithm can be improved by adding a proportional  term and/or an integral term of error in D-type ILC algorithm are  generalized using an operator. Then, a sufficient condition for convergence  and robustness of the generalized ILC algorithm are investigated  against initial state error. As a special case of the operator, a nonlinear  ILC algorithm is also proposed and it is shown that the effect of initial  state error can be reached to zero in a given finite time. It is shown that  the bound of error reduction can be effectively controlled by tuning gains  of the proposed nonlinear ILC algorithm. In order to confirm validityof  the proposed algorithms, two examples are presented.
We present a Personalized Health Information Generation and Delivery System that leverages case based reasoning techniques to dynamically author a Personalized Health Information Package based on an individual&apos;s current health profile. The work features a compositional adaptation approach, whereby relevant health information elements from the solution component of multiple similar past cases are carefully selected and systematically combined to yield a new personalized health information package. We have implemented a generic Java-based case based reasoning engine that applies a novel compositional adaptation algorithm to author a HTML-based personalized health information package that can be emailed to users.
In this paper, we describe an automated method for fast, ground-based acquisition of large-scale 3D city models. Our experimental set up consists of a truck equipped with one camera and two fast, inexpensive 2D laser scanners, being driven on city streets under normal traffic conditions. One scanner is mounted vertically to capture building facades, and the other one is mounted horizontally. Successive horizontal scans are matched with each other in order to determine an estimate of the vehicle&apos;s motion, and relative motion estimates are concatenated to form an initial path. Assuming that features such as buildings are visible from both ground-based and airborne view, this initial path is globally corrected by Monte-Carlo Localization techniques. Specifically, the final global pose is obtained by utilizing an aerial photograph or a Digital Surface Model as a global map, to which the ground-based horizontal laser scans are matched. A fairly accurate, textured 3D cof the downtown Berkeley area has been acquired in a matter of minutes, limited only by traffic conditions during the data acquisition phase. Subsequent automated processing time to accurately localize the acquisition vehicle is 235 minutes for a 37 minutes or 10.2 km drive, i.e. 23 minutes per kilometer.
We present an algorithm for planning safe navigation strategies for biped robots moving in obstaclecluttered environments. From a discrete set of plausible statically-stable, single-step motions, a forward dynamic programming approach is used to compute a sequence of feasible footstep locations. In contrast to existing navigation strategies for mobile robots, our method is a global method that takes into account the unique ability of legged robots such as bipedal humanoids  to traverse obstacles by stepping over them. Heuristics designed to minimize the number and complexity of the step motions are used to encode cost functions used for searching a footstep transition graph. We show preliminary results of an experimental implementation of the algorithm using a model of the H6 humanoid navigating on an offce floor littered with obstacles.
Visual information retrieval (VIR) is a research area with more than 300 scientific publications every year. Technological progress lets surveys become out of date within a short duration. This paper intends to shortly describe selected important advances in VIR in recent years and point out promising directions for future research. A software architecture for visual media handling is proposed that allows handling image and video content equally. This allows to integrate both types of media in a singe system. The major advances in feature design are sketched and new methods for semantic enrichment are proposed. Guidelines are formulated for further development of feature extraction methods. The most relevant retrieval processes are described and an interactive method for visual mining is suggested that really puts &quot;the human in the loop&quot;. For evaluation, the classic recall- and precision-based approach is discussed as well as a new procedure based on MPEG-7 and statistical data analysis. Finally, an &quot;ideal&quot; architecture for VIR systems is outlined. The selection of VIR topics is subjective and represents the author&apos;s point of view. The intention is to provide a short but substantial introduction to the field of VIR.
Camera self calibration, based on a purely rotational movement of the camera, receives the most attention among different camera self-calibration methods due to its algorithmic simplicity. The existing purely rotational methods, however, assume camera rotates around its optical center, therefore yielding no translation offset. This assumption is not realistic, since in practice, the precise location of the optical center is often unknown, and the rotation is often performed about an unknown but fixed point near the optical center. The conventional methods tend to ignore the offset, and therefore, could lead to significant errors with the estimated camera parameters. In this paper, we introduce a new rotation-based camera self-calibration method, which explicitly accounts for the unknown translation offset. To this end, the problem is mathematically formulated and solved for differently taking the translation into consideration. To obtain the camera parameters with unknown camera rotations, our algorithm requires the camera to rotate around an unknown but fixed axis twice, by the same yet unknown angle. This is not an unreasonable assumption for precalibrating a camera on an active head. Experiments with both synthetic and real data show that the systematic errors caused by ignoring the translational offset will be effectively eliminated by our approach.
In this paper we propose an Rprop modification that builds  on a mathematical framework for the convergence analysis to equip Rprop  with a learning rates adaptation strategy that ensures the search direction  is a descent one. Our analysis is supported by experiments illustrating  how the new learning rates adaptation strategy works in the test cases to  ameliorate the convergence behaviour of the Rprop. Empirical results indicate  that the new modification provides benefits when compared against  the Rprop and a modification proposed recently, the Improved Rprop.
We present in this paper some criteria for empirical comparison of SDC methods for continuous microdata. Based on re-identification experiments, we try to optimize the tradeoff between information loss and disclosure risk. SDC methods compared include additive noise, distortion by probability distribution, microaggregation, resampling, rank swapping and a novel approach based on lossy compression. Generic information loss measures (not targeted to specific data uses) are defined, and two approaches to empirical re-identification are used: Euclidean record linkage and probabilistic record linkage. Some weighting schemes to aggregate information loss and disclosure risk measures are discussed and empirical results are given for one of them.
The annual construction of an Exam Timetable is a common problem for all institutions of higher education. Quite often it is done by hand or with the limited help of a simple administration system and usually involves taking the previous year&apos;s timetable and modifying it so it will work for the new year. Many British institutions are now introducing the concept of the modular degree. This gives the students much greater flexibility in what courses they take as well as giving a much greater choice. For the timetabler, this, and the recent growth in student numbers, means that the timetable will be more constrained than ever. It is no longer good enough to use the previous year&apos;s timetable. Every year a new timetable must be produced to take account of staff, student and course changes causing a necessarily large amount of administrative work. We present a prototype system for University Timetabling of both exams and courses based on the use of Genetic Algorithms. This will include an in...
This paper presents two sets of observations relating static and dynamic analysis. The first concerns synergies between static and dynamic analysis. Wherever one is utilized, the other may also be applied, often in a complementary way, and existing analyses should inspire different approaches to the same problem. Furthermore, existing static and dynamic analyses often have very similar structure and technical approaches. The second observation is that some static and dynamic approaches are similar in that each considers, and generalizes from, a subset of all possible executions. Researchers need to develop new analyses that complement existing ones. More importantly, researchers need to erase the boundaries between static and dynamic analysis and create unified analyses that can operate in either mode, or in a mode that blends the strengths of both approaches.
In this paper, we discuss issues in performability    modeling of &quot;software rejuvenation,&quot; a form of software fault tolerance based on occasionally cleaning up the operational environment. System factors which playakey role in such a model are identified. Among these, we commentontwo issues of particular interest when modeling software  rejuvenation:(1)  the representation of the degradation in operational environment, and,(2)  the inclusion in the model of the system monitor, on which the decision to rejuvenate is based. We also survey howeachofthesefactorshave been accounted for in previous performability models and show possible directions for future work.
The number of minimally invasive cardiological interventions has increased over the last few years and therefore computer based training systems find growing interest. They offer a better learning schedule compared to traditional master-apprentice models due to the repeatability of the learning  situation and the possibility to learn individual tasks.
The domain name system is the standard mechanism on the Internet to advertise and access important   information about hosts. At its inception, DNS was not designed to be a secure protocol. The biggest   security hole in DNS is the lack of support for data integrity authentication, source authentication, and   authorization. To make DNS more robust, a security extension of the domain name system (DNSSEC)   was proposed by the Internet Engineering task force (IETF) in late 1997. The basic idea of the DNS   security extension is to provide data integrity and origin authentication by means of cryptographic   digital signatures. However, the proposed extension suffers from some security flaws.   In this thesis, we discuss the security problems of DNS and its security extension. As a solution, we   present the design and implementation of a Byzantine-fault-tolerant domain name system. The system   consists of 3f+1 tightly coupled name servers and guarantees safety and liveness properties assuming   no more than f replicas are faulty within a small window of vulnerability. To authenticate   communication between a client and a server to provide per-query data authentication, we propose to   use symmetric key cryptography. To address scalability concerns, we propose a hierarchical   organization of name servers with a hybrid of iterative and recursive query resolution approaches. The   issue of cache inconsistency is addressed by designing a hierarchical cache with an invalidation   protocol using leases. Because of the use of hierarchical state partitioning and caching to achieve   scalability in DNS, we develop an efficient protocol that allows replicas in a group to request   operations from another group using very few messages. We show that the scalable Byzantine-fault-tol...
An implementation of a network based Intrusion Detection System using a SelfOrganizing Map (SOM) as a clustering tool is described. The system uses a SOM to classify Ethernet data in real-time. A graphical tool constantly displays the clustered data to reflect network activities. The impact of using  different techniques for data collection, data  preprocessing and classifier design is discussed. The system shows promise in its ability to classify regular traffic verses irregular traffic as a result of a Denial of Service (DoS) attack on a given host.
Vector quantization (VQ) has been used extensively in the past for image compression. The quantized image can be further compressed via a standard entropy coder (such as the arithmetic coder). In this paper, we present a simple equivalent to VQ, where unsupervised neural nets (NN) are used to find the appropriate codevectors. Furthermore, by imposing additional constraints to the VQ-NN system, we match the entropy coder characteristics and improve the overall image compression by an additional 10%.
In this paper we develop new and efficient algorithms for the problems of pattern matching and identification of repeated patterns in biological weighted sequences. Biological Weighted Sequences can model important biological processes such as the DNA Assembly Process or the DNA-Protein Binding Process. Thus, pattern matching or identification of repeated patterns in biological weighted sequences is a very important procedure in the translation of gene expression and regulation. Our results are based on a very elegant idea of Karp et al. and are both efficient and simple to implement.
Introduction  In a round of boxing, a fighter will make decisions and carry out actions to give himself an advantage, by blocking or evading punches while using well placed attacks to defeat an opponent in the ring. A boxer must constantly decide what action will give him the best chance of winning based on the perceived state of the opponent and the boxer&apos;s own physical state. These decisions must occur with little time for thought or analysis, using instinct and technique from training. The goal of this project is to capture an approximation to the reactive motion required of a boxer in a ring in real time.  1.1 Previous Work  There has been significant interest in developing behavioural systems for controlling synthetic characters. Tu and Terzopoulos&apos;s artificial fishes used a deterministic system built around an intention generator for making decisions [8]. In the same year, Karl Sims presented his evolutionary approach to creating and controlling characters, but they are limited t
Image indexing for biomedical content is a prohibitively expensive task if done manually. This leads to the demand for effective automated or computer assisted indexing methods. We are doing research on this problem for a set of digitized x-ray films from the second National Health and Nutrition Examination Survey (NHANES II). The 10,000 cervical spine and 7,000 lumbar spine images from this survey are retrievable through a resource we have developed, the Web-based Medical Information Retrieval System (WebMIRS). Using WebMIRS, the user may retrieve x-rays based on text data, such as demographics, health questionnaire responses, or physican&apos;s examination results. Two National Institutes of Health workshops have identified visual features of the images specifically related to osteoarthritis, but the images have never been indexed for these features, which include anterior osteophytes, disc space narrowing, and subluxation. We are investigating the methods by which the indexing and retrieval of the images using these particular features, may be achieved, in a validated manner acceptable to the biomedical community. In this paper we review our work to date, plus the work of collaborating researchers, and present some possible outcomes that appear to be realistic  1. 
Object-oriented programming (OOP) has proven a very useful paradigm for supporting client-server computing  within the context of local-area networks, where stable assumptions can be made about the available  resources and services and where interactions between clients and servers are relatively simple. By implementing  servers as objects, access to services can be kept separate from implementation, thus making client-server applications  both more flexible and easier to maintain. Now that we are moving from single enterprise computing  to the inter-organizational information world of the Internet and WWW, object-oriented programming must  adapt itself to new client-server requirements. Specifically, there is need of coping with situations where new  services can be dynamically added to servers, and where clients may need to coordinate the access to multiple  services, rather than to single individual ones. In this paper, we describe the object model of the Coordination  Language Facility, a programming framework that extends OOP with constructs that support dynamic services  and multi-service coordination. We illustrate the use of these constructs through the application domain of  distributed workflow.
This paper presents a new architecture of a classifier system for learning in virtual environments. The  model will be integrated in our multi-user platform to provide interaction between intelligent agents and  user clones. An agent is an autonomous entity equipped with sensors and effectors. Its behavior is guided  by rewards coming from the environment that produce rules called classifiers. The knowledge is shared  between agents by using the &quot;sending-message&quot; protocol to increase the global efficiency of the group.  The classifier system is specially adapted to a multi-task environment and incorporates a short-term  memory to record the recent events of the simulation. These ideas have been implemented and used to  develop a virtual soccer where the user plays with autonomous agents that combine communication and  evolution.  Keywords: Agent, Evolution, Adaptation, Classifier Systems, Virtual Environments.  1 INTRODUCTION  Learning methods for adaptation and evolution of  agents in...
Introduction  Despite its relative importance within the field of Human-Computer Interaction, there seems to be growing skepticism of the ability of cognitive science to enrich our understanding of the use of complex technologies. At the practical level, for example, it has been suggested that the designs and evaluation techniques used by actual designers of computer systems owe little to the models developed within cognitive science (Carroll 1990, Barnard 1991). Moreover, theoretically, cognitive science has been subject to wide ranging criticism concerning for example, its conceptualisation of social action (Winograd and Flores 1986, Lave 1988, Coulter 1989), its approach to user modelling (Suchman 1987, Gilbert 1987) and its intentional models of language and discourse (MciIlvenny 1990, Frohlich and Luff 1990). In line with these criticisms, several researchers have argued that studies of should move away from plan based, goal orientated models of system use and begin to consider th
Multiple classifier systems based on neural networks can give improved  generalisation performance as compared with single classifier systems. We examine
This paper reports on experiments exploring the application of a Stochastic  Optimality-Theoretic approach in the corpus-based learning of some aspects of  syntax. Using the Gradual Learning Algorithm, the clausal syntax of German has  to be learned from learning instances of clauses extracted from a corpus. The particular  focus in the experiments was placed on the usability of a bidirectional approach,  where parsing-directed, interpretive optimization is applied to determine  the target candidate for a subsequent application of generation-directed, expressive  optimization. The results show that a bidirectional bootstrapping approach  is only slightly less effective than a fully supervised approach.
We present an approach to integrating the refinement relation between infinite integer types (used in specification languages) and finite integer types (used in programming languages) into software verification calculi. Since integer types in programming languages have finite ranges, in general they are not a correct data refinement of the mathematical integers usually used in specification languages. Ensuring the correctness of such a refinement requires generating and verifying additional proof obligations. We tackle this problem considering Java and UML/OCL as example. We present a sequent calculus for Java integer arithmetic with integrated generation of refinement proof obligations. Thus, there is no explicit...
The reachability problem for cryptographic protocols with nonatomic keys can be solved via a simple constraint satisfaction procedure.
Chess playing programs running on small computers, such as PocketPCs, can beat most human players. This paper reports a feasibility study to determine if the techniques programs use to play chess can be applied to the more economically interesting problem of negotiation. This study allowed us to identify the essential differences between playing chess and negotiating and to demonstrate possible solutions to the problems we encountered.
This paper explores the use of multisensory information fusion technique with Dynamic Bayesian networks (DBNs) for modeling and understanding the temporal behaviors of facial expressions in image sequences. Our approach to the facial expression understanding lies in a probabilistic framework by integrating the DBNs with the facial action units (AUs) from psychological view. The DBNs provide a coherent and unified hierarchical probabilistic framework to represent spatial and temporal information related to facial expressions, and to actively select the most informative visual cues from the available information to minimize the ambiguity in recognition. The recognition of facial expressions is accomplished by fusing not only from the current visual observations, but also from the previous visual evidences. Consequently, the recognition becomes more robust and accurate through modeling the temporal behavior of facial expressions. Experimental results demonstrate that our approach is more admissible for facial expression analysis in image sequences.
. This paper presents how the image-based rendering technique of viewdependent texture-mapping (VDTM) can be efficiently implemented using projective texture mapping, a feature commonly available in polygon graphics hardware. VDTM is a technique for generating novel views of a scene with approximately known geometry making maximal use of a sparse set of original views. The original presentation of VDTM in by Debevec, Taylor, and Malik required significant per-pixel computation and did not scale well with the number of original images. In our technique, we precompute for each polygon the set of original images in which it is visibile and create a &quot;view map&quot; data structure that encodes the best texture map to use for a regularly sampled set of possible viewing directions. To generate a novel view, the view map for each polygon is queried to determine a set of no more than three original images to blended together in order to render the polygon with projective texture-mapping. Invisible t...
The problem we are trying to tackle in this research is that of the availability of critical applications while they are being updated.
rogramming (computation of normal forms) as well as logic programming (computation of answers). Essentially, it consists of computing an appropriate substitution such that when applied to the current goal it becomes reducible, and then reducing it [10].  This work has been partially supported by CICYT under grant TIC 95-0433-C03-03 and by HCM project CONSOLE.  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works, requires prior specific permission and/or a fee. Permissions
In this paper we address power conservation for clusters of workstations or PCs. Our approach is to develop systems that dynamically turn cluster nodes on -- to be able to handle the load imposed on the system efficiently -- and off -- to save power under lighter load. The key component of our systems is an algorithm that makes load balancing and unbalancing decisions by considering both the total load imposed on the cluster and the power and performance implications of turning nodes off. The algorithm is implemented in two different ways: (1) at the application level for a cluster-based, localityconscious network server; and (2) at the operating system level for an operating system for clustered cycle servers. Our experimental results are very favorable, showing that our systems conserve both power and energy in comparison to traditional systems.  1 
In aquatic ecosystems, species diversity is known to be higher in poor nutrient conditions. The enrichment of nutrition often induces the loss of biodiversity. This phenomenon is called the paradox of enrichment, since higher nutrient levels can support more species. Furthermore, the species diversity is usually high in most natural communities of phytoplankton. However, the niches of planktonic algae seem almost identical in apparently homogeneous, aquatic environments. Therefore, the high species diversity of phytoplankton is incomprehensible and called the paradox of plankton. Mathematical studies show that local coexistence of competitive species is rare. In a competitive community, the most superior species eliminates all the inferior species in the long run. Experimental results using chemostats also support this theoretical prediction. Thus we have no sound explanation for the local coexistence of many planktonic species in low nutrient conditions. Here we build a lattice model of ten planktonic species. All ten species are under competition for space in a relatively large lattice space. We report a few cases of simulation run. Simulation shows that, in an ecological time scale, coexistence of many species is observed when all species have low identical birth rates. We also show that, when the average birth rates are high, the most superior species exclude all the inferior species immediately. Our results suggest that competition for space does not function among species, when the densities of species are extremely low. The results of current simulation experiments may be related to the paradox of enrichment as well as that of plankton.
Web information retrieval is significantly more challenging than traditional well-controlled, small  document collection information retrieval. One main difference between traditional information retrieval  and Web information retrieval is the Web&apos;s hyperlink structure. This structure has been  exploited by several of today&apos;s leading Web search engines, particularly Google. In this survey paper,  we focus on Web information retrieval methods that use eigenvector computations, presenting the  three popular methods of HITS, PageRank, and SALSA.
Introduction  Traditional development of embedded systems separates hardware and software into independent design streams. Alternatively, co-design involves the interaction between the two streams, possibly once selection of the main hardware engine is complete  [1 ]  . Unfortunately, if the solution involves high-performance multi-processor architectures co-design requires considerable prior experience of architectural alternatives, topologies and programming environments. In fact, the large number of degrees of freedom in an open-ended co-design problem is equivalent to an NP-complete problem  [ 2 ]  .  Recently, system development based on a restriction of choice to pipelined processor farming (PPF) has proved successful for the domain of continuous-flow real-time vision systems  [ 3 ]  . In the systems of interest, data rather than control exigencies frequently dominate. Realtime constraints are soft, conditioned by various throu
Due to the nature of multiplicative Rayleigh fading, symmetric space time block codes, and joint estimation and detection schemes, isometry (ambiguities in channel estimation and data detection) degrades MIMO system performances. Training breaks isometry but reduces capacity. Asymmetric space time block code mitigates isometry by replacing training with data-bearing asymmetric codewords. This paper presents a continuous fading state space model, introduces a Kalman filter and innovations based joint estimation and detection scheme, and discusses the problem of and the solutions to isometry. The analysis and simulation demonstrate that asymmetric codes is a novel and efficient way to break isometry.
The Water Framework Directive requires to include public besides the water experts and policy makers into development and implementation of River Basin Management (RBM) plans (see Article 14). In such a context, the EU research project HarmoniCOP, studies a method to improve Public Participation based on Social Learning (SL) concepts. SL refers to the growing capacity of a social network to develop and perform collective actions. The different stakeholder groups in a basin are supposed to realize that a complex issue such as RBM can be better resolved in a collective way, taking account the diversity of interests, of mental frames, of knowledge and relying on disseminated information and knowledge. Information and Communication tools (ICtools) can play an important role to support the Social Learning dimension of the Public Participation. This paper presents a HarmoniCOP project synthesis of the definition of different concepts and proposes a framework of analysis. A first part consists in a preliminary qualitative characterization of the role of the IC-tools stemming from a bibliography analysis. Twenty IC-tools are already inventoried and four criteria are proposed: communication direction, public size, usage purpose (management of information and knowledge, elicitation of perspectives, interaction support and simulation), phases in the PP process. A second part presents a framework of analysis based on a joint approach of psychologists and engineering sciences experts. This framework will be tested in a number of empirical investigations to assess the tools used in historical and real-time case studies from three perspectives: their technical characteristics, their impact on PP and SL and their usability as perceived by the users. Finally, we present some perspecti...
Content delivery networks have evolved beyond traditional distributed caching. With services such as Akamai&apos;s EdgeComputing it is now possible to deploy and run enterprise business Web applications on a globally distributed computing platform, to provide subsecond response time to end users anywhere in the world. Additionally, this distributed application platform provides high levels of fault-tolerance and scalability ondemand to meet virtually any need. Application resources can be provisioned dynamically in seconds to respond automatically to changes in load on a given application.
The Aurora financial management system developed at the University of Vienna is  a modular decision support tool for portfolio and asset-liability management. It is based  on a multivariate Markovian birth-and-death factor model for the economic environment,  a pricing model for the financial instruments and an objective function which is flexible  enough to express risk aversion. The core of the
We propose a new way of browsing contextualized-news articles. Our prototype browser system is called a Time-based ContextualizedNews  Browser (T-CNB). The T-CNB concurrently and automatically presents a series of related pages for one news source while browsing the user-specified page. It extracts the past related pages from a user-specified news articles on the web. The related pages outline the progress of user-specified news articles. We call the related pages &apos;contextual pages&apos;. Using the T-CNB, a user only needs to specify one news article on the web. The user then automatically receives past related news articles, which provide a wider understanding of the topic. The T-CNB automatically generates and presents contextualized news articles.
Geography is frequently assigned a special status in disclosure risk evaluations and during  development of disclosure control treatments. There are two main reasons for this in the United  Kingdom, which apply to other countries to varying extents, depending on how administrative areas are  defined, and whether statistics for small areas are routinely produced.
This work presents the implementation of the Controlled Random Search (CRS) [1], [2] algorithm for global optimization within a parallel processing environment consisting of a cluster of multivendor workstations. This allows us to extract and optimize parameters or calibrate a general simulator, without needing the analytical form of the implemented model.
This paper considers compositions of conditional term rewriting systems as a basis for a  modular approach to the design and analysis of equational logic programs. In this context,  an equational logic program is viewed as consisting of a set of modules, each module defining  a part of the program&apos;s functions. We define a compositional semantics for conditional  term rewriting systems which we show to be rich enough to capture computational properties  related to the use of logical variables. We also study how such a semantics may be  safely approximated, and how the results of such approximations may be composed to yield  a bottom-up abstract semantics adequate for modular data-flow analysis. A compositional  analysis for equational unsatisfiability illustrates our approach.
Survivability architectures enhance the survivability of critical information systems by providing a mechanism that allows the detection and treatment of various types of faults. In this paper, we discuss four of the issues that arise in the development of such architectures and summarize approaches that we are developing for their solution.
Pictures or nature rarely present uniform colored regions. Most drawings include  shaded and/or color shaded surfaces. Therefore for any graphic designer the color  shading tool is of major importance. The tools included in a graphics library have to  respect two important conditions: they have to be fast and they have to be intuitive. Most
We analyze the security of the CTR + CBC-MAC (CCM)  encryption mode. This mode, proposed by Doug Whiting, Russ Housley,  and Niels Ferguson, combines the CTR (&quot;counter&quot;) encryption mode  with CBC-MAC message authentication and is based on a block cipher  such as AES. We present concrete lower bounds for the security of CCM  in terms of the security of the underlying block cipher. The conclusion  is that CCM provides a level of privacy and authenticity that is in line  with other proposed modes such as OCB.
We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using different learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection.
An evolutionary synthesis methodology was introduced with special concern for the design and optimization of distributed embodied systems. Its efficacy was validated in a case study on the design of collective sensory configurations for intelligent vehicles. Candidate sensory configurations were tested in sample traffic scenarios simulated in an embodied and sensor-based simulator, and in more abstracted and computationally efficient evaluation tests. Sample results evolved under different design preferences are presented, including approximate Pareto fronts representing the engineering design trade-offs characterizing the problem investigated in the case study.
This paper describes a new algorithm for disparity estimation using trinocular stereo. The three cameras are placed in a right angled configuration. A graph is then constructed whose nodes represent the individual pixels and whose edges are along the epipolar lines. Using the well known uniqueness and ordering constraint for pair by pair matches simultaneously, a path with the least matching cost is found using dynamic programming and the disparity filled along the path. This process is repeated iteratively until the disparity at all the pixels are filled up. To demonstrate the effectiveness of our approach, we present results from real world images and compare it with the traditional line by line stereo using dynamic programming.
The thorough evaluation of global optimization algorithms and software demands devotion, time and (hardware) resources, in addition to professional objectivity.
We describe a new way to model deletions on formal languages, called deletion along trajectories. We examine its closure properties, and show that it serves as an inverse to shuffle on trajectories, recently introduced by Mateescu et al. This leads to results on the decidability of equations of the form L T X = R, where L; R are regular languages and X is unknown. 1 
This paper introduces a new technique to detect the coronary arteries as well as other heart&apos;s peripheral vessels. After finding the location of the myocardium through a graph theoretic segmentation method, the algorithm models the heart with a biaxial ellipsoid. For each point of this ellipsoid, we compute the collection of intensities that are normal to the surface. This collection is then filtered to detect the cardiovascular structures. Ultimately, the vessels centerline points are detected using a vessel tracking algorithm, and linked together to form a complete coronary artery tree.
 This proposal suggests a redesign of the teaching of programming and othe  oftware topics in universities on the basis of object-oriented principles. It argues  t  that the new &quot;inverted curriculum&quot; should give a central place to libraries, and  ake students from the reuse consumer&apos;s role to the role of producer through a  process of &quot;progressive opening of black boxes&quot;.
Photogrammetry is the art and science of deriving accurate 3-D metric and descriptive object information from multiple analogue and digital images. Recently, there has been an increasing interest in utilizing imagery in different fields such as archaeology, architecture, mechanical inspection, and surgery. The availability of reasonably priced, off-the-shelf, and high-quality digital cameras is encouraging such interest. Any camera needs to be accurately calibrated and tested before it can be used for accurate derivation of 3-D information. Traditional camera calibration is performed by qualified and trained professionals using a test field with numerous control points (i.e., points with known ground coordinates). To expedite the process of camera calibration, this paper outlines a new approach that is based on linear features within an easy-to-establish test field. Therefore, non-photogrammetrists can utilize the new calibration procedure with the least effort. Moreover, the simplicity of the calibration procedure allows for the evaluation of the camera stability through analyzing the estimated internal characteristics of the implemented camera from repetitive calibration sessions. The proposed technique is considerably more flexible and possesses higher degree of robustness when compared with traditional camera calibration exercises. The introduced calibration methodology allows for the utilization of digital cameras in a vast range of application areas (e.g., three dimensional archiving of models and monuments as well as the capability of generating 3-D perspective views). This paper introduces the calibration procedure, some analysis of the expected accuracy, and the suggested methodology for 3-D modelling of historical sites. Experimental results using real data...
In this paper, we analyze the role of the social group in a Ubiquitous Computing (Ubicomp) environment as a source of contextual information. A model is presented to address the social group member&apos;s perceptions of how devices in a Ubicomp environment should aid them in collaboration. Based on the model a distributed context-aware group membership management scheme is developed. We then present a prototype implementation for a context-aware ephemeral group membership management scheme, along with a sample application, and experimental results that demonstrate the feasibility of our system.
Proving or disproving faithfulness (a property describing robustness to rational manipulation in action as well as information revelation) is an appealing goal when reasoning about distributed systems containing rational participants. Recent work formalizes the notion of faithfulness and its foundation properties, and presents a general proof technique in the course of proving the ex post Nash faithfulness of a theoretical routing problem [11]. In this paper, we use...
An investigation has been conducted on two well known similarity-based learning approaches to text categorization: the k-nearest neighbor (k-NN) classifier and the Rocchio classifier. After identifying the weakness and strength of each technique, a new classifier called the kNN model-based classifier (kNNModel) has been proposed. It combines the strength of both k-NN and Rocchio.
Programs written in garbage-collected languages like Java often have large working sets and poor locality. Worse, a full garbage collection must visit all live data and is likely to touch pages that are no longer resident, triggering paging. The result is a pronounced drop in throughput and a spike in latency. We show that just a slight reduction in available memory causes the throughput of the SPECjbb benchmark to drop by 66% and results in garbage collection pauses lasting for several seconds.
Respiratory motion compensation for cardiac imaging requires knowledge of the heart&apos;s motion and deformation during breathing. We propose a method for measuring the natural tidal respiratory motion of the heart using free breathing coronary angiograms. A 3D deformation field describing the cardiac and respiratory motion of the coronary arteries is recovered from a biplane acquisition. Cardiac and respiratory phase are assigned to the images from an ECG signal synchronized to the image acquisition, and from the diaphragmatic displacement as observed in the images. The resulting motion field is decomposed into cardiac and respiratory components by fitting the field with periodic 2D parametric functions, where one dimension spans one cardiac cycle, and the second dimension spans one respiratory cycle. The method is applied to patient datasets, and an analysis of respiratory motion of the heart is presented.
Signal Transition Graphs (STGs) are one of the most popular models for the specification of asynchronous circuits. A STG can be implemented if it admits a so-called consistent and complete binary encoding. Checking this is EXPSPACE-hard for arbitrary STGs, and so a lot of attention has been devoted to the subclass of free-choice STGs, which offers a good compromise between expressive power and analizability. In the last years, polynomial time synthesis techniques have been developed for free-choice STGs, but they assume that the STG has a consistent binary encoding. This paper presents the first polynomial algorithm for checking consistency.
In this paper we present a tutorial overview of some of the issues that arise in the design of switched linear control systems. Particular emphasis is given to issues relating to stability and control system realisation. A benchmark regulation problem is then presented. This problem is most naturally solved by means of a switched control design. The challenge to the community is to design a control system that meets the required performance specifications and permits the application of rigorous analysis techniques. A simple design solution is presented and the limitations of currently available analysis techniques are illustrated with reference to this example. 1. Introductory remarks  Recent years have witnessed an enormous growth of interest in dynamic systems that are characterised by a mixture of both continuous and discrete dynamics. Such systems are commonly found in engineering practice and are referred to as hybrid or switching systems. The widespread application of such systems is motivated by ever increasing performance requirements, and by the fact that high performance control systems can be realised by switching between relatively simple LTI systems. However, the potential gain of switched systems is offset by the fact that the switching action introduces behaviour in the overall system that is not present in any of the composite subsystems. For example, it can be easily shown that switching between stable sub-systems may lead to instability or chaotic behaviour of the overall system, or that switching between unstable sub-systems may result in a stable overall system. In this paper we present a tutorial introduction to the design of switched linear systems. We begin by discussing how switching arises naturally in many situations. Examples include: the de...
We present a gateway between the WWW and the Gnutella peer-topeer network that permits searchers on one side to be able to search and retrieve files on the other side of the gateway. This work improves the accessibility of files across different delivery platforms, making it possible to use a single search modality. We outline our design and implementation, present access statistics from a test deployment and discuss lessons learned.
During the last decade, artificial neural networks (ANNs) have reached maturity and established themselves as a useful tool for information processing, especially for complex data, where prior knowledge and models are limited. Unfortunately, ANNs are themselves also complex, usually consisting of many interconnected non-linear processing units (neurons), what makes them hard to inspect and analyze. As a part of Baden-Wrttemberg state project VirtuGrade, we have developed a number of interactive visualization tools for educational purposes. They help users to gain insight of what is happening inside a neural network and improve their understanding of the subject. As a platform, we use Java applets, embedded in an on-line book covering the topic.  INTRODUCTION  Artificial neural networks (ANNs), or simply neural networks, have been developed since the mid-fifties, following theoretical foundations laid already in the early forties. Inspired by their biological counterparts, artificial ne...
This paper presents a search architecture that combines classical search techniques with spread activation techniques applied to a semantic model of a given domain. Given an ontology, weights are assigned to links based on certain properties of the ontology, so that they measure the strength of the relation. Spread activation techniques are used to find related concepts in the ontology given an initial set of concepts and corresponding initial activation values. These initial values are obtained from the results of classical search applied to the data associated with the concepts in the ontology. Two test cases were implemented, with very positive results. It was also observed that the proposed hybrid spread activation, combining the symbolic and the sub-symbolic approaches, achieved better results when compared to each of the approaches alone.
Linear grouping is a technique where linear relationship information is gathered for a set of  rigid points. This information is used to accelerate vertex transformation, vertex clipping, texture  transformation, etc. The linear grouping technique is an optimization problem, originally  implemented in a best-first branch-and-bound search. In this paper, we introduce several new  extensions and improvements to linear grouping. In particular, the search algorithm is greatly  improved.     Keywords: real-time rendering, visualization, rendering hardware, transformation, level of detail,   OpenGL, linear grouping.       1. 
this paper to investigate the comparative performances of two global optimization techniques, genetic algorithms and simulated annealing, in underwater inversion problems. Our interest lies in accurate and simultaneously computationally efficient inversion for both environmental and geometrical parameters. The methods are evaluated in terms of their ability to obtain the global minimum, the speed of approaching that, and the variation in the inversion results
Modeling subspaces of a distribution of interest in high dimensional spaces is a challenging problem in pattern analysis. In this paper, we present a novel framework for pose invariant face detection through multi-view face distribution modeling. The approach is aimed to learn a set of low-dimensional subspaces from an originally nonlinear distribution by using the mixtures of probabilistic PCA [16]. From the experiments, we found the learned PPCA models are of low dimensionality and exhibit high local linearity, and consequently offer an efficient representation for visual recognition. The model is then used to extract features and select &quot;representative&quot; negative training samples. Multi-view face detection is performed in the derived feature space by classifying each face into one of the view classes or into the nonface class, by using a multi-class SVM array classifier. The classification results from each view are fused together and yields the final classification results. The experimental results demonstrate the performance superiority of our proposed framework while performing multi-view face detection.
Vertebra shape can effectively describe various pathologies found in spine x-ray images. There are some critical regions on the shape contour which help determine whether the shape is pathologic or normal. We selected a subset of 250 segmented vertebra boundaries for study from a collection of 17,000 digitized x-rays of cervical and lumbar spine taken as a part of the second National Health and Nutrition Examination Survey (NHANES II). A board certified expert radiologist marked nine morphometric landmark points on the contour of these cervical and lumbar images. Image indexing could mimic the model used by the radiologists to mark the images, e.g. 6-, 9-, or 10-point, thereby improve the query and retrieval of vertebra shapes from the image database. In this paper, we present a technique to automatically select nine points from the boundary contour. The comparison between two 9-point models using the 2 L distance and retrieval rank results derived respectively from the 9-point model marked by the expert and the 9-point model selected with our algorithm provides a good measure of how well the two models match.
This paper presents an action selection framework  based on an assemblage of self-organizing neural  networks called Cooperative Extended Kohonen  Maps. This framework encapsulates two features  that significantly enhance a robot&apos;s action selection  capability: self-organization in the continuous state  and action spaces to provide smooth, efficient and  fine motion control; action selection via the cooperation  and competition of Extended Kohonen Maps  to achieve more complex motion tasks. Qualitative  and quantitative comparisons for single- and multirobot  tasks show our framework can provide better  action selection than do potential fields method.
Cellular automata (CA) are an abstract model of  ne-grain parallelism, as the node  update operations are rather simple, and therefore comparable to the basic operations of  the computer hardware. In a classical CA, all the nodes execute their operations in  parallel and in perfect synchrony. We consider herewith the sequential version of  CA, called SCA, and compare these SCA with the classical, parallel CA. In  particular, we show that there are 1-D CA with very simple node state update rules  that cannot be simulated by any comparable SCA, irrespective of the node update  ordering. Consequently, the  ne granularity of the basic CA operations and, therefore,  the  ne-grain parallelism of the classical, synchronous CA, insofar as the \interleaving  semantics&quot; is concerned, is not  ne enough. We also share some thoughts on how to  extend the results herein, and, in particular, we try to motivate the study of genuinely  asynchronous cellular automata.
Some existing approaches to multi-application smart card  design rely on the card containing data and importing the code  of functions (methods) to be performed on data. A complementary  solution is proposed in this paper to relax the requirement ---or rather  bottleneck--- that all confidential data and processing be supported  by the card. Our approach is based on running some applications  outside the card using encrypted data processing, specifically privacy  homomorphisms. Examples of privacy homomorphisms are given, one of  which is very recent and allows full arithmetic on encrypted data while  remaining secure against known-cleartext attacks.
The standard models used to generate random binary constraint satisfaction  problems are described. At the problem sizes studied experimentally, a  phase transition is seen as the constraint tightness is varied. However, Achlioptas   et al. showed that if the problem size (number of variables) increases  while the remaining parameters are kept constant, asymptotically almost all  instances are unsatisable. In this paper, an alternative scheme for one fo the  standard models is proposed in which both the number of values in each variables  &apos;s domain and the average degree of the constraint graph are increased  with problem size. It is shown that with this scheme there is asymptotically  a range of values of the constraint tightness in which instances are trivially  satisable with probability at least 0.5 and a range in which instances are  almost all unsatisable; hence there is a crossover point at some value of the  constraint tightness between these two ranges. This scheme is compa...
Annotating biomedical text for Named Entity Recognition (NER) is usually a tedious and expensive process, while unannotated data is freely available in large quantities. It therefore seems relevant to address biomedical NER using Machine Learning techniques that learn from a combination of labelled and unlabelled data. We consider two approaches: one is discriminative, using Support Vector Machines, the other generative, using mixture models. We compare the two on a biomedical NER task with various levels of annotation, and different similarity measures. We also investigate the use of Fisher kernels as a way to leverage the strength of both approaches. Overall the discriminative approach using standard similarity measures seems to out-perform both the generative approach and the Fisher kernels.
The Virtual Interface Architecture (VIA) was introduced to define a common set of features that are suitable to build high--speed networks. Today the interface of VIA serves as access point to a wide range of system area networks. M-VIA is a software that provides the VIA interface on top of several Ethernet cards. The overhead of TCP/IP protocols is avoided by running M-VIA. To benefit from this performance we developed a LAM/MPI module that utilizes VIA functions to transmit data. The key concepts of data transmission and memory management are presented by this paper. Furthermore a comparative performance analysis is included.
The UMLS® Metathesaurus®, the largest thesaurus in
the biomedical domain, provides a representation of
biomedical knowledge consisting of concepts classified
by semantic type and both hierarchical and nonhierarchical
relationships among the concepts. This
knowledge has proved useful for many applications
including decision support systems, management of
patient records, information retrieval (IR) and data
mining. Gaining effective access to the knowledge is
critical to the success of these applications. This
paper describes MetaMap, a program developed at
the National Library of Medicine (NLM) to map biomedical
text to the Metathesaurus or, equivalently, to
discover Metathesaurus concepts referred to in text.
MetaMap uses a knowledge intensive approach based
on symbolic, natural language processing (NLP) and
computational linguistic techniques. Besides being
applied for both IR and data mining applications,
MetaMap is one of the foundations of NLM’s Indexing
Initiative System which is being applied to both semiautomatic
and fully automatic indexing of the biomedical
literature at the library.
We deal with randomized competitive algorithms for non-preemptive call control on tree-like switching networks. We give an optimal O(log n) competitive algorithm for non-preemptive call scheduling on trees. We then extend the problem to include variable call rates, call durations, and arbitrary call benefits, and obtain a polylog competitive algorithm. We also show that many similar algorithms for different problems that can deal with constant values of parameters such as rates and benefits can be transformed into randomized algorithms that can deal with varying values of the parameters.
We propose a very concise calculus for modelling object-oriented concepts such as objects, message sending, encapsulation and incremental modification. We show how to deal with recursion and provide some object-oriented examples. State and encapsulated inheritance are modelled by means of an incremental modification operator.
This paper introduces three new heuristics for the Euclidean Traveling Salesman Problem (TSP). One of the heuristics called Initialization Heuristics (IH) is applicable only to the Euclidean TSP, while other two heuristics RemoveSharp and LocalOpt can be applied to all forms of symmetric and asymmetric TSPs. A Hybrid Genetic Algorithm (HGA) has been designed by combining a variant of an already existing crossover operator with these heuristics. One of the heuristics is for generating initial population, other two are applied to the offspring either obtained by crossover or by shuffling. The last two heuristics applied to offspring are greedy in nature, hence to prevent getting struck up at local optimum we have included proper amount of randomness by using the shuffling operator. We studied the effect of these heuristics by conducting experiments, which show that the results obtained by our Hybrid GA outperformed the results obtained by existing GA in certain problems. These heuristics matched &quot;Best Known&quot; solutions in most cases. In others it produced results with one% tolerance, when compared with those of nature-inspired algorithms such as Simulated Annealing (SA), Evolutionary Computation (EP) and Ant Colony System (ACS). Implementation of these heuristics is simple. Our convergence rate is found to be high and the optimal solution is obtained in a fewer number of iterations.
In [6] solutions for the nesting problem are produced using the No Fit Polygon (NFP), simulated  annealing (SA) and a new evaluation method. It showed that SA could out perform hill climbing, thus  suggesting that evolutionary approaches produce better solutions than a standard search algorithm. In this  paper this work is developed. Genetic algorithms (GA) and tabu search (TS) are compared with the results  already obtained with SA. The evaluation method is described, along with a description of the NFP.  Computational results are given.  Keywords : Evolutionary, Nesting, No Fit Polygon  1. Introduction  The nesting problem can be described as having to place a number of smaller shapes onto a larger shape (sometimes called a stock sheet). In doing so the smaller pieces must not overlap one another and must stay within the confines of the larger shape. The usual objective in placing the smaller pieces is to minimise the waste of the larger shape. Only two dimensions (height and width) ...
The computation of large problems in scientifical and industrial fields needs for effcient computer systems. A rising number parallel computers are used to deliver necessary computation resources. The physical restrictions of circuit integration limit the speed of single processor solutions. The use of consumer components is a cost--effective way to built a parallel computer. The deployment of a high--speed network enables most parallel applications to run fast. Optimized communication libraries are important to enable the use of high--speed networks with parallel applications. This field is subject to active research and development.
Since many applications require the verification of large sets  of signatures, it is sometimes advantageous to perform a simultaneous  verification instead of checking each signature individually. The simultaneous  processing, called batching, must be provably equivalent to the  sequential verification of all signatures.
We present a new global optimization approach for solving exactly  or inexactly constrained distance geometry problems. Distance geometry  problems are concerned with determining spatial structures from  measurements of internal distances. They arise in the structural interpretation  of nuclear magnetic resonance data and in the prediction  of protein structure. These problems can be naturally formulated as  global optimization problems which generally are large and difficult.  The global optimization method that we present is related to our previous  stochastic/perturbation global optimization methods for finding  minimum energy configurations, but has several key differences that  are important to its success. Our computational results show that  the method readily solves a set of artificial problems introduced by  Mor&apos;e and Wu that have up to 343 atoms. On a set of considerably  more difficult protein fragment problems introduced by Hendrickson,  the method solves all the problems w...
Emerging high-lg el hardware description and synthesis technolVV&quot;. in conjunction with fiel$&quot;&quot;&quot;---.[---LV$ff. gate arrays (FPGAs) have significantl l owered the threshol for hardware devel$$ffq t.Opportunities exist to integrate these technolDH.[ into a tool forexplq---$q and evalH&quot;ff ing microarchitectural designs.This paper presents a case study in devel$D------ the synthesizabl high-la el model of a superscal. processor and producing a working prototype in FPGA.Using an experimental operation-centric hardware  descriptionl anguage, we have created the synthesizabl model of a superscalH specul&quot;$--- e out-of-order core for the integer subset ofSimpl$.[ffLD PISA.A prototypeimplff mentation is produced by synthesizing the high-l&quot; el model for the Spyder FPGA prototyping board.In addition, we have modified thebasel---V processor model to create derivative processor designs that addnewl proposed experimental  mechanisms.The derivative model are useful both in testing the compl.[Hq&quot;&quot; and correctness of new mechanisms and in assessing the mechanisms&apos; impact onimpl&quot;L$ tation area and cycl time.
In this paper, we present the middleware architecture of MAFTIA, an  ESPRIT project aiming at developing an open architecture for transactional  operations on the Internet. The former is a modular and scalable  cryptographic group-oriented middleware suite, suitable for supporting  reliable multi-party interactions under partial synchrony models, and subject  to malicious as well as accidental faults.  1 Introduction  In this paper, we present the middleware architecture of MAFTIA, a project aiming at developing an open architecture for transactional operations on the Internet. MAFTIA exploits common approaches to fault tolerance, of both accidental and malicious faults. The middleware platform is a distributed, modular and scalable cryptographic group-oriented suite, suitable for supporting reliable multi-party interactions under partial synchrony models, subject to malicious as well as accidental faults. A combination of intrusion prevention and tolerance measures is sought, under a ...
Introduction  Tables of business magnitude data, which are the main output from business surveys, have a relatively high disclosure risk. Business populations tend to be skewed, with many small and medium-sized businesses and just a few very dominant businesses. This means that, for sampling efficiency, the large businesses are usually in full-coverage strata, so there is no confidentiality protection from sampling for these units. There is generally good public knowledge about the industry, size and region of businesses, and their approximate market share. This information can enable close approximations of confidential information for those cells dominated by just a few large businesses. In particular businesses can use their own data to deduce the characteristics of other businesses in the same cell. Cell suppression is a common method for protecting tables of business magnitude data. Cell suppression is a two-stage process. A dominance rule, such as the (n,k) rule, is used to ident
This project identified the common weakness of different commercial PDM systems in the market and critical issues need to solve to meet the requirements of dynamic design environment. To address these issues, an innovative design modelling methodology was developed. With the new modelling methodology as underlying foundation, a design process and information management system could have advanced capabilities including data exchange at fine granularity level, loosely coupled data flow and workflow, rapid deployment and reconfiguration etc. To demonstrate the merits of the new modelling methodology, a generic platform for fast deployment of design process and information management system was developed. Finally, a prototype of water heater design management system was developed as show case using the generic platform.
The main objective of this paper is to introduce a novel method of feature extraction for character data and develop a neural network system for recognising different Latin characters. In this paper we describe feature extraction, neural network development for character recognition and perform further neural network analysis on noisy image segments to explain the qualitative aspects of handwriting.  1. Introduction  The recognition of hand-written characters using artificial techniques including neural networks is an important area of research for realising commercial advances in OCR technology. In the past neural networks have been used for character recognition for classifying written input from different sources. In this paper we describe a novel method of feature extraction for recognising Latin characters. The character image is segmented for extracting primitives in each segment. The extracted primitives are used for training a three layered neural network for recognising unseen...
Commitment schemes are building blocks for guaranteeing fairness in higher-level cryptographic protocols such as mental poker protocols and others. A party Alice commits to a value v (a bit or a bitstring) without revealing it. Alice should not be able to cheat by opening the commitment as   v nor to deny having committed at all. Most commitment schemes in the literature rely on hash functions, which should be strongly collision-free for the scheme to be secure. Yet collision-freeness can only be empirically checked and cannot be met with total certainty. We present a commitment scheme which avoids hash functions by using a public-key cryptosystem instead.
The Web Ontology Language (OWL) defines three classes of documents: Lite, DL and Full. All RDF/XML documents are OWL Full documents, some OWL Full documents are also OWL DL documents, and some OWL DL documents are also OWL Lite documents. This paper discusses parsing and species recognition -- that is the process of determining whether a given document falls into the OWL Lite, DL or Full class. We describe two alternative approaches to this task, one based on abstract syntax trees, the other on RDF triples, and compare their key characteristics.
This paper describes a finite-state morphological analyzer of Modern Standard Arabic words that can be tested on the Internet. An overview of the system is provided, including the history, the finite-state technology, and the dictionary coverage. This research system is scheduled for testing and commercial development in 2001.
This work presents a design for a human computer interface capable of recognizing 25 gestures from the  international hand alphabet in real-time. Principal Component Analysis (PCA) is used to extract features from  images of gestures. The features represent gesture images in terms of an optimal coordinate system, in which  the classes of gestures make up clusters. The system is divided into two parts: an off-line and an on-line part.
We study the maximum flow possible between a single-source and multiple terminals in a random graph (modeling a wired network) and a random geometric graph (modeling an ad-hoc wireless network). We show for both models that the maximum flow is tightly concentrated around its mean. Specifically for n nodes in a network such that any two nodes have a probability p of connectivity, the maximum flow between a single source and multiple terminals is approximately (n-1)p for large n. Similar results are obtained for the case of n randomly located nodes on the unit square.
We present in this paper the first empirical comparison of SDC methods  for microdata which encompasses both continuous and categorical microdata. Based  on re-identification experiments, we try to optimize the tradeoff between information  loss and disclosure risk. First, relevant SDC methods for continuous and categorical  microdata are identified. Then generic information loss measures (not targeted to  specific data uses) are defined, both in the continuous and the categorical case.
In a recent paper, T. Kohonen and P. Somervuo have shown that self-organizing maps (SOMs) are not restricted to numerical data. They can also be defined for symbol strings, provided that one defines an average function for strings and that the adaptation process is performed off-line (batch). In this paper, we present two different methods for computing averages of strings, as well as an on-line version of the self-organizing map for strings. Both methods for computing averages are faster than the original one used by Kohonen and Somervuo, and one of them is suitable for on-line computation.
This paper presents a rectilinear path finding algorithm for two-dimensional grids, given two distinct points and a set of obstacles. The algorithm finds applications in Computer Aided Design (CAD) of Very Large Scale Integration (VLSI) and robotic navigation in constrained environments
Users of computing devices are increasingly likely to be subject to situationally  determined distractions that produce exceptionally high cognitive load. The question  arises of how a system can automatically interpret symptoms of such cognitive load in the  user&apos;s behavior. This paper examines this question with respect to systems that process  speech input. First, we synthesize results of previous experimental studies of the ways in  which a speaker&apos;s cognitive load is reflected in features of speech. Then we present a conceptualization  of these relationships in terms of Bayesian networks. For two examples of  such symptoms---sentence fragments and articulation rate---we present results concerning  the distribution of the symptoms in realistic assistance dialogs. Finally, using artificial data  generated in accordance with the preceding analyses, we examine the ability of a Bayesian  network to assess a user&apos;s cognitive load on the basis of limited observations involving these  two symptoms.
A model was introduced in [Fraga97] for integrating replication techniques in heterogeneous systems. The model adopts a reflective structure based on the meta-object approach [10]. Also, this model is founded in ORBs that support group communication in heterogeneous environments. The OMG still does not have specifications for fault tolerance. The MetaFT model has a great flexibility allowing, for instance, to modify the coordination protocols according to the fault tolerance level desired, without any implications for the application code. The advantage is that it allows to use different meta-object protocol to assist different quality of service (QoS) parameters to fault tolerance. This paper explicit our experiences in developing replication techniques following the model using two different CORBA platforms and presents some performance analyzes.
Position-based routing in a mobile ad hoc network requires geographic addresses. Thus, a  node that wants to send a packet to some target node has to know the target&apos;s (approximate) current  position. In order to provide each node&apos;s position to the other network nodes, a distributed location  service has to be used. J. Li et al. recently put forward a promising approach called the `Grid Location  Service&apos; (GLS). In this paper we provide some analyses and evaluations of GLS by means of simulation  with ns-2 beyond the results of the original paper. We present quantitative results with respect to location  query failure rate and bandwidth consumption. We analyze in detail why queries failed and how the  query failure rate can be decreased for scenarios with a low density of nodes.
Static program checking tools can find many serious bugs in software, but due to analysis limitations they also frequently emit false error reports. Such false positives can easily render the error checker useless by hiding real errors amidst the false. Effective error report ranking schemes mitigate the problem of false positives by suppressing them during the report inspection process [17, 19,20]. In this way, ranking techniques provide a complementary method to increasing the precision of the analysis results of a checking tool. A weakness of previous ranking schemes, however, is that they produce static rankings that do not adapt as reports are inspected, ignoring useful correlations amongst reports. This paper addresses this weakness with two main contributions. First, we observe that both bugs and false positives frequently cluster by code locality. We analyze clustering behavior in historical bug data from two large systems and show how clustering can be exploited to greatly improve error report ranking. Second, we present a general probabilistic technique for error ranking that (1) exploits correlation behavior amongst reports and (2) incorporates user feedback into the ranking process. In our results we observe a factor of 2-8 improvement over randomized ranking for error reports emitted by both intra-procedural and inter-procedural analysis tools.
Various concurrency control algorithms differ in the time when conflicts are detected, and in  the way they are resolved. In that respect, the Pessimistic and Optimistic Concurrency Control  (PCC and OCC) alternatives representtwo extremes. PCC locking protocols detect conflicts as  soon as they occur and resolve them using blocking. OCC protocols detect conflicts at transaction  commit time and resolve them using rollbacks (restarts). For real-time databases, blockages  and rollbacks are hazards that increase the likelihood of transactions missing their deadlines.
Rate control is an important issue in video streaming applications for both wired and wireless networks. A widely accepted rate control method in wired networks is equation based rate control [1], in which the TCP Friendly rate is determined as a function of packet loss rate, round trip time and packet size. This approach, also known as TFRC, assumes that packet loss in wired networks is primarily due to congestion, and as such is not applicable to wireless networks in which the bulk of packet loss is due to error at the physical layer. In this paper, we propose multiple TFRC connections as an end-to-end rate control solution for wireless video streaming. We show that this approach not only avoids modifications to the network infrastructure or network protocol, but also results in full utilization of the wireless channel. NS-2 simulations and experiments over 1xRTT CDMA wireless data network are carried out to validate, and characterize the performance of our proposed approach.
We provide here an overview of the new and rapidly emerging research area of privacy preserving data mining. We also propose a classification hierarchy that sets the basis for analyzing the work which has been performed in this context. A detailed review of the work accomplished in this area is also given, along with the coordinates of each work to the classification hierarchy. A brief evaluation is performed, and some initial conclusions are made.
We present PINCO, an in-network compression scheme for energy constrained, distributed, wireless sensor networks. PINCO reduces redundancy in the data collected from sensors, thereby decreasing the wireless communication among the sensor nodes and saving energy. Sensor data is buffered in the network and combined through a pipelined compression scheme into groups of data, while satisfying a user-specified end to end latency bound. We introduce a PINCO scheme for single-valued sensor readings. In this scheme, each group of data is a highly flexible structure so that compressed data can be recompressed without decompressing, in order to reduce newly available redundancy at a different stage of the network. We discuss how PINCO paremeters affect its performance, and how to tweak them for different performance requirements. We also include a performance study demonstrating the advantages of our approach over other data collection schemes based on simulation and prototype deployment results. I. 
Context has many aspects, which may vary widely, such as the device, environment and user. The perception of data in different contexts also varies widely. We present a new, flexible approach to meeting needs and limits arising from context: contextual mediation. In this paper limits are defined as goals over managed system resources. These can be met by the selection of data, taking into account preferences over its semantic and syntactic properties. The specification of this selection is presented in detail and the supporting framework is described. We illustrate our description with examples from a context-aware map application and present experimental results and experiences which demonstrate that contextual mediation enhances the usability of the application in restrictive contexts of use. Keywords Context awareness  Contextual mediation   Map adaptation  Specification of adaptation  1 
In this paper we present a formal approach to modelling context  effect in perceptual domains: namely how the perception of an object  is effected by other objects. Our approach is operational and perceptually  motivated in the sense that we focus on how objects are perceived  as being constructed from certain components. Based on a psychological  theory of perception, called Structural Information Theory, we develop  an algebraic model for context sensitive perception. We illustrate our  model by using the domain of alphabetic strings and discuss its extension  to the domain of visual objects. Finally, we remark on how this approach  can be applied to model context effect in non-perceptual situation, and  then make some observations on the general problem of context.
Flow sensitive static analyses are often more precise than  their flow insensitive counterparts, but much more expensive. To cope  with this problem, we defined, some years ago, a watchpoint semantics  as an abstract interpretation of a trace semantics. It is not itself a trace  semantics, but it lets us specify flow and control sensitive analyses which  are focused on a given set of program points only, called watchpoints. This means that
This paper presents a fast approach for vision-based self-localization in RoboCup. The vision system extracts the features required for localization without processing the whole image and is a first step towards independence of lighting conditions. In the field of self-localization, some new ideas are added to the well-known MonteCarlo localization approach that increase both stability and reactivity, while keeping the processing time low.
The paper is concerned with automatic classification of new lexical items into synonymic sets on the basis of their cooccurrence data obtained from a corpus. Our goal is to examine the impact that different types of linguistic preprocessing of the cooccurrence material have on the classification accuracy. The paper comparatively studies several preprocessing techniques frequently used for this and similar tasks and makes conclusions about their relative merits. We find that a carefully chosen preprocessing procedure achieves a relative effectiveness improvement of up to 88% depending on the classification method in comparison to the window-based context delineation, along with using much smaller feature space.
MetaCrystal enables users to visualize and control the degree of overlap between the results returned by different search engines. Several linked overview tools support rapid exploration, facilitate complex filtering operations and guide users toward relevant information. MetaCrystal addresses the problem of the effective fusion of different search results by helping users to visually combine and filter the top results returned by the different engines. Users can apply weights to the search engines to create their own ranking functions. They can control the degree of overlap by modifying the URL directory depth used to match documents or by changing the number of top documents being compared.
Instruction cache performance is very important to instruction fetch efficiency and overall processor performance. The layout of an executable has a substantial effect on the cache miss rate during execution. This means that the performance of an executable can be improved significantly by applying a code-placement algorithm that minimizes instruction cache conflicts. We describe an algorithm for procedure placement, one type of codeplacement algorithm, that significantly differs from previous approaches in the type of information used to drive the placement algorithm. In particular, we gather temporal ordering information that summarizes the interleaving of procedures in a program trace. Our algorithm uses this information along with cache configuration and procedure size information to better estimate the conflict cost of a potential procedure ordering. We compare the performance of our algorithm with previously published procedure -placement algorithms and show noticeable improvemen...
Video information processing and retrieval is a key aspect of future multimedia technologies and applications. Commercial videos encode several planes of expression through a rich and dense use of colors, editing effects, viewpoints and rhythms, which are exploited together to attract potential purchasers. Databases of commercials can be accessed in order to analyze how a commercial has been developed, retrieve commercials similar to an example, catalog commercials according to the kind of message conveyed to the user. In this paper, we present a system allowing the retrieval of commercial streams based on their salient semantics. Semantics is regarded from the semiotics perspective: collections of signs and semantic features like colors, editing effects, motion, etc. are used as basic blocks with which the meaning of a commercial is constructed. In our system, it is possible to retrieve commercials according to both the meaning they convey and to their similarity to examples.
This paper introduces new work in multiscale image statistics, a local framework that supports adaptive measurement of image structure where data may be represented by multiple incommensurable values. Data such as those represented by the Visible Human Project data often include multiple modalities such as color channels, multiple pulse sequences of magnetic resonance imaging, and X-ray CT data. Multiscale statistics can establish local correlations, covariances, and entropy measurements across the image. Such measurements have applications in nonlinear filtering, texture analysis, deformable registration and image segmentation.
This paper introduces RankOpt, a linear binary  classifier which optimises the area under the  ROC curve (the AUC). Unlike standard binary  classifiers, RankOpt adopts the AUC statistic as  its objective function, and optimises it directly  using gradient descent. The problems with using  the AUC statistic as an objective function are that  it is non-differentiable, and of complexity O n    in the number of data observations. RankOpt  uses a differentiable approximation to the AUC  which is accurate, and computationally efficient,  being of complexity O n    . This enables the gradient  descent to be performed in reasonable time.
Constraint programming is a methodology for solving di-  cult combinatorial problems. In the methodology, one makes three design  decisions: the constraint model, the search algorithm for solving the  model, and the heuristic for guiding the search. Previous work has shown  that the three design decisions can greatly inuence the eciency of a  constraint programming approach. However, what has not been explicitly  addressed in previous work is to what level, if any, the three design  decisions can be made independently. In this paper we use crossword  puzzle generation as a case study to examine this question. We draw  the following general lessons from our study. First, that the three design  decisions|model, algorithm, and heuristic|are mutually dependent. As  a consequence, in order to solve a problem using constraint programming  most eciently, one must exhaustively explore the space of possible models,  algorithms, and heuristics. Second, that if we do assume some form  of independence when making our decisions, the resulting decisions can  be sub-optimal by orders of magnitude.
In this paper, we introduce a novel approach to mesh editing with the Poisson equation as the theoretical foundation. The most distinctive feature of this approach is that it modifies the original mesh geometry implicitly through gradient field manipulation. Our approach can produce desirable and pleasing results for both global and local editing operations, such as deformation, object merging, and smoothing. With the help from a few novel interactive tools, these operations can be performed conveniently with a small amount of user interaction. Our technique has three key components, a basic mesh solver based on the Poisson equation, a gradient field manipulation scheme using local transforms, and a generalized boundary condition representation based on local frames. Experimental results indicate that our framework can outperform previous related mesh editing techniques.
We contend that, at least in the  rst stages of de  nition of  the early and late requirements, the software development process should  be articulated using knowledge level concepts. These concepts include  actors, who can be (social, organizational, human or software) agents,  positions or roles, goals, and social dependencies for de  ning the obligations  of actors to other actors. The goal of this paper is to instantiate  this claim by describing how Tropos, an agent-oriented software engineering  methodology based on knowledge level concepts, can be used in  the development of a substantial case study consisting of the meeting  scheduler problem.
this paper we deal with Extended Entity-Relationship (EER) diagrams    used to model temporal databases. The temporal conceptual model ER V T has been introduced both to formally clarify the meaning of the various temporal constructs appeared in the literature [2, 4], and to check the possibility to perform reasoning on top of temporal schemas [5]. ER V T supports valid time for entities, attributes, and relationships in the line of TIMEER [10] and ERT [15], while supporting dynamic constraints for entities as presented in MADS [14]. ER V T is able to distinguish between snapshot constructs---i.e. each of their instances has a global lifespan---and temporary constructs---i.e. each of their instances have a limited lifespan. Dynamic constructs capture the object migration from a source entity to a target entity
This paper describes the conversion of an existing standalone, multi-agent, Intelligent Tutoring System (ITS) to one that can operate trough the Internet. The particular ITS works as a Virtual Reality game and is meant to help students learn by providing them a more motivating environment and tutoring help. The system has three agents that have explicit roles. The companion that gives friendly advice, the advisor that provides help and the guard that asks the questions. In order to achieve the goal of the conversion we used a new technology, that of the Web services which has proved very satisfactory for the purposes of our research. We describe the conversion of our standalone multi-agent application to a web-based one, and we discuss the benefits of web services for the purposes of educational software.
Reconstruction from a single view of architectural scenes is possible thanks to the automatic identification of structural perspective elements in the view. Reliable and accurate reconstruction depends strongly on a robust estimation of vanishing points. In this paper we compare different approaches for estimation of vanishing points, and we justify our choice of Danish estimator. The intersection of pairs of pencils of perspective lines through vanishing points generates maps of quadrilaterals in the image which provide automatic grouping criteria for interpreting the scene. Quadrilaterals are not enough for an accurate reconstruction. Triplets of pencils of perspective lines provide a wire-framed structure of traditional architectonic scenes used for visualization tasks by constructing trapezoidal and cuboid maps superimposed to views with a low computational cost. By identifying multiple junctions in images we obtain the relative orientation between adjacent planes containing bundles of perspective lines. The introduction of multivector representation simplifies the visualization and management of 3D information arising from the lifting of adjacent quadrilaterals to 3D cuboids for visualization tasksffff    1. 
Visitors of web sites are generally heterogeneous and have different needs. The aim of the AVANTI project is to cater to these individual needs by adapting the content and the presentation of web pages to each individual user. The special needs of elderly and handicapped users are also partly considered. A model of the characteristics of user groups and individual users and a domain model are exploited in the adaptation process.
In the semantic Web architecture, Web ontology languages are built on top of RDF(S). However, serious difficulties have arisen when trying to layer expressive ontology languages, like OWL, on top of RDF-Schema. Although these problems can be avoided, OWL (and the whole semantic Web architecture) becomes much more complex than it should be. In this paper, a possible simplification of the semantic Web architecture is suggested, which has several important advantages with respect to the layering currently accepted by the W3C Ontology Working Group.
We present Adaptive Sensing, an energy-efficient topology configuration method for environment monitoring using densely deployed wireless sensor networks. Adaptive Sensing puts redundant nodes into passive mode as auxiliary nodes to be used later on, thereby extending the system lifetime. Sensor data is collected at powerful macronodes periodically and a low-order model is fitted to compute a prediction area for each sensor. This prediction area is used to measure the redundancy level of the sensors covered in it. The redundant nodes are put into passive mode by macronodes while keeping the distortion in the output low. We analyze the energy-distortion tradeoff in monitoring applications, introduce the Adaptive Sensing algorithm based on this analysis. We also include a performance study demonstrating the advantages of our approach based on simulation results.
Due to drastic reductions in feature sizes, device interconnects have begun to occupy a  considerable portion of the layout in modern VLSI chips. A related consequence is that interconnect   parasitics, which are unintended harmful side-effects that arise as a consequence of the  layout, can potentially have a significant adverse impact on chip performance. Therefore, the  electrical characteristics of these parasitics have to be taken into account in the design of IC  chips. This involves processing data that is both large in size and complex in nature, calling for  effective data management. Current solutions employ a file-based approach that suffers from  many drawbacks, necessitating a search for alternative methods of data management.  Our research explores the use of a database technology-based approach to address the parasitic  data management problem. In this paper, we present a detailed object model for representing  interconnect parasitics. We also describe the design and imp...
We present a model of dialogue for embodied virtual agents that can communicate with multiple (human and virtual) agents in a multi-modal setting, including face-to-face spoken and nonverbal, as well as radio interaction, spanning multiple conversations in support of an extended complex task. The model builds on previous work in embodied agents and multi-layer dialogue models, and is being deployed in a peacekeeping mission rehearsal exercise setting.
In environments where exact synchronization between source data objects and cached copies  is not achievable due to bandwidth or other resource constraints, stale (out-of-date) copies are  permitted. It is desirable to minimize the overall divergence between source objects and cached  copies by selectively refreshing modified objects. We call the online process of selecting which  objects to refresh in order to minimize divergence best-effort synchronization. In most approaches  to best-effort synchronization, the cache coordinates the process and selects objects  to refresh. In this paper, we propose a best-effort synchronization scheduling policy that exploits  cooperation between data sources and the cache. We also propose an implementation of  our policy that incurs low communication overhead even in environments with very large numbers  of sources. Our algorithm is adaptive to wide fluctuations in available resources and data  update rates. Through experimental simulation over synthetic and real-world data, we demonstrate  the effectiveness of our algorithm, and we quantify the significant decrease in divergence  achievable with source cooperation.
We identify a class of potentials for which the semiclassical estimate N    (1/ff)    ff 0  dr ff  -V(r)ff  [-V(r)]  of the number N of (S-wave) bound states provides a (rigorous) lower limit: N ff  {{N    }},  where the double braces denote the integer part. Higher partial waves can be included via the standard replacement of the potential V(r)with the effective ff-wave potential V      1)/r    . An analogous upper limit is also provided for a different class of potentials, which is however quite severely restricted.
We present an algorithm for planning goal-directed footstep navigation strategies for biped robots through obstacle-filled environments and uneven ground. Planning footsteps is more general than most existing navigation methods designed for wheeled robots, since the options of stepping over or upon obstacles are available. Given a height map of the terrain and a discrete set of possible footstep motions, the planner uses an A* search to generate a sequence of footstep locations to reach a given goal state. The planner evaluates footstep locations for viability using a collection of heuristic metrics designed to encode the relative safety, effort required, and overall motion complexity. We show preliminary results of the planner over several simulated terrains, as well as a simplified, online version of the algorithm running on the H7 humanoid robot. In the latter case, a stereo vision system is used to sense obstacles in the immediate environment and identify a target goal location, which is used to update the current optimal footstep sequence to the goal from the robot&apos;s present location.
With a standard bridged Ethernet network, a possible delay of high-priority packets by lower-priority packets that are just being served within a bridge or device cannot be avoided. The results are higher transaction times and jitter values, especially when using long chains of bridges. With this characteristic, Ethernet cannot be used in timecritical synchronous applications. In this paper we analyze the real-time characteristic of a new distributed timeslotting method for Ethernet, which is part of the involved bridges to realize minimized transaction times. This algorithm will be used by the emerging standard PROFINET IRT    and has therefore practical relevance. The simulation results will be compared with analytical results of a standard Ethernet System with priority support.
We present an initial analysis of the National Library of Medicine&apos;s (NLM) Gene Indexing initiative. Gene Indexing occurs at the time of indexing for all 4600 journals and over 500,000 articles added to  PubMed/MEDLINE each year. Gene Indexing links articles about the basic biology of a gene or protein within eight model organisms to a specific record in the NLM&apos;s LocusLink database of gene products.
This paper presents an analytic model for evaluating the MAC layer queueing delays at wireless nodes using the Distributed Coordination Function of IEEE 802.11 MAC specifications. Our model is valid for finite loads and can account for arbitrary arrival patterns, packet size distributions and number of nodes. Each node is modeled as a discrete time G/G/1 queue and we obtain closed form expressions for the delay and queue length characteristics at each node. We derive the service time distribution for the packets at each node while accounting for a number of factors including the channel access delay due to the shared medium, impact of packet collisions, the resulting backoffs as well as the packet size distribution. Our analytical results are verified through extensive simulations and are more accurate than existing models.
Motivated by recent financial crises in East Asia and the U.S. where the downfall of a small number of firms had an economy-wide impact, this paper generalizes existing reduced-form models to include default intensities dependent on the default of a counterparty. In this model, firms have correlated defaults due not only to an exposure to common risk factors, but also to firm-specific risks that are termed &quot;counterparty risks.&quot; Numerical examples illustrate the effect of counterparty risk on the pricing of defaultable bonds and credit derivatives such as default swaps.
In this paper, we present CoLD (colorectal lesions detector) an innovative detection system to support colorectal cancer diagnosis and detection of pre-cancerous polyps, by processing endoscopy images or video frame sequences acquired during colonoscopy. It utilizes second-order statistical features that are calculated on the wavelet transformation of each image to discriminate amongst regions of normal or abnormal tissue. An artificial neural network performs the classification of the features. CoLD integrates the feature extraction and classification algorithms under a graphical user interface, which allows both novice and expert users to utilize effectively all system&apos;s functions. It has been developed in close cooperation with gastroenterology specialists and has been tested on various colonoscopy videos. The detection accuracy of the proposed system has been estimated to be more than 95%. As it has been resulted, it can be used as a supplementary diagnostic tool for colorectal lesions.
We present the first known implementation of elliptic curve cryptography over F2  p for sensor networks based on the 8-bit, 7.3828-MHz MICA2 mote. Through instrumentation of UC Berkeley&apos;s TinySec module, we argue that, although secret-key cryptography has been tractable in this domain for some time, there has remained a need for an efficient, secure mechanism for distribution of secret keys among nodes. Although public-key infrastructure has been thought impractical, we argue, through analysis of our own implementation for TinyOS of multiplication of points on elliptic curves, that public-key infrastructure is, in fact, viable for TinySec keys&apos; distribution, even on the MICA2. We demonstrate that public keys can be generated within 34 seconds, and that shared secrets can be distributed among nodes in a sensor network within the same, using just over 1 kilobyte of SRAM and 34 kilobytes of ROM.
Interpretation, Type Systems and ControlFlow Analysis and EPSRC grant GR/R53401. Permission to make digital/hard copy of all or part of this material without fee for personal or classroom use provided that the copies are not made or distributed for profit or commercial advantage, the ACM copyright/server notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior specific permission and/or a fee. ACM Transactions on Programming Languages and Systems, Vol. TBD, No. TDB, Month Year, Pages 1--58.
Introduction  Microdata files released at the Israel Central Bureau of Statistics currently undergo a series of ad-hoc decisions with regard to protecting the confidentiality of statistical entities. In general, there are three levels of microdata files: PUF which is released upon request to legitimate researchers after they outline the details of their research, MUC which is available to researchers in universities and approved research institutions that have a special contract with the CBS, and files available in an on-site research facility at the CBS where researchers can work and access original files (without direct identifiers) for data analysis. Because of the current lack of a systematic way of evaluating the risks of the files, mistakes are common. One example is the release of multiple PUF files for the same survey or census, each having detailed coding for different sets of variables. These files can easily be linked using common variables and the entire original file can b
Network researchers face a significant problem when deploying software in routers. Router  platforms are generally not open systems. In this paper we discuss the problems this poses, and  present an eXtensible Open Router Platform (XORP) which has the key goals are extensibility, performance  and robustness. XORP is both a research tool and a stable deployment platform, thus easing  the transition of ideas from the lab to the real world.
Image-based modeling systems use photographic images to ease the creation of and to enhance the realism of computer models. However, many of these systems require knowledge about camera parameters and 3D scene information, which may not be easily obtained. This paper proposes an interactive, image-based modeling system termed &quot;Image Origami&quot; that allows rapid construction of 3D environments from 2D images of architectural scenes without requiring a priori camera calibration. This method is based on reversing the 3D to 2D image projection model by allowing points marked in a 2D image to move along lines in 3-space defined by a 3D camera position and the point&apos;s projected 2D image plane location in 3space. The user specifies vertices, edges, and constraints, and can fold and bend images into 3D shapes based on these constraints. After constraints are entered, the system can automatically calculate the best-fit 3D approximation. The user is only required to enter information that is readily apparent, such as edge locations in an image and angles between edges and faces. To produce a more realistic texturemapped model, this system applies an occlusion removal technique.
It has previously been shown that a  recommender based on immune system idiotypic  principles can outperform one based on  correlation alone. This paper reports the results  of work in progress, where we undertake some  investigations into the nature of this beneficial  effect. The initial findings are that the immune  system recommender tends to produce different  neighbourhoods, and that the superior  performance of this recommender is due partly to  the different neighbourhoods, and partly to the  way that the idiotypic effect is used to weight  each neighbour&apos;s recommendations.
Faces usually are the most interesting objects in certain categories of video like home videos and news clips. In this paper a novel sensor fusion based face tracking system is presented that tracks faces in compressed video,  and aids automatic video indexing. Tracking is done by  fusing the measurements from three independent sensors -- motion and colour based trackers (derived from [2]) and a face detector (presented in [1]) using a novel hierarchical  framework based on Kalman filter state vector fusion. The tracking results show that the fused results are better than those of any individual sensors or their mean.
The notion of refactoring —transforming the source-code of an objectoriented program without changing its external behaviour — has increased the need for a precise definition of refactorings and their properties. This paper introduces a graph representation of those aspects of the source code that should be preserved by a refactoring, and graph rewriting rules as a formal specification for the refactoring transformations themselves. To this aim, we use type graphs, forbidden subgraphs, embedding mechansims, negative application conditions and controlled graph rewriting. We show that it is feasible to reason about the effect of refactorings on object-oriented programs independently of the programming language being used. This is crucial for the next generation of refactoring tools.
The growth of world wide web has dramatically changed our lives. Different tools and new techniques based on the world wide web make it possible to perform various collaborative processes and businesses: internet chatrooms, internet phones, video conferences, internet classrooms, and now electronic commerces, etc.  Various research and projects have been done regarding how to surf the world wide web in different collaborative way and through different media: pictures, animations, voices, etc. In this paper, we give a case study in which we implemented a so called Collaborative Web Surfing (CWS) system. Besides others, this system has the following attractive features:  1. It supports the synchronized change of URLs. The leader can lead the surfing of web pages and other audiences will be brought to the corresponding web page that the leader is browsing automatically.  2. It supports voice chat and text chat. Different people can talk to one another in voice like using internet phone an...
Peer-to-peer architectures have been proposedto  bring an earthquake to interactions on the Internet by enabling real-time direct sharing of computer resources and services. In this paper we use the peer-to-peer model to deliver e-services in a timely and reliable way. The challenge is to use the collective ability of many devices - wireless and wired - to work together to perform a task, solve a problem or complete a transaction. The proposed peer-to-peer based system is autonomous, decentralized and scalable. Our system is based on a multiple feedback loop structure that coordinates the applications and system resources in an integrated mannerff monitors the behavior of the eservices transparentlyff and, schedules the system resources dynamically.
We consider the problem of detecting hiding in the least significant bit (LSB) of images. Since the hiding rate is not known, this is a composite hypothesis testing problem. We show that under a mild condition on the host probability mass function (PMF), the optimal composite hypothesis testing problem is solved by a related optimal simple hypothesis testing problem. We then develop practical tests based on the optimal test and exhibit their superiority over Stegdetect, a popular steganalysis method used in practice.
This paper describes the artistic projects undertaken at Immersion Music, Inc. (www.immersionmusic.org) during its three-year existence. We detail work in interactive performance systems, computer-based training systems, and concert production.
In this paper we describe the difficulties we faced to evaluate performance of heterogeneous cluster so that we get reliable performance analysis of a given cluster or in that matter &quot;cluster of cluster&quot; system. We also describe the options that we thought for such an analysis. We started with homogeneous machines and tried to develop a model to identify the issues that would arise in a general cluster system (heterogeneous or homogeneous) due to the model.
This paper presents a survey and comparison of the significant diversity measures in the genetic programming literature. The overall aim and motivation behind this study is to attempt to gain a deeper understanding of genetic programming dynamics and the conditions under which genetic programming works well. Three benchmark problems (Artificial Ant, Symbolic Regression and Even5-parity) are used to illustrate different diversity measures and to analyse their correlation with performance. The results show that diversity is not an absolute indicator of performance and that phenotypic measures appear superior to genotypic ones. Finally we conclude that interesting potential exists with tracking ancestral lineages.
Traditional statically composed software can be globally optimized for execution speed already at compile-time. For dynamically composed component-oriented software a different execution model has to be employed to achieve similar performance. When components are compiled, little is known about the environment they will be used in later. Only after the deployment phase the required global information is available to aggressively optimize the overall system. This poster shows how runtime code recompilation can be employed to provide an efficient execution model for dynamically composed software.
A vector enhancement of Said and Pearlman&apos;s Set Partitioning in Hierarchical Trees (SPIHT) methodology, named VSPIHT, has recently been proposed for embedded wavelet image compression. A major advantage of vector based embedded coding with fixed length VQs over scalar embedded coding, is its superior robustness to noise. In this work we show that vector set partitioning can effectively alter the balance of bits in the bit stream so that significantly fewer critical bits carrying significance information is transmitted, thereby improving inherent noise resilience. Additionally, the degradation in reconstruction quality caused by errors in non-critical quantization information, can be reduced by appropriate VQ indexing, or designing channel optimized VQs for the successive refinement systems. For very noisy channels unequal error protection to the critical and non critical bits with either block codes or convolution codes are used. Extensive simulation results are presented.  1. INTRODUC...
Throughout the last years a huge amount of work has been devoted to the definition of hypertext models. Even more resources have been directed towards the domain of virtual (dynamic/ computational) hypertext, among others motivated by the idea of building open systems. Surprisingly enough, almost nobody stressed the role of the underlying model in such virtual systems. That is precisely the aim of this...
Traditional ABR flow control mechanisms share, in a fair way, the available bandwidth according to the instantaneous peak rate requirements of active traffic streams. Such an approach does not allow bursty users, who besides the peak rate value additional measures of burstiness (e.g., the mean rate), to express their true preferences for network usage. Furthermore, fair sharing is achieved at the short time scales of the duration of the bursts, and cannot express fairness properties defined over longer time scales, such as average throughput. We describe an approach where resource sharing is done according to effective usage. Users bid for some amount of effective bandwidth, and the network controls the effective bandwidth of their traffic by adjusting the explicit rate ER (maximum rate the user is allowed to send traffic) in order to achieve economically fair resource sharing. The feedback loop operates in much longer time scales than the round trip delays, and its performance relies ...
This paper presents a high performance switched current memory cell. The cell is designed for  reduced supply voltages and lowpower. For audio applications, an accuracy around 16 bits is  mandatory.
Motion-compensated temporal filtering is an essential ingredient of recently developed wavelet-based scalable video coding schemes. Lifting implementation of these decompositions represents a versatile tool for spatio-temporal optimizations and numerous improvements have thus been proposed. In this paper, we propose an alternative structure for the temporal prediction in the 5/3 filterbank. It significantly reduces the ghosting artefacts in the temporal approximation subband frames, providing a higher quality scalability and improved compression performance, for an equivalent complexity.
Data locality is critical to achieving high performance on large-scale parallel machines. Non-local data accesses result in communication that can greatly impact performance. Thus the mapping, or decomposition, of the computation and data onto the processors of a scalable parallel machine is a key issue in compiling programs for these architectures.
This thesis addresses image-based change detection. Motivation was provided by the lack of algorithms that incorporate in their solution diverse types of pre-existing and complementary information and have the ability to interact with a spatiotemporal environment. The main
This paper presents new look-ahead schemes for backtracking  search when solving constraint satisfaction  problems. The look-ahead schemes compute a heuristic  for value ordering and domain pruning, which influences  variable orderings at each node in the search space. As a basis
Mutual information (MI) has emerged in recent years as an  effective similarity measure for comparing images. One drawback of MI,  however, is that it is calculated on a pixel by pixel basis, meaning that  it takes into account only the relationships between corresponding individual  pixels and not those of each pixel&apos;s respective neighborhood. As a  result, much of the spatial information inherent in images is not utilized.
One of the primary concerns for the network providers in designing or upgrading a network is deciding where to add capacity. This in turn requires uniquely identifying the congestion hot-spots. However, the current models for predicting network behavior are constrained by the fact that they require prior identification of the bottleneck, which is in direct contrast with the objective. In this paper we propose a Non-Linear Complementarity model for TCP/IP networks which can not only identify bottleneck links but also predict steady state link loss probabilities. We also show that full-row rank (where row represents number of links in the topology) of Incidence Matrix is a sufficient condition for all models to locate the bottleneck links. However, in this paper we show that bottleneck links can be uniquely identified with the Non-Linear Complementarity model without any regulations on the Incidence Matrix. Moreover in this paper we also show the relationship between the utility optimization, fixed-point approach and our proposed approach. Specifically we show how these models can be derived from each other.
One way of designing a taxonomy is by identifying a number of different  aspects, or facets of the domain and then designing one taxonomy per  facet. In such a faceted taxonomy, the indexing of objects is done by combining  terms from different facets. A faceted taxonomy has several advantages by  comparison to a single hierarchical taxonomy, such as conceptual clarity, compactness  and scalability. However, a major drawback of faceted taxonomies  is the possibility of forming a large number of invalid combinations of terms,  i.e. combinations of terms that do not apply to any object of the underlying  domain. The presence of such invalid combinations causes serious problems  during object indexing or browsing. To alleviate this problem, we propose an  algebra of taxonomies whose operators allow the effcient and flexible specification  of only valid combinations of terms. This algebra can be used in order  to construct very big compound taxonomies in a very systematic and effcient  way.
The rehabilitation of contaminated sites involves several considerations in terms of environmental, technological and socio-economic aspects. A decision support system becomes therefore necessary in order to manage problem complexity and to define effective rehabilitation interventions. DESYRE (Decision Support sYstem for Rehabilitation of contaminated sites) is a software system which integrates risk assessment with socio-economic analysis and technological assessment in order to provide decision-makers with different remediation scenarios to be evaluated. The structure of the system allows a subsequent analysis, from socioeconomic analysis and site characterization, to risk assessment before and after remediation technologies selection, until the definition of remediation scenarios. The system integrates several analytical tools, such as geostatistics, Fuzzy logic, risk assessment and geographical information systems (GIS). The present paper focuses on the role of the Multi-Criteria Decision Analysis (MCDA), which represents the core of the DSS. In the DESYRE framework, MCDA is applied for the definition of the pool of the suitable remediation technologies. The analytic hierarchy process is applied to rank technologies and develop alternative remediation scenarios. The scenarios are described by a set of indices which can be aggregated by decision makers to rank alternative options. Future research developments suggest the MCDA application also for the evaluation of the remediation scenarios by different stakeholders, in a Group Decision Making (GDM) context.
Research domains such as active networks, ad-hoc networks, ubiquitous computing, pervasive computing, grid computing, and sensor networks, clearly show that computer networks will become more complex and heterogeneous. In many cases, central management and control of the network are far from trivial since both the topology and the connected devices change rapidly in such highly dynamic environments, while load circumstances may vary arbitrarily. The software architecture in a node needs to support flexibility. We have developed an architecture tailored to protocol stack software that allows customizing internal resource management in order to handle overload conditions gracefully. We show that the investment in explicit support for modularity and architectural constraints pays off: the paper elaborates on a case study in which dynamic adaptation of access control behavior leads to significant performance improvements.
Tropos is a novel agent-oriented software development methodology  founded on two key features: (i) the notions of agent, goal, plan and various other  knowledge level concepts are fundamental primitives used uniformly throughout  the software development process; and (ii) a crucial role is assigned to requirements  analysis and specification when the system-to-be is analyzed with respect  to its intended environment. This paper provides a (first) detailed account of the  Tropos methodology. In particular, we describe the basic concepts on which Tropos  is founded and the types of models one builds out of them. We also specify  the analysis process through which design flows from external to system actors  through a goal analysis and delegation. In addition, we provide an abstract syntax  for Tropos diagrams and other linguistic constructs.
Wireless sensor networks have the advantage of spanning a large geographical region and being able to collaboratively detect and track non-local spatio-temporal events. This paper presents the application of geometric duality in sensor selection and non-local phenomena tracking. Using the dual-space transformation, which maps a non-local phenomenon, e.g., the edge of a half-plane shadow, to a single point in the dual space and maps locations of distributed sensor nodes to a set of lines that partitions the dual space, one can turn off majority of the sensors to achieve resource preservation without losing the detection and tracking accuracy. To scale up the system, we propose a hierarchical architecture that consists of a small number of computationally powerful nodes and a massive number of ad hoc resource constrained motes. By taking advantage of the continuity of physical phenomena and the duality principle, we can greatly reduce power consumption in non-local phenomena tracking and extend the life time of the network.
Data in the Internet are scattered on different sites indeliberately, and accumulated and updated frequently but not synchronously. It is infeasible to collect all the data together to train a global learner for prediction. Even exchanging learners trained on different sites is costly. In this paper, aggregative-learning is proposed. In this paradigm, every site maintains a local learner trained from its own data. Upon receiving a request for prediction, an aggregative-learner of a local site activates and sends out many mobile agents taking the request to potential remote learners. The prediction of the aggregative-learner is made by combining the local prediction and the responses brought back by the agents. Experiments show that the prediction of a local learner could be significantly improved through employing the aggregative-learning paradigm.
Multimedia applications place new requirements on networks as compared to traditional data applications: (i) they require relatively high bandwidths on a continuous basis for long periods of time; (ii) involve multipoint communications and thus are expected to make heavy use of multicasting; and (iii) tend to be interactive and thus require low latency. These requirements must be taken into account when routing multimedia traffic in a network. This report presents a performance evaluation of routing algorithms in the multimedia environment, where the requirements of multipoint communications, bandwidth and latency must be satisfied. We present an exact solution to the optimum multicast routing problem, based on integer programming, and use this solution as a benchmark to evaluate existing heuristic algorithms, considering both performance and cost of implementation (as measured by the average run time), under realistic network and traffic scenarios.
Development of methodologies to achieve a priori parameter estimation of hydrological models is fundamental in ungauged basins, to reduce the number of parameters to be calibrated or to obtain parameter values where calibration is not possible. This work shows that conceptual rainfall-runoff models can be applied to ungauged watersheds by developing relationships between model parameters and watershed characteristics. In fact, the calibration of MEDOR, a four parameter daily lumped conceptual rainfall-runoff model specific for the Mediterranean catchments, is affected by the equifinality issue. Systematic scanning of the Nash criterion objective function demonstrates that a Production Equifinality Relationship (PER) exists between loss function parameters. This basin-specific relation can be determined using the annual balance of rainfall-runoff and daily rainfall data generated by a stochastic model calibrated for the region. Moreover, the analysis shows the importance of the stochastic structure of rainfall in the calibration of MEDOR. Thus, the parameters cannot be determined solely from the physical properties of the basin. Coupled to a stochastic model of rainfall of a given region, MEDOR generates equifinality relations between runoff coefficients (C E ) defining a surface in the parameters space. Several large areas have been identified in the Mediterranean region having a single reference C E surface (e.g., East coast of the Mediterranean Sea). The runoff coefficient of a given watershed located in one of these areas constrains the specific equifinality relation. In a case study, the PER of two Lebanese watersheds were determined using the same single reference C E surface and spatial soil depth information. Thus, it is possible to evaluate the predictive unce...
. Business information received from advanced data analysis and data mining is a critical success factor for companies wishing to maximize competitive advantage. The use of traditional tools and techniques to discover knowledge is ruthless and does not give the right information at the right time. Data mining should provide tactical insights to support the strategic directions. In this paper, we introduce a dynamic approach that uses knowledge discovered in previous episodes. The proposed approach is shown to be effective for solving problems related to the efficiency of handling database updates, accuracy of data mining results, gaining more knowledge and interpretation of the results, and performance. Our results do not depend on the approach used to generate itemsets. In our analysis, we have used an Apriori-like approach as a local procedure to generate large itemsets. We prove that the Dynamic Data Mining algorithm is correct and complete.  Keywords: Data Mining, Dynamic Approach,...
A number of techniques have been proposed to achieve a better separation of concerns in programming systems. How does one evaluate the benefits and drawbacks of the various alternatives, and offer guidance to the designers of future systems? This paper proposes a set of challenge problems for separation of concerns, each of which deals with different types or dimensions of concerns. Drawing from my experience with large, high-level object-oriented systems and from the separation of concerns literature, this paper classifies separation of concerns problems into a number of categories and chooses representative examples from each. I intend to continue this work by evaluating a range of current systems using these problems as benchmarks, and then using this experience to design better support for separation of concerns in the context of high-level object-oriented languages like Cecil.  1 Challenge Problems  Challenge problems have long been useful in evaluating program design and implemen...
This paper focuses on a particular algorithm, Efficient Global Optimization (EGO) that uses kriging metamodels. Several infill sampling criteria are reviewed, namely criteria for selecting the points added to the data set for fitting the metamodel. The infill sampling criterion has a strong influence on how efficiently and accurately EGO locates the optimum. Variance-reducing criteria substantially reduce the RMS error of the resulting metamodels, while other criteria influence how locally or globally EGO searches. Criteria that place more emphasis on global searching require more iterations to locate optima and do so less accurately than criteria emphasizing local search
It is shown that determining whether a quantum computation has a non-zero probability of accepting is at least as hard as the polynomial time hierarchy. This hardness result also applies to determining in general whether a given quantum basis state appears with nonzero amplitude in a superposition, or whether a given quantum bit has positive expectation value at the end of a quantum computation. This result is achieved by showing that the complexity class NQP of Adleman, Demarrais, and Huang [1], a quantum analog of NP, is equal to the counting class coC=P.
This paper considers the use of browser plugins and Java code ffwithin standard HTTP mechanismsff to serve private conffdential documents securely over the World Wide Web to a group of mobile or otherwise distributed users. Web security mechanisms typically require use of either an underlying security system for transport mechanism ffe.g., SSLff7ffff alternate servers and data streams ffe.g., S-HTTPff9ffff, security-oriented plugins within the browser ffe.g, ff6ffff, or helper applications ffe.g., ff11ffff. The method described here operates by providing a per-user security mechanism coded in Java which operates as part of a standard web-browser environment. This system appears to be very appropriate for serving lower-security, non-public documents, ffles and images to a group of heterogeneous users over the world wide web. It can also be appropriate in circumstances where the standard security mechanisms are not available. We also describe an adaptation which provides automatic per-user ffwatermarking&quot; of...
This paper discusses the design of a virtual tabletennis environment, and the design of neural network based controllers to play in that environment. The motivation behind the work is to provide an interesting and entertaining forum in which to carry out research on adaptive control and planning problems that stretch the limits of current neural network paradigms. 1 Introduction  Currently there is much interest in neural networks and genetic programming methods for control of real robots. However, to experiment with such things requires expensive hardware that can be time consuming to set up and maintain. An alternative to experimenting with real robots is to experiment with virtual robots. The complexity of designing controllers for such virtual robots depends on the `physics&apos; of the virtual environment, and the task at hand. This paper describes the design of a virtual table-tennis environment and some initial work on the design of algorithmic and neural network based bat controller...
Real-time gaze tracking is a promising interaction technique for virtual environments. Immersive projection-based virtual reality systems such as the CAVE    allow users a wide range of natural movements. Unfortunately, most head and eye movement measurement techniques are of limited use during free head and body motion. An improved head-eye tracking system is proposed and developed for use in immersive applications with free head motion. The system is based upon a head-mounted video-based eye tracking system and a hybrid ultrasound-inertial head tracking system. The system can measure the point of regard in a scene in real-time during relatively large head movements. The system will serve as a flexible testbed for evaluating novel gaze-contingent interaction techniques in virtual environments.
One potentially promising approach for exploiting the best features of a variety of different computer architectures is to partition an application program to simultaneously execute on two or more different machines interconnected with a high-speed network. A fundamental problem with this heterogeneous computing, however, is the difficulty of partitioning an application program across the machines. This paper presents a partitioning strategy that relates the relative performance of two heterogeneous machines to the communication cost of transferring partial results across their interconnection network. Experiments are described that use this strategy to partition two different application programs across the sequential front-end processor of a Connection Machine CM-200, and its parallel back-end array.
Fragment-based caching has been proposed as a promising technique for dynamic Web content delivery and caching [2, 3, 4]. Most of these approaches either assume the fragment-based content is served by Web server automatically, or look at server-side caching only. There is no method of extracting fragments from an existing dynamic Web content, which is of great importance to the success of fragment-based caching. Also, current technologies for supporting dynamic fragments do not allow to take into account changes in fragment spatiality, which is a popular technique in dynamic and personalized Web site design. This paper describes our effort to address these shortcomings. The first, DyCA, a Dynamic Content Adapter, is a tool for creating fragment-based content from original dynamic content. Our second proposal is an augmentation to the ESI standard that will allow it to support looking up fragment locations in a mapping table that comes attached with the template. This allows the fragments to move across the document without needing to reserve the template.
  We study model checking problems for pushdown systems and linear time logics. We show that the global model checking problem (computing the set of configurations, reachable or not, that violate the formula) can be solved in O(gP 3
We investigate methods for exploiting structural information in semi-structured documents in order to improve classification performance of the popular Naive Bayes text classifier. A novel method based on natural language modeling is introduced which effectively combines the expressive power of a structureaware classifier with more reliable parameter estimation of the flat-text model. We provide strong intuitive and empirical justification for this approach. All methods proposed are effective, efficient and widely applicable.
This paper proposes a methodology for the channel assignment problem in the cellular communication industry. The problem considers the assignment of a limited channel bandwidth to satisfy a growing channel demand without violating electromagnetic interference constraints. The initial solution is generated using random constructive heuristic. This solution is then improved using a hyper-heuristic technique based on the great deluge algorithm. Our experimental results, on benchmarks data sets, gives promising results.
A procedure is described for synthesizing an image with the same texture as a given input image.
In this paper we introduce a new model of solving pattern recognition tasks called PRISM. The main concept behind PRISM is the slicing of information through multiple planes across different feature axes to generate a number of cells. The number of cells created and their volume depends upon the number partitions per axes. In this context we define resolution as the number of partitions per axes. In this paper  we make the following contributions. First, we provide a brief survey of the class separability measures and feature partitioning schemes used for pattern recognition. Second, we define the PRISM framework and the algorithm for data assignment to cells. Third, we detail four important concepts in PRISM: purity,  neighbourhood separability, collective entropy, and data compactness. The first two measures define the data complexity, the next measure relates to uncertainty, and the last measure defines the alternative to statistical data variance in the PRISM framework. Fourth, we investigate the variability in the estimates of these measures depending on the placement of partitions on each feature axis. Finally, we give an overview of experimental successes achieved with PRISM in the areas of classification complexity estimation and feature selection.
 The area of Propositional Satisfiability (SAT) has been the subject of intensive research in recent years, with significant theoretical and practical contributions. From a practical perspective, a large number of very effective SAT solvers have recently been proposed, most of which based on improvements made to the original Davis-Putnam-Logemann-Loveland (DPLL) backtrack search SAT algorithm. The new solvers are capable of solving very large, very hard real-world problem instances, which more traditional SAT solvers are totally incapable of. Despite the significant improvements in state-of-the-art backtrack search SAT solvers, a few relevant questions remain. Is a well-organized and well-implemented DPLL algorithm enough per se, or should the algorithm definitely include additional search techniques? Which search techniques are indeed effective for most problem instances? Which search techniques cooperate effectively and which do not? This paper is a first step towards answering
This paper reviews three recent books on data mining written from three different perspectives, i.e. databases, machine learning, and statistics. Although the exploration in this paper is suggestive instead of conclusive, it reveals that besides some common properties, different perspectives lay strong emphases on different aspects of data mining. The emphasis of the database perspective is on efficiency because this perspective strongly concerns the whole discovery process and huge data volume. The emphasis of the machine learning perspective is on effectiveness because this perspective is heavily attracted by substantive heuristics working well in data analysis although they may not always be useful. As for the statistics perspective, its emphasis is on validity because this perspective cares much for mathematical soundness behind mining methods.
this paper we have explored the use of evolutionary algorithms to aid the selection of features and the classification of P300 signals in BCI. This approach has this is a complex configuration, to help evolution we later added a pure a linear part to the general polynomial
Wireless sensor networks consist of many inexpensive wireless nodes, each having sensing capability with some computational and communication power. Asymmetric cryptographic algorithms are not suitable for providing security on wireless sensor networks due to limited computation, power, and storage resources available on sensor nodes. Therefore, the energy-efficient security protocol proposed in this paper uses symmetric cryptographic algorithms to support security. To mitigate the drawbacks of symmetric cryptographic algorithms, the session key is changed dynamically, in addition to employing code-hopping technique in non-blocking OVSF codes.
Most of the color image processing algorithms do not consider the non-linearity of human vision, and the limitations of the human vision like- capability to discriminate between two similar colors and maximum number of colors human vision can respond to. In this paper, the nonlinearity and physiological limitations of human vision have been used for sampling the RGB color space. The large redundancy, non-linearity and huge color space dimensionality in the real life color image data reduces the efficiency of color image processing, coding and analysis algorithms. This approach, along with sampling the color space, automatically reduces the redundancy in the data. This reduction in redundancy in color image data has been carried out, while maintaining the visual similarity with the original image. Further a new form of the RGB space named J R , J G and J B which is linear and perceptually uniform has been proposed. Finally some applications of this approach have been presented.
We consider the problem of estimating the length of a shortest  path in a DAG whose edge lengths are known only approximately  but can be determined exactly at a cost. Initially, each edge e is known  only to lie within an interval [l e ; he ]; the estimation algorithm can pay  ce to find the exact length of e. In particular, we study the problem of  finding the cheapest set of edges such that, if exactly these edges are  queried, the length of the shortest path will be known, within an additive   ? 0 that is given as an input parameter. We study both the  general problem and several special cases, and obtain both easiness and  hardness approximation results.
Accessing remote data anywhere and at anytime constitutes  an important advantage in many business environments. However, when  working with mobile devices, users face many problems related to: (1)  device restrictions: mobile devices are resource-constrained, more vulnerable  and fragile than stationary devices (devise exposure problems)  and, (2) the communication media: wireless communications are often  unstable, asymmetric and expensive (media problems). To alleviate these  problems, we present a service, the Lockers Rent Service, for keeping  the data of mobile users in a secure and safe space in a proxy at the  xed network, thus providing a solution to the device exposure problems.
sical layer evolve independently of IP---to both Sonet/SDH, and DWDM. IP runs over a huge variety of physical layers regardless of the underlying technology. However, a lot of repetition exists between the packet-switched IP layer and the circuitswitched physical layer. For example, a network must route both IP datagrams and circuit paths, yet, they use different routing protocols, and their implementations are incompatible. This makes simple and obvious operations infeasible. For example, if traffic increases between two neighboring routers, no simple or standard way exists to automatically increase the capacity between them. The problem is with how circuit switches interact with IP routers. TCP switching presents a method of interaction, promising automatic and dynamic circuit allocation. Packet-switched Internet With the speed benefits offered by circuit switching, why does the Internet use packet switching? Because of data traffic&apos;s bursty Pablo MolineroFern
This paper is concerned with the use of AI techniques in ecology. More specifically, we present a novel application of inductive logic programming (ILP) in the area of quantitative structure-activity relationships (QSARs). The activity we want to predict is the biodegradability of chemical compounds in water. In particular, the target variable is the half-life for aerobic aqueous biodegradation. Structural descriptions of chemicals in terms of atoms and ff The work described in this paper was conducted while the author was at the University of Freiburg, Machine Learning Lab. Georges-Ko hler-Allee Geb. 079, D-79110 Freiburg i. Br., Germany
An evidence theoretic classification method is proposed in this paper. In order to classify a pattern we consider its neighbours, which are taken as parts of a single source of evidence to support the class membership of the pattern. A single mass function or basic belief assignment is then derived, and the belief function and the pignistic (&quot;betting rates&quot;) probability function can be calculated. Then the (posterior) conditional pignistic probability function is calculated and used to decide the class label for the pattern.
The Object Constraint Language (OCL) is the established language for the specification of properties of objects and object structures in UML models. One reason that it is not yet widely adopted in industry is the lack of proper and integrated tool support for OCL. Therefore, we present a prototype tool, which analyzes the syntax and semantics of OCL constraints together with a UML model and translates them into the language of the theorem prover PVS. This defines a formal semantics for both UML and OCL, and enables the formal verification of systems modeled in UML. We handle the problematic fact that OCL is based on a three-valued logic, whereas PVS is only based on a two valued one.
In this paper we propose an error concealment algorithm for compressed video sequences. For packetization and transmission, a two layer ATM is utilized so that the location of information loss is easily detected. The coded image can be degraded due to channel error, network congestion, and switching system problems. Seriously degraded images may therefore result due to information loss represented by DCT coe$cients and motion vectors, and due to the interdependency of information in predictive coding. In order to solve the error concealment problem of intra frames, two spatially adaptive algorithms are introduced; an iterative and a recursive one. We analyze the necessity of an oriented high pass operator we introduce, and the requirement of changing the initial condition in iterative regularized recovery algorithm. Also, the convergence of iteration is analyzed. In recursive interpolation algorithm, the edge direction of the missing areas is estimated from the neighbors, and estimated edge direction is utilized for steering the direction of interpolation. For recovery of the lost motion vectors, an overlapped region matching algorithm is introduced. Several experimental results are presented. 
. Computing lower bounds to the best-cost extension of a tuple  is an ubiquous task in constraint optimization. A particular case of  special interest is the computation of lower bounds to all singleton tuples,  since it permits domain pruning in Branch and Bound algorithms.  In this paper we introduce MCTE(z), a general algorithm which allows  the computation of lower bounds to arbitrary sets of tasks. Its time and  accuracy grows as a function of z allowing a controlled tradeoff between  lower bound accuracy and time and space to fit available resources. Subsequently,  a specialization of MCTE(z) called MBTE(z) is tailored to  computing lower bounds to singleton tuples. Preliminary experiments on  Max-CSP show that using MBTE(z) to guide dynamic variable and value  orderings in branch and bound yields a dramatic reduction in the search  space and, for some classes of problems, this reduction is highly costeffective  producing significant time savings and is competative against  specialized algorithms for Max-CSP.  1 
this paper are applied to Census 2000 full microdata files Tier 3 access
Communication systems check integrity to protect information against alteration introduced by natural means such as noise and by malicious security attacks. This paper proposes that some integrity checks used for security should also be used for error control, since there are similarities between the  functions used for both purposes, and repeated checking can have a high cost. The paper extensively examines where integrity functions should be implemented in a network, and the dependencies between functions implemented in a node, since these limit the extent to which such amalgamation of function is  possible. The arguments presented in this paper mean that end-system-to-end-system (e.g. Transport layer) error checks will need to be cryptographically strengthened if they are to remain justifiable in the future.
The aim of this paper is to merge two approaches of software development: the component approach and the formal development approach. Developping  software components is now a technique widely used by the software industry. These two approaches are not so distant if we consider Bertrand Meyer&apos;s opinion: it is more complicated to reuse a component without contracts. One of the difficulties with the design by contract approach is to find the contracts. This difficulty can be removed by the use of the B method. In the B method, the software properties (the contracts) are expressed in the specifications. We present in this paper an approach to generate code in the spirit of the component approach from B specifications.
The production of a timetable for courses and exams in an  academic institution is a periodic activity, the frequency of which depends  on the institution: yearly, semi-annually, or quarterly. A timetable  must satisfy all the hard constraints and as many soft constraints as  possible. Timetable construction is one step in timetable production.
This paper proposes language concepts that facilitate the separation of an application into independent reusable building blocks and the integration of pre-build generic software components into applications that have been developed by third party vendors. A key element of our approach are on-demand remodularizations, meaning that the abstractions and vocabulary of an existing code base are translated into the vocabulary understood by a set of components that are connected by a common collaboration interface. This general concept allows us to mix-and-match remodularizations and components on demand.
. An analysis of physiological wake/sleep data is presented. We apply a recent method for the analysis of nonstationary time series with multiple operating modes. In particular, it is possible to detect and to model a switching of the dynamics and also a less abrupt, time consuming drift from one mode to another. This is achieved by an unsupervised algorithm that segments the data according to inherent modes, and a subsequent search through the space of possible drifts. The application to wake/sleep data demonstrates that analysis and modeling of realworld time series can be improved when the drift paradigm is taken into account. In the case of wake/sleep data, we hope to gain more insight into the physiological processes that are involved in the transition from wake to sleep. 1 Introduction  Modeling dynamical systems through a measured time series is commonly done by reconstructing the state space with time-delay coordinates [7, 9]. The prediction of the time series can then be accom...
Conventional vision systems designed to reconstruct the surfaces of objects generally handle only diffuse objects or objects with a minimal amount of specularity. The presence of specular reflections produced by highly specular surfaces confuse traditional vision systems and may mask the true location of an object and lead to incorrect measurements.  This article describes the development of a fixed vision system which can recover the local surface structure of highly specular objects. The system utilizes a commercial trinocular stereo vision system and a low-power twodimensional illuminant. The local surface structure of an object is obtained by projecting coded light patterns onto the object. As many objects are neither fully specular nor fully diffuse, the statistical method of mixture models is used to divide an object into specular and diffuse components in order to recover local surface structure from both specular and diffuse regions.  The system was originally designed to assist in the insitu  repair and maintenance of man-made orbital objects. One of the key challenges facing computer vision systems used in space is the presence of specular surfaces on virtually all man-made orbital objects. Because it was designed to be used in outer space, the system was designed to operate without traditional high-powered illuminants, such as laser beams, whose radiation can interfere with sensitive space instruments. While the system was designed to operate in outer space, it offers many other practical non-space applications.  1. 
This paper describes a methodology for agent oriented software engineering, called Tropos. Tropos is based on three key ideas. First, the notion of agent and all the related mentalistic notions (for instance: goals and plans) are used in all phases of software development, from the early analysis down to the actual implementation. Second, Tropos covers also the very early phases of requirements analysis, thus allowing for a deeper understanding of the environment where the software must operate, and of the kind of interactions that should occur between software and human agents. Third, Tropos adopts a transformational approach to process artifacts refinement. The methodology is partially illustrated with the help of a case study.
model for outdoor distributed embedded systems. Central to SP are the concepts of space and spatial reference, which provide applications with a virtual resource naming in networks of embedded systems. A network resource is referenced using its expected physical location and properties. Together with other SP features, such as reference consistency and access timeout, they help programmers cope with highly dynamic network configurations in a network-transparent fashion.
This paper will describe ongoing efforts to ease the information retrieval process on metadata, using a SuperTable/Scatterplot framework recently named VisMeB (Visual Metadata Browser). Based on the combination of a SuperTable (in two design variants) and a Scatterplot (also in two design variants), users can follow different search strategies to achieve results. Usability testing has showen, that an integration of our proposed visualizations supports most user&apos;s search style.
Users of database applications, especially in the e-commerce domain, often resort to exploratory &quot;trial-and-error&quot; queries since the underlying data space is huge and unfamiliar, and there are several alternatives for search attributes in this space. For example, scouting for cheap airfares typically involves posing multiple queries, varying flight times, dates, and airport locations. Exploratory queries are problematic from the perspective of both the user and the server. For the database server, it results in a drastic reduction in effective throughput since much of the processing is duplicated in each successive query. For the client, it results in a marked increase in response times, especially when accessing the service through wireless channels. In this paper, we investigate the design of automated techniques to minimize the need for repetitive exploratory queries. Specifically, we present SAUNA, a serverside query relaxation algorithm that, given the user&apos;s initial range query and a desired cardinality for the answer set, produces a relaxed query that is expected to contain the required number of answers. The algorithm incorporates a rangequery -specific distance metric that is weighted to produce relaxed queries of a desired shape (e.g., aspect ratio preserving), and utilizes multi-dimensional histograms for query size estimation. A detailed performance evaluation of SAUNA over a variety of multi-dimensional data sets indicates that its relaxed queries can significantly reduce the costs associated with exploratory query processing. Keywords: Database, Query Relaxation, Histogram 1 
A personal and rather discursive account is given of the  background to the start of work in the early 1970s at Newcastle on software  fault tolerance, and of how work has developed to encompass forward as well  as backward error recovery, and parallel and distributed software as well as  sequential programs. A major theme of the paper is that of the links between  this work and that carried out elsewhere in connection with the topic of objectoriented  programming, in particular on concepts such as generic classes and  functions, exception-handling, delegation and reflection.
Interval Temporal Logic (ITL) is a finite-time linear temporal logic with applications in hardware verification,  temporal logic programming and specification of multimedia documents. Due to the inherently nonelementary  complexity of its decision problem, effcient ITL-based verification tools have been diffcult to develop.
In an on-demand video system, requests for a video file arriving within a period of time can be batched together and served with a single multicast stream; thereby reducing the bandwidth requirement compared with the unbatched case. In this paper, we study various batching schemes to meet delay and profitability requirements --- in the windowsize based and moving-average schemes, maximumand average user delay respectively are guaranteed; in the batch-size based scheme, minimal profitability is maintained; in the adaptive scheme, both the delay and profitability can be balanced. We analyse and compare these schemes in terms of the delay experienced by the users, and their profitability (the number of users in a batch, the number of concurrent streams, etc.). I. Introduction  Video-on-demand (VOD) refers to video services in which a user is able to request from a server any video content at any time. VOD encompasses many applications such as movie-on-demand, news-on-demand, distance lear...
this article, we prove that a family of SAT algorithms including WalkSAT/SKC is PAC for 2-SAT. Unfortunately, the case for 3-SAT or general SAT seems much harder, and remains open. However, we have strong empirical evidence suggesting that even if WalkSAT/SKC is essentially incomplete for 3-SAT and general SAT, this does not have a signi cant aect on its behavior in practice.
This paper presents an efficient online mode estimation  algorithm for a class of sensor-rich, distributed  embedded systems, the so-called hybrid systems.  A central problem in distributed diagnosis of hybrid  systems is efficiently monitoring and tracking  mode transitions. Brute-force tracking algorithms  incur cost exponential in the numbers of sensors  and measurements over time and are impractical for  sensor-rich systems. Our algorithm uses a model  of system&apos;s temporal discrete-event behavior such  as a timed Petri net to generate a prior so as to focus  distributed signal analysis on when and where  to look for mode transition signatures of interest,  drastically constraining the search for event combinations.  The algorithm has been demonstrated for  the online diagnosis of a hybrid system, the Xerox  DC265 printer.  1 
Embedded systems require maximum performance from a processor within significant constraints in power consumption and chip cost. Using software pipelining, high-performance digital signal processors can often exploit considerable instruction-level parallelism (ILP), and thus significantly improve performance. However, software pipelining, in some instances, hinders the goals of low power consumption and low chip cost. Specifically, the registers required by a software pipelined loop may exceed the size of the physical register set.
In this paper, we show that under the constant delay model the placement problem is equivalent to minimizing a weighted sum of wire lengths. The weights can be efficiently computed once in advance and still accurately reflect the circuit area throughout the placement process. The existence of an efficient and accurate cost function allows us to directly optimize circuit area. This leads to better results compared to heuristic edge weight estimates or optimization for secondary criteria such as wire length.. We leverage
re called the circuits&apos; dc operating points. For example, 2  inherently nonlinear bistable circuits that possess two stable isolated equilibrium points are used in a variety of electronic designs, such as static random-access memory cells, latches, flip-flops, and shift registers. The operation of a Schmitt trigger is also intimately related to the circuit&apos;s ability to possess multiple dc operating points. Oscillator circuits employ structures that require the presence of nonlinear components. All these circuits exhibit a rich variety of nonlinear behaviors and can possess multiple operating points.  Recent advances in computer aided design (CAD) tools for circuit simulation have set designers free from the need to perform lengthy and tedious, but often only approximate, calculations to compute circuit currents and voltages. The SPICE circuit simulator [1, 2, 3] has become an industry standard, and many SPICE-like CAD tools are in use today. Nevertheless, the problem of computin
Studying similarity of objects by looking at their shapes arises naturally in many applications. However, under different viewpoints one and the same object appears to have different shapes. In addition, the correspondence between their feature points are unknown to the viewer.
This paper describes RoxyBot, one of the top-scoring agents in the First International Trading  Agent Competition, TAC-2000. A TAC agent simulates one vision of future travel agents: it  represents a set of clients in simultaneous auctions, trading complementary (e.g., airline tickets  and hotel reservations) and substitutable (e.g., symphony and theater tickets) goods. RoxyBot  faced two key technical challenges in TAC: (i) allocation---assigning purchased goods to clients  at the end of a game instance so as to maximize total client utility, and (ii) completion---  determining the optimal quantity of each resource to buy and sell given client preferences,  current holdings, and market prices. For the dimensions of TAC, an optimal solution to the  allocation problem is tractable, and RoxyBot uses a search algorithm based on A    to produce  optimal allocations. An optimal solution to the completion problem is also tractable, but in  the interest of minimizing bidding cycle time, RoxyBot solves the completion problem using  beam search with a greedy heuristic, producing approximately optimal completions. RoxyBot&apos;s  completer relies on an innovative data structure called a priceline.
In a highly innovative market, wireless systems nowadays undergo very  shortproducjTH croduc  Due to these tough timing  cingyFTOHyE  the  time-czyEffOffj proc-c  of prototyping is  oftenneglecffTH  jeopardizing the entire produc becucffsucucffffjH  HeavyapplicyEffO ofautomatic toolsco  allow for rapid  prototypingovercHOOy  this unfortunate situation and de-risking theproduc  cucVyEffff  However,  theapplicOyEff ofautomatic  tools alone does not speed up the  prototypingprocot suffcotypin  By  reffecyEff  oncyzOzz designprocyHjUj several paradigms for faster prototyping are  cyffzffFTyE  named the Five-Ones  Approach: One team, One environment, OneceyH  OnedocVFffVyEffzj  and One cey revision tool. Based on suc a Five-Ones Approacy  acOOUTyEHV  prototyping environment to implement a prototyping design from ffrst idea to ffnal implementation is presented in this paper.
This paper describes the results of a questionnaire on examination timetabling sent to the  registrars of ninety five British Universities. The survey asked questions in three specific  categories. Firstly, universities were asked about the nature of their examination timetabling  problem: how many people, rooms, periods are involved and what difficulties are  associated with the problem? Secondly, we asked about how the problem is solved  currently, whether a manual or automated system is used. Lastly, we asked what qualities  are required in a good timetable. We conclude by making some comments, based on  the survey replies, as to what sort of criteria a general automated timetabling system must  meet.  1. 
In a world-wide computational Grid, thousands of users compete for computing,  storage and network facilities, so optimising the use of these resources is critical for  both the users and resource providers. Users typically want their jobs to be executed  as fast as possible, while the goal of a Grid infrastructure is to assure some specifi c  quality of service for all users. We have developed an optimisation strategy based on  an economic model where data-seeking agents trade with data-storing agents in order  to negotiate optimal prices for exchanging data fi les.
... increasingly exploited in malicious attacks. Byzantine fault tolerance allows replicated systems to mask some software errors but it is expensive to deploy. This paper describes a replication technique, BASE, which uses abstraction to reduce the cost of Byzantine fault tolerance and to improve its ability to mask software errors. BASE reduces cost because it enables reuse of off-the-shelf service implementations. It improves availability because each replica can be repaired periodically using an abstract view of the state stored by correct replicas, and because each replica can run distinct or non-deterministic service implementations, which reduces the probability of common mode failures. We built an NFS service where each replica can run a different off-the-shelf file system implementation, and an object-oriented database where the replicas ran the same, non-deterministic implementation. These examples suggest that our technique can be used in practice -- in both cases, the implementation required only a modest amount of new code, and our performance results indicate that the replicated services perform comparably to the implementations that they reuse.
Recently, a nonlinear transformation of autocorrelation coefficients named Phase AutoCorrelation (PAC) coefficients has been considered for feature extraction [1]. PAC based features show improved robustness to additive noise as a result of two operations, performed during the computation of PAC, namely energy normalization and inverse cosine transformation. In spite of the improved robustness achieved for noisy speech, these two operations lead to some degradation in recognition performance for clean speech. In this paper, we try to alleviate this problem, first by introducing the energy information back into the PAC based features, and second by studying alternatives to inverse cosine function. Simply appending the frame energy as an additional coefficient in the PAC features has resulted in noticeable improvement in the performance for clean speech. Study of alternatives to inverse cosine transformation leads to a conclusion that linear transformation is the best for clean speech, while nonlinear functions help to improve robustness in noise.
Considering each occurrence of a word w in a recurrent  infinite word, we define the set of return words of w to be the set of all  distinct words beginning with an occurrence of w and ending exactly  just before the next occurrence of w in the infinite word. We give a  simpler proof of the recent result (of the second author) that an infinite  word is Sturmian if and only if each of its factors has exactly two return  words in it. Then, considering episturmian infinite words, which are a  natural generalization of Sturmian words, we study the position of the  occurrences of any factor in such infinite words and we determinate the  return words. At last, we apply these results in order to get a kind of  balance property of episturmian words and to calculate the recurrence  function of these words.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii I Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 A. Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1. The ffartiffcial life bridge&quot; . . . . . . . . . . . . . . . . . . . . . . 1 2. From nature to technology . . . . . . . . . . . . . . . . . . . . . 3 3. From technology to nature . . . . . . . . . . . . . . . . . . . . . 4 B. Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 II Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 A. Background: Machine learning . . . . . . . . . . . . . . . . . . . . . 10 1. Evolutionary algorithms . . . . . . . . . . . . . . . . . . . . . . . 11 2. Endogenous fftness . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3. Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . 18 B. Local selection . . . . . . . . . . . . . . . . . . . . . . . . . . ...
This paper deals with Modern Greek (henceforward MG) words ending in -menos: (1) To the nifiko wedding-dress.NEUT.N itan was ra-meno sew.PRTC.NEUT.N apo by ton the rafti tailor me with hrisi golden klosti. thread &quot;The wedding dress was sewn by the tailor with golden thread&quot;. Agreeing with the proposal of Markantonatou et al. (1996), according to which Modern Greek words ending in -menos should be considered to be participles rather than adjectives for reasons we present in Sections (3) and (3.2) below, our aim here is twofold: 1. to try to account for the fact that participles in -menos appear in the typical position of adjectives in Modern Greek (see Section (3.1)), and 2. to provide a formal account in LFG for participle-adjective formation in Modern Greek (see Section (5.2)) As far as the first of our aims is concerned, we show in Section (4) that participleadjective formation in Modern Greek is better accounted for in the spirit of Bresnan &apos;s (1996) proposal for participle-adjective conversion in English, and not in the spirit of the predictions of Ackerman (1992) and Markantonatou (1995), which we also present briefly in the same section. The main contribution of this paper, though, is the formalization in LFG of Bresnan&apos;s (1996) proposal for participle-adjective formation that we present in Section (5.2). The formalization we propose does not only cover the Modern Greek and English data at hand, but we are confident that it can easily be extended in order to account for the phenomenon of participle-adjective formation cross-linguistically. 2 Modern Greek words in -menos  Most of the literature todate has focused on the question whether the words ending in -menos in Modern Greek are adjectives or they bear a verbal nature, i.e., they are participles. The analyses p...
We perform a detailed theoretical and empirical comparison of the dual and hidden variable encodings of non-binary constraint satisfaction problems. We identify a simple relationship between the two encodings by showing how we can translate between the two by composing or decomposing relations. This translation suggests that we will tend to achieve more pruning in the dual than in the hidden variable encoding. We prove that achieving arc-consistency on the dual encoding is strictly stronger than achieving arc-consistency on the hidden variable, and this itself is equivalent to achieving generalized arc-consistency on the original (non-binary) problem. We also prove that, as a consequence of the unusual topology of the constraint graph in the hidden variable encoding, inverse consistencies like neighborhood inverse consistency and path inverse consistency collapse down onto arc-consistency. Finally, we propose the &quot;double encoding&quot;, which combines together both the dual and the hidden variable encodings.
Learning general functional dependencies is  one of the main goals in machine learning.
For a qualitative and quantitative assessment of context prediction and recognition methods, real-world data sets are inevitable. By collecting sensor data on a single notebook over a period of a few months we got a rather large log file of homogeneous and heterogeneous features reflecting the users activities during this time frame. In this paper we present which devices were exploited as sensors, which information was logged and how this information was stored for further processing by classification algorithms.
Commercially available routers typically have a monolithic operating  system that cannot be easily tailored and upgraded and support new network  protocols. PromethOS is a modular router architecture based on Linux  2.4 which can be dynamically extended by plugin modules that are installed  in the networking kernel. To install and configure plugins we present a novel  signaling protocol that establishes explicitly routed paths transiting selected  nodes in a predefined order. Such paths can be non-simple, where a given  node is being visited more than once.
Autonomic Computing is an exciting research direction that aims to provide self-configuration, selfoptimization, self-healing and self-protection capabilities to computer systems.
Development of fast computational methods to solve the Poisson-Boltzzb  equation (PBE) for molecular electrostatics is important because of the central role played by electrostatic interactions in many biological processes. The accuracy and stability of the solution to the PBE is quite sensitive to the boundary layer between the solvent and the solute which defines the solvent-accessible surface. In this paper, we propose a new interface-layer-focused PBE solver for efficiently computing the electrostatic potential for large molecules. Our method analytically constructs the solvent-accessible surface of molecules and then builds nested iso-surface layers outwards and inwards from the surface using the distance field around the surface. We then develop a volume simplification algorithm to adaptively adjust the density of the irregular grid based on the importance to the PBE solution. We also  generaliz  finite difference methods on our irregular grids using Taylor series expansions. Our algorithm achieves about three times speedup in the iterative solution process of PBE, with more accurate results on an analytical solvable testing case, compared with the popular optimiz6 DelPhi program. Our approach can also be applied directly to solve partial differential equations arising in other application domains.
Data replication is one of the main techniques by which database systems can hope to meet the stringent temporal constraints of current time-critical applications, especially Web-based directory and electronic commerce services. A pre-requisite for realizing the benefits of replication, however, is the development of high-performance concurrency control mechanisms. We present in this paper  MIRROR (Managing Isolation in Replicated Realtime Object Repositories), a concurrency control protocol specifically designed for firm-deadline applications operating on replicated real-time databases. MIRROR augments the optimistic two-phase locking (O2PL) algorithm developed for non real-time databases with a novel and simple to implement state-based conflict resolution mechanism to fine-tune real-time performance.  Using a detailed simulation model, we compare MIRROR &apos;s performance against the real-time versions of a representative set of classical protocols for a range of transaction workloads an...
The structural layout information of scanned document pages is valuable for a wide range of document processing applications such as automatic document searching, document delivery and automated data entry. This paper describes the  classification of scanned document pages into different classes of physical layout structures. The page layout classification technique proposed in this paper uses a combination of geometry-based and content-based zone features calculated from optical character recognition (OCR) output.
We present an extension to the TyCO type system that is able to identify linear channels. We prove some technical results (e.g. type preservation w.r.t. reduction) and present an algorithm for inferring channels usage from process expressions. Our major contribution is the inference of linear information in a calculus with recursive equations rather than replication. 1 
In the research of rule extraction from neural networks, fidelity describes how well the rules mimic the behavior of a neural network while accuracy describes how well the rules can be generalized. This paper identifies the fidelity-accuracy dilemma. It argues to distinguish rule extraction using neural networks and rule extraction for neural networks according to their differing goals, where fidelity and accuracy should be excluded from the rule quality evaluation framework, respectively.
Under high loads, a Web server may be servicing manyhundreds of connections concurrently. In traditional
Large repositories of data contain sensitive information that must be protected  against unauthorized access. The protection of the confidentiality of this information  has been a long-term goal for the database security research community and for the  government statistical agencies. Recent advances in data mining and machine learning  algorithms, have increased the disclosure risks that one may encounter when releasing  data to outside parties.
In evolution strategies with neighborhood attraction (EN)  the concepts of neighborhood cooperativeness and learning rules known  from neural maps are transferred onto the individuals of evolution strategies.
We demonstrate the first instance of a real  on-line robot learning to develop feasible  flying (flapping) behavior, using evolution. Here we
With today&apos;s high-level plan languages like GOLOG or rpl it is possible for mobile robots to cope with complex problems. Unfortunately, instructing the robot what to do or interacting with it is still awkward. Usually, instructions are given by loading the appropriate program and interacting amounts to little more than pressing buttons positioned on the robot itself.
The version of Kerberos presented by Burrows et al. [5] is  fully mechanised using the Inductive Method. Two models are presented,  allowing respectively the leak of any session keys, and of expired session  keys. Thanks to timestamping, the protocol provides the involved parties  with strong guarantees in a realistically hostile environment. These  guarantees are supported by the generic theorem prover Isabelle.
For non-trivial (open) queueing networks, trac-based decomposition often represents the only feasible { and at the same time fast { solution method besides simulation. The network is partitioned into individual nodes which are analyzed in isolation with respect to approximate internal trac representations. Since the correlations of network trac may have a considerable impact on performance measures, they must be captured to some extent by the employed trac descriptors. The decomposition methodology presented in this paper is based on Markovian arrival processes (MAPs), whose correlation structure is determined from the busy-period behavior of the upstream queues. The resulting compact MAPs in connection with sophisticated moment matching techniques allow an ecient decomposition of large queueing networks. Compared with [14], the output approximation of MAP/PH/1(/K) queues { the crucial step in MAP-based decomposition { is re  ned in such a way that also higher moments of the number of customers in a busy period can be taken into account. Numerical experiments demonstrate the substantially enhanced precision due to the improved output models and plumb the new opportunities in the trade-o between accuracy and eciency.
A vision system capable of extracting features in tropical environment for vehicle guidance is described in this report. The system is designed to operate in semi-structured terrain having mud and gravel roads lined irregularly with vegetation. The scene captured by a CCD colour camera is digitised into 24-bit colour images with a resolution of 320x240 pixels. The system classifies driveable, i.e. roads, and nondriveable areas, i.e. vegetation and sky, of the scene based on results of colour segmentation. Training samples of driveable and non-driveable features of the terrain to be explored, stored in a database, are used to classify blocks of pixels using only the hue information content of the images. The final output is the centroid of driveable areas of the scene in front of the vehicle and it is computed at a rate of 5 Hz.The system is based on 2D model of the real world.
The existence of semantically tagged Web pages is crucial to bring the Semantic Web to life. But it is still costly to develop and maintain Web applications that offer data and meta-data. Several standard Web engineering methodologies exist for designing and implementing Web applications. In this paper we introduce a technique to extend existing Web engineering techniques to develop semantically tagged Web applications. The novelty of this technique is the definition and implementation of a mapping from XML Schema to ontologies that can be used to automatically generate RDF meta-data from XML content documents.
The paper investigates a method for identifying risky records within a microdata file. The method, Special Uniques Identification, involves inferring population uniqueness for a set of key variables from sample uniqueness for a collapsed form of that key variable set. The method is shown to be useful with being considerably higher probability of than the probability of population uniqueness given sample uniqueness on the uncollapsed variables. I. 
This paper presents a pragmatic approach to the simulation of an autonomous mobile system, the Bremen Autonomous Wheelchair. The software architecture allows to directly switch between simulation and reality. The major idea of this paper is not to try to build a perfect simulation of the target system from scratch, but to let the simulation evolve during the experiments with the real system. INTRODUCTION  There is no disputing that simulations are useful tools in the development of autonomous mobile robotics systems. Hence, most commercial mobile robotics platforms are shipped with simulator packages that imitate the system&apos;s behavior, e.g. the Nomad 200 or the Pioneer. However, these simulations are very specialized. As the mobile robot is only able to move in 2-D space, the environments that can be modeled are also restricted to two dimensions. In addition, the packages only simulate the sensors the original robot is equipped with, and it is impossible to include additional sensors. ...
This paper presents an approach for the global optimization of constrained  nonlinear programming problems in which some of the constraints  are non-analytical (non-factorable), defined by a computational  model for which no explicit analytical representation is available.
In his paper we describe an ex rac ive me hod of crea ing very shor  summaries or gis s ha cap ure he essence of a ne ws s ory using a linguis ic  echnique called lexical chaining. The recen in er es in robus gis ing and i le  genera ion echniques origina es from a need o imp rove he indexing and  browsing capabili ies of in erac ive digi al mul im edia sys ems. More specifically hese sys ems deal wi h s reams of con inuous da a,  lik e a news programme, ha require fur her anno a ion before hey can be presen ed o he user  in a meaningful way. We au oma ically evalua e he performance of our lexical  chaining-based gis er wi h respec o four baseline ex rac ive gis ing me hods  on a collec ion of closed cap ion ma erial  ak en fr om a series of news broadcas s. We also repor resul s of a human-based evalua ion of summary quali y.  Our resul s show ha our novel lexical chaining approach o his problem ou - performs s andard ex rac ive gis ing me hods.  1  
The purpose of this reading paper is to empirically analyse and compare several heuristic algorithms that can be applied to combinatorial optimisation problems. The main focus of the paper is heuristic paradigms that are generally applicable to a wide variety of problem types. The three techniques investigated here are repeated local search, simulated annealing, and genetic algorithms. Each heuristic is investigated individually, with a small amount of theoretical discussion, to determine how the performance of each method can be changed by altering algorithm parameters. The Traveling Salesman Problem is used as a problem indicative of combinatorial optimisation to test the three heuristics. It is an NP-hard problem that shares many properties common to other combinatorial optimisation problems and it is hoped that the behaviour of the heuristics when applied to the Traveling Salesman Problem will be similar to that experienced for other combinatorial problems. To gain some insight int...
Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with finite state spaces. In this work we show that ALP solutions are not limited only to MDPs with finite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors.
In the paper we present detailed analyses of two machine translation systems when applied to documents of a previously unseen domain: agricultural texts from the European Union. The two systems compared are a statistical machine translation (SMT) system using the freely available ISI ReWrite Decoder (Germann, 2003a), and the rule-based machine translation system MATS (Sågvall Hein et al., 2002). For the purpose of comparison we use a sentence-aligned Swedish-English corpus of approximately 75,000 words per language, where 90% are used for training and 10% are used for evaluation. In the paper we discuss the outcome of automatic evaluation and the results of our manual quality assessment.
This paper describes a new and different paradigm for real-time command and control that we show can provide a static, polynomial-time scheduling algorithm. Current efforts in real-time scheduling have been unable to predict system performance, and use a unpredictable &quot;dynamic&quot; scheduling algorithm. The Nation&apos;s Air Traffic Control (ATC) System has been unable to satisfy performance requirements, and is extremely expensive. An editorial in &quot;USA Today&quot; (419 -1999) cites expenditures in ATC at $41 billion. But, current multiprocessor technology cannot do the job.
Ring network architectures that employ spatial reuse permit concurrent transmissions of messages over different links. While spatial reuse increases network throughput, it may also cause starvation of nodes. To alleviate this problem, various policies have been suggested in the literature. In this paper we concentrate on a class of such policies that achieve fairness by allocating transmission quotas to nodes. For such policies, weprovide mechanisms for improving delays and increasing overall throughput without compromising fairness.
This article presents an analytical and computational framework for the compression of particle data resulting from hierarchical approximate treecodes such as the Barnes-Hut and Fast Multipole Methods. Due to approximations introduced by hierarchical methods, various parameters (such as position, velocity, acceleration, potential) associated with a particle can be bounded by distortion radii. Using this distortion radii, we develop storage schemes that guarantee error bounds while maximizing compression. Our schemes make extensive use of spatial and temporal coherence of particle behavior and yield compression ratios higher than 12:1 over raw data, and 6:1 over gzipped (LZ) raw data for selected simulation instances. We demonstrate that for uniform distributions with 2M particles, storage requirements can be reduced from 24 MB to about 1.8 MB (about 7 bits per particle per timestep) for storing particle positions. This is significant because it enables faster storage/retrieval, better temporal resolution, and improved analysis. Our results are shown to scale from small systems (2K particles) to much larger systems (over 2M particles). The associated...
We instance our experience with six public-domain global optimization software products and report comparative computational results obtained on a set of eleven test problems. The techniques used by the software under study include integral global optimization, genetic algorithms, simulated annealing, clustering, random search, continuation, Bayesian, tunneling, and multi-level methods. The test set contains practical problems: least median of squares regression, protein folding, and multidimensional scaling. These include non-dierentiable, and also discontinuous objective functions, some with an exponential number of local minima. The dimension of the search space ranges from 1 to 20. We evaluate the software in view of engineers addressing black box global optimization problems, i.e. problems with an objective function whose explicit form is unknown and whose evaluation is costly. Such an objective function is common in industry. It is for instance given under the form of computer programmes involving a simulation. Key words: Global optimization, software, test problems, black box, direct methods, genetic algorithms, simulated annealing, clustering, non-convex optimization.
Application partitioning is the task of breaking up the functionality of an application into distinct entities  that can operate independently, usually in a distributed setting. Many distributed applications are created by  partitioning their centralized versions. Traditional application partitioning entails re-coding the application functionality  to use a middleware mechanism for communication between the different entities. This process is tedious  and error-prone. Automating the partitioning process while preserving correctness and ensuring good performance  of partitioned applications can greatly facilitate development of a large class of distributed applications. We review  the main advantages and challenges of automatic application partitioning and present the J-Orchestra system. JOrchestra  is an automatic partitioning system for Java programs. J-Orchestra takes as input Java applications in  bytecode format and transforms them into distributed applications, running on distinct Java Virtual Machines.
Accommodating the uncertain latency of load instructions is one of the most vexing problems in in-order microarchitecture design and compiler development. Compilers can generate schedules with a high degree of instruction-level parallelism but cannot effectively accommodate unanticipated latencies; incorporating traditional out-of-order execution into the microarchitecture hides some of this latency but redundantly performs work done by the compiler and adds additional pipeline stages. Although effective techniques, such as prefetching and threading, have been proposed to deal with anticipable, long-latency misses, the shorter, more diffuse stalls due to difficult-to-anticipate, first- or second-level misses are less easily hidden on inorder architectures. This paper addresses this problem by proposing a microarchitectural technique, referred to as two-pass pipelining, wherein the program executes on two in-order back-end pipelines coupled by a queue. The &quot;advance&quot; pipeline executes instructions greedily, without stalling on unanticipated latency dependences (executing independent instructions while otherwise blocking instructions are deferred). The &quot;backup&quot; pipeline allows concurrent resolution of instructions that were deferred in the other pipeline, resulting in the absorption of shorter misses and the overlap of longer ones. This paper argues that this design is both achievable and a good use of transistor resources and shows results indicating that it can deliver significant speedups for in-order processor designs.
Let x(t) be a trajectory of the gradient of a real analytic function and  suppose that x0 is a limit point of x(t). We prove the gradient conjecture of R. Thom  which states that the secants of x(t) at x0 have a limit. Actually we show a stronger  statement: the radial projection of x(t) from x0 onto the unit sphere has finite length.
Synthesis of asynchronous circuits from Signal Transition Graphs (STGs) involves resolving state coding conflicts. The refinement process is generally done automatically using heuristics and often produces sub-optimal solutions, which have to be corrected manually. This paper presents a framework for an interactive refinement process aimed to help the designer. It is based on the visualization of conflict cores, i.e., sets of transitions causing coding conflicts, which are represented at the level of finite and complete prefixes of STG unfoldings.
Replication is known to offer high availability in the presence of failures. This paper considers the case of a client making invocations on a group of replicated servers. It identifies attributes that typically characterise group invocation and replica management, and the options generally available for each attribute. A combination of options on these attributes constitutes a policy. The paper proposes an implementation framework which, by its group-oriented nature, simplifies the task of supporting these policies. It then considers a client (in UCL, London) making invocations on a replica group (in Newcastle, UK) over the Internet. It evaluates the response latencies for four policies that seem appropriate for this set-up. The evaluation takes into account the timing of server crashes with respect to client invocations; both real and virtual failures are considered, the latter being not uncommon in the Internet environment. The experiments are carried out using a CORBA compliant system called NewTop.
This paper presents an abstraction layer for short vector SIMD ISA extensions like Intel&apos;s SSE, AMD&apos;s 3DNow!, Motorola&apos;s AltiVec, and IBM&apos;s Double Hummer. It provides unified access to short vector instructions via intermediate level building blocks. These primitives are C macros that allow, for instance, portable and highly effcient implementations of discrete linear transforms like FFTs and DCTs.
In the energy spectrum of an occlusion sequence, the distortion term has the same orientation as the velocity of the occluding signal. Recent works  claimed that this oriented structure can be used to distinguish the occluding  velocity from the occluded one. Here, we argue that the orientation structure of the distortion cannot always work as a reliable feature due to the rapidly decreasing  energy contribution. This already weak orientation structure is further blurred by a superposition of distinct distortion components. We also indicate that the  superposition principle of Shizawa and Mase for multiple motion estimation needs to be adjusted.
Web-based learning is very advantageous for teachers and learners, specifically if the  system adapts itself to the user&apos;s personal needs. Yet, many of the currently available  systems suffer from the limitations of the underlying Web paradigm. We implemented  the SmexWeb    framework for Web-based learning systems. This framework increases  interactivity between learner and teaching system in two particular ways. The system  transfers more valuable information from the client to the server by partially bypassing  HTTP as underlying protocol. Furthermore we use a technique we call &quot;Passive  Navigation&quot; to give the system a chance to take over control if the user remains inactive  for a certain amount of time. To verify our framework we built an interactive course, a  so-called SmexWeb application, that was used by first year computer science minor  students.
A fundamental step towards broadening the use of real world image-based visual servoing is to deal with the important issues of reliability and robustness. In order to address this issue, a closed loop control law is proposed that simultaneously accomplishes a visual servoing task and is robust to a general class of external errors. This generality allows concurrent consideration of a wide range of errors including: noise from image feature extraction, small scale errors in the tracking and even large scale errors in the matching between current and desired features. This is achieved with the application of widely accepted statistical techniques of robust M-estimation. The M-estimator is integrated by an iteratively re-weighted method. The Median Absolute Deviation is used as an estimate of the standard deviation of the inlier data and is compared with other methods. This combination is advantageous because of its high efficiency, high breakdown point and desirable influence functions. The robustness and stability of the control law is shown to be dependent on a subsequent measure of position uncertainty. Furthermore the convergence criteria of the control law are investigated. Experimental results are presented which demonstrate visual servoing tasks which resist severe outlier contamination.
CLF/Mekano is a distributed object infrastructure oriented towards the high-level coordination of coarse  grain components. Unlike other infrastructures of the same class, such as CORBA or DCOM, coordination in  CLF/Mekano is built-in at the lowest level, namely at the inter-component communication protocol level, and  not as a side service (such as the event, transaction or negotiation services of Corba). Although the CLF protocol  is &quot;light-weight&quot; (it relies on very few concepts and only 8 communication &quot;verbs&quot;), it makes the design and  implementation of components more complex, but also more valuable if it can be re-used. The Mekano library  has been developed in order to deal with this complexity, targeting re-usability. It provides ready-to-use generic  classes of customizable components, as well as useful component parts which can be re-assembled according  to application specific needs. Of course, further layers of domain-specific libraries (so called business object  libraries), can then be developed on top of Mekano, to provide ready-to-use components dedicated to specific  business needs (in the line of Enterprise Java Beans).
This paper studies the system configuration problem for the Forward Resource Reservation (FRR)-enabled Optical Burst Switching (OBS) networks, where a pre-transmitted Burst Header Packet (BHP) reserves resources in an aggressive manner Specifically, we determine the channel holding time adjustment to optimize the system performance in terms of the minimum reservation overhead and the maximum net performance gain, respectively. Theoretical analysis is conducted to find the adjustment threshold for different optimization objectives. Numerical and simulation results have also justified the effectiveness of our solutions.
Facade texturing is a key point for realistic rendering of virtual city models, since it suggests to the viewer a level of detail that is much higher than actually provided by the geometry. Facade textures are usually derived from terrestrial imagery acquired from a position on the ground. One problem frequently encountered is the disturbance of texture images by partial occlusion of the faade with other objects. These occluding objects can be moving objects such as pedestrians and cars or static objects such as trees and street signs. This paper presents a method for the detection and removal of these disturbances and allows for the generation of occlusion-free texture images by multi-image fusion.
Advances in genome science have created a   surge of data. These data critical to scientific   discovery are made available in thousands of   heterogeneous public resources. Each of these   resources provides biological data with a   specific data organization, format, and quality,   object identification, and a variety of capabilities   that allow scientists to access, analyze, cluster,   visualize and navigate through the datasets. The   heterogeneity of biological resources and their   increasing number make it difficult for scientists   to exploit and understand them. Learning the   properties of a new resource is a tedious and   time-consuming process, often made more   difficult by the many changes made on the   resources (new or changed information,   capabilities) that stress scientists keeping their   knowledge up-to-date. Therefore many scientists   master a few resources while ignoring others   that may provide additional data and useful   capabilities.     1. 
In this paper we discuss several issues related to automated text classification of web sites. We analyze the nature of web content and metadata in relation to requirements for text features. We find that HTML metatags are a good source of text features, but are not in wide use despite their role in search engine rankings. We present an approach for targeted spidering including metadata extraction and opportunistic crawling of specific semantic hyperlinks. We describe a system for automatically classifying web sites into industry categories and present performance results based on different combinations of text features and training data. This system can serve as the basis for a generalized framework for automated metadata creation.
Cooperative Information Systems (CISs) have to manage with data located in different large information repositories distributed over communication networks. Multidatabase systems have been proposed as a solution to work with different pre-existing autonomous databases. Query processing in multidatabase systems requires a particular study because of the autonomy and heterogeneity of the component databases. In this paper we present the query processing of a multidatabase system with a global view expressed in Description Logics (DL). Query optimization techniques like semantic and caching optimization in this system are also explained.
ID: 21185374  Target Disease: Diabetes Mellitus, Non-Insulin-  Dependent; Diabetes  Other Diseases: Atherosclerosis; Coronary Artery  Disease  Findings: CANADIAN; Obesity; Related; Presence  Genes: hnf1a s319; s319; hnf1a  Alleles: none  Mutations: hnf1a  Variants: none  Polymorphisms: none  Genotypes: none  Chromosomes: none  Although word-sense ambiguity led to spurious fin- dings such as Related and Presence, the identification of genomic phenomena and clinical findings associa- ted with NIDDM in this abstract are encouraging. Such results appear to indicate that further develop- ment of this research project is warranted. We would like to report on our progress to date and outline future enhancements that are intended to expand the scope of this research effort, improve the accuracy of the identification of the values addressed, and extract relationships between disease and genes and among the identified genetic components.
This paper reports on the development of a social-oriented mobile robot, that is  able to reliably navigate in a semi-structured environment and to interact with people in  some specific way. The navigation of the robot is achieved using a behaviour-based  approach, where properly ordered sequences of actions, turns and transitions bring the  robot from place to place. The robot can detect the presence and position of people  through its vision system, that has been adapted to detect a special badge carried by the  user. The result of this work is a &quot;concierge robot&quot; able to offer practical information to  the visitors of a public building.  
Privacy concerns have become an important issue in data mining. A popular way to preserve privacy is to randomize the dataset to be mined in a systematic way and mine the randomized dataset instead.
This paper investigates data refinement by forward simulation for specifications  whose semantics is given by partial relations. The most well-known  example of such a semantics is that for Z. The standard model-theoretic approach  is based on totalisation and lifting. The paper examines this model, exploring  and isolating the precise roles played by lifting and totalisation in the standard  account by introducing a simpler, normative theory of forward simulation data  refinement (SF-refinement) which captures refinement directly in the language  and in terms of the natural properties of preconditions and postconditions. This  theory is used in conjunction with four other model-theoretic approaches to determine  the extent to which the standard approach is canonical, and the extent to  which it is arbitrary.
This paper describes the design of a scripting language aimed at expressing task (unit of computation) composition and inter-task dependencies of distributed applications whose execution could span arbitrary large durations. This work is motivated by the observation that an increasingly large number of distributed applications are constructed by composing them out of existing applications, and are executed in an heterogeneous environment. The resulting applications can be very complex in structure, containing many notification and dataflow dependencies between their constituent applications. The language enables applications to be structured with the properties of modularity, interoperability, dynamic reconfigurability and fault-tolerance.
Architects and interior designers often want to design a room, and an important part of this design is the lighting. Based on the theory of radiant intensity, the calculation of light energy can be broken down into two parts: configuration factors and enclosures. The configuration factors, which represents the fraction of light leaving one surface and arriving at another are computed using one program. This program is very slow, and was therefore rewritten in parallel. A second program calculates the illumination of each surface in an enclosure, which requires a light source and the database generated by the first program. This program is also very slow and was rewritten in parallel. An analysis of the parallel versions of both of these codes is given in this paper. The result is a third program which combines the two codes. This code demonstrated a remarkable speedup over the previous version. As these codes do not implement hidden surfaces, but assume that every surface can see every other surface, a final code is described which incorporates this ability. EPCC-SS95-15 2 1
. The state equation is a verification technique that has been applied - not always under this name -- to numerous systems modelled as Petri nets or communicating automata. Given a safety property P, the state equation is used to derive a necessary condition for P to hold which can be mechanically checked. The necessary conditions derived from the state equation are known to be of little use for systems communicating by means of shared variables, in the sense that many of these systems satisfying the property but not the conditions. In this paper, we use traps, a well-known notion of net theory, to obtain stronger conditions that can still be efficiently checked. We show that the new conditions significantly extend the range of verifiable systems.  Keywords: State equation, traps, approximation techniques, linear programming 1. Introduction  The application of linear algebra and integer programming techniques to verification problems has been the subject of a large number of papers [3,...
In this paper, we explore an architecture, called K-Trek, that enables mobile users to travel across knowledge distributed over a large geographical area (ranging from large public buildings to a national park). Our aim is providing, distributing, and enriching the environment with location-sensitive information for use by agents on board of mobile and static devices. Local interactions among K-Trek devices and the distribution of information in the larger environment adopt some typical peer-to-peer patterns and techniques. We introduce the architecture, discuss some of its potential knowledge management applications, and present a few experimental results obtained with simulation.
. In this paperff we presentnovel imageffderivedff invariant feaff tures that accurately capture both the geometric and color properties of an imaged object. These features can distinguish between objects that have the same general appearance ffe.g.ff different kinds of ffshffff in adff dition to the typical task of distinguishing objects from different classes ffe.g. ffsh vs. airplanesff. Furthermoreff these image features are insensiff tivetochanges in an objectffs appearance due to rigidffbody motionff affne shape deformationff changes of parameterizationff perspective distortionff view pointchange and changes in scene illumination. The new features are readily applicable to searching large image databases for speciffc imff ages. We present experimental results to demonstrate the validityofthe approachff which is robust and tolerant to noise. 1 Introduction  The advent of highffspeed networks and inexpensive storage devices makes the construction of large image databases feasible. More and more images are now ...
A major force affecting many forest ecosystems is the encroachment of residential, commercial and industrial development. Analysis of the complex interactions between development decisions and ecosystems, and how the environmental consequences of these decisions influence human values and subsequent decisions will lead to a better understanding of the environmental consequences of private choices and public policies. Determining conditions on the interactions between human decisions and natural systems that lead to long-term sustainability of forest ecosystems is one goal of this work. Interactions between human stakeholders are represented using multi-agent models that act on forest landscape models in the form of land-use change. Feedback on the effects of these actions is received through ecological habitat metrics and hydrological responses. Results are presented based on a study of a riparian area of the DallasFort Worth (Texas, U.S.A.) region facing intense residential development.
The problem of the unequal sphere packing in a 3-dimensional polytope is analyzed. Given a set of unequal spheres and a polytope, the double goal is to assemble the spheres in such a way that (i) they do not overlap with each other and (ii) the sum of the volumes of the spheres packed in the polytope is maximized. This optimization has an application in automated radiosurgical treatment planning and can be formulated as a nonconvex optimization problem with quadratic constraints and a linear objective function. On the basis of the special structures associated with this problem, we propose a variety of algorithms which improve markedly the existing simplicial branch-and-bound algorithm for the general nonconvex quadratic program. Further, heuristic...
Although the Internet can be used to provide high connectivity between parties, it does not always provide strong protection for private communications. Here we describe a strong cryptographic solution to this problem using one-time pads. One-time pads provide cryptographically secure communication and can be implemented using low-power computing devices such as Pocket PC&apos;s, Palm-compatible devices, and even devices for ubiquitous computing such as Javacard and iButton. A method for the cryptographically secure transmission of data between remote users is described, and prototype implementations are presented for standard computational platforms, handheld devices and Javacard enabled devices.
Let    be a family of cliques of a graph G =(V,E). Suppose that each clique C of   is associated with an integer r(C), where r(C)    0. A vertex vr-dominates a clique C of G if d(v, x)    r(C) for all x    C, where d(v, x) is the standard graph distance. A subset D    V is a clique r-dominating set of G if for every clique C    is a vertex u    D which r-dominates C. A clique r-packing set is a subset P    that there are no two distinct cliques C ff ,C ffff    Pr-dominated by a common vertex of G. The clique r-domination problem is to find a clique r-dominating set with minimum size and the clique r-packing problem is to find a clique r-packing set with maximum size. The formulated problems include many domination and clique-transversal-related problems as special cases. In this paper an efficient algorithm is proposed for solving these problems on dually chordal graphs which are a natural generalization of strongly chordal graphs. The efficient algorithm is mainly based on the tree structure and special vertex elimination orderings of dually chordal graphs. In some important particular cases where the algorithm works in linear time the obtained results generalize and improve known results on strongly chordal graphs.
Augmented Reality (AR) provides a natural interface to the &quot;calm&quot; pervasive technology anticipated in large-scale Ubiquitous Computing environments. However, the range of classic AR applications has been limited by the scope, range and cost of sensors used for tracking. Hybrid tracking approaches can go some way to extending this range. We propose an approach, called Ubiquitous Tracking, in which data from widespread and diverse heterogeneous tracking sensors is automatically and dynamically fused, and then transparently provided to applications. A formal model represents spatial relationships between objects as a graph attributed with quality-of-service parameters. This paper presents a software implementation, in which a dynamic data flow network of distributed software components is thereby constructed in response to queries and optimisation criteria specified by applications. This implementation is demonstrated using a small laboratory example, and larger setups modelled in a simulation environment.
The reduction in the average required transmit power in order to achieve the desired system performance is determined when using a receive antenna array employing polarization diversity as opposed to spatial diversity only. Cross-polardiscrimination (XPD) and envelope correlation are used to model the correlation between the horizontal and vertical polarizations. The characteristic function for an M-branch diversity receiver, using Maximal-Ratio-Combining, is used to obtain the analytical expressions for the average required transmit power, assuming Rayleigh fading channels, perfect channel information at the transmitter and the receiver, and perfect fast or average power control. Based on the measurement data of XPD and envelope correlation in an urban environment, two spatially separated dual-polarized antennas are shown to outperform two spatially separated vertical antennas between 2.25-2.4 dB and 4-4.3 dB for the perfect fast power control and the perfect average power control, respectively.
The Across Trophic Levels System Simulator (ATLSS) is a suite of ecological models designed to assess the impact of changes in hydrology on biotic components of the southern Florida ecosystem. ATLSS implements a multimodeling approach that utilizes process models for lower trophic levels, structured population models for middle trophic levels (fish and macroinvertebrates), and individual-based models for large consumers. ATLSS requires hydrologic input to assess the effects of alternative proposed restoration scenarios on trophic structure. An ATLSS model (ALFISH) for functional fish groups in freshwater marshes in the Everglades of southern Florida has been extended to create a new model (ALFISHES) to evaluate the spatial and temporal patterns of fish density in the resident fish community of the Everglades mangrove zone of Florida Bay. The ALFISHES model combines field data assessing the impact of salinity on fish biomass with hydrologic data from the Southern Inland and Coastal System (SICS) model. The estuarine landscape is represented by a grid of 500    500-meter cells across the coastal areas of the Florida Bay. Each cell is divided into two habitat types; flats, which are flooded during the wet season, and creeks, which remain wet and serve as refugia during the dry season. Daily predictions of water level and salinity are obtained from the SICS model output, which is resampled at the 500-meter spatial resolution of the ALFISHES model. The model output may be used to assess the impact of changes in hydrology on fish biomass and its availability to wading bird and other consumer populations. With the development of restoration scenario capabilities in the SICS model, the SICS/ALFISHES coupling should prove an effective tool for evaluating the potential impact o...
In this paper we propose a family of algorithms combining tree-clustering  with conditioning that trade space for time. Such algorithms are useful for reasoning in probabilistic and deterministic networks  as well as for accomplishing optimization tasks. By analyzing  the problem structure the user can select from a spectrum of algorithms, the one that best meets a given time-space specification. To determine the potential of this approach, we analyze the structural  properties of problems coming from the circuit diagnosis domain. The analysis demonstrate how the tradeoffs associated with various hybrids  can be explicated and be used for each problem instance.
This paper presents a nonlinear circuit-coupled 2D finite element model for use in the dynamic simulation of electrical machines with multiple sets of windings, connected to an external electrical circuit, and with rotating parts. An inductance is introduced to account for the 3D end effects on phase inductance, due to the end windings and axial fringing field. The model is applied to the simulation of two types of electrical machines, a permanent magnet generator and a switched reluctance motor. The modeling procedures are described and simulation results are presented. The calculated performances are compared with test results to validate the modeling. 
Traditionally, distributed query optimization techniques generate static query plans at compile time. However, the optimality of these plans depends on many parameters (such as the selectivities of operations, the transmission speeds and workloads of servers) that are not only diffcult to estimate but are also often unpredictable and fluctuant at runtime. As the query processor cannot dynamically adjust the plans at runtime, the system performance is often less than satisfactory. In this paper, we introduce a new highly adaptive distributed query processing architecture. Our architecture can quickly detect fluctuations in selectivities of operations, as well as transmission speeds and workloads of servers, and accordingly change the operation order of a distributed query plan during execution. We have implemented a prototype based on the Telegraph system [1]. Our experimental study shows that our mechanism can adapt itself to the changes in the environment and hence approach to an optimal plan during execution.
Providing quality of service (QoS) in large-scale networks like the Internet inherently needs to deal with heterogeneous network QoS systems. Therefore, the interworking between different network QoS systems is of high importance. In this paper, the interworking with respect to a basic characteristic of network QoS systems, the time scale of the system, is under investigation. The time scale of a network QoS system is its speed of reaction to individual requests for differentiated treatment of units of service. A slow time scale system will prefer requests to arrive with a low frequency and persist unaltered for a substantial period of time while a fast one is able to support much higher arrival rates of requests and is thus more amenable for short-lived units of service. Obviously, when overlaying a slow time scale QoS system over a faster one, there is no problem. However, and that is a more likely case, for the overlay of a fast time scale system on a slow one, there is a mismatch to be mediated at the edge between the two. The technique that is applied at an edge device for this mediation is called decoupling of time scales. Decoupling can also be viewed as aggregation of requests in time in contrast to spatial aggregation on the data path. In the paper we develop an adaptive heuristic scheme to deal with decoupling and evaluate this scheme by extensive simulations.
In this paper we present a new framework for novelty detection. The framework evaluates neural networks as adaptive classifiers that are capable of novelty detection and retraining on the basis of newly discovered information. We apply our newly developed model to the application area of  object recognition in video. The application is however not limited to scene analysis and the basic methodology can be easily extended to other areas. This paper details the tools and methods needed  for novelty detection such that data from unknown classes can be reliably rejected without any a priori knowledge of its characteristics. The rejected data is post-processed to determine which  samples can be manually labelled of a new type and used for retraining. In this paper we compare the proposed framework with a naive solution and discuss the results of retraining neural network to recognise further unseen data containing the newly added objects.
The main problem in bringing autonomy to any vehicle lies in the design of a suitable guidance law. For a truly autonomous operation, the vehicle needs to have a reliable Navigation, Guidance and Control (NGC) system of which the guidance system is the key element, which generates suitable trajectories to be followed. In this survey paper, various guidance laws found in the literature and their relevance to autonomous underwater vehicles (AUVs) are discussed. Since existing guidance laws for underwater vehicles have emulated from tactical airborne missile systems, a number of approaches for the missile guidance systems are considered. Finally, potential guidance strategies for AUVs are proposed.
In this paper, we examine the problem of estimating the capacity of bottleneck links and available bandwidth of endto -end paths under non-negligible cross-traffic conditions. We present a simple stochastic analysis of the problem in the context of a single congested node and derive several results that allow the construction of asymptotically-accurate bandwidth estimators. We first develop a generic queuing model of an Internet router and solve the estimation problem assuming renewal cross-traffic at the bottleneck link. Noticing that the renewal assumption on Internet flows is too strong, we investigate an alternative filtering solution that asymptotically converges to the desired values of the bottleneck capacity and available bandwidth under arbitrary (including non-stationary) cross-traffic. This is one of the first methods that simultaneously estimates both types of bandwidth and is provably accurate. We finish the paper by discussing the impossibility of a similar estimator for paths with two or more congested routers.
Mobile code provides significant opportunities and risks. Java bytecode is used to provide executable content to web pages and is the basis for dynamic service configuration in the Jini framework. While the Java Virtual Machine includes a bytecode verifier that checks bytecode programs before execution, and a bytecode interpreter that performs run-time tests, mobile code may still behave in ways that are harmful to users. We present techniques that insert runtime tests into Java code, illustrating them for Java applets and Jini proxy bytecodes. These techniques may be used to contain mobile code behavior or, potentially, insert code appropriate to profiling or other monitoring efforts. The main techniques are class modification, involving subclassing non-final classes, and method-level modifications that may be used when control over objects from final classes is desired.
Zerotree coding is an intelligent approach for quantization of Discrete Wavelet coefficients and thus achieving image compression. Embedded Zerotree Coding by Shapiro [3] is very effective in this regard, having the property that the bits in the bit stream are generated in order of importance, yielding a fully embedded code. In this paper we have proposed a Modified Embedded Zerotree (MEZT) scheme which is computationally more efficient, making it more suitable for real time video compression applications. MEZT is a bit plane wise single pass algorithm, unlike Shapiro&apos;s two pass technique. This leads to reduction in processing time. Depending on the priority of bit budget or compression ratio, the number of bit planes to be encoded can be decided. More over, since most of the necessary tasks are bit operations, it is more suitable for VLSI implementation.
this document and invoke other services
In this paper, we present a novel 3D face modeling approach from a monocular  video captured using a conventional camera. The proposed algorithm  relies on matching a generic 3D face model to the outer contours of the face  to be modeled and a few of its internal features. At the first stage of the  method, we estimate the head pose by comparing the edges extracted from  video frames, with the contours extracted from a generic face model. Next,  the generic face model is adapted to the actual 3D face by global and local  deformations. An affine model is used for global deformation. The 3D model  is locally deformed by computing the optimal perturbations of a sparse set of  control points using a stochastic search optimization method. The deformations  are integrated over a set of poses in the video sequence, leading to an  accurate 3D model.
Predictability -- the ability to foretell that an implementation will not violate a  set of specified reliability and timeliness requirements -- is a crucial, highly desirable  property of responsive embedded systems. This paper overviews a development  methodology for responsive systems, which enhances predictability by eliminating  potential hazards resulting from physically-unsound specifications. The backbone
Performance evaluation is an important issue in building efficient Web servers. It is desirable to measure the performance regularity of Web servers for overall performance behavior of the system in the full spectrum of working area, which is generally overlooked and has not received proper attention. In this paper we aim to raising the awareness of the importance of the performance regularity of a Web server by introducing Gini performance coefficient (GPC) as a scale-invariant metric for measuring the performance regularity. We present the theorems that relate the performance regularity of a Web server to the GPC, thereby providing a quantitative yardstick that complements the system capacity metric such as maximum throughput for measuring the system performance. Several representative systems that were used in the public benchmark study are examined under the proposed metric. The results are completely in line with our theoretical analysis.
The Semantic Web relies on the complex interaction of several technologies involving ontologies. Therefore, sophisticated Semantic Web applications typically comprise more than one software module. Instead of coming up with proprietary solutions, developers should be able to rely on a generic infrastructure for application development in this context. We call such an infrastructure Application Server for the Semantic Web whose design and development are based on existing Application Servers. However, we apply and augment their underlying concepts for use in the Semantic Web and integrate semantic technology within the server itself. We provide a short overview of requirements and design issues of such a server and present our implementation and ongoing work KAON SERVER.
In this paper we propose a novel intelligent control scheme that involves a cluster of Agents embedded in an Intelligent Co-ordinator. The resultant Intelligent Co-ordinator can be added to any existing wastewater treatment plant supervisory control and data acquisition system. In the proposed approach use is made of Computational Intelligence techniques and Qualitative Control. The work reported here is the object of ongoing bilateral research project between Greece and Slovenia. The objective of the project is to seek ways to enhance the performance of existing wastewater treatment plants using intelligent control techniques.
Specifying and enforcing constraints and invariants such as architectural constraints and data typing, strongly enhances the safety and reliability of the software system. Next to design and development constraints, the composition of software systems in component-based software also introduces composition time constraints and dependencies. In data-centered software systems, for example, the software composer implicitly creates dataflow dependencies between software components. Describing composition time constraints and enforcing these constraints at deploy time or at run-time strongly improves the safety and reliability of the software. In this paper, we present an approach for expressing and enforcing dataflow dependencies in data-centered software systems, and conclude with a validation of the approach in a servlet-based case study.
Cysteines were introduced, one at a time, at amino acid positions 55-75 in the cytoplasmic  region connecting helices I and II in rhodopsin. In each of the 21 cysteine mutants, the reactive native  cysteine residues (C140 and C316) were replaced by serine. Except for N55C, all mutants formed rhodopsinlike  chromophores and had normal photobleaching characteristics. The efficiency of GT activation was  reduced only for K66C, K67C, L68C, and P71C. The reactivity of the substituted cysteine in each mutant  toward 4,4-dithiodipyridine (4-PDS) was investigated in the dark. The mutants F56C to L59C and I75C  were unreactive to 4-PDS under the conditions used, suggesting that they are embedded in the micelle or  protein interior. The mutants V63C, H65C-T70C, and N73C reacted rapidly, while the remainder of the  mutants reacted more slowly, and varied in reactivity with sequence position. For the mutants derivatized  with 4-PDS, the rate of release of thiopyridone from the resulting thiopyridinyl-cysteine disulfide bond  by dithiothreitol was investigated in the dark and in the light. Marked changes in the rates of thiopyridone  release in the light were found at specific sites. Collectively, the data reveal tertiary interactions of the  residues in the sequence investigated and demonstrate structural changes due to photoactivation.
this paper, we basically relax some of these strong hypotheses by allowing more redundancy in the dictionaries, through the concept of block incoherence, which basically describes a dictionary that can be represented as the union of incoherent blocks. We show that even pure greedy algorithms can strongly benefit from such design by proving a recovery condition under which Matching Pursuit will always pick up correct atoms during the signal expansion. Based on this result, we design an algorithm that constructs a near block incoherent dictionary starting from any initial dictionary. A tree structured greedy algorithm is then proposed as a way of constructing sparse approximations with block incoherent dictionaries. This algorithm presents the important advantage of being much faster than a classical Matching Pursuit. In the same time, it only minimally degrades the quality of approximation thanks to the recovery condition, derived for block incoherent dictionaries. The performance of the proposed algorithm are demonstrated in the context of image representation
Image texture is an important visual primitive in image search and retrieval applications. To characterize texture information of images, a texture descriptor is proposed to capture both the perceptual information and the statistics of textures. The descriptor consists of two parts: a Perceptual Browsing Component (PBC) and a Similarity Retrieval Component (SRC). The emphasis of this paper is to introduce the extraction method for the PBC which is based on multidimensional decomposition of images using Gabor filters. By analyzing the one dimensional projections resulting from filtered images, PCB gives a quantitative measure about the structuredness of textures and also identifies the directionality and coarseness (scale). Our experimental results demonstrate that PBC can capture these texture properties quite well.
Recovery of epipolar geometry is a fundamental problem in computer vision. The introduction of the &quot;joint image manifold&quot; (JIM) allows to treat the recovery of camera motion and epipolar geometry as the problem of fitting a manifold to the data measured in a stereo pair. The manifold has a singularity and boundary, therefore special care must be taken when fitting it.
Accidents and incidents involving safety-critical software systems  often provide lessons to the systems&apos; users and designers, to industry, and to the  software engineering community at large. Proper identification and  documentation of these lessons is critical in order to prevent the recurrence of an  untoward event. In this paper we examine two commercial aviation incidents  involving failures of safety-critical software systems. Based on our analysis of  the incidents and the official investigations that followed, we conclude that the  aviation community is missing important lessons regarding safety-critical  software systems, especially concerning the broad role these systems play in  preserving the safety of commercial air travel. This is primarily because  incidents involving such systems are not being investigated and documented  with sufficient rigor to identify these lessons and disseminate them throughout  the aviation community effectively.
The solution of large-scale scheduling problems involving the production of hundreds different products using a variety of process unit operations are typical for chemical and pharmaceutical companies. These problems however are translated to mathematical models involving a computationally infeasible number of variables and constraints independent of the mathematical modeling approach one choose to follow. In this paper, the continuous-time formulation proposed by Ierapetritou and Floudas (Ind. Eng. Chem. Res. 37 (1998) 4341) is used as a basic scheduling model. A number of different heuristic-based decomposition approaches are presented including time-based decomposition, required production method and resource-based decomposition. Lagrangean relaxation (LR) and Lagrangean decomposition (LD) are then employed that give rise to an upper bound of the original scheduling problem. Finally, an iterative solution framework is proposed that exploits the lower bound obtained through the heuristic-based approaches and the upper bound based on the LR and LD to result in a refined schedule for large-scale scheduling problems. Two examples are used to illustrate the application of the approaches presented and compare their efficiencies.
The enormity and rapid growth of the web-graph forces quantities such as its pagerank to be computed under missing information consisting of outlinks of pages that have not yet been crawled. This paper examines the role played by the size and distribution of this missing data in determining the accuracy of the computed pagerank, focusing on questions such as (i) the accuracy of pageranks under missing information, (ii) the size at which a crawl process may be aborted while still ensuring reasonable accuracy of pageranks, and (iii) algorithms to estimate pageranks under such missing information. The first couple of questions are addressed on the basis of certain simple bounds relating the expected distance between the true and computed pageranks and the size of the missing data. The third question is explored by devising algorithms to predict the pageranks when full information is not available. A key feature of the &quot;dangling link estimation&quot; and &quot;clustered link estimation&quot; algorithms proposed is that, they do not need to run the pagerank iteration afresh once the outlinks have been estimated.
This paper considers the problem of estimating the parameters of two-dimensional (2-D) moving average random (MA) fields. We first address the problem of expressing the covariance matrix of nonsymmetrical half-plane, noncausal, and quarter-plane MA random fields in terms of the model parameters. Assuming the random field is Gaussian, we derive a closedform expression for the Cram er--Rao lower bound (CRLB) on the error variance in jointly estimating the model parameters. A computationally efficient algorithm for estimating the parameters of the MA model is developed. The algorithm initially fits a 2-D autoregressive model to the observed field and then uses the estimated parameters to compute the MA model. A maximumlikelihood algorithm for estimating the MA model parameters is also presented. The performance of the proposed algorithms is illustrated by Monte-Carlo simulations and is compared with the Cramer--Rao bound.
PubMed on Tap is a testbed system that supports search of and retrieval from the National Library of Medicine&apos;s MEDLINE database from a PDA. The goal of the PubMed on Tap   project is to discover and implement design principles for point-of-care delivery of clinical   support information. The project explores user interface issues, information content and   organization, and system performance. Here we present our progress in these areas.   1. 
Contour Estimation, Bayesian Estimation, Random Fields, Dynamic Programming,  Multigrid Methods.
this paper we describe another virtual exercise. In contrast to the courses mentioned above, no externally funded project has been set up for its development. Moreover, we did not have the explicit goal to develop a virtual course. The virtual course just evolved by changes to a traditional course that have been necessary due to several financial and organisational constraints. The result of the different actions finally was a course that can completely be performed remotely using a web-browser
Confidence annotation allows a spoken dialog system to accurately assess the likelihood of misunderstanding at the utterance level and to avoid breakdowns in interaction. We describe experiments that assess the utility of features from the decoder, parser and dialog levels of processing. We also investigate the effectiveness of various classifiers, including Bayesian Networks, Neural Networks, SVMs, Decision Trees, AdaBoost and Naive Bayes, to combine this information into an utterancelevel confidence metric. We found that a combination of a subset of the features considered produced promising results with several of the classification algorithms considered, e.g., our Bayesian Network classifier produced a 45.7% relative reduction in confidence assessment error and a 29.6% reduction relative to a handcrafted rule.
Current geographic information systems tend to follow an inherently static approach to geospatial information management. Small amounts of information are typically synthesized into map-like application-specific data snapshots. This static approach leaves large amounts of information unused and offers limited communication capabilities. Accordingly, it is unsuitable for today&apos;s applications, where geospatial information becomes increasingly dynamic and spatiotemporal in nature. In this paper we present a framework for the integration of digital images and complementary GIS datasets in a model which provides explicit information about spatiotemporal change. We propose the SpatioTemporal Gazetteer as a model that makes more effective use of multiple information resources than the traditional map model, and in particular incorporates components to track changes to objects over time. We present the general components of the model and discuss information flow in such a system. We also prese...
We believe it is important for web graphic standards such as SVG to support user interaction and diagrams that can adapt their layout and appearance to their viewing context so as to take into account viewing device charateristics and the viewer&apos;s requirements. In [1] we suggested that adding expression-based attributes to SVG and using one-way constraints to evaluate these dynamically would considerably improve SVG&apos;s support for adaptive layout and user interaction. We describe a minimal backward compatible extension to SVG 1.1, called Constraint SVG (CSVG), that provides such expression-based attributes and its implementation on top of Batik. CSVG also provides another significant extension to SVG 1.1: it allows the author to define new custom elements using XSLT.
An algebraic version of Kashiwara and Schapira&apos;s calculus of constructible functions is used to describe local topological properties of real algebraic sets, including Akbulut and King&apos;s numerical conditions for a stratified set of dimension three to be algebraic. These properties, which include generalizations of the invariants modulo 4, 8, and 16 of Coste and Kurdyka, are defined using the link operator on the ring of constructible functions.
Interactive Web services consist of a mixture of HTML fragments and program code. The fragments, which are maintained by designers, are combined to form HTML pages that are shown to the clients. The code, which is maintained by programmers, is executed on the server to handle the business logic. Current Web service frameworks provide little help in separating these constituents, which complicates cooperation between programmers and HTML designers. We propose a system based on XML templates and formalized contracts allowing a flexible separation of concerns. The contracts act as interfaces between the programmers and the HTML designers and permit tool support for statically checking that both parties fulfill their obligations. This ensures that (1) programmers and HTML designers work more independently focusing on their own expertises, (2) the Web service implementation is better structured and thus easier to develop and maintain, (3) it is guaranteed that only valid HTML is sent to the clients even though it is constructed dynamically, (4) the programmer uses the XML templates consistently, and (5) the form input fields being sent to the client always match the code receiving those values. Additionally, we describe tools that aid in the construction and management of contracts and XML templates. 
This paper gives an overview of the methods offered by t-ARGUS to identify sensitive cells, and to select secondary suppressions
This paper reviews some developments in error estimation and mesh selection for collocation methods for ordinary differential equations. The basic idea
We give an unified convergence analysis of ensemble learning methods  including e.g. AdaBoost, Logistic Regression and the Least-Square-Boost  algorithm for regression. These methods have in common that they iteratively  call a base learning algorithm, which returns hypotheses that are  then linearly combined. We show that these methods are related to the  Gauss-Southwell method known from numerical optimization and show linear  convergence for all these methods. Our analysis includes ff 1 regularized  cost functions leading to a clean and general way to regularize ensemble  learning.
This paper examines biologically inspired cognitive map models, which provide an artificial navigating agent  with a topological map of places after an exploration and learning phase in a previously unknown environment.
Optical tracking systems allow three-dimensional input for virtual environment applications with high precision and without annoying cables. Spontaneous and intuitive interaction is possible through gestures. In this paper, we present a finger tracker that allows gestural interaction and is simple, cheap, fast, robust against occlusion and accurate. It is based on a marked glove, a stereoscopic tracking system and a kinematic 3-d model of the human finger. Within our augmented reality application scenario, the user is able to grab, translate, rotate, and release objects in an intuitive way. We demonstrate our tracking system in an augmented reality chess game allowing a user to interact with virtual objects.  (a) (b)  Figure 1. Manipulation of virtual objects by  grab and release gestures: Natural interaction  is possible using the finger tracker  described in this paper together with augmented  reality displays. In this image, a user  plays chess against the computer by moving  virtual chess men with his finger on a real  board.  1 
Component-based applications require runtime support to be able to guarantee non-functional properties.
Hierarchical caching is deployed to scale up the explosive Web growth, and the expiration-based mechanism is adopted as an economic means to support the weak consistency in this context. However, given a hierarchy, the user perceived performance heavily depends on its position. Normally, a user near the hierarchy leaf suffers higher miss rate and longer response time. Such an intrinsic property can discourage users from participating in any hierarchical caching systems. In this paper, we analyze the performance of a proposed approach, i.e., freshness and retrieval threshold based cache prefetching, to mitigate the bias against leaf users. We also use ns--2 to further substantiate our analysis. By adopting this approach with the appropriate parameters, the fairness among users within a caching hierarchy can be considerably improved.
We describe a system for transforming an input video into a highly abstracted, spatio-temporally coherent cartoon animation with a range of styles. To achieve this, we treat video as a space-time volume of image data. We have developed an anisotropic kernel mean shift technique to segment the video data into contiguous volumes. These provide a simple cartoon style in themselves, but more importantly provide the capability to semi-automatically rotoscope semantically meaningful regions.
We have been developing a GIS-based information system on social archaeology in Northeast Thailand since 2000, and we opened the contents to the public through a web page in early 2003 as a result of the first stage of our activity. Our system, the EcoNETVIS, is comprised of legacy information, such as paper-based topographic maps, aerial photographs of the 1950s, scenic photographs, and academic descriptions. They are mostly from the lifetime research results of Srisakra Vallibhotama, since our activity started with the aim of archiving and conserving his academic materials. To archive them properly was an urgent issue, as much of his unpublished knowledge and related materials would otherwise eventually become useless. Practical collaboration was needed to work out an effective solution. Our collaborative activity moved up to the next stage after two of us from the fields of social archaeology strongly recognized the efficiency of developing the spatial information system and its influence in academic and educational activities. It is, of course, very time-consuming to construct an exhaustive database that integrates information on social archaeology with geo-references, since data must be processed manually. The spatial resolution of our system would be much coarser than that of remote sensing. Nevertheless, the two fields can be linked together by use of geo-references, and close overlay will provide fresh viewpoints for social and human scientists.
In the Spring of 2003, it was realised a survey of the baptistery of Cremona (Italy) combining photogrammetric and laser scanner techniques. All the survey is composed by 22 photogrammetric images taken with a Nikon D100 18/2.8 digital camera and 14 scans collected by Optech&apos;s ILRIS-3D Laser Scanner. The geo-referencing was carried out with some natural and artificial targets displaced in the structure of the baptistery. The artificial targets used for this survey, were purposely projected, in order to get the best radiometric response and univocal geometrical definition; instead the natural target were surveyed in a traditional topographic way with a Total Station. The solid modelling of all the monument was made using a commercial software called PolyWorks; this software allows to align different scans with different reference systems and mapping some topographic information recording that on the 3D model. This
In this work we consider face recognition from face motion manifolds. An information-theoretic approach with Resistor-Average Distance (RAD) as a dissimilarity measure between distributions of face images is proposed. We introduce a kernel-based algorithm that retains the simplicity of the closed-form expression for the RAD between two normal distributions, while allowing for modelling of complex, nonlinear manifolds. Additionally, it is shown how errors in the face registration process can be modelled to significantly improve recognition. Recognition performance of our method is experimentally demonstrated and shown to outperform state-of-the-art algorithms. Recognition rates of 97-100% are consistently achieved on databases of 35-90 people.
We present initial work on a theory of coordination and parallelism in Glue Semantics (GLUE; Dalrymple 1999, 2001). We will explore points of convergence and divergence between our approach to coordination and similar Categorial Grammar (CG) approaches. We also compare our approach to a previous GLUE approach to coordination (Kehler et al. 1995, 1999) and argue that our approach is superior on the grounds that it preserves a very strong notion of resource-sensitivity (Dalrymple et al. 1993). We conclude by discussing parallelism in connection with the Coordinate Structure Constraint (CSC; Ross 1967). The CSC is a putatively robust condition on extraction which has been argued to be a feature of the CG approach to coordination and of other related approaches. It is standardly assumed to have two parts, the Conjunct Constraint and the Element Constraint (Grosu 1973). The Conjunct Constraint is quite robust, but the Element Constraint has been challenged repeatedly, most recently by Kehler (2002), who argues that the CSC is not a syntactic condition, but rather follows from conditions on discourse coherence and parallelism. We discuss a constraint language on the structure of GLUE derivations, and show how Kehler&apos;s theory of discourse cohesion can be related to parallelism in such derivations.
A new mapping heuristic is developed, based on the recently proposed Mean Field Annealing (MFA) algorithm. An efficient implementation scheme, which decreases the complexity of the proposed algorithm by asymptotical factors, is also given. Performance of the proposed MFA algorithm is evaluated in comparison with two wellknown heuristics; Simulated Annealing and Kernighan-Lin. Results of the experiments indicate that MFA can be used as an alternative heuristic for solving the mapping problem. Inherent parallelism of MFA is exploited by designing an efficient parallel algorithm for the proposed MFA heuristic.  1 Introduction  Today, with the aid of VLSI technology, parallel computers not only exist in research laboratories, but are also available on the market as powerful, general purpose computers. Wide use of parallel computers in various compute intensive applications makes the problem of mapping parallel programs to parallel computers more crucial. The mapping problem arises while d...
Human-robot interaction has been identified as one of the major open research directions in mobile robotics. This paper considers a specific type of interaction: short-term and spontaneous interaction with crowds of people. Such patterns of interactions are found frequently when service robots operate in public places (e.g. information kiosks, receptionists, tourguide robots). This paper describes three components of a successfully implemented interactive robot: a motorized face as focal point for interaction, an architecture that suggests the robot has moods, and a method for learning how to interact with people. The approach has been implemented and tested using a mobile robot, which was recently deployed at a Smithsonian museum in Washington, DC. During a two week installation period it interacted with 50,000 people and we found that the robot&apos;s interactive capabilities were essential for its high on-task performance, and thus its practical success. 1 Introduction  Human-robot inter...
Even though intrusion detection systems have been studied  for a number of years several problems remain; chiefly low detection rates  and high false alarm rates.
We introduce a software system for real-time classification of violin bow strokes (articulations). The system uses an electromagnetic motion tracking system to capture raw gesture data. The data is analyzed to extract stroke features. These features are provided to a decision tree for training and classification. Feedback fromfeatureand classification data i s presented visually in an immersive graphic environment.
This article explores an application of Bayesian programming to behaviours for synthetic video games characters. We address the problem of real-time reactive selection of elementary behaviours for an agent playing a first person shooter game. We show how Bayesian programming can lead to condensed and easier formalisation of finite state machine-like behaviour selection, and lend itself to learning by imitation, in a fully transparent way for the player.
This paper presents a simple and effcient method of protection  against fault analysis when the underpinning cryptosystem uses  modular arithmetic. The proposed method applies whatever the modular  function to be evaluated and the used algorithms. Moreover, it only  requires a very little overhead of extra computations, especially when  the modulus is represented in diminished-radix form or when at least  one factor of the modulus is known.
When tables of quantitative data are generated from a datafile, the  release of those tables should not reveal information concerning individual  respondents. This disclosure of individual respondents in the  microdata file can be prevented by applying disclosure control methods  at the table level, but this may create inconsistencies across tables.
The Unified Modeling Language (UML) offers an extensive set of diagrams for modeling. However, the semantics of specific diagrams aren&apos;t always clear so as to decide how to model specific aspects of parallel applications. In this paper we introduce an approach for modeling parallel applications with UML by utilizing the UML extension mechanisms. We will demonstrate that these extension mechanisms represent a powerful means to model a wide range of structural and behavioral patterns of the parallel programming paradigm at various levels of details by modeling some of the most important parallel programming concepts.
Once a baseline level of full automation is possible for an Automated Highway System, there are numerous choices to be made in deploying enhanced capabilities to improve safety, throughput, and travel time. Identifying a set of orthogonal capabilities enables describing multiple deployment paths within a common framework. Sixteen AHS configurations can be formed from a proposed set of orthogonal capabilities including: number of vehicles grouped into an entity (free agent vs. platoon), number of automated lanes (single or multiple), obstacle strategy (exclusion or detection), and system vigilance (trusting or vigilant). This systematic approach reveals a maximally enhanced end-state configuration: a platooned, multi-lane, obstacle detecting, vigilant AHS that could be attained using any of 24 incremental deployment paths. A mapping technique is presented that can assist in risk management by depicting alternative deployment paths and constraints within a single framework. KEY WORDS inc...
We investigate the single link failure recovery problem and its application to the alternate path routing problem for ATM networks. Specifically, given a 2-connected graph G, a specified node s, and a shortest paths tree T s = fe 1 ; e 2 ; : : : ; e n\Gamma1 g of s, where e i = (x i ; y i ) and x i = parent Ts (y i ), find a shortest path from y i to s in the graph Gne i for 1 i n \Gamma 1. We present an O(m + n log n) time algorithm for this problem and a linear time algorithm for the case when all weights are equal. When the edge weights are integers, we present an algorithm that takes O(m+ T sort (n)) time where T sort (n) is the time required to sort n integers. We show that any solution to the single link recovery problem can adapted to solve the alternate path routing problem in ATM networks.
The Soil and Water Assessment Tool (SWAT) model, coupled to a GIS, was applied to the Naivasha basin in Kenya, a closed basin experiencing diverse climatic conditions from semi arid to humid, comprises with the only fresh water lake in the grate rift valley. The intent of this study was to estimates the lake water level fluctuations by integrating the SWAT with a water balance model, estimating the lake water level based on the lake volumes.
The main goal of this paper is to argue for an approach to optimization in syntax that is not global (as is standardly assumed), but local, in the sense that syntactic optimization procedures can affect only small portions of syntactic structure. Local optimization presupposes harmonic serialism (rather than harmonic parallelism), i.e., a derivational organization of grammar. In line with this, I set out to reconcile optimality theory with the minimalist program (see Chomsky (2000), Chomsky (2001)), a derivational approach in which phrase structure is created incrementally. I argue that local optimization is both conceptually attractive (because it significantly reduces complexity) and supported by empirical evidence. As a case study, I develop an analysis of a shape conservation phenomenon in German that involves repair-driven movement operations at the clause edge. I show that, other things being equal, local optimization succeeds where global optimization fails.
this paper captures relation with other variables, such as the lag of evaluation, the performance measure, the number of performing class and the statistical test
Too often, professional ethics issues are trivialized in software engineering  education.To begin to remedy this situation, we have built two interactive, adaptive  learning scenarios that place students in the role of a software project manager confronting  many critical project decisions,each with an ethical dimension. As students  move through a scenario,making and justifyingtheir decisions,their behaviourcan be  monitored and used both to adapt the scenario to each student as they proceed, and in  post hoc analysis to identify different classes of ethical behaviour. In this paper we  discuss five different classes of student behaviour that emerged from the analysis of  protocols collected during the use of these scenarios in a third year undergraduate  software engineering class. We speculate that the existence of these general student  models can be used in several ways to further enhance the learning of ethics  1 
this paper, we propose a design for web information services based on user profile communication. We illustrate the approach by an example application, in which conference and workshop calls are collected and distributed to interested users. We present the results of an initial study of this system, which points towards the most important characteristics of this collaboration. Finally, we discuss future extensions for the system
Our main results are: 1) after adding any number of Cohen reals  to a model of GCH the space U(ff 1 ) is co-absolute with a power of ff 2 and  2) there is a model in which U(ff1 ) is not co-absolute with any product of  discrete spaces.
Registration of 2-D projection images and 3-D volume images is still a largely unsolved problem. In order to register a pre-operative CT image to an intra-operative 2-D x-ray image, one typically computes simulated x-ray images from the attenuation coeffcients in the CT image (Digital Reconstructed Radiograph, DRR). The simulated images are then compared to the actual image using intensity-based similarity measures to quantify the correctness of the current relative pose. However, the spatial information present in the CT is lost in the process of computing projections. This paper first introduces a probabilistic extension to the computation of DRRs that preserves much of the spatial separability of tissues along the simulated rays. In order to handle the resulting non-scalar data in intensity-based registration, we propose a way of computing entropy-based similarity measures such as mutual information (MI) from probabilistic images. We give an initial evaluation of the feasibility of our novel image similarity measure for 2-D to 3-D registration by registering a probabilistic DRR to a deterministic DRR computed from patient data used in frameless stereotactic radiosurgery.
A method of rotation-invariant texture classification based on a complete space-frequency model is introduced. A polar, analytic form of a two-dimensional (2-D) Gabor wavelet is developed, and a multiresolution family of these wavelets is used to compute information-conserving microfeatures.From  these microfeatures a micromodel, which characterizes spatially localized amplitude, frequency, and directional behavior of the texture, is formed. The essential characteristics of a texture sample, its macrofeatures, are derived from the estimated selected parameters of the micromodel. Classification of texture samples is based on the macromodel derived from a rotation invariant subset of macrofeatures. In experiments, comparatively high correct classification rates were obtained using large sample sets.  Index Terms---Gabor filters, texture classification, wavelets.  I. INTRODUCTION  T  HE SPECTRUM of texture analysis techniques ranges from those focusing on structural features to those emph...
Golem is an holonomic robot designed to be compliant with Robocup regulations for the middle-sized league. The project consists in three parts: mechanics and hardware, vision system and software. We adopted the universal three wheels model to achieve a great freedom of movement. In order to take advantage of this particular feature, the vision system has to give a full sight of the environment. This objective is achieved by a mirror mounted on the mobile base. Decision making and planning are based on the knowledge of the relative prosition of all objects in the field (ball, walls, robots, goals) and especially on the recognition of teammates and opponents. Every robot can play as goalkeeper, defender or attacker.
A fast mode decision method for Intra prediction in H.264 is proposed in this work to reduce the encoder complexity. The proposed algorithm adopts a multi-stage sequential mode decision process that uses joint spatial and transform domain features to filter out unlikely candidate modes and, in the final stage, a simplified rate-distortion optimization method with a rate-distortion model is applied to a very limited number of candidates. It incorporates an early termination mechanism to save computations. Experimental results are given to demonstrate that the proposed scheme achieves a speed-up factor of 10-15 as compared with the current RD optimized mode decision with little quality degradation.
this report is supported by grants from the National Science Foundation ffIIS-0083075, ffITR-0205679 and ffITR-0205724. No endorsement implied. Mark Ackerman at the University of Michigan Ann Arbor; Les Gasser at the University of Illinois, Urbana-Champaign; John Noll at the Santa Clara University; Chris Jensen, Mark Bergman, and Xiaobin Li at the UCI Institute for Software Research, and also Julia Watson at The Ohio State University are collaborators on the research project that produced this chapter. 1   1 
Our VIA-capable PCI-SCI bridge was already introduced to the community in several papers (e.g. [3], [4]). While in these publications the focus were set at the advantages of the integration of VIA characteristics into an SCI architecture and the positive impact on message passing libraries, this paper is intended to provide the reader with a couple of implementation details.  Functionality and cooperation of many of the hardware components are described and deepened by tracing a remote write operation throughout the whole system from PCI bus to PCI bus. This trace also discloses the composition of the latency.  
: We present an approach to pricing and resource sharing for ABR services which allows bursty users to express their true preferences for network usage. Users bid for some amount of effective bandwidth, and the network controls the effective bandwidth of their traffic by adjusting the explicit rate (maximum rate the user can send traffic) in order to achieve economically fair resource sharing. Experiments demonstrate how our approach can differentiate connections with different mean rates, and show that it leads to adequate bandwidth utilization for anticipated link capacities and traffic burstiness.  1 Introduction  There are well understood models, see [9], which allow for the optimal pricing of networks such as the telephone network, where resources are allocated for the whole duration of a connection. These models are relatively simple since there is usually a straightforward mapping of a service to a one-dimensional quantity that models the amount of resources the service consumes...
Introduction  Research into using reinforcement learning to find optimal solutions to tasks where only partial information is available, i.e. partially observable Markovian decision processes (POMDPs), has traditionally focused on augmenting learning algorithms with memory or the ability to build internal models of the world. Our approach differs in that we consider agents with active perception, i.e. agents who exercise control over the sensory input they obtain from the world. Our conjecture is that agents should be able to learn to use active perception to find optimal solutions to what are otherwise POMDPs; and further, that this is possible using basic reinforcement learning techniques that do not employ memory or internal models.  This abstract presents our preliminary empirical investigation into this conjecture. We present some suggestive results, though at this stage our work is a limited proof of concept, focusing on a single grid world problem with no comparison against the e
Nearest Neighbour algorithms for pattern recognition have been widely studied. It is now well-established that they  offer a quick and reliable method of data classification. In this paper we further develop the basic definition of the  standard k-nearest neighbour algorithm to include the ability to resolve conflicts when the highest number of nearest  neighbours are found for more than one training class (kNN model). We also propose aNN model of nearest neighbour  algorithm that is based on finding the nearest average distance rather than nearest maximum number of neighbours.  These new models are explored using image understanding data. The models are evaluated on pattern recognition  accuracy for correctly recognising image texture data of five natural classes: grass, trees, sky, river reflecting sky and  river reflecting trees. On noise contaminated test data, the new nearest neighbour models show very promising results  for further studies when compared with neural networks.  1  ...
An enumerative technique is presented which supports reachability and timeliness analysis of time-dependent models based. The technique assumes a dense model of time and uses equivalence classes to enable discrete and compact enumeration of the state space. Properties of timed reachability among states are recovered through the analysis of timing constraints embedded within equivalence classes. In particular, algorithms are given to evaluate a tight profile for the set of feasible timings of any untimed run. Run time refinement of static profiles supports a mixed static/dynamic strategy in the development of a failure avoidance mechanism for dynamic acceptance and guarantee of hard real time processes.
Quality of Service (QoS) guarantees are required by an increasing number of applications to ensure a  minimal level of fidelity in the delivery of application data units through the network. Application-level  QoS does not necessarily follow from any transport-level QoS guarantees regarding the delivery of the  individual cells (e.g. ATM cells) which comprise the application&apos;s data units. The distinction between  application-level and transport-level QoS guarantees is due primarily to the fragmentation that occurs  when transmitting large application data units (e.g. IP packets, or video frames) using much smaller  network cells, whereby the partial delivery of a data unit is uselessff and, bandwidth spent to partially  transmit the data unit is wasted. The data
Variational inference methods, including  mean field methods and loopy belief propagation,  have been widely used for approximate  probabilistic inference in graphical models.
Before new automated technologies can be incorporated into the air-traffic-management system, they need to be analyzed and tested extensively. This analysis and testing is required not only to demonstrate functionality, but also to determine overall system performance and dependability. The risks and costs of operational testing preclude its routine use, especially to test system survivability where intentional serious failures will be forced to occur to ensure that they are handled properly. Fortunately, simulation provides a practical alternative. This paper describes a simulation system designed to assess the performance and survivability of various automated technologies in terminal area flight. It also presents some preliminary results of the system&apos;s use. 
We present a Semantic Web application that we call CS AKTive Space. The application exploits a wide range of semantically heterogeneous and distributed content relating to Computer Science research in the UK. This content is gathered on a continuous basis using a variety of methods including harvesting and scraping as well as adopting a range models for content acquisition. The content currently comprises around ten million RDF triples and we have developed storage, retrieval and maintenance methods to support its management. The content is mediated through an ontology constructed for the application domain and incorporates components from other published ontologies. CS AKTive Space supports the exploration of patterns and implications inherent in the content and exploits a variety of visualisations and multi dimensional representations. Knowledge services supported in the application include investigating communities of practice: who is working, researching or publishing with whom. This work illustrates a number of substantial challenges for the Semantic Web. These include problems of referential integrity, tractable inference and interaction support. We review our approaches to these issues and discuss relevant related work.
Recently a new generation of P2P systems, offering distributed hash table (DHT) functionality, have been proposed. These systems greatly improve the scalability and exact-match accuracy of P2P systems, but offer only the exact-match query facility. This paper outlines a research agenda for building complex query facilities on top of these DHT-based P2P systems. We describe the issues involved and outline our research plan and current status.
Simulating the dynamics of rigid bodies plays an important role in virtual reality applications such  as virtual assembly planning and ergonomy studies but also in the  eld of computer animation.
Tree structures are used extensively in domains such as computational biology, pattern recognition, XML databases, computer networks, and so on. One important problem in mining databases of trees is to find frequently occurring subtrees. However, because of the combinatorial explosion, the number of frequent subtrees usually grows exponentially with the size of the subtrees. In this paper, we present CMTreeMiner, a computationally effcient algorithm that discovers all closed and maximal frequent subtrees in a database of rooted unordered trees. The algorithm mines both closed and maximal frequent subtrees by traversing an enumeration tree that systematically enumerates all subtrees, while using an enumeration DAG to prune the branches of the enumeration tree that do not correspond to closed or maximal frequent subtrees. The enumeration tree and the enumeration DAG are defined based on a canonical form for rooted unordered trees--the depth-first canonical form (DFCF).
: We survey 25 years of research on decidability issues for Petri nets. We collect results on the decidability of important properties, equivalence notions, and temporal logics. 1. Introduction  Petri nets are one of the most popular formal models for the representation and analysis of parallel processes. They are due to C.A. Petri, who introduced them in his doctoral dissertation in 1962. Some years later, and independently from Petri&apos;s work, Karp and Miller introduced vector addition systems [47], a simple mathematical structure which they used to analyse the properties of &quot;parallel program schemata&apos;, a model for parallel computation. In their seminal paper on parallel program schemata, Karp and Miller studied some decidability issues for vector addition systems, and the topic continued to be investigated by other researchers. When Petri&apos;s ideas reached the States around 1970, it was observed that Petri nets and vector addition systems were mathematically equivalent, even though thei...
Social behaviour in intelligent agent systems is often considered to be achieved by deliberative, in-depth reasoning techniques. This paper shows, how a purely reactive multi-agent system can learn to evolve cooperative behaviour, by means of learning from previous experiences. In particular, we describe a learning multi agent approach to the problem of controlling power ow in an electrical power-grid. The problem is formulated within the framework of dynamic programming. Via a global optimization goal, a set of individual agents is forced to autonomously learn to cooperate and communicate. The ability of the purely reactive distributed systems to solve the global problem by means of establishing a communication mechanism is shown on two prototypical network con  gurations.
In this paper an efficient face candidates selector is proposed for face detection tasks in still gray level images. The proposed method acts as a selective attentional mechanism. Eye-analogue segments at a given scale are discovered by finding regions which are roughly as large as real eyes and are darker than their neighborhoods. Then a pair of eye-analogue segments are hypothesized to be eyes in a face and combined into a face candidate if their placement is consistent with the anthropological characteristic of human eyes. The proposed method is robust in that it can deal with illumination changes and moderate rotations. A subset of the FERET data set and the BioID face database are used to evaluate the proposed method. The proposed face candidates selector is successful in 98.75% and 98.6% cases respectively.
It is useful to prove that an implementation correctly follows a specification. But even with a provably correct implementation, given a choice, would a node choose to follow it? This paper explores how to create distributed system specifications that will be faithfully implemented in networks with rational nodes, so that no node will choose to deviate. Given a strategyproof centralized mechanism, and given a network of nodes modeled as having rational-manipulation faults, we provide a proof technique to establish the incentive-, communication-, and algorithm-compatibility properties that guarantee that participating nodes are faithful to a suggested specification. As a case study, we apply our methods to extend the strategyproof interdomain routing mechanism proposed by Feigenbaum, Papadimitriou, Sami, and Shenker (FPSS) [7], defining a faithful implementation.
When a client submits a set of XPath queries to  a XML database on a network, the set of answer  sets sent back by the database may include redundancy  in two ways: some elements may appear  in more than one answer set, and some elements  in some answer sets may be subelements  of other elements in other (or the same) answer  sets. Even when a client submits a single query,  the answer can be self-redundant because some elements  may be subelements of other elements in  that answer. Therefore, sending those answers as  they are is not optimal with respect to communication  costs. In this paper, we propose a method  of minimizing communication costs in XPath processing  over networks. Given a single or a set of  queries, we compute a minimal-size view set that  can answer all the original queries. The database  sends this view set to the client, and the client  produces answers from it. We show algorithms  for computing such a minimal view set for given  queries. This view set is optimal; it only includes  elements that appear in some of the final answers,  and each element appears only once.
This paper introduces three new probabilistic encryption  schemes using elliptic curves over rings. The cryptosystems are based  on three specific trapdoor mechanisms allowing the recipient to recover  discrete logarithms on different types of curves. The first scheme is an  embodiment of Naccache and Stern&apos;s cryptosystem and realizes a discrete  log encryption as originally wanted in [23] by Vanstone and Zuccherato.
When no channel knowledge is available at the OFDM transmitter, the water pouring strategy is inappropriate and the information sent on one subchannel can irremediably be lost if a deep fade occurs. The role of uniform spreading in Linear Precoded OFDM (LP-OFDM) also known as OFDM-CDMA [1], [2] is to exploit all the frequency diversity of the channel. However, due to the intercarrier interference introduced by the precoder, more complex equalization schemes are required. This paper proposes a new Multiresolution decoding algorithm simple to implement thanks to the Walsh-Hadamard recursive structure. For a target BER, the algorithm refines the accuracy (called here resolution) of the estimated symbols based on a tree-like structured algorithm and can achieve near Maximum Likelihood performance with only two stages. Moreover, some theoretical results are provided to optimize the parameters involved in the detection process.
A conversation protocol is a top-down specification framework which specifies desired global behaviors of a web service composition. In our earlier work [6] we studied the problem of realizability, i.e., given a conversation protocol, can a web service composition be synthesized to generate behaviors as specified by the protocol. Several sufficient realizability conditions were proposed in [6] to ensure realizability. Conversation protocols studied in [6], however, are essentially abstract control flows without data semantics. This paper extends the work in [6] and achieves more accurate analysis by considering data semantics. To overcome the state-space explosion caused by the data content, we propose a symbolic analysis technique for each realizability condition. In addition, we show that the analysis of the autonomy condition can be done using an iterative refinement approach.
We formulate regression as maximizing the minimum probability (ff) that the true regression  function is within    of the regression model. Our framework starts by posing  regression as a binary classification problem, such that a solution to this single classification  problem directly solves the original regression problem. Minimax probability machine  classification (Lanckriet et al., 2002a) is used to solve the binary classification problem, resulting  in a direct bound on the minimum probability ff that the true regression function is  within    of the regression model. This minimax probability machine regression (MPMR)  model assumes only that the mean and covariance matrix of the distribution that generated  the regression data are known; no further assumptions on conditional distributions  are required. Theory is formulated for determining when estimates of mean and covariance  are accurate, thus implying robust estimates of the probability bound ff. Conditions under  which the MPMR regression surface is identical to a standard least squares regression surface  are given, allowing direct generalization bounds to be easily calculated for any least  squares regression model (which was previously possible only under very specific, often  unrealistic, distributional assumptions). We further generalize these theoretical bounds  to any superposition of basis functions regression model. Experimental evidence is given  supporting these theoretical results.
Understanding and debugging of concurrent systems is a critical activity in systems design. Means supporting the visual inspection of drawings as well as the animation of the execution are expected to play a signiffcant role in future. Since Pictorial Janus ffPJff has been deffned as a completely visual language including an advanced animation semantics it serves well as a highfflevel language for the debugging and rapid development of concurrent systems. In this article, we present an interactive PJ speciffcation and analysis environment for the rapid prototyping of concurrent systems. The environmentcovers an editor with integrated animation facilities and an interpreter enhanced by debugging facilities.
Computer systems should provide what you want, when you want it (the WYWWYWI Principle, pronounced &quot;why why why&quot;), but they frequently do not. Our research encourages a new philosophy of design based on the WYWWYWI principle, and the tools for authors to provide this easily. Comprehensive metainformation embodies the WYWWYWI principle. Metainformation includes the structural relationships, content-based relationships, user-declared link-based relationships, and metadata around an element of interest. Combined, the metainformation goes a long way towards establishing the full semantics for (the meaning of and context around) a system&apos;s elements. We take a three-pronged approach to providing metainformation on a grand scale. First, we provide a systematic methodology for systems analysts to determine the relationships around elements of interest in their information domains---Relationship Analysis. Relationship Analysis will result in a comprehensive set of a domain&apos;s structural relationships. Second, we provide a Metainformation Engine, which automatically generates sets of structural and content-based relationships around elements of interest as links, as well as metadata within static and virtual documents. Third, we provide an infrastructure for widespread link-based services within both static and virtual documents. This approach provides the inspiration as well as a sound foundation for a ubiquitous embracing of the WYWWYWI principle in the everyday systems people use, both on the Web and beyond.  
Abstract  The design of Computer-Supported Cooperative Work (CSCW) systems involves a variety of disciplinary approaches, drawing as much on sociological and psychological perspectives on group and individual activity as on technical approaches to designing distributed systems. Traditionally, these have been applied independently---the technical approaches focussing on design criteria and implementation strategies, the social approaches focussing on the analysis of working activity with or without technological support.
Interoperability has become one of the big problems of e-commerce since it was born. A number of B2B standards like ebXML, UDDI, RosettaNet, xCBL, etc. emerged recently to solve the interoperability problem.
The paper describes an application composition and execution environment implemented as a transactional workflow system that enables sets of inter-related tasks to be carried out and supervised in a dependable manner. The paper describes how the system meets the requirements of interoperability, scalability, flexible task composition, dependability and dynamic reconfiguration. The system is general purpose and open: it has been designed and implemented as a set of CORBA services, running on top of a given ORB. The system serves as an example of the use of middleware technologies to provide a fault-tolerant execution environment for long running distributed applications.
We provide a system for surfing the web at a high level of abstraction, which is an analogy of the web browser, but which displays entire sites at a time. It allows a principled investigation of what is present, based on an overview of all available information. We show a site&apos;s relation to other sites, the broad nature of the information contained and how it is structured, and how it has changed over time. Our current system maintains a continuously updated archive of 40 million sites representing 1.9 billion web pages, and enables real-time navigation through the sea of web sites.
We present a new approach for automatic selection and transfer of corresponding image points in digital video (DV) image sequences. This approach utilizes and extends some well-known techniques and theories (e.g. the optical flow theory) as well as a designed simple error-detection mechanism to achieve a much better efficiency than the well-known Lucas-Kanade optical flow estimation (LK) method. Our test results use the SONY DCR-PC115 DV image sequences and show that the trackable range of 3-4 pixels in the LK method can apparently be enlarged to 30 pixels in this new approach. The proposed error-detection mechanism simply utilizes the average gradient, normalized cross-correlation, and a simple image registration aided by least squares adjustment. Test results show that it can efficiently detect and delete wrong tracked points, and thus improve the quality of point transfer, and provide accurate coordinates of a large number of corresponding image points in DV image sequences. This work aims at high precision automatic image triangulation for the automatic real-time mobile mapping vehicle system (MMVS). Some future works need to be done.
New applications in fields such as augmented or virtualized  reality have created a demand for dense, accurate real-time stereo reconstruction.
Although initially proposed as the deployable alternative to IP multicast, application-layer overlay network actually revolutionizes the way network applications can be built, since each overlay node is an end host, which is able to carry out more functionalities than simply forwarding packets. This paper addresses the on-demand media distribution problem in the context of overlay network. We take advantage of the strong buffering capabilities of end hosts, and propose a novel overlay multicast strategy, oStream. We have performed extensive analysis and performance evaluation with respect to the scalability and the efficiency of oStream. With respect to the required server bandwidth, we show that oStream defeats the theoretical lower bound of traditional multicast-based approaches, under both sequential and non-sequential stream access patterns. oStream is also robust against bursty requests. With respect to bandwidth consumption on the backbone network, we show that the benefit introduced by oStream overshadows the topological inefficiency (e.g., link stress and stretch) introduced by using applicationlayer multicast.
This paper introduces an approach for clustering  /classification which is based on the use  of local, high-order structure present in the  data. For some problems, this local structure  might be more relevant for classification  than other measures of point similarity used  by popular unsupervised and semi-supervised  clustering methods. Under this approach,  changes in the class label are associated to  changes in the local properties of the data.
This paper discusses monitoring of slope deformations by vision metrology with a CCD camera. Reflective targets are placed over a slope, and their object coordinates is measured by a photogrammetric technique. Precision and sensitivity of slope deformation measurement using vision metrology are investigated. Deformation of targets placed on a slope was detected by measurement at two time epochs using hypothesis testing, and a series of equations is derived for the detection. The strengths of the observation networks were evaluated from three view points, i.e. precision of target object coordinates, sensitivity of observations and reliability of observation. Model experiments were carried out to verify the method&apos;s validity. A slope model of 1.1 m  0.5 m in size was  constructed. An reasonable exposure configuration is looked for, which is capable of detecting displacement of about 2 mm pro 30  m. It is thus clarified that sufficient precision, sensitivity and reliability are achievable for practical use by a total of 12 exposures:  four for each of three locations.   * Corresponding author.   1. 
Many discovery problems, e.g., subgroup or association rule discovery, can naturally be cast as n-best hypotheses problems where the goal is to  nd the n hypotheses from a given hypothesis space that score best according to a certain utility function. We present a sampling algorithm that solves this problem by issuing a small number of database queries while guaranteeing precise bounds on con  dence and quality of solutions. Known sampling approaches have treated single hypothesis selection problems, assuming that the utility be the average (over the examples) of some function | which is not the case for many frequently used utility functions. We show that our algorithm works for all utilities that can be estimated with bounded error. We provide these error bounds and resulting worst-case sample bounds for some of the most frequently used utilities, and prove that there is no sampling algorithm for a popular class of utility functions that cannot be estimated with bounded error. The algorithm is sequential in the sense that it starts to return (or discard) hypotheses that already seem to be particularly good (or bad) after a few examples. Thus, the algorithm is almost always faster than its worst-case bounds.
Prototype-based languages are often described as being more  flexible and expressive than class-based languages. This greater flexibility  makes prototype-based languages well-suited for rapid prototyping and  exploratory programming, but comes with a serious loss of safety. Examples of  this are the encapsulation problem and the prototype corruption problem most  prototype-based languages suffer from. These problems preclude prototypebased  languages from being widely used. We propose a prototype-based  language that eliminates these problems and thus reintroduces safety in  prototype-based languages.
this article, a comprehensive survey of the medium access control (MAC) approaches for wireless mobile ad hoc networks is presented. The complexity in MAC design for wireless ad hoc networks arises due to node mobility, radio link vulnerability and the lack of central coordination. A series of studies on MAC design has been conducted in the literature to improve medium access performance in different aspects as identified by the different performance metrics. Tradeoffs among the different performance metrics (such as between throughput and fairness) dictate the design of a suitable MAC protocol. We compare the different proposed MAC approaches, identify their problems and discuss the possible remedies. The interactions among the MAC and the higher layer protocols such as routing and transport layer protocols are discussed and some interesting research issues are also identified. Copyright ff 2003 John Wiley &amp; Sons, Ltd
Various facial region biometrics have been used extensively in the areas of recognition and authentication. However, some regions of the face provide more information than is currently being fully utilized in these specific capacities. Biometrics associated exclusively with the eye region hold a key to identifying and classifying particular affective and cognitive states. This paper focuses on 1) methods for identifying and deriving the appropriate biometric data inherent to the eye region that is most useful in specific HCI scenarios and, 2) outlining a framework for classification of these biometric data into affective and cognitive states relative to a particular HCI context.
this paper we will focus on IPSec
It is shown that five apparently irreconcilable claims about the clausal syntax of Irish can be reconciled in a natural, base-generated LFG analysis that builds on the standard LFG theory of endocentricity and coheads/extended heads, the LFG projection architecture, and Toivonen&apos;s (2001) work on nonprojecting categories and c-structure adjunction. The analysis also builds on McCloskey&apos;s (1996) analysis of Irish adjunction, but does not posit complementizer lowering. The principal theoretical consequences of the analysis are 1) the reconciliation of the five claims, in particular a synthesis of McCloskey&apos;s position that the Irish preverbal particles are complementizers and Sells&apos;s (1984) position that they are head-adjoined to the verb, 2) the elaboration of Toivonen&apos;s (2001) theory of c-structure adjunction, 3) correct predictions about not only adjunction to matrix and subordinate clauses, but also adjunction to appositives.
This paper provides a formal foundation of the concept of  Abstract Class Interface Descriptions (ACIDs). It gives a definition of both  ACIDs and the operations defined on them and proves a number of  properties concerning their interactions. In class libraries and frameworks  documented with ACIDs these properties can be used to provide a better  understanding of their layered structure and to help in assessing the impact  of changes.
Considering the existing tendency toward the use of wireless  devices we propose in this paper a new service that helps users of those  devices in the (tedious, repetitive and many times costly in terms of  communication cost) task of obtaining software from the Web.
To strengthen the connection between requirements and design during the early stages of architectural design, a designer would like to have notations to help visualize the incremental refinement of an architecture from initially abstract descriptions to increasingly concrete components and interactions, all the while maintaining a clear focus on the relevant requirements at each step. We propose the combined use of a goal-oriented language GRL and a scenarios-oriented architectural notation UCM. Goals are used in the refinement of functional and non-functional requirements, the exploration of  alternatives, and their operationalization into architectural constructs. The scenario notation is used to depict the incremental elaboration and realization of requirements into architectural design. The approach is illustrated with an example from the telecom domain.
We present a scheme for retrieval by spatial reasoning in a spatial information system such as a Geographic Information System. We base this scheme on a spatial algebra consisting of topological (point, line and region), metric and morphological operators. We detail the operations available in this algebra and show how context can be built in for flexible retrieval of spatial data.
A branch and bound algorithm for computing globally optimal solutions to nonconvex nonlinear programs in continuous variables is presented. The algorithm is directly suitable for a wide class of problems arising in chemical engineering design. It can solve problems defined using algebraic functions and twice differentiable transcendental functions, in which finite upper and lower bounds can be placed on each variable. The algorithm uses rectangular partitions of the variable domain and a new bounding program based on convex/concave envelopes and positive definite combinations of quadratic terms. The algorithm is deterministic and obtains convergence with final regions of finite size. The partitioning strategy uses a sensitivity analysis of the bounding program to predict the best variable to split and the split location. Two versions of the algorithm are considered, the first using a local NLP algorithm (MINOS) and the second using a sequence of lower bounding programs in the search fo...
Roughly speaking a protocol is a way of exchanging messages so that each party gains some specified information. Correctness and privacy are fundamental constraints to the notion of a secure protocol. Essentially, correctness guarantees that the information the parties learn in executing a protocol is what was specified. Privacy guarantees that they cannot learn more than that. We investigate correctness and privacy for the primitive notion of oblivious transfer protocols. As introduced by Rabin, in an oblivious transfer a party S (the sender) owning a secret message m discloses this message to another party R (the receiver) with probability  1 2 . S does not find out whether R did get the message or not. A different version of this notion (1-out-of-2-oblivious transfer) was introduced by Even, Goldreich and Lempel. Traditionally, correctness and privacy were guaranteed assuming that the parties had bounded computational resources and that some appropriate mathematical problems were in...
As computer and network intrusions become more and more of a concern, the need for better capabilities to assist in the detection and analysis of intrusions also increases. System administrators typically rely on log files to analyze usage and detect misuse. However, as a consequence of the amount of data collected by each machine, multiplied by the tens or hundreds of machines under the system administrator&apos;s auspices, the entirety of the data available is neither collected nor analyzed. This is compounded by the need to analyze network traffic data as well.
Finding the l nearest neighbors to a query in a vector space is an important primitive in text and image retrieval. Here we study an extension of this problem with applications to XML and image retrieval: we have multiple vector spaces, and the query places a weight on each space. Match scores from the spaces are weighted by these weights to determine the overall match between each record and the query; this is a case of score aggregation. We study approximation...
In this paper we consider the problem of controlling the six  degrees of freedom of a manipulator using the projection of 3D lines in the  image plane of central catadioptric systems. Most of the effort in visual  servoing are devoted to points, only few works have investigated the use  of lines in visual servoing with traditional cameras and none has explored  the case of omnidirectional cameras. First a generic interaction matrix for  the projection of 3D straight lines is derived from the projection model  of the entire class of central catadioptric cameras. Then an image-based  control law is designed and validated through simulation results.
Many discovery problems, e.g., subgroup or association rule discovery, can naturally be cast as n-best hypothesis problems where the goal is to nd the n hypotheses from a given hypothesis space that score best according to a given utility function. We present a sampling algorithm that solves this problem by issuing a small number of database queries while guaranteeing precise bounds on condence and quality of solutions. Known sampling algorithms assume that the utility be the average (over the examples) of some function, which is not the case for many frequently used utility functions. We show that our algorithm works for all utilities that can be estimated with bounded error. We provide such error bounds and resulting worst-case sample bounds for some of the most frequently used utilities, and prove that there is no sampling algorithm for another popular class of utility functions. The algorithm is sequential in the sense that it starts to return (or discard) hypotheses that already...
Electronic business motivates automatic bargaining: computers may not be as good bargainers as expert human bargainers, but by talking to a large number of traders on the Internet, they stand a better chance of getting good deals. While an end-seller may be interested in getting the highest profit from each negotiation, traders have to balance between making large profits in few deals (thus missing opportunities in the failed negotiations) and making smaller profits in larger number of deals. When a deal is to be constructed through a chain of middlemen, these middlemen have to cooperate (in order to construct the deal) while trying to maximize their own profits. The middlemen can be seen as propagating constraints along the chain, with the aim to maximize its profit while satisfying all the bargainers&apos; constraints (failing that, the chain will break down). In this paper, a simple chainbargaining problem is defined. It is used to study constraint propagation strategies, which are essential components of automatic bargaining.
Most known text classifiers represent documents  as bags of words and process documents  as a whole. In practice, e.g., when  handling long documents with sections, it  would be useful to have algorithms that can  seamlessly switch from labeling documents to  labeling regions, or individual tokens, and be  able to account for the sequence and context  of words at least in a limited way. We discuss  how both of these issues can be addressed by  treating text classification as an instance of  a generalized token labeling problem which  we define. We show that Hidden Markov  models (HMMs) cover this range of labeling  problems and point out that in our setting a  HMM is in fact a direct generalization of the  naive Bayes classifier and allows limited context  to be treated. We derive algorithms for  the associated learning problems that cover  the full range from completely labeled data,  to labels that impose only weak constraints  on the possible state sequences. In experiments,  we demonstrate the clear advantages  of HMMs for classification.
Batch training algorithms with a different learning rate for eachweight are investigated. The adaptive learning rate algorithms of this class that apply inexact one-dimensional subminimization are analyzed and their global convergence is studied. Simulations are conducted to evaluate the convergence behavior of two training algorithms of this class and to compare them with several popular training methods.  
Wittmeyer’s pseudoinverse iterative algorithm is formulated
as a dynamic connectionist Data Compression and Reconstruction (DCR) network, and subnets of this type are supplemented by the winner-take-all paradigm. The winner is selected upon the goodness-of-fit of the input reconstruction. The network can be characterised as a competitive-cooperative-competitive architecture by virtue of the contrast enhancing properties of the pseudoinverse subnets. The network is capable of fast learning. The adopted learning method gives rise to increased sampling in the vicinity of dubious boundary regions that resembles the phenomenon of categorical perception. The generalising abilities of the scheme allow one to utilise single bit connection strengths. The network is robust against input noise and contrast levels, shows little sensitivity to imprecise connection strengths, and is promising for mixed VLSI implementation with on-chip learning properties. The features of the DCR network are demonstrated on the NIST database of handprinted characters.
Multiple-Phased Systems, whose operational life can be partitioned in a set of disjoint periods, called Â¿phasesÂ¿; include several classes of systems such as Phased Mission Systems and Scheduled Maintenance Systems. Because of their deployment in critical applications, the dependability modeling and analysis of Multiple-Phased Systems is a task of primary relevance. However, the phased behavior makes the analysis of Multiple-Phased Systems extremely complex. This paper is centered on the description and application of DEEM, a dependability modeling and evaluation tool for Multiple Phased Systems. DEEM supports a powerful and efficient methodology for the analytical dependability modeling and evaluation of Multiple Phased Systems, based on Deterministic and Stochastic Petri Nets and on Markov Regenerative Processes.
As a first step toward realizing a dynamical system that evolves while spontaneously determining its own rule for time evolution, function dynamics (FD) is analyzed. FD consists of a functional equation with a self-referential term, given as a dynamical system of a one-dimensional map. Through the time evolution of this system, a dynamical graph (a network) emerges. This graph has three interesting properties: (i) vertices appear as stable elements, (ii) the terminals of directed edges change in time, and (iii) some vertices determine the dynamics of edges, and edges determine the stability of the vertices, complementarily. Two aspects of FD are studied, the generation of a graph (network) structure and the dynamics of this graph (network) in the system.
A simulation model is successful if it leads to policy action, i.e., if it is implemented. Studies show that for a model to be implemented, it must have good correspondence with the mental model of the system held by the user of the model. The user must feel confident that the simulation model corresponds to this mental model. An understanding of how the model works is required. Simulation models for implementation must be developed step by step, starting with a simple model, the simulation prototype. After this has been explained to the user, a more detailed model can be developed on the basis of feedback from the user. Software for simulation prototyping is discussed, e.g., with regard to the ease with which models and output can be explained and the speed with which small models can be written.
Hedging of fixed income securities remains one of the most challenging problems faced by financial institutions. The predominantly used measures of duration and convexity do not completely capture the interest rate risks borne by the holder of these securities. Using historical data for the entire yield curve, we perform a principal components analysis and find that the first four factors capture over 99.99% of the yield curve variation. Incorporating these factors into the pricing of arbitrary fixed income securities via Monte Carlo simulation, we derive perturbation analysis (PA) estimators for the price sensitivities with respect to the factors. Computational results for mortgage-backed securities (MBS) indicate that using these sensitivity measures in hedging provides far more protection against interest risk exposure than the conventional measures of duration and convexity.
As ubiquitous computing emerges in our lives and cities new opportunities for artistic and otherwise cultural interventions in urban space follow, but so far not much work has been done in order to articulate the socio-cultural significance of these new opportunities. This paper is part of a general attempt to develop a coherent understanding of the implications and potentials of ubiquitous computing in the context of everyday city life. On a more specific level the paper examines how the notion of social friction can be helpful in the development and analysis of ubiquitous computing in relation to art and design. Social friction is articulated as a critical position, which could be applied as a strategy for design. Our approach consists of a theoretical analysis and precedes concrete development and real-life experiments. As such the paper aims to establish a steppingstone from which to launch actual digital designs. We argue that by designing for the social friction, which is an intrinsic characteristic of everyday life, new forms of social and cultural potentials can be released. By means of discussing CityNova, a vision for a possible use of ubiquitous computing in urban space, we explore how this approach might lead to systems that create new ways of experiencing the city.
This paper addresses the optimization of FIR filters for low power. We propose a search algorithm to find the combination of the number of taps and coeffcient bit-width that leads to the minimum number of total partial sums, and hence to the least power consumption. We show that the minimum number of taps does not necessarily lead to the least power consumption in fully parallel FIR filter architectures. This is particularly true if the reduction of the bit-width of the coeffcients is taken into account. We show that power is directly related to the total number of partial sums in the FIR filter, which in turn is determined by the number of bits set to 1 in the coeffcients. We have developed a search algorithm that achieves up to 36% less power consumption when compared to an implementation using the minimum number of taps.
Educational research has highlighted the importance of maintaining an orderly classroom environment and providing both clear and well-organized instruction tailored to the needs of individual students. Time spent on direct instruction and particularly the direct instruction of basic skills is associated with school learning (Wang, Haertel &amp; Walberg, 1993). With the increased interest in constructivistic conceptions of learning and teaching today, educators with constructivistic orientations contend that various forms of knowledge and skills are applied more generally when constructed by the learners themselves as opposed to explicitly taught: &quot;knowledge is made, not acquired&quot; (Phillips, 2000, p. 7). Such a view nevertheless often leads to an inclination to reject direct instruction by the teacher (see, for example, Brooks &amp; Brooks, 1993). It should be noted, however, that many of the discussions of constructivistic orientations to learning and instruction are at the level of slogan and clichÃ© (Duffy &amp; Cunningham, 1996; Finn &amp; Ravitch, 1996; Kozloff, 1998). In addition, the term constructivism has come to serve as an umbrella term for a diversity of views (Phillips, 1995; 2000).
We believe that a broad class of future applications will span both the Internet and the telephone network because such multiplanar applications have several economic and architectural advantages over conventional ones. We also envision the close interlinking of the telephone network and the Internet to form a multimodal network. In this paper, we describe these applications and networks, outline their architecture, and present our experiences in constructing a prototype multiplanar application.
We devise a simple model to study the phenomenon of free-riding and the effect of free identities on user behavior in peer-to-peer systems. At the heart of our model is a strategic user of a certain type, an intrinsic and private parameter that reflects the user&apos;s generosity. The user decides whether to contribute or free-ride based on how the current burden of contributing in the system compares to her type. We derive the emerging cooperation level in equilibrium and  quantify the effect of providing free-riders with degraded service on the emerging cooperation. We find that this penalty  mechanism is beneficial mostly when the &quot;generosity level&quot; of the society (i.e., the average type) is low. To quantify the social cost of free identities, we extend the model to account for dynamic scenarios with turnover (users joining and leaving) and with whitewashers: users who strategically leave the system and re-join with a new identity. We find that the imposition of penalty on all legitimate newcomers incurs a significant social loss only under high turnover rates in conjunction with intermediate societal generosity levels.
We present an algorithm for complete path planning for translating polyhedral robots in 3D. Instead of exactly computing an explicit representation of the free space, we compute a roadmap that captures its connectivity. This representation encodes the complete connectivity of free space and allows us to perform exact path planning. We construct the roadmap by computing deterministic samples in free space that lie on an adaptive volumetric grid. Our algorithm is simple to implement and uses two tests: a complex cell test and a star-shaped test. These tests can be efficiently performed on polyhedral objects using max-norm distance computation and linear programming. The complexity of our algorithm varies as a function of the size of narrow passages in the configuration space. We demonstrate the performance of our algorithm on environments with very small narrow passages or no collision-free paths.
The model used in this report focuses on the analysis  of ship waiting statistics and stock fluctuations under different  arrival processes. However, the basic outline is the  same: central to both models are a jetty and accompanying  tankfarm facilities belonging to a new chemical plant in the   Port of Rotterdam. Both the supply of raw materials and   the export of finished products occur through ships loading   and unloading at the jetty. Since disruptions in the plants  production process are very expensive, buffer stock is  needed to allow for variations in ship arrivals and overseas  exports through large ships.   Ports provide jetty facilities for ships to load and unload  their cargo. Since ship delays are costly, terminal operators  attempt to minimize their number and duration. Here, simulation  has proved to be a very suitable tool. However, in port  simulation models, the impact of the arrival process of ships  on the model outcomes tends to be underestimated. This article  considers three arrival processes: stock-controlled,   equidistant per ship type, and Poisson. We assess how their  deployment in a port simulation model, based on data from a  real case study, affects the efficiency of the loading and  unloading process. Poisson, which is the chosen arrival  process in many client-oriented simulations, actually performs worst in terms of both ship delays and required storage capacity. Stock-controlled arrivals perform best with regard to ship delays and required storage capacity.   In the case study two types of arrival processes were  considered. The first type are the so-called stock-controlled  arrivals, i.e., ship arrivals are scheduled in such a way, that  a base stock level is maintained in the tanks. Given a base  stock level of a raw material or ...
The effective use of humanoid robots in space will depend upon  the efficacy of interaction between humans and robots. The key to  achieving this interaction is to provide the robot with sufficient skills for  natural communication with humans so that humans can interact with the  robot almost as though it were another human. This requires that a number  of basic capabilities be incorporated into the robot, including voice  recognition, natural language, and cognitive tools on-board the robot to  facilitate interaction between humans and robots through use of common  representations and shared humanlike behaviors.
Couper (2002) outlines the &quot;challenges and opportunities&quot; of recent and stillemerging technological developments on the conduct of survey research. This paper focuses on one such development -- the use of computer-assisted survey instruments in place of paper-andpencil questionnaires -- and it focuses on one particular opportunity which this development presents: the ability to improve efficiency, &quot;flow,&quot; and naturalness, and in general make the interview experience a more pleasant one for all participants, while still controlling question wording and sequencing. Moral arguments can be raised in defense of such efforts; the potential for important practical benefits, including improved survey cooperation, lends more mundane but perhaps more potent support. Although the research literature is surprisingly scant, there is some evidence that improved instrument design can reduce nonresponse. A recent effort by the U.S. Census Bureau to redesign the core instrument for the Survey of Income and Program Participation (SIPP) offers additional support. Motivated in large measure by evidence of increasing unit nonresponse and attrition, the primary goal of the SIPP redesign effort was to improve the interview process, and in particular to seek ways to avoid violations of conversational norms (e.g., Grice, 1975). A great many of the SIPP interview process improvements would not have been feasible without the computerization of the survey instrument. This paper briefly summarizes many of the technology-based changes implemented in the SIPP instrument, and briefly describes a set of field experiments used to develop and refine the new procedures and to evaluate their success in achieving SIPP&apos;s redesign goals. Keywords: burden, conversational norms, efficiency, flow, nonresponse/...
RAID-II is a high-bandwidth, networkattached storage server designed and implemented at the University of California at Berkeley. In this paper, we measure the performance of RAID-II and evaluate various architectural decisions made during the design process. We first measure the end-to-end performance of the system to be approximately 20 MB/s for both disk array reads and writes. We then perform a bottleneck analysis by examining the performance of each individual subsystem and conclude that the disk subsystem limits performance. By adding a custom interconnect board with a high-speed memory and bus system and parity engine, we are able to achieve a performance speedup of 8 to 15 over a comparative system using only off-theshelf hardware.
Introduction  The force of induction F on a charge q is given by  FA=-qtcdd,(1)  where A is the usual magnetic vector potential defined by  A r  rJr  rrc  -  s    ,(2)  where J is the current density. Slowly varying effects are assumed here, where the basic theory may be given as a true relativity theory, involving the separation distance between two charges and its time derivatives.  This force of induction, Eq. (1), yields Faraday&apos;s law of electromagnetic induction for the special case of an electromotive force (emf) around a fixed closed loop. In particular,  emf  d  d  d  d  d    &apos;  &amp;  (  0  ) =-  =-  =-    s s s  sF  q  s  tc tc  an  tc  an  A  B    ,(3)  where F is the magnetic flux through the loop.  It is observed in the laboratory that an emf is also induced when =A tc 0 , and the magnetic flux through the loop is changed by moving the loop, so Faraday&apos;s law becomes  emf = -      .-(4)  Francisco Mller&apos;s (1987) experiments show that induction occurs locally and that the force 
This paper describes the design, implementation and evaluation of a user-verification system for a smart gun, which is based on grip-pattern recognition. An existing pressure sensor consisting of an array of 44    44 piezoresistive elements is used to measure the grip pattern. An interface has been developed to acquire pressure images from the sensor. The values of the pixels in the pressure-pattern images are used as inputs for a verification algorithm, which is currently implemented in software on a PC. The verification algorithm is based on a likelihoodratio classifier for Gaussian probability densities. First results indicate that it is feasible to use grip-pattern recognition for biometric verification.
The frequency shifts predicted by the `relativistic&apos; Doppler  effect are derived in the photon picture of light. It turns  out that, in general, the results do not depend exclusively  on the relative velocity between observer and light source.
Protein-protein interactions are of great interest to biologists. A variety of high-throughput techniques have been devised, each of which leads to a separate definition of an interaction network. The concept of differential association rule mining is introduced to study the annotations of proteins in the context of one or more interaction networks. Differences among items across edges of a network are explicitly targeted. As a second step we identify differences between networks that are separately defined on the same set of nodes. The technique of differential association rule mining is applied to the comparison of protein annotations within an interaction network and between different interaction networks. In both cases we were able to find rules that explain known properties of protein interaction networks as well as rules that show promise for advanced study.
In order to manage the use of roles for the purpose of access control, it is important to look at attributes beyond the consideration of capability assignment. Fundamentally, a generic attribute description using a constraint-based approach will allow many of the important aspects of role, such as scope, activation and deactivation, to be included. Furthermore, the commonly accepted concept of role hierarchy is challenged from the point of view of subsidiarity in real organisations, with the suggestion that role hierarchy has limited usefulness that does not seem to apply widely.
In this paper, we consider the problem of assigning sensors to track targets so as to minimize the expected error in the resulting estimation for target locations. Specifically, we are interested in how disjoint pairs of bearing or range sensors can be best assigned to targets in order to minimize the expected error in the estimates. We refer to this as the focus of attention (FOA) problem. In its
In the field of Computer-Aided anything, acronyms abound. They are, after all, useful tools. However, there is a risk that we become constrained by them and, as a result, fail to see beyond them.
Our approach to extracting information from the web analyzes the structural content of web pages through exploiting the latent information given by HTML tags. For each specific extraction task, an object model is created consisting of the salient fields to be extracted and the corresponding extraction rules based on a library of HTML parsing functions. We derive extraction rules for both single-slot and multiple-slot extraction tasks which we illustrate through two sample domains.
In kernel methods, an interesting recent development  seeks to learn a good kernel from  empirical data automatically. In this paper,  by regarding the transductive learning  of the kernel matrix as a missing data  problem, we propose a Bayesian hierarchical  model for the problem and devise the  Tanner-Wong data augmentation algorithm  for making inference on the model. The  Tanner-Wong algorithm is closely related to  Gibbs sampling, and it also bears a strong resemblance  to the expectation-maximization  (EM) algorithm. For an effcient implementation,  we propose a simplified Bayesian hierarchical  model and the corresponding TannerWong  algorithm. We express the relationship  between the kernel on the input space  and the kernel on the output space as a  symmetric-definite generalized eigenproblem.
This paper presents the current state in an ongoing development of the Genetic  Improvisation Model (GIM): a framework for the design of real-time improvisational  systems. The aesthetic rationale for the model is presented, followed by  a discussion of its general principles. A discussion of the Emonic Environment,  a networked system for audiovisual creation built on GIM&apos;s principles, follows
In this paper we analyse the mean-variance hedging approach in an incomplete market under the assumption of additional market information, which is represented by a given, finite set of observed prices of non-attainable contingent claims. Due to no-arbitrage arguments, our set of investment opportunities increases and the set of possible equivalent martingale measures shrinks. Therefore, we obtain a modified mean-variance hedging problem, which takes into account the observed additional market information. Solving this by means of the techniques developed by Gourieroux, Laurent and Pham (1998), we obtain an explicit description of the optimal hedging strategy and an admissible, constrained variance-optimal signed martingale measure, that generates both the approximation price and the observed option prices.
SIS PRUEBA is a software tool to integrate usability and user-centred design principles in the development process of services within Telefnica Mviles Espaa (TME), the largest mobile telecommunications operator in Spain.
Predicting the native conformation using computational protein models requires a large number  of energy evaluations even with simplified models such as hydrophobic-hydrophilic (HP)  models. Clearly, energy evaluations constitute a significant portion of computational time. We  hypothesize that given the structured nature of algorithms that search for candidate conformations  such as stochastic methods, energy evaluation computations can be cached and reused,  thus saving computational time and effort. In this paper, we present a caching approach and  apply it to 2D triangular HP lattice model. We provide theoretical analysis and prediction of  the expected savings from caching as applied this model. We conduct experiments using a sophisticated  evolutionary algorithm that contains elements of local search, memetic algorithms,  diversity replacement, etc. in order to verify our hypothesis and demonstrate a significant level of savings in computational effort and time that caching can provide.
This paper considers the modes of interaction between one or several human operators and an active sensor network -- a fully decentralized network of sensors some or all of which have actuators and are in that sense active. The primary goal of this study is to investigate the conditions under which the human involvement will not jeopardize scalability of the overall system. Two aspects of human-robot interaction are considered: the ways in which the global view of the system may be conveyed to the operators, and how the operators may influence the behavior of the system during the course of its operation. The results of analysis favor peer-topeer information-based interactions between the operators and the network whereby the humans act as extended sensors and communication nodes of the network itself. Experiments on an indoor active sensor network are described.
Recently, an approach has been presented to minimize Disjoint Sumof  -Products (DSOPs) based on Binary Decision Diagrams (BDDs). Due  to the symbolic representation of cubes for large problem instances, the  method is orders of magnitude faster than previous enumerative techniques.
NASA has embarked on a long-term program to develop  human-robot systems for sustained, affordable space exploration.
Packet classification is an enabling function for a variety of Internet applications including Quality of Service, security, monitoring, and multimedia communications. In order to classify a packet as belonging to a particular flow or set of flows, network nodes must perform a search over a set of filters using multiple fields of the packet as the search key. In general, there have been two major threads of research addressing packet classification: algorithmic and architectural. A few pioneering groups of researchers posed the problem, provided complexity bounds, and offered a collection of algorithmic solutions. Subsequently, the design space has been vigorously explored by many offering new algorithms and improvements upon existing algorithms. Given the inability of early algorithms to meet performance constraints imposed by high speed links, researchers in industry and academia devised architectural solutions to the problem. This thread of research produced the most widely-used packet classification device technology, Ternary Content Addressable Memory (TCAM). New architectural research combines intelligent algorithms and novel architectures to eliminate many of the unfavorable characteristics of current TCAMs. We observe that the community appears to be converging on a combined algorithmic and architectural approach to the problem. Using a taxonomy based on the high-level approach to the problem and a minimal set of running examples, we provide a survey of the seminal and recent solutions to the problem. It is our hope to foster a deeper understanding of the various packet classification techniques while providing a useful framework for discerning relationships and distinctions.
this report, Paul Lindgreen as secretary and as editor of the interim report [Lin90a]
In this paper we compare the average performance of one class of low-discrepancy quasi-Monte Carlo sequences for global optimization. Weiner measure is assumed as the probability prior on all optimized functions. We show how to construct van der Corput sequences and we prove their consistency. Numerical experimentation shows that the van der Corput sequence in base 2 has a better average performance.
This paper addresses the opportunity to put into place a virtual consortium for modeling and simulation. While periodic conferences such as the Winter Simulation Conference are tremendously vital to the continued growth of modeling and simulation research, they do not offer the day-to-day technical exchange that can now be made possible with matured collaborative technologies.
ue, the Netherlands, Norway, the Philippines, the Rockefeller Foundation, the Rural Industries Research and Development Corporation (Australia), South Africa, the Southern African Development Bank, Spain, Sweden, Switzerland, the United Kingdom, the United Nations Children&apos;s Fund, the United States, and the World Bank. CLASSIFIC ATION AND REGRESSION TREES, CART^TM  A USER MANUAL FOR IDENTIFYING INDIC A TORS OF VULNERABILITY TO FAMINE AND CHRONIC FOOD INSECURITY YISEHAC YOHANNES PATRICK WEBB MICROCOMPUTERS IN POLICY RESEARCH   INTERNATIONAL FOOD POLICY RESEARCH INSTITUTE  CART is a registered trademark of California Statistical Software, Inc. Copyright 1999 by the International Food Policy Research Institute 2033 K Street, N.W. Washington, D.C. 20006-1002 U.S.A. Library of Congress Cataloging-in-Publication Data available Yohannes, Yisehac Classification and Regression Trees, Cart^TM : A User Manual for Identifying Indicators of Vulnerability to Famine and Chronic Food Insecurity / Yise
An approach for segmentation of handwritten touching numeral strings is presented in this paper. A neural network has been designed to deal with various types of touching observed frequently in numeral strings. A numeral string image is split into a number of line segments while stroke extraction is being performed and the segments are represented with straight lines. Four types of primitive are defined based on the lines and used for representing the numeral string in more abstractive way and extracting clues on touching information from the string. Potential segmentation points are located using the neural network by active interpretation of the features collected from the primitives. Also, the run-length coding scheme is employed for efficient representation and manipulation of images. On a test set collected from real mail pieces, the segmentation accuracy of 89.1% was achieved, in image level, in a preliminary experiment. 1. 
The Java Modeling Language (JML) can be used to specify  the detailed design of Java classes and interfaces by adding annotations  to Java source files. The aim of JML is to provide a specification language  that is easy to use for Java programmers and that is supported by a wide  range of tools for specification type-checking, runtime debugging, static  analysis, and verification. This paper
Ensuring performance isolation and differentiation among workloads that share a storage infrastructure is a basic requirement in consolidated data centers. Existing management tools rely on resource provisioning to meet performance goals; they require detailed knowledge of the system characteristics and the workloads. Provisioning is inherently slow to react to system and workload dynamics, and in the general case, it is impossible to provision for the worst case.
This paper describes an on-line handwritten Japanese text recognition method that is liberated from constraints on writing direction (line direction) and character orientation. This method estimates the line direction and character orientation using the time sequence information of pen-tip coordinates and employs writingbox -free recognition with context processing combined. The method can cope with a mixture of vertical, horizontal and skewed lines with arbitrary character orientations. It is expected useful for tablet PC&apos;s, interactive electronic whiteboards and so on.
In order to analyze market trends and make reasonable business plans, a company&apos;s local data is not sufficient. Decision making must also be based on information from suppliers, partners and competitors. This external data can be obtained from the Web in many cases, but must be integrated with the company&apos;s own data, for example, in a data warehouse. To this end, Web data has to be mapped to the star schema of the warehouse. In this paper we propose a semi-automatic approach to support this transformation process. Our approach is based on the use a rooted labeled tree representation of Web data and the existing warehouse schema. Based on this common view we can compare source and target schemata to identify correspondences. We show how the correspondences guide the transformation to be accomplished automatically. We also explain the meaning of recursion and restructuring in mapping rules, which are the core of the transformation algorithm.
In this paper we introduce a new embedding technique to  linearly project labeled data samples into a new space where the performance  of a Nearest Neighbor classifier is improved. The approach is  based on considering a large set of simple discriminant projections and  finding the subset with higher classification performance. In order to implement  the feature selection process we propose the use of the adaboost  algorithm. The performance of this technique is tested in a multiclass  classification problem related to the production of cork stoppers for wine  bottles.
inchoative, &quot;up&quot;    pffed    16    48    &quot;before, in front of&quot;    roz    80    295    inch., &quot;disperse/ break into pieces&quot;    nad    5    33    &quot;over&quot;    pod    26    74    &quot;under&quot;    od    41    253    distantiational movement    sum    195    762        TOTAL        957        (6) the secret must be found in the different status of stem-initial CC-clusters.    (7) stem-initial CCs observed with  a. prefixal-V only +e  b. prefixal - only -e  c. both mix     +e only: 17 CCs -e only: 38 CCs ct, dn, dff, jm, lstn, mk, pn, ps, rv, ffv, sch, sr, v, tn, vff, zff, ffr  bl, bff, cl, cv, ffl, fff, fr, hl, hm, hv, chl, chrchl, km, kr, kff, kv, mff, mr, pl, pt, sh, sv, k, n, p, r, tl, tr, tv, vd, vr, zbr, zp, zt, ffh, ffm, ffff, ffv  mix: 35 CCs br, fft, dm, dr, dv, hn, hr, hff, chv, jd, kd, kl, ml, mn, pj, pr, pff, sk, sl, sm, sn, sp, st, l, t, tff, vff, vl, vff, v, vz, zd, zl, zn, zv TOTAL nb CC: 90 (8) A given root belongs to one and only one of these three groups. (9)    CC mix represented by how many it
In this paper we present a comprehensive approach to conceptual structuring and intelligent navigation of text databases. Given any collection of texts, we first automatically extract a set of index terms describing each text. Next, we use a particular lattice conceptual clustering method to build a network of clustered texts whose nodes are described using the index terms. We argue that the resulting network supports an hybrid navigational approach to text retrieval - implemented into an actual user interface - that combines browsing potentials with good retrieval performance. We present the results of an experiment on subject searching where this approach outperformed a conventional Boolean retrieval system.
Energy management has become one of the great challenges in portable computing. This is the result of the increasing energy requirements of modern portable devices without a corresponding increase in battery technology.  Sleep is a new energy reduction technique for handheld devices that is most effective when the handheld&apos;s processor is lightly loaded, such as when the user is reading a document or looking at a web page. When possible, rather than using the processor&apos;s idle mode, Sleep tries to put the processor in sleep mode for short periods (less than one second) without affecting the user&apos;s experience. To enhance the perception that the system is on, an image is maintained on the display and activity is resumed as a result of external events such as touch-screen and button activity. We have implemented Sleep on a prototype pocket computer, where it has reduced energy consumption by up to 60%.
In this tutorial we provide answers to the top ten inputmodeling questions that new simulation users ask, point out common mistakes that occur and give relevant references. We assume that commercial input-modeling software will be used when possible, and only suggest non-commercial options when there is little else available. Detailed examples will be provided in the tutorial presentation.
This paper discusses the initial efforts to implement simulation modeling as a visual management and analysis tool at an automotive foundry plant manufacturing engine blocks. The foundry process was modeled using Pro Model to identify bottlenecks and evaluate machine performance, cycle times and production data (total parts, rejects, throughput, products/hr) essential for efficient production control. Results from the current system identified assembly machine work area as the bottleneck (although utilization was greater than 95% for two assembly machines) resulting in high work-in-process (WIP) inventory level, low resource and machine utilization. Based on these results, optimum numbers were identified through use of scenarios by varying the number of assembly machines and processing time of each machine. In addition to these scenarios, strategies for production control involving buffer sizes were also made.
We present an algorithm for conjunctive and disjunctive Boolean equation systems (BESs), which arise frequently in the verification and analysis of finite state concurrent systems. In contrast to the previously best known O(e&amp;sup2;) time solutions, our algorithm computes the solution of such a fixpoint equation system with size e and alternation depth d in O(e log d) time.
This article is meant to provide the reader with details regarding the present state of the project, describing the current architecture of the system, its latest innovations and other systems 10  that make use of the NetSolve infrastructure. Copyright ff 2002 John Wiley &amp; Sons, Ltd
This report presents the InfoVis Toolkit, designed to support the creation,  extension and integration of advanced 2D Information Visualization components into interactive  Java Swing applications. The InfoVis Toolkit provides specific data structures to  achieve a fast action/feedback loop required by dynamic queries. It comes with a large  set of components such as range sliders and tailored control panels required to control and  configure the visualizations. These components are integrated into a coherent framework  that simplifies the management of rich data structures and the design and extension of  visualizations. Supported data structures currently include tables, trees and graphs. Supported  visualizations include scatter plots, time series, Treemaps, node-link diagrams for  trees and graphs and adjacency matrix for graphs. All visualizations can use fisheye lenses  and dynamic labeling. The InfoVis Toolkit supports hardware acceleration when available  through Agile2D, an implementation of the Java Graphics API based on OpenGL, achieving  speedups of 10 to 60 times. The report
This paper addresses the simulation of the dynamics of complex systems by using hierarchical graph and multi-agent system. A complex system is composed of numerous interacting parts that can be described recursively. First we summarize the hierarchical aspect of the complex system. We then present a description of hierarchical graph as a data structure for structural modeling in parallel with dynamics simulation by agents. This method can be used by physiological modelers, ecological modelers, etc as well as in other domains that are considered as complex systems. An example issued from physiology will illustrate this approach.
uses to deliver value to its customers. In today&apos;s competitive environment, the globalization of markets has rapidly substituted the traditional integrated business. The competitive success of an organization no longer depends only on its own efforts, but relies on the efficiency of the entire supply chain. Therefore, building an effective supply chain is fast becoming paramount in today&apos;s marketplace. Distributed Supply Chain (DSC) Simulation has been identified as one of the best means to test and analyze the performance of supply chains. The Generic Runtime Infrastructure for Distributed Simulation (GRIDS) is a middleware that supports the reuse and interoperation of DSC simulations. This paper reports the experience on employing the GRIDS to support the distributed collaboration of an automobile manufacture supply chain simulation. Several advantages of GRIDS are also discussed here which make it an ideal middleware for DSC simulations.
this paper) and (2) develop a visual method for each characterization. The mariner community needs enhanced characterizations of environmental uncertainty now, but the accuracy of the characterizations is still not sufficient enough and therefore formal user evaluations cannot take place at this point in development. We received feedback on the applicability of our techniques from domain experts. We used this in conjunction with previous results to compile a set of development guidelines (some obvious, others not)
This paper proposes the InstantGrid framework for on-demand  construction of grid points. In contrast to traditional approaches, InstantGrid  is designed to substantially simplify software management in grid  systems, and is able to instantly turn any computer into a grid-ready  platform with the desired execution environment. Experimental results  demonstrate that a 256-node grid point with commodity grid middleware  can be constructed in five minutes from scratch.
We introduce a generic framework for proof carrying code, developed and mechanically verified in Isabelle/HOL. The framework defines and proves sound a verification condition generator with minimal assumptions on the underlying programming language, safety policy, and safety logic. We demonstrate its usability for prototyping proof carrying code systems by instantiating it to a simple assembly language with procedures and a safety policy for arithmetic overflow.
this paper. Ref [15] addresses the knowledge consensus problem when teams of agents only have local communication between nearest neighbors. Since the set of nearest neighbors is constantly changing, the overall system becomes a hybrid system. The paper shows that if the union over all bidirectional communication graphs is connected for finite periods of time, then consensus is achieved. While the results in this paper are not as strong, only unidirectional communication links are assumed
In any multi-hop routing scheme, cooperation by the intermediate nodes are essential for the succesful delivery of traffic. However, the effort exerted by the intermediate nodes are often unobservable by the source and/or destination nodes. We show it is possible to overcome this problem of hidden action by designing contracts, in the form of payments, to induce cooperation from the intermediate nodes. Interestingly, the ability to monitor per-hop or per-path outcomes, even if costless to implement, may not improve the welfare of the participants or the performance of the network.
This paper develops a framework to measure the impact of agricultural research on urban poverty. Increased investments in agricultural R&amp;D can lower food prices by increasing food production, and lower food prices benefit the urban poor because they often spend more than 60% of their income on food. Application of the framework to China shows that these food price effects are large and that the benefits for the urban poor have been about as large as the benefits for the rural poor. KEYWORDS: developing countries, China, agricultural research, urban, poverty ii  ACKNOWLEDGMENTS The authors are grateful for helpful comments received from Peter Hazell, Robert Evanson and participants in a session at the American Agricultural Economics Association annual meeting in Chicago, August 5-8, 2001. iii  TABLE OF CONTENTS 1. 
To enable effcient access to multimedia content, the media data has to be augmented by semantic metadata and functionality. The semantic representation has to be integrated with domain ontologies to fully exploit domain-specific knowledge. This knowledge can be used for refining ambiguous user queries by closing the conceptual gap between the user and the information to be retrieved. In our previous research, we have introduced Enhanced Multimedia Meta Objects (EMMOs) as a new approach for semantic multimedia meta modeling, as well as the query algebra EMMA, which is adequate and complete with regard to the EMMO model. This paper focuses on the refinement of EMMA queries by incorporating ontological knowledge.
This paper summarises the achievements of a multidisciplinary Bioinformatics project which has the objective of providing a general mechanism for efficient computerisation of typewritten/hand-annotated archive card indexes, of the type found in most museums, archives and libraries. In addition to efficiently scanning, recognising and databasing the content of the cards, the original card images must be maintained as the ultimate source record, and a flexible database structure is required to allow taxonomists to reorganise and update the resulting online archive. Implementation mechanisms for each part of the overall system are described, and conversion performance for a demonstrator database of 27,578 Pyralid moth archive cards is reported. The system is currently being used to convert the full NHM archive of Lepidoptera totalling 290,886 cards.
this paper we present a detailed examination of the technical problems we have encountered in undertaking high-throughput analyses of alternative splicing over the last four years, and the specific solutions we have developed for these problems, in seeking to minimize both false positive and false negative errors
this paper we describe   ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff a new distributed,   robotic sensor methodology developed for applications including  characterization of environmental structure and phenomena.  NIMS exploits deployed infrastructure that provides the benefits  of precise motion, aerial suspension, and low energy sustainable  operations in complex environments. NIMS nodes may explore a  three-dimensional environment and enable the deployment of  sensor nodes at diverse locations and viewing perspectives.  NIMS characterization of phenomena in a three dimensional  space must now consider the selection of sensor sampling points  in both time and space. Thus, we introduce a new approach of  mobile node adaptive sampling with the objective of minimizing  error between the actual and reconstructed spatiotemporal  behavior of environmental variables while minimizing required  motion. In this approach, the NIMS node first explores as an  agent, gathering a statistical description of phenomena using a   ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffapproach. By iteratively   increasing sampling resolution, guided adaptively by the  measurement results themselves, this NIMS sampling enables  reconstruction of phenomena with a systematic method for  balancing accuracy with sampling resource cost in time and  motion. This adaptive sampling method is described analytically  and also tested with simulated environmental data. Experimental  evaluations of adaptive sampling algorithms have also been  completed. Specifically, NIMS experimental systems have been  developed for monitoring of spatiotemporal variation of  atmospheric climate phenomena. A NIMS system has been  deployed at a field biology station to map phenomena in a 50m  width and 50m span transect in a forest environme...
this paper) is scripting. Here, the user provides a simple ASCII file containing commands that steer the visualization. Typically, the commands are held in plain English to make using the underlying scripting language easier. Typical examples for scripting-driven AV systems include JAWAA (Akingbade et al., 2003), JSamba (Stasko, 1998), JHAV E (Naps et al., 2000) and Animal (Roling and Freisleben, 2002)
In speaker identification, we match a given (unkown) speaker to the set of known speakers in a database. The database is constructed from the speech samples of each known speaker. Feature vectors are extracted from the samples by short-term spectral analysis, and processed further by vector quantization for locating the clusters in the feature space. We study the role of the vector quantization in the speaker identification system. We compare the performance of different clustering algorithms, and the influence of the codebook size. We want to find out, which method provides the best clustering result, and whether the difference in quality contribute to improvement in recognition accuracy of the system.
We present here a framework for developing a generic talking head capable of reproducing the anatomy and the facial  deformations induced by speech movements with a set of a few parameters. We will show that the speaker-specific articulatory movements can be straightforward encoded into the normalized MPEG-4 Facial Animation Parameters and Facial Definition  Parameters.
Multiple representation occurs when information about the same geographic entity is represented electronically more than once. This occurs frequently in practice, and it invariably results in the occurrence of inconsistencies among the different representations. We propose to resolve this situation by introducing a multiple representation management system (MRMS), the schema of which includes rules that specify how to identify representations of the same entity, rules that specify consistency requirements, and rules used to restore consistency when necessary. In this paper, we demonstrate by means of a prototype and a realworld case study that it is possible to implement a multiple representation schema language on top of an objectrelational database management system. Specifically, it is demonstrated how it is possible to map the constructs of the language used for specifying the multiple representation schema to functionality available in Oracle. Though some limitations exist, Oracle has proven to be a suitable platform for implementing an MRMS.
In this paper, we present a compressed pattern matching method for searching user queried words in the CCITT Group 4 compressed document images, without decompressing. The feature pixels composed of black changing elements and white changing elements are extracted directly from the CCITT Group 4 compressed document images. The connected components are labeled based on a line-by-line strategy according to the relative positions between the changing elements of the current coding line and the changing elements of the reference line. Word boxes are bounded by merging the connected components. A two-stage matching strategy is constructed to measure the dissimilarity between the template image of the user&apos;s query word and the words extracted from document images. Experimental results confirmed the validity of the proposed approach.
The components of a key frame selection algorithm for a paper-based multimedia browsing interface called Video Paper are described. Analysis of video image frames is combined with the results of processing the closed caption to select key frames that are printed on a paper document together with the closed caption. Bar codes positioned near the key frames allow a user to play the video from the corresponding times. This paper describes several component techniques that are being investigated for key frame selection in the Video Paper system, including face detection and text recognition. The Video Paper system implementation is also discussed.
This paper proposes a method of using ontology hierarchy in  automatic topic identification. The fundamental idea behind this work is to  exploit an ontology hierarchical structure in order to find a topic of a text. The  keywords which are extracted from a given text will be mapped onto their  corresponding concepts in the ontology. By optimizing the corresponding  concepts, we will pick a single node among the concepts nodes which we  believe is the topic of the target text. However, a limited vocabulary problem  is encountered while mapping the keywords onto their corresponding  concepts. This situation forces us to extend the ontology by enriching each of  its concepts with new concepts using the external linguistics knowledge-base  (WordNet). Our intuition of a high number keywords mapped onto the  ontology concepts is that our topic identification technique can perform at its  best.
This module provides the information about the CAFCR course: &quot;Multi-Objective Embedded Systems Design, based on CAFCR&quot;. Distribution  This article or presentation is written as part of the Gaud project. The Gaud project philosophy is to improve by obtaining frequent feedback. Frequent feedback is pursued by an open creation process. This document is published as intermediate or nearly mature version to get feedback. Further distribution is allowed as long as the document remains complete and unchanged. All Gaud documents are available at:  http://www.extra.research.philips.com/natlab/sysarch/ version: 0 status: draft 5th July 2004 Contents 1 Multi-Objective Embedded Systems design, based on CAFCR 1  1.1 
Annotating a video-database requires an intensive human effort that is time consuming and error prone. However this task is mandatory to bridge the gap between low-level video features and the semantic content. We propose a partition sampling active learning method to minimize human effort in labeling. Formally, active learning is a process where new unlabeled samples are iteratively selected and presented to teachers. The major problem is then to find the best selection function that maximizes the knowledge gain acquired from new samples. In contrast with existing active learning approaches, we focus on the selection of multiple samples. We propose to select samples such that their contribution to the knowledge gain is complementary and optimal. Hence, at each iteration we ensure to maximize the knowledge gain. Our method offers many advantages; among them the possibility to share the annotation effort among several teachers.
A regional transportation system and the movement of large traffic volumes through it, are characteristic of stochastic systems. The standard traffic management or transportation planning approach uses a slice in time view of the system. Static, mean values of system variables are used for the basis of incident-caused, congestion management decisions. By reason of the highly variable nature of transportation systems, discrete event simulation is used in the planning process. The simulation model is highly dependent on the spatial accuracy of real world coordinates of nodes and the lengths of the roadway network links. Link travel times, queue spill back and turn lane queue size are directly related to the magnitude of incident-caused congestion, and the roadway system&apos;s ability to recover from it. The incorporation of accurate Geographic Information System (GIS) data with a powerful transportation simulation software package and properly designed data collection and analysis techniques are invaluable in support of transportation incident management decisions.
Nowadays, XML is becoming the standard for electronic information representation and exchange in our lives. Access to information presented in hyperlinked XML documents and other formats have always been in demand by users. In this paper, we describe the architecture, implementation, and evaluation of the P-RANK system built to address the requirement for efficient ranked keyword search over hyperlinked XML documents. Our contributions include presenting a new efficient keyword search system using a genuine data structure called the P-tree, a novel ranking method based on dimension rank voting, and a fast rank sorting method using the EIN-ring.
In this paper we present tractable algorithms for learning a logical model of actions&apos; effects and preconditions in deterministic partially observable domains. These algorithms update a representation of the set of possible action models after every observation and action execution. We show that when actions are known to have no conditional effects, then the set of possible action models can be represented compactly indefinitely. We also show that certain desirable properties hold for actions that have conditional effects, and that sometimes those can be learned efficiently as well. Our approach takes time and space that are polynomial in the number of domain features, and it is the first exact solution that is tractable for a wide class of problems. It does so by representing the set of possible action models using propositional logic, while avoiding general-purpose logical inference. Learning in partially observable domains is difficult and intractable in general, but our results show that it can be solved exactly in large domains in which one can assume some structure for actions&apos; effects and preconditions. These results are relevant for more general settings, such as learning HMMs, reinforcement learning, and learning in partially observable stochastic domains.
We show that temporal logic and combinations of temporal logics  and modal logics of knowledge can be effectively represented in artificial  neural networks. We present a Translation Algorithm from  temporal rules to neural networks, and show that the networks  compute a fixed-point semantics of the rules. We also apply the  translation to the muddy children puzzle, which has been used as a  testbed for distributed multi-agent systems. We provide a complete  solution to the puzzle with the use of simple neural networks, capable  of reasoning about time and of knowledge acquisition through  inductive learning.
F1 and F2 frequencies of the vowels /i/, /a/ and /u/ were measured in speech directed to an infant and to adults. The vowels were taken from content words as well as function words. The results showed that the vowel triangles in speech to the infant were expanded compared to those in speech to adults, but only in the content words. For function words, the opposite pattern was found: adults produced more expanded vowels in adult-directed speech than in infant-directed speech.
So far, boosting has been used to improve the quality of moderately accurate learning algorithms,  by weighting and combining many of their weak hypotheses into a final classifier with theoretically  high accuracy. In a recent work (Sebban, Nock and Lallich, 2001), we have attempted to adapt  boosting properties to data reduction techniques. In this particular context, the objective was not  only to improve the success rate, but also to reduce the time and space complexities due to the  storage requirements of some costly learning algorithms, such as nearest-neighbor classifiers. In  that framework, each weak hypothesis, which is usually built and weighted from the learning set,  is replaced by a single learning instance. The weight given by boosting defines in that case the  relevance of the instance, and a statistical test allows one to decide whether it can be discarded  without damaging further classification tasks. In Sebban, Nock and Lallich (2001), we addressed  problems with two classes. It is the aim of the present paper to relax the class constraint, and  extend our contribution to multiclass problems. Beyond data reduction, experimental results are  also provided on twenty-three datasets, showing the benefits that our boosting-derived weighting  rule brings to weighted nearest neighbor classifiers.
that are stored in a fragile state, on a volatile medium. They require conservation and restoration. Automated tools for video restoration will be crucial in preserving our cultural heritage, since manual image restoration is a tedious and time-consuming process.
The need for new approaches to the consistent simulation of related phenomena at multiple levels of resolution is great. While many fields of application would benefit from a complete and approachable solution to this problem, such solutions have proven extremely difficult. We present a multi-resolution simulation methodology which uses numerical optimization as a tool for maintaining external consistency between models of the same phenomena operating at different levels of temporal and/or spatial resolution. Our approach follows from previous work in the disparate fields of inverse modeling and spacetime constraintbased animation. As a case study, our methodology is applied to two environmental models of forest canopy processes that make overlapping predictions under unique sets of operating assumptions, and which execute at different temporal resolutions. Experimental results are presented and future directions are addressed.
We present an iterative algorithm for robustly estimating the egomotion and refining and updating a coarse, noisy and partial depth map using a depth based parallax model and brightness derivatives extracted from an image pair. Given a coarse, noisy and partial depth map acquired by a range-finder or obtained from a Digital Elevation Map (DEM), we first estimate the ego-motion by combining a global ego-motion constraint and a local brightness constancy constraint. Using the estimated camera motion and the available depth map estimate, motion of the 3D points is compensated. We utilize the fact that the resulting surface parallax field is an epipolar field and knowing its direction from the previous motion estimates, estimate its magnitude and use it to refine the depth map estimate. Instead of assuming a smooth parallax field or locally smooth depth models, we locally model the parallax magnitude using the depth map, formulate the problem as a generalized eigen-value analysis and obtain better results. In addition, confidence measures for depth estimates are provided which can be used to remove regions with potentially incorrect (and outliers in) depth estimates for robustly estimating ego-motion in the next iteration. Results on both synthetic and real examples are presented.
ion. In 5 patients, additional pancreatic tumors or distant metastases only suspected during PET scanning were confirmed. Image fusion improved the sensitivity of malignancy detection from 76.6% (CT) and 84.4% (PET) to 89.1% (image fusion). Compared with CT alone, image fusion increased the sensitivity of detecting tissue infiltration to 68.2%, but at the cost of decreased specificity. Conclusion: The most important supplementary finding supplied by image fusion is a more precise correlation with focal tracer hot spots in PET. Image fusion improved the sensitivity of differentiating between benign and malignant pancreatic lesions with no significant change in specificity. All image modalities failed to stage lymph node involvement.  Key Words: PET; CT, spiral; image manipulation or reconstruction; pancreas; computer applications, detection  J Nucl Med 2004; 45:1279--1286  With  an incidence rate of 10 cases per 100,000 people per year, cancer of the pancreas is the third most common ma
Arabic rule in Middle Europe)     contrastive vowel length in these languages. Czech vowel length has been extensively studied since the 19th century.  However, no generalisation of any kind could be uncovered. Diachronically, it does not relate to either Indo-European  or Common Slavic vowel length, nor does it show any kinship with Baltic tones and East/ South Slavic accent.  Synchronically, closed syllable shortening (krva vs. krav, kravka &quot;cow NOMsg, GENpl, dim&quot;) appears to coexist with  closed syllable lengthening (nff vs. noe, nffky &quot;knife NOMsg, GENsg, scissors&quot;). In sum, any attempt to propose a  regularity underlying this system seems desperate. Vowel length in Czech is therefore reputed to be anarchic and  unpredictable. This situation is mirrored in grammars by pages of amorphous lists of grammatical categories that exhibit  length or shortness.   Czech vowel length is driven by a simple mechanism that is known from other languages: templates. That is, a  certain amount of vocalic space is associated to a given morphological and/ or semantic category. If concatenation of  underlying long and short vowels produces more morae than the specific category allows for, shortening is observed. If  it produces less vocalic weight than the category at stake demands, lengthening ensues. This kind of templatic structure  is a typical feature of Afro-Asiatic languages, and I believe that the templatic regularities I present have not been  discovered before because nobody has ever looked at the relevant data through the prism of templates: these are  commonly held to be a typological pecularity of Afro-Asiatic, absent from Indo-European.   In order to illustrate the preceding claim, only a few of the instances of templatic activity that I have identified may  be quoted in t...
below the skeleton   b. UP = processes driven by syllable structure. Consequences of syntagmatic  relations between syllabic constituents (e.g. lenition).  ==&gt; trigger above the skeleton    2  c. DOWN           x x   | |   ff   ff   ff ff   ff      UP   R   |   N Coda   | |   x x   | |   V   ff   ff   ff        (4) Vulgar Latin [VL]: consonification of short (non-low) vowel in hiatus      a. {i, e} -&gt; j / __ V b. {u, o} -&gt; w / __ V    fiilia &gt; filja fille vidua &gt; wedwa veuve  viinea &gt; winja vigne coagulaare &gt; kwaglare cailler  (5) a. Lat. filia = 3 syll.,VL. filja = 2 syll.   b. Cw/j clusters   c.  no original Cj/w preserved: Cj = palatalizations     &gt; mod. French [j], [z], [s], [ff], [ff], [], ([L])    (+j metathesis / fusion with the preceding vowel ratjoone &gt; [ffffz]   (6) evolution of Cj:   a. classical view: all processes depend on segmental characteristics of C   b. our claim:    - there is just one (fundamentally) syllabic process    - segmental properties are secondary and never the cause 
The focus of this paper is on developing and evaluating a practical methodology for determining if and when  different types of traffic can be safely multiplexed within the same service class. The use of class rather than individual  service guarantees offers many advantages in terms of scalability, but raises the concern that not all users within a class  see the same performance. Understanding when and why a user will experience performance that differs significantly  from that of other users in its class is, therefore, of importance. Our approach relies on an analytical model developed  under a number of simplifying assumptions, which we test using several real traffic traces corresponding to different  types of users. This testing is carried out primarily by means of simulation, to allow a comprehensive coverage of  different configurations. Our findings establish that although the simplistic model does not accurately predict the  absolute performance that individual users experience, it is quite successful and robust when it comes to identifying  situations that can give rise to substantial performance deviations within a service class. As a result, it provides a  simple and practical tool for rapidly characterizing real traffic profiles that can be safely multiplexed.
Over the last decade, importance sampling has been a popular technique for the efficient estimation of rare event probabilities. This paper presents an approach for applying balanced likelihood ratio importance sampling to the problem of quantifying the probability that the content of the second buffer in a two node tandem Jackson network reaches some high level before it becomes empty. Heuristic importance sampling distributions are derived that can be used to estimate this overflow probability in cases where the first buffer capacity is finite or infinite. The proposed importance sampling distributions differ from previous balanced likelihood ratio methods in that they are specified as functions of the contents of the buffers. Empirical results indicate that the relative errors of these importance sampling estimators is bounded independent of the buffer size when the second server is the bottleneck and is bounded linearly in the buffer size otherwise.
Agent-based Modeling and Simulation (ABMS) is a relatively new development that has found extensive use in areas such as social sciences, economics, biology, ecology etc. Can ABMS be effectively used in finding answers to complex construction systems? The focus of this paper is to provide some answers to this question. Initial experimentation is conducted to understand the advantages of using ABMS either in isolation or in combination with traditional simulation methodologies. The paper provides a summary of this experimentation, conclusions and sets the agenda for future research in this area.
this paper, we want to argue that image pro-   This material is based upon work supported by the U. S. Department of Defense and by the National Science Foundation under Grant No. 9734102. Additional support was provided by Sun Microsystems
We introduce a novel approach to the cerebral white matter  connectivity mapping from diffusion tensor MRI. DT-MRI is the  unique non-invasive technique capable of probing and quantifying the  anisotropic diffusion of water molecules in biological tissues. We address  the problem of consistent neural fibers reconstruction in areas of complex  diffusion profiles with potentially multiple fibers orientations. Our  method relies on a global modelization of the acquired MRI volume as a  Riemannian manifold M and proceeds in 4 majors steps: First, we establish  the link between Brownian motion and diffusion MRI by using the  Laplace-Beltrami operator on M . We then expose how the sole knowledge  of the diffusion properties of water molecules on M is suffcient to  infer its geometry. There exists a direct mapping between the diffusion  tensor and the metric of M . Next, having access to that metric, we propose  a novel level set formulation scheme to approximate the distance  function related to a radial Brownian motion on M . Finally, a rigorous  numerical scheme using the exponential map is derived to estimate the  geodesics of M , seen as the diffusion paths of water molecules. Numerical  experimentations conducted on synthetic and real diffusion MRI datasets  illustrate the potentialities of this global approach.
We show how a tableaux algorithm for   that include range and domain axioms, prove that the extended algorithm is still a decision   concepts w.r.t. such a role box, and show how support for range and domian axioms can be exploited in order to add a new form of absorption optimisation called role absorption. We illustrate the effectiveness of the optimised algorithm by analysing the perfomance of our FaCT++  implementation when classifying terminologies derived from realistic ontologies. 1 
Accuracy of oversampled analog-to-digital (A/D) conversion, the dependence of accuracy on the sampling interval and on the bit rate are characteristics fundamental to A/D conversion but not completely understood. These characteristics are studied in this paper for oversampled A/D conversion of band-limited signals in    ( ). We show that the digital sequence obtained in the process of oversampled A/D conversion describes the corresponding analog signal with an error which tends to zero as    in energy, provided that the quantization threshold crossings of the signal constitute a sequence of stable sampling in the respective space of band-limited functions. Further, we show that the sequence of quantized samples can be represented in a manner which requires only a logarithmic increase in the bit rate with the sampling frequency, = ( log ), and hence that the error of oversampled A/D conversion actually exhibits an exponential decay in the bit rate as the sampling interval tends to zero.
Temporal databases assume a single line of time  evolution. In other words, they support timeevolving  data. However there are applications  which require the support of temporal data with  branched time evolution. With new branches created  as time proceeds, branched and temporal  data tends to increase in size rapidly, making the  need for efficient indexing crucial. We propose a  new (paginated) access method for branched and  temporal data: the BT-tree. The BT-tree is both  storage efficient and access efficient. Wehaveimplemented  the BT-tree and performance results  confirm these properties.
There are high expectations in all sectors of society for immediate access to biological knowledge of all kinds. To fully exploit and manage the value of biological resources, society must have the intellectual tools to store, retrieve, collate, analyze, and synthesize organism-level and ecological scale information. However, it currently is difficult to discover, access, and use biodiversity data because of the long history of &quot;bottom-up&quot; evolution of scientific biodiversity information, the mismatch between the distribution of biodiversity itself and the distribution of the data about it, and, most importantly, the inherent complexity of biodiversity and ecological data. This stems from, among many factors, numerous data types, the nonexistence of a common underlying (binary) language, and the multiple perceptions of different researchers/data recorders across spatial or temporal distance or both. The challenge presented to the computer science and information technology community by the biodiversity and ecological information domain is worthy of all the time and talent that can be brought to bear, because the continued existence  of the species Homo sapiens depends upon  gaining an understanding of this spaceship Earth  and our fellow passengers upon it.
Syntax Tree    *  Figure 2: Architecture diagram  UML repository The repository contains the model, represented at the metamodel level (i.e. a class is represented by an object, instance of the M2 concept named Class);  Bridge The OCL interpreter itself should not know anything of UML, but rather manipulate it through a bridge pattern. This bridge maps manipulated MOF-compliant concepts to OCL types and properties.
Proper education of a modeling and simulation professional meeting the extensive criteria imposed by the community poses significant challenges. In this paper, we explore the formation of a university-based education in modeling and simulation to meet the challenges. We examine the factors affecting the composition of a modeling and simulation course. Based on the anticipated consequences, we propose potential solutions.
The problem of evaluating machine translation (MT) systems is more challenging than it  may first appear, as diverse translations can often be considered equally correct. The task is  even more difficult when practical circumstances require that evaluation be done automatically  over short texts, for instance, during incremental system development and error analysis. While several
This paper proposes a novel technique for building layer animation models of real articulated objects  from 3D surface measurement data. Objects are scanned using a hand-held 3D sensor to acquire 3D  surface measurements. A novel geometric fusion algorithm is presented which enables reconstruction  of a single surface model from the captured data. This algorithm overcomes the limitations of previous  approaches which cannot be used for hand-held sensor data as they assume that measurements are on a  structured planar grid. The geometric fusion introduces the normal-volume representation of a triangle  to convert individual triangles to a volumetric implicit surface.
This paper develops statistical algorithms and performance limits for resolving sinusoids with nearby frequencies, in the presence of noise. We address the problem of distinguishing whether the received signal is a single-frequency sinusoid or a double-frequency sinusoid, with possibly unequal, and unknown, amplitudes and phases. We derive a locally optimal detection strategy that can be applied in a stand-alone fashion or as a refinement step for existing spectral estimation methods, to yield improved performance. We further derive explicit relationships between the minimum detectable difference between the frequencies of two tones, for any particular false alarm and detection rate, and at a given SNR. ff This work was supported in part by NSF CAREER Award CCR-9984246, and AFOSR grant F49620-03-1-0387.
this article, Professor Ajay Agrawal at Queen&apos;s University&apos;s School of Business, can be reached at aagrawal@business.queensu.ca. This paper represents the views of the author and does not necessarily reflect the opinions of Statistics Canada
Given a Morse function f over a 2-manifold with or without boundary, the Reeb graph is obtained by contracting the connected components of the level sets to points. We prove tight upper and lower bounds on the number of loops in the Reeb graph that depend on the genus, the number of boundary components, and whether or not the 2-manifold is orientable. We also give an algorithm that constructs the Reeb graph in time O(n log n), where n is the number of edges in the triangulation used to represent the 2-manifold and the Morse function.
This paper aims at finding fundamental design principles for hierarchical web caching. An analytical modeling technique is developed to characterize an uncooperative twolevel hierarchical caching system where the least recently used (LRU) algorithm is locally run at each cache. With this modeling technique, we are able to identify a characteristic time for each cache, which plays a fundamental role in understanding the caching processes. In particular, a cache can be viewed roughly as a low-pass filter with its cutoff frequency equal to the inverse of the characteristic time. Documents with access frequencies lower than this cutoff frequency have good chances to pass through the cache without cache hits. This viewpoint enables us to take any branch of the cache tree as a tandem of low-pass filters at different cutoff frequencies, which further results in the finding of two fundamental design principles. Finally, to demonstrate how to use the principles to guide the caching algorithm design, we propose a cooperative hierarchical web caching architecture based on these principles. Both model-based and real trace simulation studies show that the proposed cooperative architecture results in more than 50% memory saving and substantial central processing unit (CPU) power saving for the management and update of cache entries compared with the traditional uncooperative hierarchical caching architecture.
this paper require very little additional hardware and no extra bus lines, achieving, nonetheless, a significant reduction of the activity level on the bus
In this paper, we present a maximum entropy (maxent) approach to the fusion  of experts opinions, or classifiers outputs, problem. The maxent approach is quite  versatile and allows us to express in a clear, rigorous, way the a priori knowledge  that is available on the problem. For instance, our knowledge about the reliability  of the experts and the correlations between these experts can be easily integrated:  Each piece of knowledge is expressed in the form of a linear constraint.
Since the late 1970s dramatic economic changes have taken place in the agricultural sector in the highlands of Guatemala. The introduction of new export crops, such as snow peas, broccoli, and miniature vegetables, has led to yet another agro-export boom. Unlike earlier booms, however, this one has included all but the smallest farmers. The high rate of smallholder participation in the boom, and the initial high profitability of nontraditional exports (NTXs), fueled initial optimism that NTX production could increase smallholders&apos; ability to accumulate land and so decrease the highly skewed distribution of land in Guatemala, a country with one of the most unequal landholding patterns in all of Latin America. The picture that emerges from the analysis in this paper raises serious questions about the sustainability and equity effects of NTX crop adoption among smallholders in the long run. Two main findings illustrate the problems besetting NTX crop production. First, the land accumulation rates of adopters have dropped dramatically in the 1990s. NTX crop adopters accumulated close to three times more land than non-adopters in the 1980s. Although adopters are still accumulating more land than non-adopters in the 1990s, the gap between the two groups has narrowed substantially. Second, smaller adopters are no longer accumulating land at higher rates than their larger counterparts. In the 1980s the landholdings of smaller adopters grew significantly faster than those of the larger adopters, but this trend reversed itself in the iii 1990s. The advantages smallholders initially had in accumulating land may have been lost as a result of deteriorating agronomic conditions and volatile export markets. However, given adequate policy support, smallholders could indeed improve thei...
This paper will describe the REDTOP-2 tool  and its capabilities. Sample results obtained from exercising the tool for a number of  different existing engine designs will be presented. Results from a multi-variable sensitivity  study on a LOX/LH2 fuel-rich, single preburner staged-combustion engine will be  highlighted. Two sample applications involving vehicle designs will be discussed. The first  involves probabilistic/uncertainty analysis for an all-rocket vehicle design and the second the  rocket main propulsion system analysis of an airbreathing, two-stage RLV concept with first  stage tail-rockets and all-rocket second stage propulsion. Finally, future directions in the  development of REDTOP-2 will be discussed
Introduction  Autism is a neurodevelopmental disorder with abnormal corpus callosum (CC) size [1]. Most previous studies used the area of predefined Witelson partition [5] as a morphometric measure but other shape metrics have not been considered. We present a new computational technique for curvature estimation via piecewise quintic splines and use it in both CC nonlinear dynamic time warping algorithm [4] and detecting the regions of curvature difference.  Figure 1: Left: level set segmentation showing  partial volume effect. Right: spline smoothing. A  similar approach has been taken in [6].  Methods  A group of 2D mid sagittal cross section images of the corpus callosum was taken from males of similar age, 15 autistic, and 12 normal controls. The level set method was used to extract the boundary ff of the corpus callosum automatically by solving  ffff  fft  + F|ffff| = 0  where F is the given boundary propagation velocity [2]. Then the pixelated CC contour was reconstructed into a rough 
We describe soft versions of the global cardinality constraint  and the regular constraint, with effcient filtering algorithms maintaining  domain consistency. For both constraints, the softening is achieved by  augmenting the underlying graph. The softened constraints can be used  to extend the meta-constraint framework for over-constrained problems  proposed by Petit, Regin and Bessiere.
Graphs are a popular data structure, and graph-manipulation  programs are common. Graph manipulations can be cleanly, compactly, and  explicitly described using graph-rewriting notation. However, when a  software developer is persuaded to try graph rewriting, several problems  commonly arise. Primarily, it is difficult for a newcomer to develop a feel for  how computations are expressed via graph rewriting. Also, graph-rewriting  is not convenient for solving all aspects of a problem: better mechanisms are  needed for interfacing graph rewriting with other styles of computation.
Despite the growing interest for component-based systems, few works tackle the question of the trust we can bring into a component.
Differentiated Services (DiffServ) is scalable for deployment in today&apos;s Internet, and Multiprotocol Label Switching (MPLS) provides fast packet switching and the opportunity for traffic engineering. Thus, the combination of DiffServ and MPLS presents a very attractive strategy to backbone network providers. This paper attempts to explain the concepts of DiffServ + MPLS and illustrate its effectiveness by performing a simulation using Network Simulator (ns-2). The results show the fast rerouting feature of MPLS and how it alleviates the problem of link failures in DiffServ networks.
Several studies have shown that the performance advantages of adaptive  routing over deterministic routing are reduced when the traffic contains  strong degree of communication locality. This paper proposes a new  analytical model of an adaptive routing algorithm proposed by Duato in  [Dua94]. The main feature of this algorithm is the use of a time-out  selection function for assigning virtual channels. This has the advantage  of reducing virtual channels multiplexing to improve the network  performance, especially, in the presence of communication locality.
In this paper, we provide a sub-channel partitioning based unequal error protection (UEP) scheme for a space-time block coded orthogonal frequency division multiplexing (STBCOFDM) system. In such a scheme, video data is partitioned into high-priority (HP) and low-priority (LP) layers according to the importance of the data. At the receiver side, OFDM subchannels are partitioned into high-quality (HQ) and low-quality (LQ) groups according to the estimated channel qualities. Based on the feedback of sub-channel partitioning results, the transmitter assigns HP and LP video data to the corresponding HQ and LQ sub-channels. Through theoretical analysis, we show there is indeed a significant BER difference between the HQ and LQ sub-channels, which can be exploited by UEP. Based on the analysis, we provide a criterion for determining the appropriate transmission power. Through computer simulations, we show that the proposed scheme offers significant performance gain compared to conventional methods. We also demonstrate that the scheme is the least sensitive to channel estimation errors among all compared schemes, and is hardly influenced by the Doppler spread. The feedback overhead can also be reduced with almost no performance penalty by bundling several neighboring sub-channels together and assigning them to the same group.
Supporting fast restoration for general mesh topologies with minimal network over build is a technically challenging problem. Traditionally, ring based SONET networks have offered 50ms restoration at the cost of requiring 100% over-build. Recently, fast (local) reroute has gained momentum in the context of MPLS networks. Fast reroute, when combined with preprovisioning of protection capacities and bypass tunnels, comes close to providing fast restoration for mesh networks. Preprovisioning has the additional advantage of greatly simplifying network routing and signaling. Thus even for protected connections, online routing can now be oblivious to the offered protection, and may only involve single shortest path computations.
In this paper, we address the complex task of initializing an on-line simulation to a current system state collected from an operating physical system. The paper begins by discussing the complications that arise when the system model employed by the controller and the planner are not the same. The benefits of using the same model for control and planning are then outlined. The paper then discusses a new simulation paradigm that models controller interactions and provides a single model that is capable of supporting planning and control functions. Next, issues arising from performing a distributed simulation of the distributed control architecture that is being employed to manage the system are addressed. The definition of the state for the distributed system is then discussed and the collection of the real-time state information from the elements of this distributed system is outlined. Finally, the procedure for initializing the distributed on-line simulation from the collected real-time state information is given.
Although the Ethernet promises high probability packet delivery between all pairs of hosts, defects in the installation can sometimes prevent some pairs of hosts from communicating very well. This paper describes a method that was developed and used at Carnegie Mellon University during 1984--1985 to monitor the connectivity between all pairs of hosts on the Ethernet. The method is based on sending test packets from a central station through various cyclic routes and relating the results to the likelihood of various defects via a system probability model. The method proved to be quite effective in practice and greatly assisted our support staff in maintaining our Ethernet.
Distributed weighted fair scheduling schemes for QoS support in wireless networks have not yet become standard. In this paper we propose an Admission Control and Dynamic Bandwidth Management scheme that provides fairness in the absence of distributed link level weighted fair scheduling. In case weighted fair scheduling becomes available, our system assists it by supplying the scheduler with weights and adjusting them dynamically as network and traffic characteristics vary. To obtain these weights, we convert the bandwidth requirement of the application into a channel time requirement. Our Bandwidth Manager then allots each flow a share of the channel time depending on its requirement relative to the requirements of other flows in the network. It uses a max-min fairness algorithm with minimum guarantees. The flow controls its packet transmission rate so it only occupies the channel for the fraction of time allotted to it by the Bandwidth Manager. As available bandwidth in the network and the traffic characteristics of various flows change, the channel time proportion allotted also dynamically varies. Our experiments show that, at the cost of a very low overhead, there is a high probability that every flow in the network will receive at least its minimum  requested share of the network bandwidth.
This paper describes the emergence of improved traditional planting pits (za) in Burkina  Faso in the early 1980s as well as their advantages, disadvantages and impact. The za emerged  in a context of recurrent droughts and frequent harvest failures, which triggered farmers to start  improving this local practice. Despair triggered experimentation and innovation by farmers.  These processes were supported and complemented by external intervention. Between 1985 and  2000 substantial public investment has taken place in soil and water conservation (SWC). The  socio-economic and environmental situation on the northern part of the Central Plateau is still  precarious for many farming families, but the predicted environmental collapse has not occurred  and in many villages indications can be found of both environmental recovery and poverty  reduction.      Keywords: soil fertility, soil conservation, water conservation  iii  TABLE OF CONTENTS     1. The Context In Which Za Emerged In The Yatenga Region 1  2. Development And Dissemination Of The Za Technology 3  3. Impact On Farm Households And On Farmland 11  4. Final Remarks 24  References 26      TRADITIONAL SOIL AND WATER CONSERVATION PRACTICE  IN BURKINA FASO     Daniel Kabor    and Chris Reij          1. THE CONTEXT IN WHICH ZA EMERGED IN THE YATENGA REGION   In the 1970s the densely populated northern part of the Central Plateau faced an  acute environmental crisis. Recurrent droughts led to frequent harvest failure. Between  1975 and 1985 this region witnessed substantial out-migration to less densely populated  regions with better soils and higher rainfall   . Women had to walk longer distances to  collect firewood. Vegetation was destroyed not only for firewood, but even more to  expand cultivated land. Groundwater ...
Intelligent Agent and on its goal-oriented point of view.
STARS Program Manager TASK: PV03 CDRL: A025 14 June 1996 Data Reference: STARS-VC-A025/001/00  Version 2.0 (Signatures on File)  Principal Author(s): Approvals:  Mark Simos, Organon Motives, Inc. Dick Creps, Lockheed Martin Tactical Defense Systems  Date Teri F. Payton, Lockheed Martin Tactical Defense Systems  Carol Klingler, Lockeed Martin Tactical Defense Systems Date Larry Levine, Organon Motives, Inc. Date Dean Allemang, Organon Motives, Inc. Date Public reporting burden for this collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington, VA 22202-4302, and to the Office of Management and Budget, Paperwork Reduction Project (0704-0188), Washington, DC 20503.
this paper, the case of 2  p is considered and alternative cases will be applied to power engineering estimation problems. The specific cases of parameter estimation and measurement noise are discussed
Fluid models of IP networks have been recently proposed as a way to break the scalability barrier of traditional discrete state-space models, both simulative (e.g., ns-2) and analytical (e.g., queues and Markov chains). Fluid models adopt...
this paper, we use the wrapper methods based on a nonlinear classification algorithm in order to extract the discriminative genes that diffcult to be extracted by conventional filter methods. RFE method based on nonlinear Support Vector Machines (SVMs) [2] is employed to this end because it is successfully applied to classification of gene expression data. We investigate the genes extracted by the RFE method based on SVMs with gaussian kernel function to indicate that it can extract discriminative genes which are not chosen by conventional filter methods
Per-flow traffic measurement is critical for usage accounting, traffic engineering, and anomaly detection. Previous methodologies are either based on random sampling (e.g., Cisco&apos;s NetFlow), which is inaccurate, or only account for the &quot;elephants&quot;. We introduce a novel technique for measuring perflow traffic approximately, for all flows regardless of their sizes, at very high-speed (say, OC768). The core of this technique is a novel data structure called Space Code Bloom Filter (SCBF). A SCBF is an approximate representation of a multiset; each element in this multiset...
There is general agreement that metamodeling will play a pivotal role in the realization of the MDA, but less consensus on what the precise role of metamodeling should be and what form it should take. In this paper we first analyze the underlying motivation for metamodeling within the context of the MDA and derive a concrete set of requirements that an MDA supporting infrastructure should satisfy. We then present a number of concepts, which we believe are best suited to providing technical solutions to the identified requirements. In particular, we discuss why the traditional &quot;language definition&quot; view is insufficient for an optimal MDA foundation.
OntoLearn is a system for word sense disambiguation, used to automatically  enrich WordNet with domain concepts and to disambiguate WordNet glosses.
Peer-to-peer networks have attracted a significant amount of interest as a popular and successful alternative to traditional client-server networks for resource sharing and content distribution. However, the existence of high degrees of free riding may be an important threat against P2P networks. In this paper    , we propose a distributed and measurement-based method to reduce the degree of free riding in P2P networks. We primarily focus on developing schemes to locate free riders and on determining policies that can be used to take actions against them. We propose a model in which each peer monitors its neighboring peers, makes decisions if they exhibit any kind of free-riding, and takes appropriate actions if required. We specify three types of free riding and their symptoms observable from the activities of the neighboring peers. We employ simple formulas to determine if a peer exhibits any kind of free riding. The counter actions to be applied to the free riders are defined. We combine the mechanisms proposed to detect free riders and to take appropriate actions in an ECA rule and a state diagram.
In position-based routing protocols, each node periodically transmits a short hello message (called beacon) to announce its presence and position. Receiving nodes list all known neighbor nodes with their position in the neighbor table and remove entries after they have failed to receive a beacon for a certain time from the corresponding node. Especially in highly dynamic networks, the information stored in the neighbor table is often out-dated and does not reflect the actual topology of the network anymore such that retransmissions and rerouting are required which consume bandwidth and increase latency. Despite a considerable number of proposed position-based protocols, almost no analysis has been performed on the impact of beacons and the out-dated and inaccurate neighbor tables. We show by analytical and simulation results that performance suffers especially in highly mobile ad-hoc networks and propose several mechanisms to improve the accuracy of neighborhood information. Extensive simulations show the effectiveness of the proposed schemes to improve the network performance.
on and one in the prone position. The prone images were registered to the  respective supine images by use of an intensity-based registration algorithm, once  using only the frame and once using only the head. The difference between the  transformations produced by these two registrations describes the movement of the  patient&apos;s head with respect to the frame.  RESULTS: The maximum frame-based registration error between the supine and prone  positions was 2.8 mm; it was more than 2 mm in two patients and more than 1.5 mm  in six patients. Anteroposterior translation is the dominant component of the difference  transformation for most patients. In general, the magnitude of the movement increased  with brain volume, which is an index of head weight.  CONCLUSION: To minimize frame-based registration error caused by a change in the  mechanical load on the frame, stereotactic procedures should be performed with the  patient in the identical position during imaging and intervention.  KEY WOR
It is known that a data network may not be stable at the connection level under some  unfair bandwidth allocation policies, even when the normal offered load condition is satisfied,  i.e., the average traffic load at each link is less than its capacity. In this paper, we show  that, under the normal offered load condition, a data network is stable when the bandwidth  of the network is allocated so as to maximize a class of general utility functions. Using the  microscopic model proposed by Kelly [9] for a TCP congestion control algorithm, we argue  that the bandwidth allocation in the network dominated by this algorithm can be modelled  as our bandwidth allocation model, and hence that the network is stable under the normal  offered load condition. This result may shed light on the stability issue of the Internet since  the majority of its data traffc is dominated by the TCP.
In this paper we experimentally compare the classification uncertainty  of the randomised Decision Tree (DT) ensemble technique and the Bayesian DT  technique with a restarting strategy on a synthetic dataset as well as on some  datasets commonly used in machine learning community. For quantitative  evaluation of classification uncertainty, we use an Uncertainty Envelope dealing  with the class posterior distribution and a given confidence probability. Counting  the classifier outcomes, this technique produces feasible evaluations of the  classification uncertainty. Using this technique in our experiments, we found that  the Bayesian DT technique is superior to the randomised DT ensemble technique.
This paper describes the present results of introducing Computer Supported Collaborative Learning in a Dutch secondary school. The inquiry focuses on three main questions: 1. Does CSCL effect student&apos;s metacognition and motivation? 2. Does the role of the teacher effect number and/or quality of the contributions in the database? 3. What opinions do both students and teachers have on CSCL? Method  The study is carried out in a secondary school in the Netherlands, The Raayland College in Venray. The Raayland is a school that includes all types of secondary education: Gymnasium, pre-university education (VWO), senior secondary education (HAVO), junior secondary education (MAVO) and preparatory vocational education (VBO). It is a school with 2.300 students and 158 teachers. The Raayland College is a so-called pioneering school. About 120 of 700 high schools in the Netherlands receive extra money from the Department of Education of the government to introduce computers in their curriculum. The aim of this initiative is that partner schools share their experiences with other schools, which do not receive extra funding. The Raayland College is present on the World Wide Web. Participating in this project fits well in this context of a computer supported collaborative learning environment. Collaborative Learning, with or without computers, is not a common experience of students at Raayland College. Six classes of the Raayland College have applied collaborative learning, supported by WebKnowledge Forum, in one or two courses. Each course comprises of six lessons. WebKnowledge Forum is a software program that has been developed by Dr. Marlene Scardamalia and Dr. Carl Bereiter of the Ontario Institute for Studies in Education at the University of Toronto, in succession to the Compu...
Data is produced and consumed everyday by information systems, and its inherent quality is a fundamental aspect to operational and support business activities. However, inadequate data quality often causes severe economic and social losses in the organizational context. The problem addressed in this paper is how to assure data quality, both syntactically and semantically, at information entity level. An information entity is a model representation of a real world business entity. To address this problem, we have taken an organizational engineering approach, consisting in using a business process-modeling pattern for describing, at a high level of abstraction, how to ensure and validate business object data. The pattern defines a conceptual data quality model with specific quality attributes. We use object-oriented concepts to take advantage of concepts such as inheritance and traceability. The concepts and notation we use are an extension to the Unified Modeling Language. A case study is detailed exemplifying the use of the proposed concepts.
While the tone of this paper is informal and tongue-incheek,  we believe we raise two important issues in robotics  and multi-modal interface research; namely, how crucial  integration of multiple modes of communication are for  adjustable autonomy, which in turn is crucial for having  dinner with R2D2. Furthermore, we discuss how our multimodal  interface to autonomous robots addresses these issues  by tracking goals, allowing for both natural and mechanical  modes of input, and how our robotic system adjusts itself to  ensure that goals are achieved, despite interruptions.
In this paper we are concerned with the principles underlying the utility of  modelling concepts, in particular in the context of architecture-modelling. Firstly, some  basic concepts are discussed, in particular the relation between information, language, and  modelling. Our primary area of application is the modelling of enterprise architectures and  information system architectures, where the selection of concepts used to model different  aspects very much depends on the specific concerns that need to be addressed. The approach  is illustrated by a brief review of the relevant aspects of two existing frameworks for modelling  of (software intensive) information systems and their architectures.
This paper contributes to this methodology by presenting an improvement over previous algorithms. Sections II and III give a short outline of previous Boltzmann annealing (BA) and fast Cauchy fast annealing (FA) algorithms. Section IV presents the new very fast algorithm. Section V enhances this algorithm with a re-annealing modification found to be extremely useful for multi-dimensional parameter-spaces. This method will be referred to here as very fast reannealing (VFR)
Manually querying search engines in order to accumulate a large body of factual information is a tedious, error-prone process of piecemeal search. Search engines retrieve and rank potentially relevant documents for human perusal, but do not extract facts, assess confidence, or fuse information from multiple documents. This paper introduces KNOWITALL, a system that aims to automate the tedious process of extracting large collections of facts from the web in an autonomous, domain-independent, and scalable manner.
High dimensional American options have no analytic solution and are diffcult to  price numerically. Progress has been made in using Monte Carlo simulation to give both lower and  upper bounds on the price. Building on an idea of Glasserman and Yu we investigate the utility  of martingale basis functions in regression based approximation methods. Regression methods are  known to give lower bounds easily, however upper bounds are usually computationally expensive.
orting life. Do not extinguish that element, do not corrupt it, learn that it is divine; and do not substitute wretched scholastic feuds for the voice of nature.&quot; ---Essay on Tolerance, Voltaire (1763).  We do not live very long. By the time a man has lived 20 years, he may have begun to learn some of what was done in previous centuries. By the time a man has lived 30 years, he may have begun to do something himself, which was not done before. Yet by the time a man has lived 40 years, he begins to doubt whether any of what was done before, including his own work, is of any real value!  If we lived much longer, say for 100 or 200 years in good health, some people would continue to learn and think and do original things all through their lifetimes. Those long-lived men and women would then go far beyond us in their thinking and knowledge. Indeed, the simple-minded ideas which we follow in our philosophy and science and religion today, might look meager and infantile to such superior bein
This paper addresses our experience in using and developing the VEG (Visual Event Grammars) toolkit for the formal specification, verification, design and implementation of graphical user interfaces
K-d trees have been widely studied, yet their complete advantages are often not realized due to ineffective search implementations and degrading performance in high dimensional spaces. We outline an effective search algorithm for k-d trees that combines an optimal depth-first branch and bound (DFBB) strategy with a unique method for path ordering and pruning. This technique was developed for improving nearest neighbor (NN) search, but has also proven effective for k-NN and approximate k-NN queries.
Software agents have been recognized as one of the main building blocks of the emerging infrastructure for the Semantic Web, but their relationship with more standard components, such as Web servers and clients, is still not clear. At the server side, a possible role for agents is to enhance the capabilities of servers using their intelligence to provide more complex services and behaviors. In this paper we explore the role of agents at the server side presenting an Open Service Architecture (OSA) which extends the centralized Internet Reasoning System (IRS-II) to a distributed scenario. The architecture uses a distributed facilitation protocol which integrates Web Services with agent communication languages. Finally we present an implementation which extends Tomcat with these features.
India&apos;s semi-arid tropical (SAT) region is characterized by seasonally concentrated rainfall, low agricultural productivity, degraded natural resources, and substantial human poverty. The green revolution that transformed agriculture elsewhere in India had little impact on rainfed agriculture in the SAT. In the 1980s and 1990s, agricultural scientists and planners aimed to promote rainfed agricultural development through watershed development. A watershed is an area from which all water drains to a common point, making it an attractive unit for technical efforts to manage water and soil resources for production and conservation. Watershed projects are complicated, however, by the fact that watershed boundaries rarely correspond to human-defined boundaries. Also, watershed projects often distribute costs and benefits unevenly, with costs incurred disproportionately upstream, typically among poorer residents, and benefits realized disproportionately downstream, where irrigation is concentrated and the wealthiest farmers own most of the land. Watershed projects take a wide variety of strategies, ranging from those that are more technocratic to those that pay more attention to the social organization of watersheds. By the mid-1990s annual expenditure on watershed development in India approached $500 million, but there was relatively little information available on the success of different project approaches. This study addresses three main research questions: 1) What projects are most successful in promoting the objectives of raising agricultural productivity, improving natural resource management and reducing poverty? 2) What approaches enable them to succeed? 3) What nonproject factors also contribute to achieving these objectives? The major hypotheses are that participat...
i  Controlled Fabrication System of Fabry-Perot Optical Fiber Sensors  Wei Huo  The Bradley Department of Electrical and Computer Engineering, Virginia Tech  (Abstract)  The use of optical fiber sensors is increasing widely in industry, civil, medicine, defense and research. Among different categories of these sensors is the Extrinsic Fabry-Perot interferometer (EFPI) sensor which is inherently simple and requires only modest amount of interface electronics. These advantages make it suitable for many practical applications. Investigating a cost-effective, reliable and repeatable method for optical fiber sensor fabrication is challenging work. In this thesis, a system for controlled fabrication of FabryPerot optical fiber sensors is developed and presented as the first attempt for the long-term goal of automated EFPI sensor fabrication. The sensor fabrication control system presented here implements a real-time control of a carbon dioxide (CO 2 ) laser as sensor bonding power, an optical fiber white light interferometric subsystem for real-time monitoring and measurement of the air gap separation in the Fabry-Perot sensor probe, and real-time control of a piezoelectric (PZT) motion subsystem for sensor alignment. The design of optoelectronic hardware and computer software is included. A large number of sensors are fabricated using this system and are tested under high temperature and high pressure. This system as a prototype system shows the potential in automated sensor fabrication. Acknowledgements ii  Acknowledgements First and foremost, I would like to thank Dr. Anbo Wang, my advisor and graduate committee chair, for his thoughtful guidance and constant encouragement and for giving me the opportunity to work at Photonics Lab as his graduate student. I am extremely gr...
Hydrogen Wishes, presented at MIT&apos;s Center for Advanced Visual Studies, explores the themes of wishes and peace. It dramatizes the intimacy and power of transforming one&apos;s breath and vocalized wishes into a floating sphere, a bubble charged with hydrogen. The floating bubble represents transitory anticipation as a wish is sent on its trajectory toward fulfillment. Light, heat sensors, microphones, projected imagery, hydrogen and ordinary soap bubbles come together in this exploration of human aspiration. As in our lives, many wishes escape, but many others are catalyzed by the heat of the candle and become ethereal. The fulfilled wishes then become living artifacts within projected photographs of Earth cities as seen from outer space.
With the rise of international bond markets in the 1990s, the role of sovereign credit ratings  has become increasingly important. In the aftermath of Asian Crises a series of empirical studies  on the effects of sovereign ratings appeared. The theoretical literature on the topic, however,  remains rather scarce. We propose a model of rating agencies that is an application of global  game theory in which heterogeneous investors act strategically. The model is consistent with  the main findings by the empirical literature. In particular, it is able to explain the independent  effect of sovereign ratings on the cost of debt and the failure of rating agencies to predict crises.
An efficient methodology for simulating paths of fractional stable motion is presented. The proposed approach is based on invariance principles for linear processes. A detailed analysis of the error terms involved is given and the performance of the method is assessed through an extensive simulation study.
In economics and game theory agents are assumed to follow  a model of perfect rationality. This model of rationality  assumes that the rational agent knows all and will take  the action that maximizes her utility. We can find evidence  in the psychology and economics literature as well as in our  own lives that shows human beings do not satisfy this definition  of rationality. Thus there are many who look to study  some notion of bounded rationality. Unfortunately, models  of bounded rationality suffer from the exact phenomena that  they attempt to explain. Specifically, models of bounded rationality  are bounded. Understanding the limits of various  rationality models will make clearer their contribution and  place in the overall picture of rationality.
The primary contribution of this paper is the introduction of a new method to reduce significantly the computation time necessary to solve the multidimensional assignment (MDA) problem. In the first part of the track oriented method clusters are formed to reduce the amount of computation time necessary for correlation. For each formed target tree a mean track is formed. The different mean tracks are used to determine independent components. Each independent component corresponds with a cluster. In the second part of the method the original MDA problem is decomposed in smaller, independent MDA problems, using a root track label for each target tree.
Energy-effciency and reliability are two major design constraints influencing next generation system designs. In this work, we focus on the interaction between power consumption and reliability considering the on-chip data caches. First,  we investigate the impact of two commonly used architecturallevel  leakage reduction approaches on the data reliability. Our results indicate that the leakage optimization techniques can have very different reliability behavior as compared to an original cache with no leakage optimizations. Next, we investigate on providing data reliability in an energy-effcient fashion in the presence of soft-errors. In contrast to current commercial caches that treat and protect all data using the same error detection/correction mechanism, we present an adaptive error coding scheme that treats dirty and clean data cache blocks differently. Furthermore, we present an early-write-back scheme that enhances the ability to use a less powerful error protection scheme for a longer time without sacrificing reliability. Experimental results show that proposed schemes, when used in conjunction, can reduce dynamic energy of error protection components in L1 data cache by 11% on average without impacting the performance or reliability.
We investigate the use of dominating-set neighbor elimination as an integral part of the distribution of route requests using the Ad hoc On-demand Distance Vector (AODV) protocol as an example of on-demand routing protocols. We use detailed simulations to show that simply applying dominant pruning (DP) to the distribution of route requests in AODV results in pruning too many route requests in the presence of mobility and cross-traffic. Accordingly, we introduce several heuristics to compensate the effects of DP and show that the resulting AODV with Dominating Set heuristics (AODV-DS) has comparable or better delivery ratio, network load, and packet latency than the conventional AODV. AODV-DS exhibits over 70% savings on RREQ traffic than conventional AODV, and in some situations, AODV-DS may have a lower control overhead using Hello packets than conventional AODV without Hellos.
this paper. On January 2, 1985, Zaman Akil sent the Academy of Sciences a short summary of a longer work. At his request, the Perpetual Secretary of the Academy, Prof. Paul Germain, sent the letter to several members of the Academy, including myself. I was the only one who agreed to discuss it with the author. His strange result was dismissed a priori by my colleagues as being a purely spurious relation without justification, and which could not be understood, since Akil equated a dimensionless quantity to a physical quantity of dimensions L        . A long correspondence then ensued between Mr. Akil and myself, notwithstanding the difficulties created by the fact that Mr. Akil divides his time between London and Kuwait. This correspondence resulted in the paper published below (which was submitted to the Academy in 1988-1989) together with my &quot;note to the reader&quot; in defence of Akil&apos;s peculiar results
Prosody is one of the challenges for experts of synthesis and recognition of the speech will be involved in the next years. Todays speech synthesis systems are able to achieve better performancesthan older systems, but the produced speech do not appear as natural as human voice. Those systems are already able to synthesize speech almost perfect on the segmental point of view: most of the artefact of former synthesizer are not present anymore. Further improvements can only come from a better implementation of prosody in future system. New systems have to be able to take under control intonation, tempo and loudness of voice in order to obtain the most natural speech. Also automatic speech recognition systems make a poor use of prosodic features. Speaker independency and noise robustness are not the only challenge of future recognition systems. Further improvement can derive from a better processing of prosody: todays systems require too much effort from the user to keep tempo and loudness constant. Those systems are thus not able to deal with spontaneous speech. In this work some prosodic processing tools will be shown. An application of these tools will be the extraction on prosodic features to be used as input in automatic recognition, and the automatic prosodic labelling of corpora for speech synthesis purpose. In the first chapter of this thesis a very fast introduction to prosody will be given, the most important similar systems described in literature will be shown and motivations for this work will be discussed. The second chapter will show original algorithms for prosodic analysis developed by the author. Most of the routines, used in this work, crucially rely on the proper choice of a number of parameter. While in many other similar works they are set empirically, in the third chapter some tools are shown to effectively tune parameter. The problem is essentially reduced to a minimization of an n-variable function. Processing time of these tools will be reduced by using distributed computing. The fourth chapter will show the architecture of the proposed system, it is essentially a library of classes able to model various prosodic entities. In the fifth chapter comparison between automatic and human analysis will be shown. In the same chapter, parameter tuning tools will be compared and benefits from distributed computing will be also shown.
Recently, a time-domain equalizer (TEQ) design for multicarrier-based systems has been proposed which claims to minimize the delay spread of the overall channel impulse response. We show that this is true only in an approximate sense; depending on the channel considered, the loss in delay spread with respect to the true minimum can be significant. An iterative algorithm to find this minimum is presented, whose computational complexity is similar to that of standard TEQ designs like the MSSNR approach of Melsa et al. It is observed that the method iteratively and automatically seeks the time reference yielding best performance, an advantage with respect to the MSSNR design which must compute several TEQs over a range of time references in order to select the optimum.
We present novel intelligent tools for mining 3D medical images. We  focus on detecting discriminative Regions of Interest (ROIs) and mining associations  between their spatial distribution and other clinical assessment. To  identify these highly informative regions, we propose utilizing statistical tests to  selectively partition the 3D space into a number of hyper-rectangles. We apply  quantitative characterization techniques to extract k-dimensional signatures  from the highly discriminative ROIs. Finally, we use neural networks for classification.
The present paper is concerned with the statistical analysis of the resolution limit in a so-called &quot;diffraction-limited&quot; imaging system. The canonical case study is that of incoherent imaging of two closely-spaced sources of possibly unequal brightness. The objective is to study how far beyond the classical Rayleigh limit of resolution one can reach at a given signal to noise ratio. The analysis uses tools from statistical detection and estimation theory. Specifically, we will derive explicit relationships between the minimum detectable distance between two closely-spaced point sources imaged incoherently at a given SNR. For completeness, asymptotic performance analysis for the estimation of the unknown parameters is carried out using the Cramr-Rao bound. To gain maximum intuition, the analysis is carried out in one dimension, but can be well extended to the two-dimensional case and to more practical models.
We are currently developing a system for visualizing Usenet newsgroups at a variety of scales. A macro/landscape view depicts many newsgroups and the relationships among them; a medium view depicts the  interactions within a single group; a close-up view depicts the individual in the context of the conversational situation.
This paper describes the development of a three dimensional geometrically constrained target tracker. This tracker combines the predictions of a circular prediction algorithm and a constant velocity filter by utilizing the Covariance Intersection. This combined prediction can be updated with the consequent measurement using the linear estimator. The proposed technique is illustrated on a benchmark trajectory including circular and straight line maneuvers.
We introduce an automated and accurate system for registering pre-operative 3D MR and CT images with intraoperative 3D ultrasound images based on the vessels visible in both. The clinical goal is to guide the radio-frequency ablation (RFA) of liver lesions using percutaneous ultrasound even when the lesions are not directly visible using ultrasound. The lesions locations and desired RFA sites are indicated on pre-operative images, and those markings are made to appear within the intra-operative 3D ultrasound images.
This paper deals with the dependability evaluation of software programs of iterative  nature. In this work we define a model that is able to account for both dependencies  between input values of successive iterations and the effects of sequences of  consecutive software failures on the reliability of the controlled system. Differently  from previously proposed models, some effort is devoted to address the problem  of how to get accurate estimates for the basic parameters. A model is thus proposed  that, requiring the designers or users to provide information usually obtainable by  experimental techniques, e.g. testing, is more useful and more generally applicable. Then a 
We describe an architecture for next generation, distributed data mining  systems which integrates data services to facilitate remote data analysis  and distributed data mining, network protocol services for high performance  data transport, and path services for optical paths. We also  present experimental evidence using geoscience data that this architecture  scales the remote analysis of Gigabyte size data sets over long haul,  high performance networks.
Pervasive computing allows a user to access an application on heterogeneous devices continuously and consistently. However, it is challenging to deliver complex applications on resource-constrained mobile devices, such as cell phones and PDAs. Different approaches, such as application-based or system-based adaptations, have been proposed to address the problem. However, existing solutions often require degrading application fidelity. We believe that this problem can be overcome by dynamically partitioning the application and offloading part of the application execution to a powerful nearby surrogate. This will enable pervasive application delivery to be realized without significant fidelity degradation or expensive application rewriting. Because pervasive computing environments are highly dynamic, the runtime offloading system needs to adapt to both application execution patterns and resource fluctuations. Using the Fuzzy Control model, we have developed an offloading inference engine to adaptively solve two key decision-making problems during runtime offloading: (1) timely triggering of adaptive offloading, and (2) intelligent selection of an application partitioning policy. Extensive trace-driven evaluations show the effectiveness of the offloading inference engine.
this paper, we discuss acoustic parameters and a classifier we developed to distinguish between nasals (/m/, /n/, /ng/) and semivowels (/r/, /l/, /w/, /y/). Based on the literature and our own acoustic studies, we use an onset/offset measure to capture the consonantal nature of nasals, and an energy ratio, a low spectral peak measure and a formant density measure to capture the nasal murmur
Current Quality of Service models such as those embodied in the Differentiated Services proposal, rely on data path aggregation to achieve scalability. Data path aggregation bundles into a single aggregate multiple flows with the same quality requirements, hence decreasing the amount of state to be kept. A similar scalability concern exists on the control path, where the state required to account for individual reservations needs to be minimized. There have been several proposals aimed at control path aggregation, and the goal of this paper is to expand on these works in an attempt to gain a better understanding of the various parameters that influence the efficiency of different approaches. In particular, we focus on inter-domain control aggregation, and compare an Autonomous System (AS) sink-tree based approach with two examples of a shared AS segment based approach, in terms of the amount of state kept, both per AS and per edge router. Our main contributions are in providing a greater understanding into the design of efficient control path aggregation methods.
This paper aims at assisting empirical researchers benefit from recent advances in  causal inference. The paper stresses the paradigmatic shifts that must be undertaken  in moving from traditional statistical analysis to causal analysis of multivariate  data. Special emphasis is placed on the assumptions that underly all causal inferences,  the languages used in formulating those assumptions, and the conditional  nature of causal claims inferred from nonexperimental studies. These emphases  are illustrated through a brief survey of recent results, including the control of  confounding, the assessment of causal effects, the interpretation of counterfactuals,  and a symbiosis between counterfactual and graphical methods of analysis.
We consider the problem of identifying the orders and the model parameters of PWARX hybrid models from noiseless input/output data. We cast the identification problem in an algebraic geometric framework in which the number of discrete states corresponds to the degree of a multivariate polynomial p and the orders and the model parameters are encoded on the factors of p. We derive a rank constraint on the input/output data from which one can estimate the coefficients of p. Given p, we show that one can estimate the orders and the parameters of each ARX model from the derivatives of p at a collection of regressors that minimize a certain objective function. Our solution does not require previous knowledge about the orders of the ARX models (only an upper bound is needed), nor does it constraint the orders to be equal. Also the switching mechanism can be arbitrary, hence the switches need not be separated by a minimum dwell time. We illustrate our approach with an algebraic example of a switching circuit and with simulation results in the presence of noisy data.
this paper is applicable to binary data inputs only; investigation of the non-binary ART Pseudo-Random Vs Random  0  2  4  6  8  10  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1    pseudo random  Random 1  Random Vs Non-Random Data  0  2  4  6  8  10  12  0.002  0.1 0.3 0.5 0.7 0.9    Non-Random  random  Fig. 2a. Baseline pseudo-random data reaches maximal vigilance faster than nonrandom data. This indicates that clustering tendency is not caused by chance clustering Fig. 2b. True random data reach maximal vigilance faster than the baseline, which is an indication that their clustering tendency is caused by mere chance
In multiprogrammed systems, synchronization often turns out to be a performance bottleneck and the source of poor fault-tolerance. Wait-free and lock-free algorithms can do without locking mechanisms, and therefore do not suffer from these problems. We present an effcient almost wait-free algorithm for parallel accessible hashtables, which promises more robust performance and reliability than conventional lock-based implementations. Our solution is as effcient as sequential hashtables. It can easily be implemented using C-like languages and requires on average only constant time for insertion, deletion or accessing of elements. Apart from that, our new algorithm allows the hashtables to grow and shrink dynamically when needed. A true problem of lock-free algorithms is that they are hard to design correctly, even when apparently straightforward. Ensuring the correctness of the design at the earliest possible stage is a major challenge in any responsible system development. Our algorithm contains 81 atomic statements. In view of the complexity of the algorithm and its correctness properties, we turned to the interactive theorem prover PVS for mechanical support. We employ standard deductive verification techniques to prove around 200 invariance properties of our almost wait-free algorithm, and describe how this is achieved using the theorem prover PVS.
This paper sketches the design of PAST, a large-scale, Internet-based, global storage utility that provides scalability, high availability, persistence and security. PAST is a peer-to-peer Internet application and is entirely selforganizing. PAST nodes serve as access points for clients, participate in the routing of client requests, and contribute storage to the system. Nodes are not trusted, they may join the system at any time and may silently leave the system without warning. Yet, the system is able to provide strong assurances, efficient storage access, load balancing and scalability.
We propose selective bitplane encryption to provide secure image transmission in low power mobile environments. Two types of ciphertext only attacks against this scheme are discussed and we use the corresponding results to derive conditions for a secure use of this technique.
Bear the provision of Quality of Service (QoS) in the Internet, Differentiated Service (DiffServ) model has been  proposed as a cost-effective solution. Traffic is classified into several service classes with different priorities. The premium class traffic has the highest one. The routing algorithm used by the premium class service has significant effects not only on its own traffic, but on all other classes of traffic as well. The shortest hopcount routing scheme used in current Internet turns out to be no longer sufficient in DiffServ networks. Based on
Good quality terrain models are becoming more and more important, as applications such as runoff modelling are being developed that demand better surface orientation information than is available from traditional interpolation techniques. A consequence is that poor-quality elevation grids must be massaged before they provide useable runoff models. This paper describes improved methods for extracting good quality terrain models from topographic contour maps, which despite modern techniques are still the most available form of elevation information. Recent work on the automatic reconstruction of curves from point samples, and the generation of medial axis transforms (skeletons) has greatly helped in the visualization of the relationships between sets of boundaries, and families of curves. The insertion of skeleton points guarantees the elimination of all flat triangles. Additional assumptions about the local uniformity of slopes give us enough information to assign elevation values to these skeleton points. Various interpolation techniques were compared using visualization of the enriched contour data. Examination of the quality and consistency of the resulting maps indicates the required properties of the interpolation method in order to produce terrain models with valid slopes. The result provides us with a surprisingly realistic model of the surface - that is, one that conforms well to our subjective interpretation of what a real landscape should look like.
Given uncertainty in the input model and parameters of a simulation study, the goal of the simulation study often becomes the estimation of a conditional expectation. The conditional expectation is expected performance conditional on the selected model and parameters. The distribution of this conditional expectation describes precisely, and concisely, the impact of input uncertainty on performance prediction. In this paper we estimate the density of a conditional expectation using ideas from the field of kernel density estimation. We present a result on asymptotically optimal rates of convergence and examine a number of numerical examples.
Solving equations in equational theories is a relevant programming paradigm which  integrates logic and equational programming into one unified framework. Efficient methods  based on narrowing strategies to solve systems of equations have been devised. In  this paper, we formulate a narrowing-based equation solving calculus which makes use of  a top-down abstract interpretation strategy to control the branching of the search tree. We define
This paper describes the MUMIS project, which applies ontology  based Information Extraction to improve the results of Information  Retrieval in multimedia archives. It makes use of a domain specific  ontology, multilingual lexicons and reasoning algorithms to automatically  create a semantic annotation of sources. The innovative aspect is  the use of a cross document merging algorithm that combines the information  extracted from separate textual sources to produce an integrated,  more complete, annotation of the material. This merging and unification  process uses ontology based reasoning and scenarios which are extracted  automatically from annotated sources.
Middleware provides simplicity and uniformity for the development of distributed applications. However, the modularity of the architecture of middleware is starting to disintegrate and to become complicated due to the interaction of too many orthogonal concerns imposed from a wide range of application requirements. This is not due to bad design but rather due to the limitations of the conventional architectural decomposition methodologies. We introduce the principles of horizontal decomposition (HD) to address this problem with a mixed-paradigm middleware architecture. HD provides guidance for the use of conventional decomposition methods to implement the core functionalities of middleware and the use of aspect orientation to address its orthogonal properties. Our evaluation of the horizontal decomposition principles focuses on refactoring major middleware functionalities into aspects in order to modularize and isolate them from the core architecture. New versions of the middleware platform can be created through combining the core and the flexible selection of middleware aspects such as IDL data types, the oneway invocation style, the dynamic messaging style, and additional character encoding schemes. As a result, the primary functionality of the middleware is supported with a much simpler architecture and enhanced performance. Moreover, customization and configuration of the middleware for a wide-range of requirements becomes possible.
In this paper   we investigate a problem arising in decentralized registration of sensors. The application we consider involves a heterogeneous collection of sensors - some sensors have on-board Global Positioning System (GPS) capabilities while others do not. All sensors have wireless communications capability but the wireless communication has limited effective range. Sensors can communicate only with other sensors that are within a fixed distance of each other. Sensors with GPS capability are self-registering. Sensors without GPS capability are less expensive and smaller but they must compute estimates of their location using estimates of the distances between themselves and other sensors within their radio range. GPS-less sensors may be several radio hops away from GPS-capable sensors so registration must be inferred transitively. Our approach to solving this registration problem involves minimizing a global potential or penalty function by using only local information, determined by the radio range, available to each sensor. The algorithm we derive is a special case of a more general methodology we have developed called &quot;Emergence Engineering&quot;.
this paper I will discuss the central role of licensing constraints (henceforth &quot;LC&apos;s&quot;) in phonological systems and how they may be viewed as one of the principal engines of phonological events. LC&apos;s were originally designed to explain restrictions on the combinatorial properties of elements. Given a theory of phonological expressions (to be given below), the underlying assumption is that any syntactically well-formed combination of elements should be present in a phonological system unless explicitly excluded. Since, as far as we know, no language expresses the full range of theoretically possible combinations of elements, LC&apos;s were proposed as language-specific constraints on such possibilities. A subset of a small set of possible LC&apos;s is sufficient to define the lexical set of, say, nuclear expressions of a given linguistic system. Recent work has shown that the usefulness of LC&apos;s extend far beyond their original raison d&apos;tre. In particular it is a pleasure to recognise the 2 seminal articles of Monik Charette &amp; Asli Gksel (Charette and Gksel 1996) and (Charette and Gksel 1998) which have provided the leadership in this field and the inspiration for this present work. I will briefly review part of their work in a later section. In the following section, I give a succinct summary of the element theory of phonological representations
Introduction  Identifying and recruiting prospective technology education teachers has been an ongoing concern for more than two decades. Considerable research was conducted during the late 1970s and early 1980s relative to teacher recruitment (Craft, 1979; Devier, 1982). These studies were prompted by declining enrollments in university programs and reported shortages of industrial arts teachers in forty-one states (Miller, 1980; Tomlinson, 1982; Wright, 1985). In some cases, this shortage of teachers led to high school programs being closed or cut back, the utilization of under-qualified personnel, and the abandonment of planned expansion. Simultaneously, university programs experienced significant drops in industrial arts teaching majors as students increasingly selected industrial technology or management options over teaching (Devier &amp; Wright, 1988). This trend of declining enrollments has continued and has now reached critical proportions (Volk, 1997). Current data suggest that a
Security is a good example for Aspect-Oriented Programming, but there are few reusable software components at the level of aspects. We introduce an elementary implementation of a reusable and generic aspect library providing security functions. This aspect library is based on AspectJ and common Java security packages, and includes typical security mechanisms. We describe the principle, architecture and usage of the security aspect library, and give a practical example of application in which security has been implemented using the aspects in the library. We also discuss the advantages and disadvantages of aspect library in reusability and generality, and the future efforts we will focus on.
This paper uses a &quot;natural experiment&quot; in Canadian divorce law reform to discriminate empirically between unitary and Nash-bargained models of the household. Using time-series data from three Canadian provinces, it demonstrates that following landmark divorce law reforms in the 1970s---reforms that led to improvements in women&apos;s expected settlement upon divorce in Ontario and British Columbia, suicide rates for older, married women in these provinces registered a sharp decline. Similar declines were not registered for younger, unmarried women or men in Ontario and British Columbia, nor for older, married women in Quebec, where the legal basis for divorce did not change. These results are consistent with Nash-bargained models of the household but not with the unitary model. 
The typical processing paradigm in natural language processing is the &quot;pipeline&quot; approach, where learners are being used at one level, their outcomes are being used as features for a second level of predictions and so one. In addition to accumulating errors, it is clear that the sequential processing is a crude approximation to a process in which interactions occur across levels and down stream decisions often interact with previous decisions. This work develops a general...
Prior to the deployment of any new or replacement component within a transportation system, it should be demonstrated that the modified system meets or exceeds the safety requirements of the original system. Since the occurrence of a mishap in such a system is a rare event, it is neither cost nor time effective to build and to test a prototype in an actual system prior to deployment. The Axiomatic Safety-Critical Assessment Process (ASCAP) is a simulation methodology that models the complete system and analyzes the effects of equipment changes. By carefully constraining the amount of the overall system state space required for analyses, it probabilistically determines the sequence of events that lead to mishaps. ASCAP is applicable to any transportation system that is governed by a well-defined operational environment.
In this paper, we exhibit security flaws in MICROCAST payas  -you-watch system. From the sole knowledge of public parameters, we  show how any intruder is able to forge a coin and so to freely get access  to the service.
In 2002-2003, the American College of Medical Informatics (ACMI) undertook a study of the future of informatics training. This project capitalized on the rapidly expanding interest in the role of computation in basic biological research, well characterized in the NIH BISTI report. The defining activity of the project was the three-day 2002 Annual Symposium of the College. A committee, comprised of the authors of this report, subsequently carried out activities, including interviews with a broader informatics and biological sciences constituency, collation and categorization of observations, and generation of recommendations. The committee viewed biomedical informatics as an interdisciplinary field, combining basic informational and computational sciences with application domains including health care, biological research, and education. Consequently, effective training in informatics, viewed from a national perspective, should encompass four key elements: 1) curricula that integrate experiences in the computational sciences and application domains, rather than just concatenating them; 2) diversity among trainees, with individualized, interdisciplinary cross-training allowing each trainee to develop key competencies that he/she does not initially possess, 3) direct immersion in research and development activities, and 4) exposure across the wide range of basic informational and computational sciences. Informatics training programs that implement these features, irrespective of their funding sources, will meet and exceed the challenges raised by the BISTI report, and optimally prepare their trainees for careers in a field that continues to evolve. 
We present a review of methods for the construction and deformation of  character models. We consider both state of the art research and common  practice. In particular we review applications, data capture methods, manual  model construction, polygonal, parametric and implicit surface representations,  basic geometric deformations, free form deformations, subdivision  surfaces, displacement map schemes and physical deformation.
Field Programmable Gate Arrays (FPGAs) holds the possibility of dynamic reconfiguration. The key advantages of dynamic reconfiguration are the ability to rapidly adapt to dynamic changes and better utilization of the programmable hardware resources for multiple applications. However, with the advent of multi-million gate equivalent FPGAs, configuration time is increasingly becoming a concern. High reconfiguration cost can potentially wipe out any gains from dynamic reconfiguration. One solution to alleviate this problem is to exploit the high levels of redundancy in the configuration bitstream by compression. In this paper, we propose a novel configuration compression technique that exploits redundancies both within a configuration&apos;s bitstream as well as between bitstreams of multiple configurations. By maximizing reuse, our results show that the proposed technique performs 26.5--75.8% better than the previously proposed techniques. To the best of our knowledge, ours is the first work that performs inter-configuration compression.
Current interest in ad hoc and peer-to-peer networking technologies prompts a re-examination of models for configuration management, within these frameworks. In the future, network management methods may have to scale to millions of nodes within a single organization, with complex social constraints. In this paper, we discuss whether it is possible to manage the configuration of large numbers of network devices using well-known and no-so-well-known configuration models, and we discuss how the special characteristics of ad hoc and peer-to-peer networks are reflected in this problem.
In this paper we describe a finite-capacity algorithm that can be used for production scheduling in a semiconductor wafer fabrication facility (wafer fab). The algorithm is a beam-search-type algorithm. We describe the basic features of the algorithm. The implementation of the algorithm is based on the ILOG-Solver libraries. We describe the simulation environment, which is used to evaluate the performance of the proposed algorithm. We show some results from computational experiments with the algorithm and the simulation test-bed described.
This paper presents an overview of the use of simulation algorithms in the field of financial engineering, assuming on the part of the reader no familiarity with finance and a modest familiarity with simulation methodology, but not its specialist research literature. The focus is on the challenges specific to financial simulations and the approaches that researchers have developed to handle them, although the paper does not constitute a comprehensive survey of the research literature. It offers to simulation researchers, professionals, and students an introduction to an application of increasing significance both within the simulation research community and among financial engineering practitioners. 
This paper proposes a method of creating a web document representation using a web ontology concepts instead of `bag-ofwords &apos;. However, since the web domain has a very small vocabulary, we are unable to transform all or most of the keywords of the web document into web ontology concepts. This particular problem is solved by creating an extended part of the web ontology with words obtained from an external linguistics knowledgebase. The promising outcome as the result of Natural Language Processing (NLP) and Information Retrieval (IR) fields being merged together convinces us to create the extended ontology using NLP technique.
U)   4. AUTHOR(S)   5. CORPORATE AUTHOR     506 Lorimer St   Fishermans Bend Victoria 3207 Australia   6a. DSTO NUMBER   6b. AR NUMBER     6c. TYPE OF REPORT   Technical Report   7. DOCUMENT DATE     8. FILE NUMBER   510/207/1295   9. TASK NUMBER   NAV 00/206   10. TASK SPONSOR   Navy- CANSG   11. NO. OF PAGES     12. NO. OF REFERENCES     ...
The process of selecting requirements for a release of a  software product is challenging as the decision-making is  based on uncertain predictions of issues such as market  value and development cost. This paper presents a method  aimed at supporting software product development organisations  in the identification of process improvement proposals  to increase requirements selection quality. The  method is based on an in-depth analysis of requirements  selection decision outcomes after the release has been  launched to the market and is in use by customers. The  method is validated in a case study involving real requirements  and industrial requirements engineering experts.
In this paper we will describe an approach to evaluating learning technology which we have developed over  the last twenty-five years, outline its theoretical background and compare it with other evaluation  frameworks. This has given us a set of working principles from evaluations we have conducted at the Open  University and from the literature, which we apply to the conduct of evaluations. These working practices  are summarised in the context interactions and outcomes (CIAO!) model. We describe here how we applied  these principles, working practices and models to an evaluation project conducted in Further Education. We  conclude by discussing the implications of these experiences for the future conduct of evaluations.
One way to relieve resources when executing a program on constrained devices is to migrate parts of it to other machines in a distributed system. Ideally, a system can automatically decide where to place parts of a program to satisfy resource constrains (CPU, memory bandwidth, battery power, etc.). We describe a compiler and virtual machine infrastructure as the context for research in automatic program partitioning and optimization for distributed execution. We define program partitioning as the process of decomposing a program into multiple tasks. The main motivation for our design is to enable experimenting with optimizing program execution on resource-constrained devices with respect to memory consumption, CPU time, battery lifetime and communication.
Bluetooth and IEEE 802.11 (Wi-Fi) are two communication protocol standards which  define a physical layer and a MAC layer for wireless communications within a short range (from a  few meters up to 100 meters) with low power consumption (from less than 1 mW up to 100 mW).
not found in spite of differences in phytoplankton  species composition: at the Antarctic Polar Front, biomass was dominated by a diatom population  of Fragilariopsis kerguelensis, whereas smaller cells, including chrysophytes, were relatively more  abundant in the Antarctic Circumpolar Current beyond the influence of frontal systems. Because  mixing was often in excess of 100 m in the latter region, diatom cells may have been unable to fulfil  their characteristically high Fe demand at low average light conditions, and thus became co-limited  by both resources. Using a model that describes the    C incorporation, the consistency was  shown between the dynamics in the glucan pool in the field experiments and in laboratory  experiments with an Antarctic diatom, Chaetoceros brevis. The glucan respiration rate was almost  twice as high during the dark phase as during the light phase, which is consistent with the role of  glucan as a reserve supplying energy and carbon skeletons for conti
this paper is on IRs. We believe that a more widespread acceptance and utilization of this approach has been hindered so far by a shortage of theoretical and experimental evidence suggesting its utility and overall feasibility for practical data mining. The goal of this research is to contribute to fill this gap
This paper outlines  a new autofocus procedure for improving SAS imagery, based on wavefront sensing techniques from  the astronomical imaging field
In this paper, we discuss acoustic parameters and a classifier we developed to distinguish between nasals and semivowels. Based on the literature and our own acoustic studies, we use an onset/offset measure to capture the consonantal nature of nasals, and an energy ratio, a low spectral peak measure and a formant density measure to capture the nasal murmur. These acoustic parameters are combined using Support Vector Machine based classifiers. Classification accuracies of 88.6%, 94.9% and 85.0% were obtained for prevocalic, postvocalic and intervocalic sonorant consonants, respectively. The overall classification rate was 92.4% for nasals and 88.1% for semivowels. These results have been obtained for the TIMIT database, which was collected from a large number of speakers and contains substantial coarticulatory effects.
This paper describes an algorithm to extract adaptive and quality quadrilateral/hexahedral meshes directly from volumetric imaging data. First, a bottom-up surface topology preserving octree-based algorithm is applied to select a starting octree level. Then the dual contouring method is used to extract a preliminary uniform quad/hex mesh, which is decomposed into finer quads/hexes adaptively without introducing any hanging nodes. The positions of all boundary vertices are recalculated to approximate the boundary surface more accurately. Mesh adaptivity can be controlled by a feature sensitive error function, the regions that users are interested in, or finite element calculation results. Finally, the relaxation based technique is deployed to improve mesh quality. Several demonstration examples are provided from a wide variety of application domains. Some extracted meshes have been extensively used in finite element simulations.
On-chip cache sizes are likely to continue to grow over the next decade as working sets, available chip capacity, and memory latencies all increase. Traditional cache architectures, with fixed sizes and discrete latencies, lock one organization down at design time, which will provide inferior performance across a range of workloads. In addition, expected increases in on-chip communication delays will make the time to retrieve data in a cache a function of the data&apos;s physical location. Consequently, cache access times will become a continuum of latencies rather than a single one. This non-uniformity will make static organizations particularly limited for single-chip servers, in which multiple processors will be different distances from the cache controller. In this paper, we propose a set of adaptive, high-performance cache design, called Non-Uniform Cache Architectures (NUCAs). We extend these physical designs with logical policies that allow important data to migrate closer to the processor within the same cache. We show that these adaptive level-two NUCA designs provide 1.6 times the performance of a Uniform Cache Architecture of any size, and that the adaptive NUCA scheme outperforms static NUCA schemes by 9% for multi-megabyte, on-chip server caches with large numbers of banks.
Design Patterns are now widely accepted as a useful concept  for guiding and documenting the design of object-oriented software  systems. Still the UML is ill-equipped for precisely representing design  patterns. It is true that some graphical annotations related to parameterized  collaborations can be drawn on a UML model, but even the most  classical GoF patterns, such as Observer, Composite or Visitor cannot  be modeled precisely this way. We thus propose a minimal set of modifications  to the UML 1.3 meta-model to make it possible to model design  patterns and represent their occurrences in UML, opening the way for  some automatic processing of pattern applications within CASE tools.
Category Theory is used to describe a category of fusors. The category is formed from a model of a process begining with an event and leading to the final labeling of the event. Although many techniques of fusing information have been developed the inherent relationships among different types of fusion techniques (fusors) have not yet been fully explored. In this paper, a foundation of fusion is presented, definitions developed, and a method of measuring the performance of fusors is given. Functionals on receiver operating characteristic (ROC) curves are developed to form a partial ordering of a set of classifier families. The functional also induces a category of fusion rules. The treatment includes a proof of how to find the Bayes optimal classifier (or Bayes Optimal fusor, if available) from a ROC curve.
Gridded volumetric data sets representing simulation or tomography output are commonly visualized by displaying a triangulated isosurface for a particular isovalue. When the grid is stored in a standard format, the entire volume must be loaded from disk, even though only a fraction of the grid cells may intersect the isosurface.
In conventional applications it is easy to find detailed and structured practices that make use of models in order to describe almost every aspect of the user interface. On the other hand, new user interfaces, such as those used in virtual and augmented applications, are usually designed and developed in an ad hoc fashion or following a rather simple systematic approach. There are, however, a few notable efforts to develop model-based design methods for these new interfaces. Current practices, research efforts and holes that are still to be addressed in the development of new user interfaces are reviewed in this paper.
There exists nowadays consensus on the importance of teachers&apos; professional development. Also, most authors agree that the school&apos;s workplace conditions can exert great influence on this development. In this paper the impact of two workplace conditions, autonomy and collegiality, on elementary school teachers&apos; professional development is analysed. The qualitative research reported makes clear that this inffuence should be thought of in a balanced way. Certain forms of autonomy and collegiality -- and more specifically certain combinations of both workplace conditions -- have a far more positive influence on teachers&apos; professional development than others. 
We present an approach for registering an aerial Digital Elevation Model (DEM) with a color intensity image obtained using a camera mounted on a mobile robot. An approximate measurement of the camera pose is obtained using auxiliary sensors on-board the robot. The DEM is transformed into a depth map in the camera&apos;s coordinate system using this initial pose. The problem is now simplified to the alignment of two images, one containing intensity information, and the other, depth. Region boundaries in the intensity image are matched with discontinuities in the depth map using a robust directed Hausdorff distance. This cost function is minimized with respect to the six parameters defining the camera pose. Due to the highly non-linear nature of cost function with multiple local minima, a stochastic algorithm based on the downhill simplex principle is employed for minimization. Results on real data are presented.
A calibrated classifier provides reliable estimates of the true  probability that each test sample is a member of the class of interest.
A two transponder long baseline positioning system to measure the sway of a free towed Synthetic Aperture Sonar (SAS) is proposed. A Matlab simulation predicts a worst case sway accuracy of    cm over a 150 m long tow path with an update rate of 14 Hz. The sway is measured with respect to freely deployed transponders which remain stationary on the seabed connected via cables to floating buoys housing GPS timing receivers. Sway information is completely independent for each sonar ping and allows the deblurring of the SAS images by post processing.
We present a real-time algorithm for skin rendering which was used in the real-time animation Ruby: The DoubleCross, appearing in this year&apos;s SIGGRAPH animation festival. Our approach approximates the appearance of subsurface scattering by blurring the diffuse illumination in texture space using graphics hardware. This approach, based on the offline skin rendering technique proposed by Borshukov and Lewis, gives a realistic look and is both efficient and easy to implement. We describe algorithms to efficiently implement this technique in real-time using graphics hardware, as well as several enhancements to improve quality.
The up-coming Gbps high-speed networks are expected to support a wide range of communication-intensive, real-time multimedia applications. The requirement for timely delivery of digitized audio-visual information raises new challenges for the next generation integrated-service broadband networks. One of the key issues is the Quality-of-Service (QoS) routing. It selects network routes with sufficient resources for the requested QoS parameters. The goal of routing solutions is two-fold: (1) satisfying the QoS requirements for every admitted connection and (2) achieving the global efficiency in resource utilization. Many unicast/multicast QoS routing algorithms were published recently, and they work with a variety of QoS requirements and resource constraints. Overall, they can be partitioned into three broad classes: (1) source routing, (2) distributed routing and (3) hierarchical routing algorithms. In this paper we give an overview of the QoS routing problem as well as the existing solutions. We present the strengths and the weaknesses of different routing strategies and outline the challenges. We also discuss the basic algorithms in each class, classify and compare them, and point out possible future directions in the QoS routing area.
Introduction  The pertinent facts, leading up to this latest experiment, about acoustic and perceptual properties of Danish std have been reported and discussed in Grnnum &amp; Basbll (2001a, 2001b, 2002a, 2002b). Here is the briefest possible summary: Consonants with std are not systematically longer than consonants without std, and they may be shorter as well. Vowels with std are as long as long vowels without std, and both are 50-70% longer than short vowels. Std vowels are also found to equal long vowels perceptually, though this similarity may be overshadowed by the similarity between syllables with std, irrespective of vowel length.  2. Std onset timing and cognitive reality  Variability in the onset (when it can be determined at all) of the laryngealization which is the std, measured from vowel onset, is very considerable, with time lags ranging between 10 and 130ms. It averages around 60ms, cf. Grnnum &amp; Basbll (2001a). We need now to know whether and how this onset is perceived. Wh
this paper we propose and experimentally investigate a vision-based technique for autonomously landing a robotic helicopter. We model the solution to the landing problem discretely using a finite state machine, responsible for detecting the landing site, navigating toward it, and landing on it. Data from a single on-board camera are combined with attitude and position measurements from an on-board inertial navigation unit. These are the inputs to the on-board control system: a set of controllers running in parallel which are responsible for controlling the individual degrees of freedom of the helicopter. The resulting hybrid control system is simple, yet effective as shown experimentally by trials in nominal and perturbed conditions
Summary indicators for measuring and assessing infant and child feeding  practices are needed for research, communication and advocacy, and program evaluation.  This paper reports on progress in developing a summary measure of infant and child  feeding practices that addresses the following two challenges: infant and child feeding is  multidimensional, and appropriate practices vary by age of the child. Much previous  research in the area of infant and child feeding has focused on single practices over a  narrow age range and so has not addressed the determinants and impact of adequate or  optimal infant and child feeding.  Using data from the Ethiopia Demographic and Health Survey, an infant and child  feeding index is constructed, summarizing a range of key practices, including  breastfeeding, bottle use, feeding frequency, and diet diversity. Because it provides agespecific scoring and incorporates various practices, the index is a useful analytic tool.  The index is associated with an indicator of child growth (height-for-age) in  bivariate and multivariate analyses. Examination of individual indicators shows that this  association is driven by a strong positive association between one componentdiet  diversityand height-for-age. Further work is required to establish the nature of the  relationship between infant and child feeding indicators, nutrient adequacy, growth, and  other outcomes. But because it can be used to illustrate the association between a set of  recommended practices and growth, the index may serve as a communication tool with  policymakers.    iii Simulations show that the index accurately reflects an averaging of changes in  individual component practices, and so it may also be of use to program managers who  seek a summary measure for assessing p...
Distributedsynchronizationforparallelsimulationisgenerallyclassifiedasbeingeitheroptimisticorconservative. Whileconsiderableinvestigationshavebeenconducted toanalyzeandoptimizeeachofthesesynchronization strategies,verylittlestudyonthedefinitionandstrictness ofcausalityhavebeenconducted.Dowereallyneed topreservecausalityinalltypesofsimulations?This paperattemptstoanswerthisquestion.Wearguethat significantperformancegainscanbemadebyreconsideringthisdefinitiontodecideiftheparallelsimulation needstopreservecausality.Weinvestigatethefeasibility ofunsynchronizedparallelsimulationthroughtheuseof severalqueuingmodelsimulationsandpresentacomparativeanalysisbetweenunsynchronizedandTimeWarp simulation.
A mathematical method called subordination broadens the applicability of the  classical advection-dispersion equation for contaminant transport. In this method the time  variable is randomized to represent the operational time experienced by different  particles. In a highly heterogeneous aquifer the operational time captures the fractal  properties of the medium. This leads to a simple, parsimonious model of contaminant  transport that exhibits many of the features (heavy tails, skewness, and non-Fickian growth  rate) typically seen in real aquifers. We employ a stable subordinator that derives from  physical models of anomalous diffusion involving fractional derivatives. Applied to a onedimensional  approximation of the MADE-2 data set, the model shows excellent  agreement.
This paper develops a framework based on convex optimization and economic ideas to  formulate and solve approximately a rich class of dynamic and stochastic resource  allocation problems, fitting in a generic discrete-state multi-project restless bandit  problem (RBP). It draws on the single-project framework in the authors companion  paper &quot;Restless bandit marginal productivity indices I: Single-project case and optimal  control of a make-to-stock M/G/1 queue&quot;, based on characterization of a projects  marginal productivity index (MPI). Our framework significantly expands the scope of  Whittle (1988)s seminal approach to the RBP. Contributions include: (i) Formulation  of a generic multi-project RBP, and algorithmic solution via single-project MPIs of a  relaxed problem, giving a lower bound on optimal cost performance; (ii) a heuristic  MPI-based hedging point and index policy; (iii) application of the MPI policy and  bound to the problem of dynamic scheduling for a multiclass combined MTO/MTS  M/G/1 queue with convex backorder and stock holding cost rates, under the LRA  criterion; and (iv) results of a computational study on the MPI bound and policy,  showing the latters near-optimality across the cases investigated
This paper describes the design and implementation of a novel reliable multicast protocol, totally reliable and scalable to large number of receivers. MAF relies on Active Networks technology: active routers in the multicast tree store sender&apos;s transmissions in order to be able to later retransmit them to repair downstream losses. To address scalability, MAF organizes those active routers into a hierarchical structure obtained by dividing the multicast tree into subtrees. Since a sender initiated approach is used within each of those subtrees, MAF has the particularity of operating correctly with finite buffers. This paper also describes the implementation of MAF over the active network platform deployed by the RNRT project AMARRAGE. Index Terms--- totally reliable multicast, active networks, hierarchical structure, aggregated ACK, finite buffers.
ix 1 
Key management is an essential cryptographic primitive upon which other security primitives are built. However, none of the existing key management schemes are suitable for ad hoc networks. They are either too inefficient, not functional on an arbitrary or unknown network topology, or not tolerant to a changing network topology or link failures. Recent research on distributed sensor networks suggests that key pre-distribution schemes (KPS) are the only practical option for scenarios where the network topology is not known prior to deployment. However, all of the existing KPS schemes rely on trusted third parties (TTP) rendering them inapplicable in many ad hoc networking scenarios and thus restricting them from wide-spread use in ad hoc networks. To eliminate this reliance on TTP, we introduce distributed key pre-distribution scheme (DKPS) and construct the first DKPS prototype to realize fully distributed and selforganized key pre-distribution without relying on any infrastructure support. DKPS overcomes the main limitations of the previous schemes, namely the needs of TTP and an established routing infrastructure. It minimizes the requirements posed on the underlying networks and can be easily applied to the ad hoc networking scenarios where key pre-distribution schemes were previously inapplicable. Finally, DKPS is robust to changing topology and broken links and can work before any routing infrastructure has been established, thus facilitating the widespread deployment of secure ad hoc networks.
As technology shrinks and working frequency reaches multi gigahertz range, designing and testing interconnects are no longer trivial issues. In this paper we propose an enhanced boundary scan architecture to test high-speed interconnects for signal integrity. This architecture includes: a) a modified driving cell that generates patterns according to multiple transitions fault model; and b) an observation cell that monitors signal integrity violations. To fully comply with conventional JTAG, two new instructions are used to control cells and scan activities in the integrity test mode.
Detecting a transient signal of unknown arrival time in noise is actually a binary hypothesis test problem, where the null hypothesis (noise only) is a simple one, while the alternative hypothesis is composite. The generalized likelihood ratio test (GLRT) is a common tool to solve such problems. In this paper we show how order statistics (OS) approach can be used to solve the same problem. We show that the two hypothesis becomes simple using the OS approach so a likelihood ratio test (LRT) can be applied, and we discuss the trade-offs between the two solutions. In particular, we point out cases where the OS detector outperforms the GLRT.
Facial expression interpretation, recognition and analysis is a key issue in visual communication and man to machine interaction. In this paper, we present a factorization technique which decomposes the appearance parameters coding a natural image. This technique is then used to perform facial expression synthesis on unseen faces showing any undetermined facial expression, as well as facial expression recognition.
Fingerprints are widely used in automatic identity verification  systems. The core of such systems is the verification algorithm to match two  fingerprints. So far, various method for fingerprint matching have been  proposed, but few works investigated the fusion of two or more matching  algorithms. In this paper, various methods for fusing such algorithms have been  investigated. Experimental results showed that such fusion can outperform the  best individual verification algorithm and increase the discrimination between  genuine and impostor classes.
In this paper, I describe an alternative approach to building a semantic web that addresses some known challenges to existing attempts. In particular, powerful information extraction techniques are used to identify concepts of interest in Web pages. Identified concepts are then used to semi-automatically construct assertions in a computer-readable markup, reducing manual annotation requirements. It is also envisioned that these semantic assertions will be constructed specifically by communities of users with common interests. The structured knowledge bases created will then contain content that reflects the uses they were designed for, thereby facilitating effective automated reasoning and inference for real-world problems.
Congruence closure algorithms are nowadays central in many modern applications in automated deduction and verification, where it is frequently required to recover the set of merge operations that caused the equivalence of a given pair of terms. For this purpose we study, from the algorithmic point of view, the problem of extracting such small proofs.
In this paper, we consider Dijkstra&apos;s algorithm for the single source single target shortest  path problem in large sparse graphs. The goal is to reduce the response time for on-line  queries by using precomputed information. Due to the size of the graph, preprocessing space  requirements can be only linear in the number of nodes. We assume that a layout of the  graph is given. In the preprocessing, we determine from this layout a geometric object for  each edge containing all nodes that can be reached by a shortest path starting with that edge.
Over the past two years students taking two biology modules at the University of Derby have been assessed using computer assessments with TRIADs (Tripartite Interactive Assessment Delivery System) in both their formal end of module examinations and for scored formative assessments. We were keen to establish the student views of the use of computer assessment and thus over this period in addition to the overall evaluation of the modules the students were also given the opportunity to evaluate these assessments. In the first instance an open ended approach was taken, and students were given the opportunity to anonymously write comments on the computer examinations. The results of this were encouraging in that only a minority of students (~5%) made non-positive comments on CAA with the majority of students being very positive on their CAA experiences. In addition a range of useful comments in relation to the application of CAA were provided by students, pertaining to comparability with traditional examinations and student learning strategy these are also discussed.
It is widely understood that protein functions can be exhaustively described in terms of  no single parameter, whether this be amino acid sequence or the three-dimensional  structure of the underlying protein molecule. This means that a number of different  attributes must be used to create an ontology of protein functions. Certainly much of the  required information is already stored in databases such as Swiss-Prot, Protein Data  Bank, SCOP and MIPS. But the latter have been developed for different purposes and  the separate data-structures which they employ are not conducive to the needed data  integration. When we attempt to classify the entities in the domain of proteins, we find  ourselves faced with a number of cross-cutting principles of classification. Our question  here is: how can we bring together these separate taxonomies in order to describe  protein functions? Our proposed answer is: via a careful top-level ontological analysis  of the relevant principles of classification, combined with a new framework for the  simultaneous manipulation of classifications constructed for different purposes.
A common traffic engineering design principle is to select a small set of flows, that account for a large fraction of the overall traffic, to be differentially treated inside the network so as to achieve a specific performance objective. In this paper we illustrate that one needs to be careful in implementing such an approach because there are tradeoffs to be addressed that arise due to traffic dynamics. We demonstrate that Internet flows are very volatile in terms of volume, and may substantially change the volume of traffic they transmit as time evolves. Currently proposed schemes for flow classification, although attractive due to their simplicity, face challenges due to this property of flows. Bandwidth volatility impacts the amount of load captured in a set of flows, which usually drops both significantly and quickly after flow classification is performed. Thus if the goal is to capture a large fraction of traffic consistently over time, flows will need to be reselected often. Our first contribution is in understanding the impact of flow volatility on the classification schemes employed in a traffic engineering context. Our second contribution is to propose a classification scheme that is capable of addressing the issues identified above by incorporating historical flow information. Using actual Internet data we demonstrate that our scheme outperforms previously proposed schemes, and reduces both the impact of flow volatility on the load captured by the selected set of flows and the required frequency for its reselection.
The Winter Simulation Conference (WSC) is traditionally known as the most important annual conference serving the discrete event simulation community. The purpose of this panel session is to generate discussion about the nature of WSC in the future and about its future role in the overall simulation community. There are many reasons to do this. It is important to the communities currently served by WSC, critical to the conference itself, and in a broad sense significant to the future of simulation itself. In keeping with the track theme of discussing the future of simulation, it makes sense to discuss the future of the most important discrete-event simulation event.
We study the temporal connectivity structure of single-channel Internet-based chat participation streams. Somewhat similar to bibliometric analysis, and complementary to topic-analysis, we base our study solely on context information provided by the temporal order of participants&apos; contributions. Experimental results obtained by employing both networkanalysis indicators and an aggregate Markov modelling approach indicate the existence of distinguishable communities in the about one day worth real-world chat dynamics analysed.
GAMBL is a word expert approach to WSD in which each word expert is trained using memorybased learning. Joint feature selection and algorithm parameter optimization are achieved with a genetic algorithm (GA). We use a cascaded classifier approach in which the GA optimizes local context features and the output of a separate keyword classifier (rather than also optimizing the keyword features together with the local context features). A further innovation on earlier versions of memorybased WSD is the use of grammatical relation and chunk features. This paper presents the architecture of the system briefly, and discusses its performance on the English lexical sample and all words tasks in SENSEVAL-3.
Anthropomorphic visualization is a new approach to presenting information about participants in online spaces using the human form as the basis for the visualization. Various data about an individual&apos;s online behavior are mapped to different parts of a humanoid yet abstract form. I hypothesized that using a humanoid form to visualize data about people in online social spaces could serve two purposes simultaneously: communicate statistics about the individuals and evoke a social response. Using the
Service composition is the act of taking several component products or services, and bundling them together to meet the needs of a given customer. In the future, service composition will play an increasingly important role in e-commerce, and automation will be desirable to improve speed and efficiency of customer response. In this paper, we consider a service composition agent that both buys components and sells services through auctions. It buys component services by participating in many English auctions. It sells composite services by participating in Request-for-Quotes reverse auctions. Because it does not hold a long-term inventory of component services, it must take risks; it must make offers in reverse auctions prior to purchasing all the components needed, and must bid in English auctions prior to having a guaranteed customer for the composite good. We present algorithms that is able to manage this risk, by appropriately bidding/offering in many auctions and reverse auctions simultaneously. The algorithms will withdraw from one set of possible auctions and move to another set if this will produce a better-expected outcome, but will effectively manage the risk of accidentally winning outstanding bids/offers during the withdrawal process. We illustrate the behavior of these algorithms through a set of worked examples.
this paper most of the contents of this section (1.2) is taken
In this paper we present and evaluate the 4+4 architecture. 4+4 extends the IPv4 address space without requiring changes to existing routers. It builds on the existence of NATs and multiple address realms, but it does not use address translation and provides end-to-end address transparency. Existing address translation is used only as a transition tool. The paper also presents an implementation of 4+4 and related experimental results. We conclude that 4+4 is simple to introduce and may represent a mediumterm solution if IPv6 transition does not take off quickly enough. The source code of our implementation can be downloaded from http://ipv44.comet.columbia.edu.
this paper is as follows. We present a model for selection and segregation distortion in an infinitely large randomly mating population with one sex. To fix ideas, we first consider the competition between the wildtype and two distorter alleles. We then show how the analysis can be extended to the competition between a large number of distorters. We start by assuming that the amount of complementation is the same for all combinations of distorter alleles. In this case, each parameter configuration results in a unique stable polymorphism. We give an analytical characterization of this equilibrium, and show that it typically involves many alleles. Subsequently, we show by means of a simple example that the outcome of competition may be contingent on the initial conditions if the degree of complementation differs between distorters. Finally, we study the competition between segregation distorters in case that there is a negative trade-off between distorting effciency and complementing ability
The Virtual Home Environment (VHE) encompasses the deployment and management of adaptable services that retain any personalized service aspects, irrespective of terminal, network and geographic location. We assert that the dynamic nature of the VHE requires management capabilities that can be suitably provided through the use of mobile agent technology. In this direction, we examine four different engineering solutions for the realization of a VHE performance management component that allows service adaptation in relation to the available network Quality of Service (QoS). The mobile agent approach is compared with competing technologies in order to identify the benefits of this novel application of mobile agents, discuss its drawbacks and finally focus on the lessons learned from our prototype system. Although mobile agents are typically associated with increased performance costs, it is through agent migration that we were able to address the VHE requirements of universality, dynamic programmability and network technology independence.
The recent reduction in telecommunications spending has increased the importance of network planning to improve the return on investment on the existing network infrastructures. Therefore, tools that help in maximizing the bandwidth efficiency of the network at a minimum cost are essential. Previous work in this area focused on increasing bandwidth efficiency and reliability. In this work, in addition to increasing the bandwidth efficiency, we address the complexity of network management and operations. This issue is explicitly addressed by our novel framework, a simple polynomial time algorithm (SimPol)that  achieves optimum network performance (in terms of congestion or bandwidth consumption) using only a small number of paths. The problem formulation is based on splittable multicommodity flows. Using SimPol we show that the total number of paths is at most k + m,wherek and m are the numbers of demands and edges in the network, respectively. We extend the basic framework into an integer programming formulation to address the tradeoff between network congestion and the total number of paths. We also use SimPol to address the problem of implementing path/link policies such as bandwidth-limited paths. The performance of SimPol is evaluated through extensive simulations. We find that for large number of demands the LPbased framework provides a near-optimal solution of almost one path per demand. Using the integer programming approach, we can get exactly one path while losing about 10% to 50% in congestion depending on the number of demands. This congestion is, however, far better than the traditional shortest path routing. The framework is general and can be used in capacity planning for transport networks such as MPLS and ATM.
This paper review the state of the art in off-line Roman cursive han dw iting recognition. The input provided to an off-line han iting recognition system is an image of a digit, aw ord, or - more generally - some text, and the system produces, as output, an ASCII transcription of the input. This taskinvolves a number of processing steps, some of w ich are quite diffcult. Typically, preprocessing, normalization, feature extraction, classification, and postprocessing operations are required. We&apos;ll survey the state of the art, analyze recent trends, and try to identify challenges for future research in this field.
this paper I wish to present the model proposed some years ago by Louis de Broglie [2] for the tired photon. The problem with most alternative explanations for the cosmological redshift arises from the fact that they result from ad hoc assumptions. The model of de Broglie has none of the ad hoc character of which most tired-light mechanisms are accused: it follows from considerations at the fundamental level of his quantum theory [3]. It is only a corollary of his causal double solution theory, which stands almost side by side with the orthodox non causal theory for explanation and prediction of quantum phenomena. Another major advantage of this model is that it can be tested on a laboratory scale
Recent advances in wireless communication and microelectronics have enabled the development of low-cost sensor devices leading to interest in large-scale sensor networks for military applications. Sensor networks consist of large numbers of networked sensors that can be dynamically deployed and used for tactical situational awareness. One critical challenge is how to dynamically integrate these sensor networks with information fusion processes to support real-time sensing, exploitation and decision-making in a rich tactical environment. In this paper, we describe our work on an extensible prototype to address the challenge. The prototype and its constituent technologies provide a proof-of-concept that demonstrates several fundamental new approaches for implementing next generation battlefield information systems. Many cutting-edge technologies are used to implement this system, including semantic web, web services, peer-to-peer network and content-based routing. This prototype system is able to dynamically integrate various distributed sensors and multi-level information fusion services into new applications and run them across a distributed network to support different mission goals. Agent technology plays a role in two fundamental ways: resources are described, located and tasked using semantic descriptions based on ontologies and semantic services; tracking, fusion and decision-making logic is implemented using agent objects and semantic descriptions as well.
Previous work showed how moving particles that rest along their trajectory lead to time-nonlocal advection-dispersion equations. If the waiting times have infinite mean, the model equation contains a fractional time derivative of order between 0 and 1. In this article, we develop a new advection-dispersion equation with an additional fractional time derivative of order between 1 and 2. Solutions to the equation are obtained by subordination. The form of the time derivative is related to the probability distribution of particle waiting times and the subordinator is given as the first passage time density of the waiting time process which is computed explicitly. Preprint submitted to Elsevier Science 19 July 2004 Key words: Anomalous Diffusion, Continuous Time Random Walks, First Passage Time, Fractional Calculus, Subdiffusion, Power laws 1 
The telecommunications industry in Israel has changed significantly in recent years. This paper examines key issues that will arise in Israel as a result of these major changes and argues that the major changes in the telecommunications industry require significant changes in the regulatory structure. The paper first provides important background material on the current structure in the various sectors of the telecommunications industry in Israel. The paper then discusses the current regulatory environment and makes recommendations regarding the future regulatory structure in Israel and the scope for regulation.
n Beschlagnahme pfndbar zabavit, -ovat beschlagnahmen drh zdrffka  zdrffn  zdrh   zadrhl  Stockung  Sperr-  Schlinge  verwickelt   zadrhnout,-vat   zadrhovat   zadrffet,-ovat  scheuern, reiben  verknoten  aufhalten  duch zducha  zdus  zdusn  zdusn   zadusen   zadusen   zadchan  Atemnot  Kirchengut  Kirchen-  asthmatisch  Ersticken  erstickt  stickig   zadusit   zadusovat se   zadychat  ersticken  beteuern  anhauchen  hyb zhyb   zahynut   zahnut  Falte, Krmmung  Untergang  gebogen   zahnout   zahbat   zahynout  umbiegen  rtteln  untergehen  hff zhffevn   zahffat   zahffvac   zahffvaff   zahffvadlo  Wrme-  Erwrmen  Wrme-  Vorwrmer  &quot;&quot;   zahfft,-vat erwrmen  chod zchod   zachzen   zachzka  Klosett  Umgang  Umweg   zachzet einbiegen,  untergehen  chran zchrana  zchrann   zachrnce   zachrnkynff   zachrnffn   zachraffovac  Rettung  Rettungs-  Retter  Retterin  Rettung  Rettungs-   zachrnit   zachraffovat  (er-) retten  &quot;&quot;  klad zklad  zkladka  zkladna  zkladn   zakladac   zakladatel   zakladatelka  Grundla
Identity-based public key encryption facilitates easy introduction of  public key cryptography by allowing an entity&apos;s public key to be derived from  an arbitrary identification value, such as name or email address. The main practical  benefit of identity-based cryptography is in greatly reducing the need for,  and reliance on, public key certificates. Although some interesting identity-based  techniques have been developed in the past, none are compatible with popular  public key encryption algorithms (such as El Gamal and RSA). This limits the  utility of identity-based cryptography as a transitional step to full-blown public  key cryptography. Furthermore, it is fundamentally difficult to reconcile finegrained  revocation with identity-based cryptography.
Fast restoration is an important feature of both MPLS and optical networks. The main mechanism for achieving fast restoration is by locally routing around failures using pre-setup detour paths. Signaling and routing protocol extensions to implement this local bypass ability are currently being standardized. To make use of this ability, dynamic schemes that jointly route primary paths and all link detours for links used by the primary paths have been previously proposed. These schemes also permit sharing of reserved restoration capacity for achieving efficiency. However, this joint computation places a significantly larger computational load on the network elements than that imposed by the shortest path computation variants typically used for unprotected network connection routing. In this paper, we propose a new scheme that is operationally much simpler, shares capacity used for restoration, and permits the network to route the primary paths in a manner that is oblivious to restoration needs. Restoration of all carried traffic is guaranteed by a new link capacity partitioning scheme that maximizes the working capacity of the network without requiring any knowledge of the traffic that will be imposed on the network. Being traffic independent for a priori link capacity partitioning and being oblivious to restoration needs for on-line network routing makes this scheme operationally simple and desirable in the sense of placing no additional routing load on the constrained computing resources at the network nodes. To compute the link capacity partitions, we develop a fast combinatorial algorithm that uses only iterative shortest path computations, and is a fully polynomial time approximation scheme (FPTAS) , i.e., it achieves a (1 + ff)-factor approximation for any ff&gt;0 and ru...
Does teaching experience di!erentially shape the thinking of teachers of di!erent academic disciplines regarding schooling issues incidentally related to subject matter instruction? This question was addressed by examining the broad schooling goals established for students by novice and veteran teachers of humanistica and scienti&quot;ca subjects. Participants were 44 Israeli female teachers of grades 7}9. Frequency and intensity of goal preferences were assessed in a semi-structured interview. Results demonstrated that: (1) novices and veterans expressed di!erent goal preferences, as did humanities versus science teachers; (2) experienced humanities teachers preferred academic goals less than other teachers; and (3) the overall order of goal preference was academic&apos;social&apos;personal. The signi&quot;cance of the interaction between teacher experience and discipline taught is discussed. ff 1999 Elsevier Science Ltd. All rights reserved.
Interpretation  The abstract-interpretation technique [2] allows conservative automatic verification of partial correctness to be conducted by identifying sound over-approximations to loop invariants. An iterative computation is carried out to determine an appropriate abstract value for each program point. The result at each program point is an abstract value that summarizes the sets of reachable concrete states at that point.
This work describes a computer model of the immune system s response to infection, specifically the cytotoxic T lymphocyte (CTL) response. CTLs play an important role in the control of infectious agents, and they are essential components of our defense against HIV, cancer, and other diseases of great public interest. Immunologists are interested in manipulating and enhancing the CTL response to these diseases, whether by vaccination or drug therapy, but the process can be difficult and ad hoc. A combination of animal experimentation, limited human testing, and simple mathematical models have been the primary sources of guidance in the efforts to address these diseases. Computer models provide an alternative strategy for exploring immune system therapies. Recently developed laboratory techniques that have revealed and quantified many aspects of CTL behavior provide an unprecedented opportunity to develop detailed models. The model used in this work integrates many of these new findings into a coherent system that simulates an immune response to viral infection. This model reproduces many of the phenomena seen in CTL responses but not captured by other mathematical or computer models and can be used to explore vaccination strategies. The value of modeling goes beyond simply making predictions. It allows one to perform experiments difficult, or even impossible, to perform in the laboratory. For example, in a computer model one can replicate experiments exactly or choose to allow stochastic fluctuations to influence the outcome. In biological systems, achieving this level of control is impossible. Model-building can also be used as a vehicle for hypothesis testing by formulating one s assumptions about a system s behavior as a model. If the model s behavior does not match real-world experimental results, the initial assumptions can be changed and a new model built. The model presented here is the result of a series of such choices.
A critical issue in wireless sensor networks is represented by the limited availability of energy within network nodes; therefore making good use of energy is a must. A widely employed energy-saving technique is to place nodes in sleep mode, corresponding to a low-power consumption as well as to reduced operational capabilities. In this work, we develop a Markov model of a sensor network whose nodes may enter a sleep mode, and we use this model to investigate the system performance in terms of energy consumption, network capacity, and data delivery delay. Furthermore, the proposed model enables us to investigate the trade-offs existing between these performance metrics and the sensor dynamics in sleep/active mode. Analytical results present an excellent matching with simulation results for a large variety of system scenarios showing the accuracy of our approach.
There are a lot of approaches for measuring semantic similarities between words. This paper proposes a new method based on the analysis of a monolingual dictionary. We can view the word definitions of a dictionary as a network: its nodes are the headwords found in the dictionary and its edges represent the relations between a headword and the words present in its definition. In this view, the meaning of a word is defined by the total quantity of information, in which each element of its definition contributes. The similarity between two words is defined by the maximal quantity of information exchanged between them through the network.
Using discrete-event simulation models, a study was conducted to evaluate the current production practices of a high-volume semiconductor back-end operation. The overall goal was to find potential areas for productivity improvement that would collectively yield a 60% reduction in manufacturing cycle time. This paper presents the simulation methodology and findings pertaining to analysis of the Assembly, Burn-In, and Test operations. Many of the recommendations identified can be implemented at no additional cost to the factory. The most significant opportunities for improvement are in the Test area, the system constraint. Additionally, the model is extremely sensitive to changes in operator staffing levels, an accurate reflection of many back-end operations. The model shows that the cumulative impact of these recommendations is a 41% reduction in average cycle time, a significant contribution to the overall goal.
this report may be reproduced without the express  permission of but with acknowledgment to the International Food Policy Research Institute
There is considerable interest in Peer-topeer (P2P) traffic because of its remarkable increase over the last few years. By analyzing flow measurements at the regional aggregation points of several cable operators, we are able to study its properties. It has become a large part of broadband traffic and its characteristics are different from older applications, such as the Web. It is a stable balanced traffic: the peak to valley ratio during a day is around 2 and the Inbound/Outbound traffic balance is close to one. Although P2P protocols are based on a distributed architecture, they don&apos;t show strong signs of geographical locality. A cable subscriber is not much more likely to download a file from a close region than from a far region.
This paper develops a polyhedral approach to the design, analysis, and computation of dynamic allocation indices for scheduling binary-action (engage/rest) Markovian stochastic projects which can change state when rested (restless bandits (RBs)), based on partial conservation laws (PCLs). This extends previous work by the author [J. Nino-Mora (2001): Restless bandits, partial conservation laws and indexability. Adv. Appl. Probab. 33, 76--98], where PCLs were shown to imply the optimality of index policies with a postulated structure in stochastic scheduling problems, under admissible linear objectives, and they were deployed to obtain simple sufficient conditions for the existence of Whittle&apos;s (1988) RB index (indexability), along with an adaptive-greedy index algorithm. The new contributions include: (i) we develop the polyhedral foundation of the PCL framework, based on the structural and algorithmic properties of a new polytope associated with an accessible set system (J,   (F-extended polymatroid); (ii) we present new dynamic allocation indices for RBs, motivated by an admission control model, which extend Whittle&apos;s and have a significantly increased scope; (iii) we deploy PCLs to obtain both sufficient conditions for the existence of the new indices (PCL-in-  dexability), and a new adaptive-greedy index algorithm; (iv) we interpret PCL-indexability as a form of the classic economics law of diminishing marginal returns, and characterize the index as an optimal marginal cost rate; we further solve a related optimal constrained control problem; (v) we carry out a PCL-indexability analysis of the motivating admission control model, under time-discounted and long-run average criteria; this gives, under mild conditions, a new index characterization of optimal threshold...
Software systems generally suffer from a certain fragility in the face of &quot;disturbances&quot;  such as bugs, unforeseen user input, unmodeled interactions with other software components, and  so on. A single such disturbance can make the machine on which the software is executing  hang or crash. We postulate that what is required to address this fragility is a general means of using feedback to stabilize these systems. In this paper we develop a preliminary dynamical systems model of an arbitrary iterative software process along with the conceptual framework for &quot;stabilizing&quot; it in the presence of disturbances. To keep the computational requirements of the  controllers low, randomization and approximation are used. We describe our initial attempts to  apply the model to a faulty list sorter, using feedback to improve its performance. Methods by  which software robustness can be enhanced by distributing a task between nodes each of which are  capable of selecting the &quot;best&quot; input to process are also examined, and the particular case of a  sorting system consisting of a network of partial sorters, some of which may be buggy or even  malicious, is examined.
Recent studies in the rotation of the plane of polarization of electromagnetic waves over...
The integration and coordination of different emergency service personnel is crucial to Crisis Management.
In this paper, we introduce a parametric semantics for timed controllers called  the Almost ASAP semantics. This semantics is a relaxation of the usual ASAP semantics (also  called the maximal progress semantics) which is a mathematical idealization that can not be  implemented by any physical device no matter how fast it is. On the contrary, any correct  Almost ASAP controller can be implemented by a program on a hardware if this hardware is  fast enough. We study the properties of this semantics, show how it can be analyzed using the  tool HyTech, and illustrate its practical use on examples.
We examine the formation of networks among a set of players whose payoffs depend  on the structure of the network. We focus on games where players may bargain by  promising or demanding transfer payments when forming links. We examine several  variations of the transfer/bargaining aspect of link formation. One aspect is whether  players can only make and receive transfers to other players to whom they are directly  linked, or whether they can also subsidize links that they are not directly involved  in. Another aspect is whether or not transfers related to a given link can be made  contingent on the full resulting network or only on the link itself. A final aspect  is whether or not players can pay other players to refrain from forming links. We  characterize the networks that are supported under these variations and show how each  of the above aspects is related either to accounting for a specific type of externality, or  to dealing with the combinatorial nature of network payoffs.
In previous studies we could show that linguistic word structures correlate closely with the time  course of written word production. In the present study we investigate whether there are also  correlations between the syntactic structures of phrases and the time course of their production.
We introduce ASAP2, an improved variant of the batchmeans algorithm ASAP for steady-state simulation output analysis. ASAP2 operates as follows: the batch size is progressively increased until the batch means pass the ShapiroWilk test for multivariate normality; and then ASAP2 delivers a correlation-adjusted confidence interval. The latter adjustment is based on an inverted Cornish-Fisher expansion for the classical batch means t-ratio, where the terms of the expansion are estimated via a first-order autoregressive time series model of the batch means. ASAP2 is a sequential procedure designed to deliver a confidence interval that satisfies a prespecified absolute or relative precision requirement. When used in this way, ASAP2 compares favorably to ASAP and the well-known procedures ABATCH and LBATCH with respect to close conformance to the precision requirement as well as coverage probability and mean and variance of the half-length of the final confidence interval.
The path planning is not a trivial problem of artificial intelligence. An agent has to find a path from one state (or position) to another whilst avoiding contact with obstacles. The configuration space used for representation of all agent states is usually continuous, which makes the problem even more complex. Skeletonisation is one of approaches, which &quot;discretises&quot; continuous space and reduces it to a graph search problem.
Previous approaches to implementing temporal DBMSs have assumed that a temporal DBMS must be built from scratch, employing an integrated architecture and using new temporal implementation techniques such as temporal indexes and join algorithms. However, this is a very large and time-consuming task. This paper explores approaches to implementing a temporal DBMS as a stratum on top of an existing non-temporal DBMS, rendering implementation more feasible by reusing much of the functionality of the underlying conventional DBMS. More specifically, the paper introduces three stratum meta-architectures, each with several specific architectures. Based on a new set of evaluation criteria, advantages and disadvantages of the specific architectures are identified. The paper also classifies all existing temporal DBMS implementations according to the specific architectures they employ. It is concluded that a stratum architecture is the best short, medium, and perhaps even longterm, approach to implementing a temporal DBMS.
Description logics are valuable for modeling the conceptual structures of scientific and engineering research because the underlying ontologies generally have a taxonomic core. Such structures have natural representations through semantic networks that mirror the underlying description logic graph-theoretic structures and are more comprehensible than logical notations to those developing and studying the models. This article reports experience in the development of visual language tools for description logics with the objective of making research issues, past and present, more understandable.
The ridgelet transform [6] was introduced as a sparse expansion for functions on continuous spaces that are smooth away from discontinuities along lines. In this paper, we propose an orthonormal version of the ridgelet transform for discrete and finite -size images. Our construction uses the finite Radon transform (FRAT) [11], [20] as a building block. To overcome the periodization effect of a finite transform, we introduce a novel ordering of the FRAT coefficients. We also analyze the FRAT as a frame operator and derive the exact frame bounds. The resulting finite ridgelet transform (FRIT) is invertible, nonredundant and computed via fast algorithms. Furthermore, this construction leads to a family of directional and orthonormal bases for images. Numerical results show that the FRIT is more effective than the wavelet transform in approximating and denoising images with straight edges.
Most real-world database applications contain a substantial portion of time-referenced, or temporal, data. Recent advances in temporal query languages show that such database applications could benefit substantially from builtin temporal support in the DBMS. To achieve this, temporal query representation, optimization, and processing mechanisms must be provided. This paper presents a general, algebraic foundation for query optimization that integrates  conventional and temporal query optimization and is suitable for providing temporal support both via a stand-alone temporal DBMS and via a layer on top of a conventional DBMS. By capturing duplicate removal and retention and order preservation for all queries, as well as coalescing for temporal queries, this foundation formalizes and generalizes existing approaches.
 We discuss the diverse types and roles of ontologies in web information extraction and illustrate them on a small study from the product offer domain. Attention is mainly paid to the impact of domain ontologies, presentation ontologies and terminological taxonomies. 
We propose a new methodology for &quot;soft&quot; docking unbound protein molecules (reported at the isolated state). The methodology is characterized by its simplicity and easiness of embedment in any rigid body docking process based on point complementarity. It is oriented to allow limited free but not unrealistic interpenetration of the side chains of protein surface amino acid residues. The central step to the technique is a filtering process similar to those in image processing. The methodology assists in deletion of atomic-scale details on the surface of the interacting monomers, leading to the extraction of the most characteristic flattened shape for the molecule as well as the definition of a soft layer of atoms to allow smooth interpenetration of the interacting molecules during the docking process. Although the methodology does not perform structural or conformational rearrangements in the interacting monomers, results output by the algorithm are in fair agreement with the relative position of the monomer in experimentally reported complexes. The algorithm performs especially well in cases where the complexity of the protein surfaces is high, that is in hetero dimmer complex prediction. The algorithm is oriented to play the role of a fast screening engine for proteins known to interact but for which no information other than that of the structures at the isolated state is available. Consequently the importance of the methodology will increase in structural-function studies of thousand of proteins derived from large scale genome sequencing projects being executed all around the globe  Keywords: protein-protein interaction, docking, soft docking, filtering  1 
In recent years, the &quot;Internet Multicast Backbone,&quot; or MBone, has risen from a small, research curiosity to a largescale and widely used communications infrastructure. A driving force behind this growth was the development of multipoint audio, video, and shared whiteboard conferencing applications. Because these real-time media are transmitted at a uniform rate to all of the receivers in the network, a source must either run at the bottleneck rate or overload portions of its multicast distribution tree. We overcome this limitation by moving the burden of rate adaptation from the source to the receivers with a scheme we call receiver-driven layered multicast, or RLM. In RLM, a source distributes a hierarchical signal by striping the different layers across multiple multicast groups, and receivers adjust their reception rate by simply joining and leaving multicast groups. In this paper, we describe a layered video compression algorithm which, when combined with RLM, provides a comprehensive solution for scalable multicast video transmission in heterogeneous networks. In addition to a layered representation, our coder has low complexity (admitting an efficient software implementation) and high loss resilience (admitting robust operation in loosely controlled environments like the Internet) . Even with these constraints, our hybrid DCT/wavelet-based coder exhibits good compression performance. It outperforms all publicly available Internet video codecs while maintaining comparable run-time performance. We have implemented our coder in a &quot;real&quot; application---the UCB/LBL videoconferencing tool vic. Unlike previous work on layered video compression and transmission, we have built a fully operational system that is currently being deployed on a very large scale over the MBone.
We consider the congestion-control problem in a communication network with multiple traffic sources, each modeled as a fullycontrollable stream of fluid traffic. The controlled traffic shares a common bottleneck node with high-priority cross traffic described by a Markovmodulated fluid (MMF). Each controlled source is assumed to have a unique round-trip delay. We wish to maximize a linear combination of the throughput, delay, traffic loss rate, and a fairness metric at the bottleneck node. We introduce an online sampling-based burst-level congestioncontrol scheme capable of performing effectively under rapidly-varying cross traffic by making explicit use of the provided MMF model of that variation. The control problem is posed as a finite-horizon Markov decision process and is solved heuristically using a technique called Hindsight Optimization. We provide a detailed derivation of our congestion-control algorithm based on this technique. The distinguishing feature of our scheme relative to conventional congestion-control schemes is that we exploit a stochastic model of the cross traffic. Our empirical study shows that our control scheme significantly outperforms the conventional proportionalderivative (PD) controller, achieving higher utlization, lower delay, and lower loss under reasonable fairness. The performance advantage of our scheme over the PD scheme grows as the rate variance of cross traffic increases, underscoring the effectiveness of our control scheme under variable cross traffic.
This paper presents a new approach to the problem of building a global map from laser range data, utilizing shape based object recognition techniques originally developed for tasks in computer vision. In contrast to classical approaches, the perceived environment is represented by polygonal curves (polylines), possibly containing rich shape information yet consisting of a relatively small number of vertices. The main task, besides segmentation of the raw scan point data into polylines and denoising, is to find corresponding environmental features in consecutive scans to merge the polylinedata to a global map. The correspondence problem is solved using shape similarity between the polylines. The approach does not require any odometry data and is robust to discontinuities in robot position, e.g., when the robot slips. Since higher order objects in the form of polylines and their shape similarity are present in our approach, it provides a link between the necessary low-level and the desired high-level information in robot navigation. The presented integration of spatial arrangement information, illustrates the fact that high level spatial information can be easily integrated in our framework.
In this paper we present a storage method for sets of first  order logic terms in a relational database using function symbols based  indexing method of Discrimination trees. This is an alternative method  to a published one, based on attribute indexing. This storage enables  effective implementation of several retrieval operations: unification, generalization,  instantation and variation of a given query term in the language  of first order predicate calculus. In our solution each term has  unique occurrence in the database. This is very useful when we need to  store a large set of terms that have identical many subterms.
this paper, we outline three examples of ongoing work of this type. For an introduction to factor graphs, we refer to [1] and [2]. We will use the notation of [2]
The purpose of this study was to investigate experienced secondary school teachers&apos; (N&quot;80) current and prior perceptions of their professional identity. A questionnaire was used to explore the way teachers see (and saw) themselves as subject matter experts, didactical experts, and pedagogical experts. The teachers currently see their professional identity as consisting of a combination of the distinct aspects of expertise. Most teachers&apos; current perceptions of their professional identity reportedly di!er signi&quot;cantly from their prior perceptions of this identity during their period as beginning teachers. On the basis of their current perceptions of their professional identity, &quot;ve groups of teachers could be distinguished. These groups had di!erent learning experiences throughout their careers for each aspect of expertise. Also, teachers from di!erent subject areas did not undergo the same changes in their perceptions of their professional identity. The di!erences among the groups in teachers&apos; current perceptions of professional identity were not related to contextual, experiential, and biographical factors that might inffuence these perceptions. 
this report, a novel methodology for the efficient multiplexing and transmission of MPEG4coded video signals over wireless networks will be presented and discussed. The proposed approach relies on the joint exploitation of variable-bit-rate (VBR) multicarrier code-division multiplexing (MC-CDM), together with MPEG4 coding with Fine-Grain-Scalability (FGS) in order to provide unequal error protection to the transmitted video stream. The innovative scheme proposed employs a shared bandwidth partitioned into orthogonal sub-channels in order to multiplex different layers of MPEG-4-coded signals. The highest number of subchannels (and hence an increased frequency diversity) is assigned to the lowest-bit-rate base layer and the lowest number of sub-channels is assigned to the highest bit-rate enhancement layer. In such a way, base layer information contents are more protected against channel degradations than information contained in FGS enhancement layers, which can only yield a refinement of the quality of the decoded streams. A 2GHz LEO multicast satellite transmission to mobile users has been regarded as the application testbed for the proposed method. Results achieved in terms of PSNR point out that the VBR MC-CDM technique can provide better results than a conventional MPEG-4 single-layer MC-SS transmission. In the framework of a full-digital implementation of reconfigurable multimedia transceivers, the proposed VBR MCCDM technique may be regarded as an interesting solution for reliable multimedia transmissions in mobile environments
This paper is a practical guide to building higher-order filters with single-amplifier biquadratic MOSFET--C sections. Theory, design guidelines, and measurement electronics are discussed by example of a 7th-order current-mode filter built to the specifications of a 1 DVD read channel filter. The 7th-order filter was fabricated with the double-poly 0.6-micron CMOS process by AMS. It is continuously tunable from 4.5 MHz up to 10 MHz, covers a chip area of only 0.24 mm    , and consumes 49 mW from a 3.3-V supply. The SNR at    of harmonic distortion is between 48 dB and 50 dB over the whole tuning range. The comparatively low power consumption and chip area could be achieved by using single-amplifier biquadratic building blocks implemented as MOSFET--C filters and generating the control voltage of the MOSFET resistors with an on-chip charge pump. The technique is, with a small loss of SNR, also applicable on fabrication processes where only gate-oxide capacitors are available.
This paper reviews a number of recent books related to current developments  in machine learning. Some (anticipated) trends will be sketched. These include:  a trend towards combining approaches that were hitherto regarded as distinct and  were studied by separate research communities; a trend towards a more prominent  role of representation; and a tighter integration of machine learning techniques  with techniques from areas of application such as bioinformatics. The intended  readership has some knowledge of what machine learning is about, but brief tutorial  introductions to some of the more specialist research areas will also be given.
In this work, we propose a novel scheme to minimize drift in scalable wavelet based video coding, which gives a balanced performance between compression efficiency and quality. Our drift control mechanism maintains two frame buffers in the encoder and decoder; one for the base layer and the other for the enhancement layer. Drift control is achieved by switching between these two buffers for motion compensation and prediction. In the encoder, the residues are coded using the embedded zerotree wavelet (EZW) algorithm. Our prediction is based on the enhancement layer, which inherently introduces drift in the system, if part of the enhancement layer is not available at the receiver. A measure of drift is computed based on channel information, and a threshold is set. When the measure exceeds the threshold, i.e. when drift becomes significant, we switch the prediction to be based on the base layer, which is always available to the receiver.
A continuous time random walk is a simple random walk subordinated to a renewal process, used in physics to model anomalous diffusion. In this paper we show that, when the time between renewals has infinite mean, the scaling limit is an operator Levy motion subordinated to the hitting time process of a classical stable subordinator. Density functions for the limit process solve a fractional Cauchy problem, the generalization of a fractional partial differential equation for Hamiltonian chaos. We also establish a functional limit theorem for random walks with jumps in the strict generalized domain of attraction of a full operator stable law, which is of some independent interest.  
A wide range of database applications manage information that varies over time. Many of the underlying  database schemas of these were designed using one of the several versions, with varying syntax and  semantics, of the Entity-Relationship (ER) model. In the research community as well as in industry, it is  common knowledge that the temporal aspects of the mini-world are pervasive and important, but are also  difficult to capture using the ER model. Not surprisingly, several enhancements to the ER model have  been proposed in an attempt to more naturally and elegantly support the modeling of temporal aspects of  information. Common to the existing temporally extended ER models, few or no specific requirements  to the models were given by their designers. With the
In this paper, we examine the sources of random numbers used in signal processing. We also hope to present some interesting solutions to modern application problems using random numbers and  to provide methods in which to test the integrity of random number sequences for use in a variety of applications.
The purpose of this paper is to characterize the problem of multiple levels of abstraction in simulation modeling and to develop an approach that addresses the problem. In this paper, we describe the notion of abstraction and the technical problems associated with multiple levels of abstraction, how abstractions affect different activities during the simulation modeling process, a preliminary approach for addressing the problems associated with multiple levels of abstraction, the conceptual architecture of a simulation modeling environment that implements the proposed approach, and a summary of the research on questions of abstraction in simulation.
Unlike snapshot queries in traditional  databases, the processing of continuous  queries in Data Stream Management Systems  (DSMSs) needs to satisfy user-specified QoS  requirements. In this paper, we focus on  three major QoS parameters in a DSMS environment:   processing delay, querying frequency  and loss tolerance. To minimize processing  delays, the Earliest Deadline First (EDF)  CPU scheduling policy is recommended.
Globalisation and competitive pressure urge many organisations to radically change business processes. Although this approach can provide significant benefits such as reducing costs or improving efficiency, there are substantial risks associated with it. Using simulation for modelling and analysis of business processes can reduce that risk and increase the chance for success of Business Process Re-engineering projects. This paper investigates the potential of simulation modelling to be used for modelling business processes and supports the case for a wider use of simulation techniques by the business community. Following a discussion on business process modelling methods and tools, the usability of simulation modelling for evaluating alternative business process strategies is investigated. Examples of simulation models representing business processes are presented and discussed.
The analysis of handwritten documents from the viewpoint of determining their writership has great bearing on the criminal justice system. In many cases, only a limited amount of handwriting is available and sometimes it consists of only numerals. Using a large number of handwritten numeral images extracted from about 3000 samples written by 1000 writers, a study of the individuality of numerals for identification/verification purposes was conducted. The individuality of numerals was studied using cluster analysis. Numerals discriminability was measured for writer verification. The study shows that some numerals present a higher discriminatory power and that their performances for the verification/identification tasks are very different.
This paper presents an application of corpus-based terminology extraction in interactive information retrieval. In this approach, the terminology obtained in an automatic extraction procedure is used, without any manual revision, to provide retrieval indexes and a &quot;browsing by phrases&quot; facility for document accessing in an interactive retrieval search interface. We argue that the combination of automatic terminology extraction and interactive search provides an optimal balance between controlled-vocabulary document retrieval (where thesauri are costly to acquire and maintain) and free text retrieval (where complex terms associated to domain specific concepts are largely overseen).
Although the amount of earth science data is growing rapidly, as is  the availability of high performance networks, our ability to access large  remote earth science data sets is still very limited. This is particularly  true of networks with high bandwidth delay products (BDP), such as  those between the US and Europe. Recently, several network protocols  have emerged that improve the situation and hold the promise of  being much more effective than striped TCP. (In striped TCP, data  is striped across multiple TCP streams). In this paper, we report on  experimental studies using one of these new protocols called UDT, and  compare UDT to other approaches. In addition, we consider the effectiveness  of these new protocols when reading and writing data from  disk over high BDP networks. We also consider the problem of accessing  remote data by attribute over these same networks. We show that  with the appropriate protocol, accessing data across the Atlantic can  be improved significantly. We note that the UDT protocol used here  can be deployed as an application library for earth science applications  and neither requires upgrades to existing network infrastructure, such  as routers, nor to the Linux kernels on the servers involved.
The BootCaT toolkit (Baroni and Bernardini, 2004) is a suite of perl programs implementing a procedure to bootstrap specialized corpora and terms from the web using minimal knowledge sources. In this paper, we report ongoing work in which we apply the BootCaT procedure to a Japanese corpus and term extraction task in the hotel terminology domain. The results of our experiments are very encouraging, indicating that the BootCaT procedure can be successfully applied, with relatively small modifications, to a language very different from English and the other Indo-European languages on which we tested the procedure originally.
We propose an energy-balanced allocation of a real-time application onto a single-hop cluster of homogeneous  sensor nodes connected with multiple wireless channels. An epoch-based application consisting of a set of communicating  tasks is considered. Each sensor node is equipped with discrete dynamic voltage scaling (DVS). The time and  energy costs of both computation and communication activities are considered. We propose both an Integer Linear  Programming (ILP) formulation and a polynomial time 3-phase heuristic. Our simulation results show that for small  scale problems (with ff ffff tasks), up to 5x lifetime improvement is achieved by the ILP-based approach, compared  with the baseline where no DVS is used. Also, the 3-phase heuristic achieves up to 63% of the system lifetime  obtained by the ILP-based approach. For large scale problems (with 60 - 100 tasks), up to 3.5x lifetime improvement  can be achieved by the 3-phase heuristic. We also incorporate techniques for exploring the energy-latency tradeoffs of  communication activities (such as modulation scaling), which leads to 10x lifetime improvement in our simulations.
Model-driven development (MDD) processes are increasingly being used to develop component middleware and applications for distributed real-time and embedded (DRE) systems in various domains. DRE applications are often missioncritical and have stringent quality of service (QoS) requirements, such as timeliness, predictability and scalability. MDD software techniques are well suited for validating the operation of DRE applications since they offer a higher-level of abstraction than conventional third-generation programming languages. The state-of-the-art in model-driven DRE application development is still maturing, however. For example, conventional MDD development environments for DRE application do not yet provide seamless integration of development capabilities and model checking capabilities.
A fundamental problem in multi-view 3D face modeling is the determination of the set of optimal views required for accurate 3D shape estimation for a generic face. There is no analytical solution to this problem, instead (partial) solutions require (near) exhaustive combinatorial search, hence the inherent computational difficulty. We build on our previous modeling framework which uses an efficient contour-based silhouette method and extend it by aggressive pruning of the view-sphere with view clustering and various imaging constraints. A multi-view optimization search is performed using both model-based (eigenheads) and data-driven (visual hull) methods, yielding comparable best views. These constitute the first reported set of optimal views for silhouette-based 3D face shape capture and provide useful empirical guidelines for the design of 3D face recognition systems.
Analyzing systems by means of simulation is necessarily a time consuming process. This becomes even more pronounced when models of multiple systems must be compared. In general, and even more so in today&apos;s fast-paced environment, competitive pressure does not allow for waiting on the results of a lengthy analysis. That competitive pressure also makes it more imperative that the processing performance of systems be seriously considered in the system design. Having a generic model allows one model to be applied to multiple systems in a given domain and provides a feedback mechanism to systems designers as to the operational impact of design decisions.
The concept of web services represent the next generation of architectures for interoperability between software applications based on software industry standards. Presented here is an overview of web services, a discussion of the use of web services in the context of simulation and a demonstration of the use of web services for simulation as implemented in the Microsoft .Net software development and execution framework. The paper focuses on the vital role of industry standards in the definition and implementation of web services and relates this to the opportunities and challenges for similar standards and benefits for interoperability in simulation software.
We investigate finite-time blow-up and stability of semilinear partial differential  equations of the form @w t =@t = w t +t      t , w 0 (x) = &apos;(x)  0, x 2 R+ ,  where is the generator of the standard gamma process and  &gt; 0,  2 R,  &gt; 0 are constants. We show that any initial value satisfying c 1 x     &apos;(x),  x &gt; x 0 for some positive constants x 0 ; c 1 ; a 1 , yields a non-global solution if  a 1  &lt; 1 + , or if a 1   = 1 +  and  &gt; 1. If &apos;(x)  c 2 x    , x &gt; x 0 ; where  x 0 ; c 2 ; a 2 &gt; 0, and a 2  &gt; 1 + , then the solution w t is global and satis  es  0  w t (x)  Ct    , x  0, for some constant C &gt; 0. This extends the results  previously obtained in the case of -stable generators. Systems of semilinear  PDE&apos;s with gamma generators are also considered.
We consider the problem of clustering data lying on multiple subspaces of unknown and possibly different dimensions. We show that one can represent the subspaces with a set of polynomials whose derivatives at a data point give normal vectors to the subspace associated with the data point. Since the polynomials can be estimated linearly from data, subspace clustering is reduced to classifying one point per subspace. We do so by choosing points in the data set that minimize a distance function. A basis for the complement of each subspace is then recovered by applying standard PCA to the set of derivatives (normal vectors) at those points. The final result is a new GPCA algorithm for subspace clustering based on simple linear and polynomial algebra. Our experiments show that our method outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as K-subspace and EM. We also present applications of GPCA on computer vision problems such as vanishing point detection, face clustering, and news video segmentation.
The standard design of on-line auction systems places most of the computational  load on the server and its adjacent links, resulting in a bottleneck in  the system. In this paper, we investigate the impact, in terms of the performance  of the server and its adjacent links, of introducing active nodes into the  network. The performance study of the system is done using the stochastic  process algebra formalism PEPA.
IP-networked storage protocols such as NFS and  iSCSI have become increasingly common in today  &apos;s LAN environments. In this paper, we experimentally  compare NFS and iSCSI performance  for environments with no data sharing across machines.
This paper examines two multiple ground target tracking methods. Their specificity is that they use the road network as additional prior geographical information to further refine the targets&apos; state estimation. The first method is based on belief functions theory for associating measurements to predictions as well as for determining the road segment relative to an existing target. The second method uses a Variable Structure Interacting Multiple Model method integrated in a Multiple Hypothesis Tracking framework (MHT VS-IMM). Finally both approaches are compared suggesting the possibility of using the advantages of the evidential approach inside the well established MHT framework.
A fundamental aspect in the design of overlay networks is the path length/node degree  trade-off. Previous research has shown that it is possible to achieve logarithmic path lengths  for logarithmic or even constant node degree. While nearby contacts, with nodes that have  close identifiers, ensure a connected lattice of nodes, short path lengths demand for the use  of long range contacts. In this respect, previous work exhibits limitations in scenarios where  node distribution is unbalanced: either short path length properties do not hold or may require  node degree and/or signaling to grow with respect to the virtual identification space  instead of the number of nodes (which is usually several order of magnitudes smaller).
This paper contains some transaction related patterns from my forthcoming book, Patterns in Java, Volume 3: Design Patterns for Enterprise and Distributed Applications. A transaction is a sequence of operations that change the state of an object or collection of objects in a well defined way. Transactions are useful because they satisfy constraints about what the state of an object must be before, after or during a transaction. For example, a particular type of transaction may satisfy a constraint that an attribute of an object must be greater after the transaction than it was before the transaction. Sometimes, the constraints are unrelated to the objects that the transactions operate on. For example, a transaction may be required to take place in less than a certain amount of time. The patterns in this chapter provide guidance in selecting and combining constraints for common types of transactions. Figure 1 shows how the patterns in this chapter build on each other. Composite Transaction ACID Transaction Two Phase Commit Audit Trail Figure 1: Pattern Map  The first and most fundamental pattern to read is the ACID Transaction pattern. It describes how to design transactions that never have inconsistent or unexpected outcomes. The Composite pattern describes how to compose a complex transaction from simpler transactions. The Two Phase commit pattern describes how to ensure that a composite transaction is atomic. The Audit Trail pattern describes how to maintain an historical of ACID transactions. You may notice the lack of code examples in this paper. It is the author&apos;s opinion that the patterns in this paper are too high level for concrete code examples to be useful. The application of these transaction related patterns can be readily understood at the design level. How...
There are different approaches to mobile robot navigation. Landmark-based  localization has shown to be the alternative to simple dead-reckoning, but often landmarks  are environmental specific, and recognition algorithms are computationally very  expensive. This paper presents an approach to landmark-based navigation using emergency  exit pannels and corridors as cues, without odometric information. Experiments  are carried out to verify appart each landmark identification subsystem and both behaviors  are combined together in a complete path through the environment.
In this article we focus on evolving information systems. First a delimitation of the concept of  evolution is provided, resulting in a first attempt to a general theory for such evolutions. The theory
Recent work in Bayesian classifiers has shown that a better and more flexible representation of domain knowledge results in better classification accuracy. In previous work [1], we have introduced a new type of Bayesian classifier called Case-Based Bayesian Network (CBBN)  classifiers. We have shown that CBBNs can capture finer levels of semantics than possible in traditional Bayesian Networks (BNs). Consequently, our empirical comparisons showed that CBBN classifiers have considerably improved classification accuracy over traditional BN classifiers. The basic idea behind our CBBN classifiers is to intelligently partition the training data into semantically sound clusters. A local BN classifier can then be learned from each cluster separately. Bayesian Multi-net (BMN) classifiers also try to improve classification accuracy through a simple partitioning of the data by classes. In this paper, we compare our CBBN classifiers to BMN classifiers. Our experimental results show that CBBN classifiers considerably outperform BMN classifiers. 1 
Anomalies are unusual and significant changes in a network&apos;s traffic levels, which can often span multiple links. Diagnosing anomalies is critical for both network operators and end users. It is a difficult problem because one must extract and interpret anomalous patterns from large amounts of high-dimensional, noisy data.
The average signal-to-noise ratio (SNR) of a generalized selection combining scheme, in which the  m  diversity branches (m  L,    L  is the total number of diversity branches available) with the largest instantaneous SNR&apos;s are selected and coherently combined, is derived. A Rayleigh fading channel is assumed, and a simple closed-form expression for the SNR is found which is upper bounded by the average SNR of maximal ratio combining, and lower bounded by average SNR of conventional selection combining.
In this paper, we evaluate and suggest methods to improve the performance of IEEE 802.11 based ad hoc networks from the perspective of spatial reuse. Since 802.11 employs virtual carrier sensing to reserve the medium prior to a packet transmission, the relative size of the spatial region it reserves for the impending traffic significantly affects the overall network performance. We show that the space reserved by 802.11 for a successful transmission is far from optimal and depending on the one hop distances between the sender and the receiver, we can have three scenarios with very different spatial reuse characteristics. We also introduce a new quantitative measure, the spatial reuse index, to evaluate the efficiency of the medium reservation accomplished by 802.11 virtual carrier sensing. We also propose an improved virtual carrier sensing mechanism for wireless LAN scenarios and using analysis and simulation results, show that it can significantly increase the spatial reuse and network throughput.
This paper studies the performance implications of using cryptographic controls in performance-critical systems. Full cryptographic controls beyond basic authentication are considered and experimentally validated in the concept of network file systems. This paper demonstrates that processor speeds have recently become fast enough to support cryptographic controls in many performance-critical systems. Integrity and authentication using keyed-hash and RSA as well as confidentiality using RC5 are tested. This analysis demonstrates that full cryptographic controls are feasible in a distributed network file system, by showing the performance overhead for including signature, hash and encryption algorithms on various embedded and workstation computers. The results from these experiments are used to predict the performance impact using three proposed network disk security schemes.
Introduction  Our Digial Human Memory project (Lin &amp; Hauptmann, 2002) aims to collect and index every aspect of human daily experiences in digital form. By wearing a spy camera, microphones, and a BodyMedia armband, the wearer can collect rich records in a unobtrutive fashion, and many applications can build on top of such multimodal collections. For example, digital human memory can serve as a memory prosthesis to help the wearer recall past events; the habits or anomalies of the wearer can be analyzed from digital human memory. The physiological recordings recorded by a Bodymedia armband provides complementary dimensions of the wearer&apos;s experiences, and play an important role in identifying wearer&apos;s context and activities. In this year Physiological Data Modeling Contest, we build a baseline system that models the gender and context tasks as simple binary classification problems using only unambiguous annotations. In addition, we explore two issues. First, instead of ignoring ambiguo
Team Dynamo-Pavlov of Uppsala is an effort at the Department  of Information Technology at Uppsala University in Sweden, to  establish a soccer team in the four legged league of Robocup. The core  develoment team of the project is a group of 4th year computer science  students taking a project course in the fall of 2002. In 2003 a smaller  group of students have been working with the code to compete in German  Open and Robocup 2003.
For the past two decades, fractals (e.g., the Hilbert and Peano space-filling curves) have been considered the natural method for providing a locality-preserving mapping. The idea behind a locality-preserving mapping is to map points that are nearby in the multi-dimensional space into points that are nearby in the one-dimensional space. In this paper, we argue against the use of fractals in locality-preserving mapping algorithms, and present examples with experimental evidence to show why fractals produce poor locality-preserving mappings. In addition, we propose an optimal locality-preserving mapping algorithm, termed the Spectral Locality-Preserving Mapping algorithm (Spectral LPM, for short), that makes use of the spectrum of the multi-dimensional space. We give a mathematical proof for the optimality of Spectral LPM, and also demonstrate its practical use.
This paper describes an evaluation of the Kea automatic keyphrase extraction algorithm. Tools that automatically identify keyphrases are desirable because document keyphrases have numerous applications in digital library systems, but are costly and time consuming to manually assign. Keyphrase extraction algorithms are usually evaluated by comparison to author-specified keywords, but this methodology has several well-known shortcomings. The results presented in this paper are based on subjective evaluations of the quality and appropriateness of keyphrases by human assessors, and make a number of contributions. First, they validate previous evaluations of Kea that rely on author keywords. Second, they show Kea&apos;s performance is comparable to that of similar systems that have been evaluated by human assessors. Finally, they justify the use of author keyphrases as a performance metric by showing that authors generally choose good keywords.
directories, etc---exist in both a physical (paper) and virtual (Web) form. Few approaches to knowledgment management and digital libraries fully exploit the opportunities afforded by this fact. Motivated by the goal of seamless integration of physical artifacts and their Web counterparts, we describe a large-scale case study of one aspect of this relationship. Based on a corpus of hundreds of real-world product catalogs, we measure the effectiveness of hand-held scanner /OCR devices for the task of automatically retrieving a catalog&apos;s authoritative Web counterpart (the vendor&apos;s home page). We find that, despite OCR errors, text fragments scanned from product catalogs can serve as reasonably effective queries for retrieving the Web counterparts. Furthermore, the effectiveness of the technique increases with multiple scanned text fragments. Our main technical contribution is a novel machine learning approach to adaptively merging the retrieved documents from multiple scans.
We present an application of multiple-objectives evolutionary optimization to the problem of engineering the distribution of the interdomain traffic in the Internet. We show that this practical problem requires such a heuristic due to the potential conflicting nature of the traffic engineering objectives. Furthermore, having to work on the parameter&apos;s space of the real problem makes such techniques as evolutionary optimization very easy to use. We show the successful application of our algorithm to two important problems in interdomain traffic engineering.
Recently, there has been a wide interest in using ontologies on the Web. As a basis for this, RDF Schema (RDFS) provides means to define vocabulary, structure and constraints for expressing metadata about Web resources. However, formal semantics are not provided, and the expressivity of it is not enough for full-fledged ontological modeling and reasoning. In this paper, we will show how RDFS can be extended in such a way that a full knowledge representation (KR) language can be expressed in it, thus enriching it with the required additional expressivity and the semantics of this language. We do this by describing the ontology language OIL as an extension of RDFS. An important benefit of our approach is that it ensures maximal sharing of meta-data on the Web: even partial interpretation of an OIL ontology by less semantically aware processors will yield a correct partial interpretation of the meta-data. We conclude that our method of extending is equally applicable to other KR formalisms.
In this paper we show how the resilience approach can give a generic  solution to the problems of looping and high-bandwidth output in autonomous  agents. A resilient approach to looping is for the agent to delay responding again  to a source that has recently triggered a task. A resilient approach to high-bandwith  output is for the agent to delay output when the overall &quot;noise&quot; level in the environment  is high. The conditions under which the delays are triggered may be  determined by data on past system behaviour. Our generic approach allows agents  to limit themselves, without requiring them to perform semantic analyses.
With the widespread and increasing use of data warehousing in industry, the design of effective  data warehouses and their maintenance has become a focus of attention. Independently of this,  the area of temporal databases has been an active area of research for well beyond a decade. This
Welch bound equality (WBE) signature sequences maximize the uplink sum capacity in direct-spread synchronous code division multiple access (CDMA) systems. WBE sequences have a nice interference invariance property that typically holds only when the system is fully loaded and the signature set must be redesigned and reassigned as the number of active users changes to maintain this property. An additional equiangular constraint on the signature set, however, maintains interference invariance. Finding such signatures requires imposing equiangular side constraints on an inverse eigenvalue problem. This paper presents an alternating projection algorithm that can design WBE sequences that satisfy equiangular side constraints. The proposed algorithm can be used to find Grassmannian frames as well as equiangular tight frames. Though one projection is onto a closed but non convex set, it is shown that this algorithm converges to a fixed point, and these fixed points are partially characterized.
Introduction  It has been revealed that the function of transmembrane (TM) proteins (20-30% in most genomes [1]) can be classified and identified with the information of its TM topologies, i.e., the number of TM segments (TMSs), the position of TMS and the orientation of the TMS to the membrane lipid bilayer [6]. Therefore, developing the TM topology prediction method with high reliability is critical task for the elucidation of TM protein functions. Although many TM topology prediction methods have been proposed, the prediction accuracies of these methods are still not high enough, i.e., at most 50-60% as to whole TM topology [3]. In this study, we propose a new consensus approach (ConPred elite) with reliabilities of 0.98 and 0.95 for prokaryotic and eukaryotic TM protein sequences, respectively, by combining the results from five currently used TM topology prediction methods. We applied this method to TM proteins extracted from 87 prokaryotic and 12 eukaryotic proteomes.  2 Material
this paper, we introduce a system called Papyrus for distributed data mining over commodity and high performance networks and give some preliminary experimental results about its performance. We are particularly interested in data mining over clusters of workstations, distributed clusters connected by high performance networks (super-clusters), and distributed clusters and super-clusters connected by commodity networks (meta-clusters)
This paper addresses the problem of automated design of a  computer system for an embedded application. The computer  system to be designed consists of a VLIW processor and/or a  customized systolic array, along with a cache subsystem  comprising a data cache, instruction cache and second-level  unified cache. Several algorithms for &quot;walking&quot; the design space  are described, and experimental results of custom designed  systems for two applications are presented
A novel blind initialization procedure for iterative decision feedback equalizers in block-based transmission systems is proposed and investigated. It relies on an initial stage using Regalia&apos;s blockbased Constant Modulus iterative algorithm for the blind computation of a linear equalizer; then a switch to decision feedback mode is performed. It is shown how the building blocks of the decision feedback equalizer (feedforward and feedback filters, automatic gain control and phase rotation) can be blindly estimated. Due to the unknown lag introduced by the blind linear equalizer, delay synchronization of the feedforward and feedback filters is also required. These filters are then refined over successive decision feedback iterations. This approach can also be used as a blind channel identifier for other block receiver designs, such as soft ISI cancelers and decoder-aided (i.e. turbo) equalizers. 1. 
AweSimff is a general-purpose simulation system which  takes advantage of Windowsff technology to integrate programs  and provide componentware. AweSim includes the   Visual SLAMff simulation language to build network,   subnetwork, discrete event, and continuous models. Network  models require no programming yet allow user-coded  inserts in Visual Basic or C. Discrete event and continuous  models can be created using the object-oriented technology  of Visual Basic, C or Visual C++ and can be combined  with network models. This tutorial will demonstrate the  process of using AweSim&apos;s componentware, describe examples  of user interfaces that allow integration with other   applications, and present a sample model.   1 
Configuration management is an essential...This article addresses security in configuration management systems and proposes strategies for increasing security by randomized scheduling of actions constrained by a set of precedence relations...
This paper provides a novel approach for optimal route planning making efficient use of the underlying geometrical structure. It combines classical AI exploration with computational geometry. Given a set
We study the provision of deterministic rate guarantees over single crossbar switches. Birkhoff decomposition yields a general approach for this problem, but the required complexity can be very high and the quality of service can be unsatisfactory for practical traffic sources.
developments such as the increasingly widespread acceptance of video surveillance in public places. However, the decade&apos;s most striking developments (with respect to ubiquitous computing) have undoubtedly been the emergence of the Web as a global information and service resource and the widespread adoption of digital mobile telephony, letting users experience nearly ubiquitous wireless communications.  The World Wide Web  The Web&apos;s emergence has fundamentally changed the way many people interact with computers. It has also created a culture that is substantially more amenable to the deployment of ubiquitous computing environments than that which existed when Weiser first articulated his vision.  Most obviously, the Web has created a nearly ubiquitous information and communications infrastructure. We can now access a huge wealth of knowledge and services from almost any computer, including low-power mobile devices such as smart phones and PDAs. However, the Web has had other, more subtl
Nowadays there are a lot of vector drawings available for inclusion into documents, which tend to be achieved and accessed by categories. However, to find a drawing among hundreds of thousands is not easy. While text-driven attempts at classifying image data have been recently supplemented with query-by-image content, these have been developed for bitmap-type data and cannot handle vectorial information. In this paper we present an approach to index and retrieve ClipArt images by content, using topological and geometric information automatically extracted from drawings. Additionally, we introduce a set of simplification heuristics to eliminate redundant information and useless elements.
We leverage the buffering capabilities of end-systems to achieve scalable, asynchronous delivery of streams in a peer-to-peer environment. Unlike existing cache-and-relay schemes, we propose a distributed prefetching protocol where peers prefetch and store portions of the streaming media ahead of their playout time, thus not only turning themselves to possible sources for other peers but their prefetched data can allow them to overcome the departure of their source-peer. This stands in sharp contrast to existing cache-and-relay schemes where the departure of the source-peer forces its peer children to go the original server, thus disrupting their service and increasing server and network load. Through mathematical analysis and simulations, we show the effectiveness of maintaining such asynchronous multicasts from several source-peers to other children peers, and the efficacy of prefetching in the face of peer departures. We confirm the scalability of our dPAM protocol as it is shown to significantly reduce server load.
This paper presents an overview of techniques for improving the efficiency of option pricing simulations, including quasiMonte Carlo methods, variance reduction, and methods for dealing with discretization error.
This paper presents a machine learning method to predict polyadenylation signals (PASes) in  human DNA and mRNA sequences by analysing features around them. This method consists of  three sequential steps of feature manipulation: generation, selection and integration of features. In  the first step, new features are generated using k-gram nucleotide acid or amino acid patterns. In  the second step, a number of important features are selected by an entropy-based algorithm. In the  third step, support vector machines are employed to recognize true PASes from a large number of  candidates. Our study shows that true PASes in DNA and mRNA sequences can be characterized  by different features, and also shows that both upstream and downstream sequence elements are  important for recognizing PASes from DNA sequences. We tested our method on several public  data sets as well as our own extracted data sets. In most cases, we achieved better validation  results than those reported previously on the same data sets. The important motifs observed are  highly consistent with those reported in literature.
We use a formal tool to extract Finite State Machines (FSM) based representations (lists of states and transitions) of sequential circuits described by flip-flops and gates. These complete and optimized representations helps the designer to understand the accurate behaviour of the circuit. This deep understanding is a prerequisite for any verification or test process. An example is fully presented to illustrate our method. This simple pipelined processor comes from our experience in computer architecture and digital design education. ([2])  1. 
Effects of the windowing process, widely investigated by the scientific literature for narrow--band components embedded in white noise, is not sufficiently detailed when signals are corrupted by colored noise. Such a phenomenon can  heavily affect the spectral parameters estimation of the noisy signal. In this paper effects of the windowing on the output of analog--to--digital converters with ffff topology, which present a spectrally shaped quantization noise, is analyzed. In particular, the spectral leakage of both narrow-- and wide-- band components is investigated and a criterion for choosing the most appropriate window for any given modulator resolution is given. The proposed analysis validates the use of the Hanning sequence as the optimum two term cosine window to be employed for characterizing low order ffff modulators.
We develop a probability forecasting model through a synthesis of Bayesian beliefnetwork  models and classical time-series analysis. By casting Bayesian time-series analyses  as temporal belief-network problems, weintroduce dependency models that capture richer  and more realistic models of dynamic dependencies. With richer models and associated  computational methods, we can movebeyond the rigid classical assumptions of linearityin  the relationships among variables and of normality of their probability distributions.
We proposed a method for recognizing matrices which contain abbreviation symbols, and a format for representing the structure of matrices, and reported experimental results in our paper [1]. The method consisted of 4 processes; detection of matrices, segmentation of elements, construction of networks and analysis of the matrix structure. In the paper, our work was described with a focus on the construction of networks and the analysis of the matrix structure. However, we concluded that improvements in the other two processes were very important for obtaining a high accuracy rate for recognition. In this paper, we describe the two improved processes, the detection of matrices and the segmentation of elements, and we report the experimental results.
The need for a sharable resource that can provide deep anatomical knowledge and support inference for biomedical applications has recently been the driving force in the creation of biomedical ontologies. Previous attempts at the symbolic representation of anatomical relationships necessary for such ontologies have been largely limited to general partonomy and class subsumption. We propose an ontology of anatomical relationships beyond class assignments and generic part-whole relations and illustrate the inheritance of structural attributes in the Digital Anatomist Foundational Model of Anatomy. Our purpose is to generate a symbolic model that accommodates all structural relationships and physical properties required to comprehensively and explicitly describe the physical organization of the human body.
Mayday is an architecture that combines overlay networks with lightweight packet filtering to defend against denial of service attacks. The overlay nodes perform client authentication and protocol verification, and then relay the requests to a protected server. The server is protected from outside attack by simple packet filtering rules that can be efficiently deployed even in backbone routers. Mayday generalizes
Applications that require good network performance often use parallel TCP streams and TCP modifications to improve the effectiveness of TCP. If the network bottleneck is fully utilized, this approach boosts throughput by unfairly stealing bandwidth from competing TCP streams. Improving the effectiveness of TCP is easy, but improving effectiveness while maintaining fairness is difficult. In this paper, we describe an approach we implemented that uses a long virtual round trip time in combination with parallel TCP streams to improve effectiveness on underutilized networks. Our approach prioritizes fairness at the expense of effectiveness when the network is fully utilized. We compared our approach with standard parallel TCP over a wide-area network, and found that our approach preserves effectiveness and is fairer to competing traffic than standard parallel TCP.
Seven Tones    ([13]) is a search engine specialized in linguistics and  languages. Its current database, which is stored on a single machine, contains  approximately 240,000 indexed web pages about linguistics and languages.
The aim of todays software development is to build applications by the reuse of binary components. This requires the composition of components and as special cases component enhancement as well as adaption. We demonstrate how to deal with these cases by furnishing components with a type consisting of two protocols --- a call and a use protocol. We model these protocols by finite automata and show how those reflect component enhancement and adaption. This mechanism allows for automatic adaption of components in changing environments. In order to
The South Asian countries are gradually diversifying with some inter-country variation in favor of high value commodities, namely fruits, vegetables, livestock and fisheries. Agricultural diversification is strongly influenced by price policy, infrastructure development (especially markets and roads), urbanization and technological improvements. Rainfed areas have benefited more as a result of agricultural diversification in favor of high value crops by substituting inferior coarse cereals. Agricultural diversification is also contributing to employment opportunities in agriculture and increasing exports. The need is to suitably integrate production and marketing of high value commodities through appropriate institutions. Market reforms in developing and strengthening desired institutions through required legal changes would go a long way in boosting agricultural growth, augmenting income of small farm holders and promoting exports.
An adequate natural language description of developments in a real-world scene may  be taken as a proof of `understanding what is going on&apos;. An algorithmic system which  generates natural language descriptions from video recordings of road traffc scenes may  be said to `understand&apos; its input to the extent the algorithmically generated text is acceptable  to humans judging it. A Fuzzy Metric-Temporal Horn Logic (FMTHL) provides  a formalism to represent both schematic and instantiated conceptual knowledge about  the depicted scene and its temporal development. The resulting conceptual representation  mediates in a systematic manner between the spatio-temporal geometric descriptions  extracted from video input and a module which generates natural language text. This  contribution outlines a thirty years effort to create such a `cognitive vision&apos; system, indicates  its current status, summarizes lessons learned along the way, and discusses open  problems against this background.
this report mostly focuses on the information extraction task (task II)
Hidden Markov fields (HMF), which are widely applied in various problems arising in image processing, have recently been generalized to Pairwise Markov Fields (PMF). Although the hidden process is no longer necessarily a Markov one in PMF models, they still allow one to recover it from observed data. We propose in this paper two original methods of parameter estimation in PMF, based on general Stochastic Gradient (SG) and Iterative Conditional Estimation (ICE) principles, respectively. Some experiments concerning unsupervised image segmentation based on Bayesian Maximum Posterior Mode (MPM) are also presented.
This article discusses  the problems and  proposes a top-down  approach to  overcome some of  the problems. A  combined yo-yo  approach aims to  exploit both  strategies&apos; benefits
Providing service differentiation in wireless networks has attracted much attention in recent research. Existing studies so far have focused on the design of differentiated media access algorithms. Some QoS metrics, such as queueing delay can not be completely addressed by these approaches. Moreover, without a formalized service differentiation goal that quantifies the outcome of differentiation, the performance of most approaches fluctuates, especially in short time-scales. This paper addresses above problems by introducing the concept of proportional service differentiation, to the domain of wireless network and focuses on providing proportional delay differentiation in wireless LANs. Due to the unique characteristic of distributed medium sharing, the 1 scheduling algorithm employed in wireline networks can not be applied directly to the context of wireless LANs. We argue that delay differentiation in wireless LAN can only be achieved through a joint packet scheduling at the network layer and distributed coordination at the MAC layer. Therefore, we present a cross-layer waiting time priority scheduling (CWTP) algorithm. CWTP consists of two tiers: an intra-node WTP scheduler at the network layer and an inter-node distributed coordination function at the MAC layer. These two tiers coordinate via a mapping function which maps the normalized waiting time at the network layer to the backoff time at the MAC layer. Two mapping schemes, namely linear mapping and piecewise linear mapping, are presented and evaluated in this paper. Extensive simulation results show that the CWTP algorithm can effectively achieve proportional delay differentiation in wireless LANs.
Although many real-world stochastic planning problems are more naturally formulated by hybrid models with both discrete and continuous variables, current state-of-the-art methods cannot adequately address these problems. We present the first framework that can exploit problem structure for modeling and solving hybrid problems efficiently. We formulate these problems as hybrid Markov decision processes (MDPs with continuous and discrete state and action variables), which we assume can be represented in a factored way using a hybrid dynamic Bayesian network (hybrid DBN). This formulation also allows us to apply our methods to collaborative multiagent settings. We present a new linear program approximation method that exploits the structure of the hybrid MDP and lets us compute approximate value functions more efficiently. In particular, we describe a new factored discretization of continuous variables that avoids the exponential blow-up of traditional approaches. We provide theoretical bounds on the quality of such an approximation and on its scale-up potential. We support our theoretical arguments with experiments on a set of control problems with up to 28-dimensional continuous state space and 22-dimensional action space.
this paper for enabling the healthcare system to improve its capability, is to unbind the large scale and complex tasks, so efficient and effective organizations can be formed around these distinct tasks. Specifically we argue for two very different systems: an efficient system to deal with health issues that affect entire populations (and that can be made efficient on a large scale) and a system to address the complexities of individual medical care in an effective and error-free way. By separating simple, large scale &quot;health care&quot; from complex, individualized &quot;medical care&quot;, we relieve physicians of tasks that can be addressed with a much higher efficiency, enabling them to focus their attention on the complex tasks for which they are uniquely trained. Not only does this create a more cost-effective health care system but it also allows for a more effective and error-free medical system
Secure systems are best built on top of a small trusted operating system: The smaller the operating system, the easier it can be assured or verified for correctness. In this
Z. A fractional advection--dispersion equation ADE is a generalization of the classical ADE in which the second-order derivative is replaced with a fractional-order derivative. In contrast to the classical ADE, the fractional ADE has solutions that resemble the highly skewed and heavy-tailed breakthrough curves observed in field and laboratory studies. These solutions, known as a-stable distributions, are the result of a generalized central limit theorem which describes the behavior of sums of finite or infinite-variance random variables. We use this limit theorem in a model which sums the length of particle jumps during their random walk through a heterogeneous porous medium. If the length of solute particle jumps is not constrained to a representative elementary Z. volume REV , dispersive flux is proportional to a fractional derivative. The nature of fractional derivatives is readily visualized and their parameters are based on physical properties that are measurable. When a fractional Fick&apos;s law replaces the classical Fick&apos;s law in an Eulerian evaluation of solute transport in a porous medium, the result is a fractional ADE. Fractional ADEs are ergodic equations since they occur when a generalized central limit theorem is employed. q 2001 Elsevier Science B.V. All rights reserved.
We introduce channel sequence types to study finitary polymorphism in the context of mobile processes modelled in the ff-calculus. We associate to each channel a set of exchange types, and we require that output processes send values of one of those types, and input processes accept values of all the types in the set. Our type assignment system enjoys subject reduction and guarantees the absence of communication errors. We give several examples of polymorphism, and we encode the ff-calculus with the strict intersection type discipline.
This paper describes the Visual Simulation Environment (VSE) software product. VSE has been developed under $1.3 million research funding, primarily from the U.S. Navy, for over a decade. It enables discrete-event, generalpurpose, object-oriented, picture-based, component-based, visual simulation model development and execution. This advanced environment can be used for solving complex problems in areas such as air traffic control and space systems, business process reengineering and workflows, complex system design evaluation, computer and communication networks, computer performance evaluation, education and training, health care systems, manufacturing systems, military/combat systems, satellite and wireless communications systems, service systems, supply chain management, and transportation systems.
A description of optimal sequences for direct-spread code division multiple access is a byproduct of recent characterizations of the sum capacity. This papers restates the sequence design problem as an inverse singular value problem and shows that it can be solved with finite-step algorithms from matrix analysis. Relevant algorithms are reviewed and a new one-sided construction is proposed that obtains the sequences directly instead of computing the Gram matrix of the optimal signatures. I. 
As we approach 100nm technology the interconnect issues are becoming one of the main concerns in the testing of gigahertz system-onchips. Voltage distortion (noise) and delay violations (skew) contribute to the signal integrity loss and ultimately functional error, performance degradation and reliability problems. In this paper, we first define a model for integrity faults on the high-speed interconnects. Then, we present a BIST-based test methodology that includes two special cells to detect and measure noise and skew occurring on the interconnects of the gigahertz system-on-chips. Using an inexpensive test architecture the integrity information accumulated by these special cells can be scanned out for final test and reliability analysis.
The Internet has fostered an unconventional and powerful style of collaboration: &quot;wiki&quot; web sites, where every visitor has the power to become an editor. In this paper we investigate the dynamics of Wikipedia, a prominent, thriving wiki. We make three contributions. First, we introduce a new exploratory data analysis tool, the history flow visualization, which is effective in revealing patterns within the wiki context and which we believe will be useful in other collaborative situations as well. Second, we discuss several collaboration patterns highlighted by this visualization tool and corroborate them with statistical analysis. Third, we discuss the implications of these patterns for the design and governance of online collaborative social spaces. We focus on the relevance of authorship, the value of community surveillance in ameliorating antisocial behavior, and how authors with competing perspectives negotiate their differences.
The overall number of nearest neighbors in bounded distance decoding (BDD) algorithms is given by N o;eff = No + NBDD ; where NBDD denotes the number of additional, noncodeword, neighbors that are generated during the (suboptimal) decoding process. We identify and enumerate the nearest neighbors associated with the original Generalized Minimum Distance (GMD) and Chase decoding algorithms. After careful examination of the decision regions of these algorithms, we derive an approximated probability ratio between the error contribution of a noncodeword neighbor (one of NBDD points) and a codeword nearest neighbor. For Chase Algorithm 1 it is shown that the contribution to error probability of a noncodeword nearest neighbor is a factor of 2  d01  less than the contribution of a codeword, while for Chase Algorithm 2 the factor is  2  dd=2e01  , d being the minimum Hamming distance of the code. For Chase Algorithm 3 and GMD, a recursive procedure for calculating this ratio, which turns out to be nonexponential in d, is presented. This procedure can also be used for specifically identifying the error patterns associated with Chase Algorithm 3 and GMD. Utilizing the probability ratio, we propose an improved approximated upper bound on the probability of error based on the union bound approach. Simulation results are given to demonstrate and support the analytical derivations.
igin, these anastomoses coil extensively over 200--300 m, before re-anastomosing with neighbouring vessels to form progressively larger secondary arteries (Olson, 1996). In the skipjack tuna Katsuwonis pelamis and the Atlantic cod Gadus morhua, the SCS forms capillary beds (Dewar et al., 1994; Burne, 1929), which are assumed to be typical of water breathing teleosts (Vogel, 1985a), before draining into the primary venous system. However, in Salaria pavo (prev. Blennius) and Zosterisessor ophiocephalus, it fails to do so (Lahnsteiner et al., 1990).  The distribution and volume of the SCS has been widely discussed. To date it has been shown that secondary vessels supply secondary capillary beds in the body surface, the fins, the buccal cavity, the pharynx and the peritoneum, and it may 591 The Journal of Experimental Biology 206, 591-599   2003 The Company of Biologists Ltd  doi:10.1242/jeb.00113  The volume of the primary (PCS) and secondary (SCS) circulatory system in the Atlantic cod 
Keywords: data mining, predictive modeling, data interchange formats, XML, SGML, ensemble learning, partitioned learning, distributed learning We introduce a markup language based upon XML for working with the predictive models produced by data mining systems. The language is called the Predictive Model Markup Language (PMML) and can be used to define predictive models and ensembles of predictive models. It provides a flexible mechanism for defining schema for predictive models and supports model selection and model averaging involving multiple predictive models. It has proved useful for applications requiring ensemble learning, partitioned learning, and distributed learning. In addition, it facilitates moving predictive models across applications and systems.
This paper presents a stochastic model of the lymphocyte recruitment in inflammed  brain microvessels. The framework used is based on stochastic process algebras for  mobile systems. The automatic tool used in the simulation is the BioSpi. We  compare our approach with classical hydrodinamical specifications
In this paper, we address the problem of dynamic allocation of storage  bandwidth to application classes so as to meet their response time requirements.
A key problem in Optical Burst Switching (OBS) is to schedule as many bursts as possible on wavelength channels so that the throughput is maximized and the burst loss is minimized. Currently, most of the research on OBS (e.g., burst scheduling and assembly algorithms) has been concentrated on reducing burst loss in an &quot;average-case&quot; sense. Little effort has been devoted to understanding the worst-case performance. Since OBS itself is an open-loop control system, it may often exhibit a worst-case behavior when adversely synchronized, thus a poor worst-case performance can lead to an unacceptable system-wide performance. In this paper, we use competitive analysis to analyze the worstcase performance of a large set of scheduling algorithms, called best-effort online scheduling algorithms, for OBS networks, and establish a number of interesting upper and lower bounds on the performance of such algorithms. Our analysis shows that the performance of any best-effort online algorithm is closely related to a few factors, such as the range of offset time, burst length ratio, scheduling algorithm, and number of data channels. A surprising discovery is that the worst-case performance of any best-effort online scheduling algorithm is primarily determined by the maximum to minimum burst length ratio, followed by the range of offset time. Furthermore, if all bursts have the same burst length and offset time, all best-effort online scheduling algorithms generate the same optimal solution, regardless how different they may look like. Our analysis can also be extended to some non-besteffort online scheduling algorithms, such as the well-known Horizon algorithm, and establish similar bounds. Based on the analytic results, we give guidelines for several widely discussed OBS problems, inclu...
One of the fundamental problems in distributed computing is how to efficiently perform routing in a faulty network in which each link fails with some probability. This paper investigates how big the failure probability can be, before the capability to efficiently find a path in the network is lost. Our main results show tight upper and lower bounds for the failure probability which permits routing, both for the hypercube and for the d-dimensional mesh. We use tools from percolation theory to show that in the d-dimensional mesh, once a giant component appears --- efficient routing is possible. A different behavior is observed when the hypercube is considered. In the hypercube there is a range of failure probabilities in which short paths exist with high probability, yet finding them must involve querying essentially the entire network. Thus the routing complexity of the hypercube shows an asymptotic phase transition. The critical probability with respect to routing complexity lies in a different location then that of the critical probability with respect to connectivity. Finally we show that an oracle access to links (as opposed to local routing) may reduce significantly the complexity of the routing problem. We demonstrate this fact by providing tight upper and lower bounds for the complexity of routing in the random graph G n,p .
Many applications, including web transfers, software distribution, video-on-demand, and peer-to-peer data downloads, require the retrieval of structured documents consisting of multiple components like images, video, and text. Large systems using these applications may be made more scalable by using efficient data distribution techniques like multicast, and by enabling clients to retrieve data from multiple servers in parallel.
With the proliferation of Internet services, many solutions have emerged to provide Quality-of-Service (QoS) guarantees when the demands for the hosted services exceed the server&apos;s capacity. In this paper, we take an analytical approach to answering key questions in the design and performance of application-level QoS techniques, especially those that are based on the multi-threading or multi-processing abstraction. Key to our analysis is the integration of the effects of concurrency into the interactions between multi-threaded services. To this end, we extend traditional time-sharing models to develop the multi-threaded round-robin (MTRR) servers, a more accurate model of operation of typical multi-threaded Internet services. For this model, we first develop powerful, yet computationallyefficient, mathematical relationships that describe the performance (in terms of throughput and response time) of multithreaded services. We then apply optimization techniques to derive the optimal allocation of threads given specific QoS objective functions. Using realistic workloads on a typical web server, we show the efficacy and accuracy of the proposed new methodology.
We propose to modify a conventional single-chip multicore so that a sequential program can migrate from one core to another automatically during execution. The goal of execution migration is to take advantage of the overall onchip cache capacity. We introduce the affinity algorithm, a method for distributing cache lines automatically on several caches. We show that on working-sets exhibiting a property called &quot;splittability&quot;, it is possible to trade cache misses for migrations. Our experimental results indicate that the proposed method has a potential for improving the performance of certain sequential programs, without degrading significantly the performance of others.
Griffn, Jaggard, and Ramachandran introduced in [4] a framework for understanding the design principles of path-vector protocols such as the Border Gateway Protocol (BGP), which is used for inter-domain routing on the Internet. They described as an application of their framework a study of Hierarchical-BGP-like systems where routing at a node is determined by the relationship with the next-hop node on a path (e.g., an ISPpeering relationship) and some additional scoping rules (e.g., the use of backup routes). These systems are called class-based path-vector systems. The robustness of these systems depends on the presence of a global constraint on the system, but an adequate constraint has not yet been given. In this paper, we give the best-known suffcient constraint that guarantees robust convergence. We show how to generate this constraint from the design specification of the path-vector system. We also give centralized and distributed algorithms to enforce this constraint, discuss applications of these algorithms, and compare them to algorithms given in previous work on path-vector protocol design.
CODEX (COrnell Data EXchange) stores secrets for subsequent access by authorized clients. It also is a vehicle  for exploring the generality of a relatively new approach to  building distributed services that are both fault-tolerant and attack-tolerant. Elements of that approach include: embracing the asynchronous (rather than synchronous) model of computation, use of Byzantine quorum systems for storing state, and  employing proactive secret sharing with threshold cryptography for implementing confidentiality and authentication of service responses. Besides explaining the CODEX protocols, experiments to measure their performance are discussed.
Predicting the secondary structure of RNA molecules from the knowledge  of the primary structure (the sequence of bases) is still a challenging  task. There are algorithms that provide good results e.g. based on  the search for an energetic optimal configuration. However the output  of such algorithms does not always give the real folding of the molecule  and therefore a feature to judge the reliability of the prediction would be  appreciated. In this paper we present results on the expected structural  behavior of LSU rRNA derived using a stochastic context-free grammar  and generating functions. We show how these results can be used to  judge the predictions made for LSU rRNA by any algorithm. In this  way it will be possible to identify those predictions which are close to  the natural folding of the molecule with a probability of 97% of success.
We propose a model and an algorithm to perform exact power estimation taking  into account all temporal and spatial correlations of the input signals. The proposed  methodology is able to accurately model temporal and spatial correlations at the logic  level, with the input signal correlations being specified at the word level using a simple  but effective formulation.
We study the nature of the relationship between performance measures and privacy guarantees in the case study of an adaptive protocol for the secure transmission of real-time audio over the Internet. The analysis is conducted on a...
A number of network simulators are now capable of simulating systems with millions of devices, at the IP packet level. With this ability comes a need for realistic network descriptions of commensurate size. This paper describes our effort to build a detailed model of the U.S. Internet backbone based on measurements taken from a variety of mapping sources and tools. We identify key attributes of a network design that are needed to use the model in a simulation, describe which components are available and which must be modeled, and discuss the pros and cons of this approach as compared to synthetic generation. As for attributes that we have to model, we also briefly discuss some measurement efforts that can potentially provide the missing pieces, and thus improve the fidelity of the model. Finally, we describe the resulting network model of the U.S. Internet backbone, which is being made publicly available. 1 
We investigate two important, common fluid flow patterns from computational fluid dynamics (CFD) simulations, namely, swirl and tumble motion typical of automotive engines. We study and visualize swirl and tumble flow using three different flow visualization techniques: direct, geometric, and texture-based. When illustrating these methods side-by-side, we describe the relative strengths and weaknesses of each approach within a specific spatial dimension and across multiple spatial dimensions typical of an engineer &apos;s analysis. Our study is focused on steady-state flow. Based on this investigation we offer perspectives on where and when these techniques are best applied in order to visualize the behavior of swirl and tumble motion.
This paper presents a new model of agency, called the KGP (Knowledge, Goals and Plan) model. This draws from the classic BDI model and proposes a hierarchical agent architecture with a highly modular structure that synthesises various reasoning and sensing capabilities of the agent in an open and dynamic environment. The novel features of the model include: its innovative use of Computational Logic (CL) in a way that facilitates both the formal analysis of the model and its computational realisability directly from the high-level specification of the agents (a first prototype for the development of KGP agents exists, based upon a correct computational counterpart of the model), the modular separation of concerns and flexibility afforded by the model in designing heterogeneous agents and in developing independently the various components of an agent, and the declarative agent control provided through a context-sensitive cycle CL theory component that regulates the agent&apos;s operational behaviour, according to the current circumstances of operation, thus breaking away from the conventional one-size-fits-all control of operation.
Recently, Milner and Moller have presented several decomposition results for processes. Inspired by these, we investigate decomposition techniques for the verification of parallel systems. In particular, we consider those of the form       q j (I) where p i and q j are (finite) state systems. We provide a decomposition procedure for all p i and q j and give criteria that must be checked on the decomposed processes to see whether (I) does or does not hold. We analyse the complexity of our procedure and show that it is polynomial in n, m and the sizes of p i  and q j if there is no communication. We also show that with communication the verification of (I) is co-NP hard, which makes it very unlikely that a polynomial complexity bound exists. But by applying our decomposition technique to Milner&apos;s cyclic scheduler we show that verification can become polynomial in space and time for practical examples, where standard techniques are exponential. Note: The authors are supported by the European Communities under ESPRIT Basic Research Action 3006 (CONCUR).
This paper describes a semantic portal on the domain of International  Affairs. This application is an integration of several technologies in the field of  the Semantic Web in a complex project. We describe an approach, tools and  techniques that allow building a semantic portal, where access is based on the  meaning of concepts and relations of the International Affairs domain. The  approach comprises an automatic ontology-based annotator, a semantic search  engine with a natural language interface, a web publication tool allowing  semantic navigation, and a 3D visualization component. The portal is being  deployed in the Royal Institute Elcano    (Real Instituto Elcano) in Spain, which  is a prestigious independent political institute whose mission is to comment on  the political situation in the world focusing on its relation to Spain. As part of  its dissemination strategy it operates a public website. The online content can  be accessed by navigating through categories or by a keyword-based, full text  search engine. The work described in this paper aims at improving access to the  content. The semantic portal is currently being tested by the Institute.
Although the syntax and semantics of mainstream agent content languages are based on those of predicate logic, the popularity of the Java programming language, the availability of various free Java-based agent development toolkits and the use of frame-based ontology modelling languages have meant that many developers of  multi-agent systems are accustomed to conceptualising their problem domain in terms of classes and objects.
Techniques for automatic query expansion from top retrieved documents have recently shown promise for improving retrieval effectiveness on large collections but there is still a lack of systematic evaluation and comparative studies. In this paper we focus on term-scoring methods based on the differences between the distribution of terms in (pseudo-)relevant documents and the distribution of terms in all documents, seen as a complement or an alternative to more conventional techniques. We show that when such distributional methods are used to select expansion terms within Rocchio&apos;s classical reweighting scheme, the overall performance is not likely to improve. However, we also show that when the same distributional methods are used to both select and weight expansion terms the retrieval effectiveness may considerably improve. We then argue, based on their variation in performance on individual queries, that the set of ranked terms suggested by individual distributional methods can be combined to further improve mean performance, by analogy with ensembling classifiers, and present experimental evidence supporting this view. Taken together, our experiments show that with automatic query expansion it is possible to  achieve performance gains as high as 21.34% over non-expanded query (for non-interpolated average precision). We also discuss the effect that the main parameters involved in automatic query expansion, such as query difficulty, number of selected documents, and number of selected terms, have on retrieval effectiveness.
Despite their benefits, programmers rarely use formal specifications, because they are difficult to write and they require an up front investment in time. To address these issues, we present a tool that helps programmers write and debug algebraic specifications. Given an algebraic specification, our tool instantiates a prototype that can be used just like any regular Java class. The tool can also modify an existing application to use the prototype generated by the interpreter instead of a hand-coded implementation. The tool improves the usability of algebraic specifications in the following ways: (i) A programmer can “run ” an algebraic specification to study its behavior. The tool reports in which way a specification is incomplete for a client application. (ii) The tool can check whether a specification and a hand-coded implementation behave the same for a particular run of a client application. (iii) A prototype can be used when a hand-coded implementation is not yet available. Two case studies demonstrate how to use the tool. 1.
The Joint Warfare System (JWARS) is being equipped with a Commander Model (CM) to perform situation assessment and Course of Action (COA) selection, and a Commander Behavior Model (CBM) to bias decisions with a commander&apos;s leadership style. The CM is a hybrid artificial intelligence system that models doctrine through the use of fuzzy rule sets, together with a tree-based lookahead algorithm for the strategy. The CBM employs behaviorbased fuzzy rule sets to augment the CM in assessing the situation, and in biasing the COA selection criteria. Extending from Myers-Briggs personality traits, the CBM links personality traits to military attitudes, consequences and values. Employing the fuzzy rule sets, the resulting sets of values are combined to select a specific COA with an auditable trail. Users will have the ability to modify both the input parameters and the underlying rules. The CM/CBM is applicable to decisions at multiple echelons.
Multitrajectory Simulation allows random events in a simulation to generate multiple trajectories. Management techniques have been developed to manage the choices of trajectories to be continued as combinatorial explosion and limited resources prevents continuing all of them. One of the seemingly most promising methods used trajectory probability as a criterion, so that higher probability trajectories were preferentially continued, resulting in a more even distribution of (surviving) trajectory probabilities, and better than stochastic approximation to a reference outcome. It was also found that this management technique introduced a failed ergodicity assumption. The higher and lower probability trajectories behave differently to a significant extent. The effect is to limit the number of trajectories which can usefully be applied to the problem, such that additional runs would fail to converge further toward the definitive reference outcome set. This may be a useful model for understanding other simulation modeling limitations.
Some recent works have shown that under an autoregressive constraint on the input signal, least-squares equationerror methods provide stable models of the estimated transfer function. Here we present an alternative proof of this fact which allows to increase the order of the autoregressive input by one, for both the monic and unit-norm approaches.
This paper presents an analysis of increased diversity in genetic programming. A selection strategy based on genetic lineages is used to increase genetic diversity. A genetic lineage is defined as the path from an individual to individuals which were created from its genetic material. The method is applied to three problem domains: Artificial Ant, Even-5-Parity and symbolic regression of the Binomial-3 function. We examine how increased diversity affects problems differently and draw conclusions about the types of diversity which are more important for each problem. Results indicate that diversity in the Ant problem helps to overcome deception, while elitism in combination with diversity is likely to benefit the Parity and regression problems.
This paper explores a statistical basis for a process often described in computer vision: image segmentation by region  merging following a particular order in the choice of regions. We exhibit a particular blend of algorithmics and statistics whose  segmentation error is, as we show, limited from both the qualitative and quantitative standpoints. This approach can be efficiently  approximated in linear time/space, leading to a fast segmentation algorithm tailored to processing images described using most  common numerical pixel attribute spaces. The conceptual simplicity of the approach makes it simple to modify and cope with hard  noise corruption, handle occlusion, authorize the control of the segmentation scale, and process unconventional data such as spherical  images. Experiments on gray-level and color images, obtained with a short readily available C-code, display the quality of the  segmentations obtained.
It has been known for some time that proportional output feedback  will stabilize certain classes of linear time-invariant systems under an  adaptation mechanism that drives the feedback gain suffciently high. More  recently, it was demonstrated that discrete implementations of the high-gain  adaptive controller also require adaptation of the sampling rate. In this paper,  we use recent advances in the mathematical field of dynamic equations on time  scales to unify the discrete and continuous versions of the high-gain adaptive  controller. A novel proof method is presented based on time scales, as is a  brief tutorial on the subject of time scales.
We present a method for feature construction  and selection that finds a minimal set of conjunctive  features that are appropriate to perform  the classification task. For problems where this  bias is appropriate, the method outperforms other  constructive induction algorithms and is able to  achieve higher classification accuracy. The application  of the method in the search for minimal  multi-level boolean expressions is presented and  analyzed with the help of some examples.
The complexities and costs associated with preserving the nation&apos;s bridge infrastructure demand innovative approaches to analysis of data and prediction of future bridge conditions. Several Bridge Management systems (BMS) have come into existence following the ISTEA act of 1991. The policy analysis module of BMS systems developed is restricted to analytical methods. With the availability of modern infrastructure, realistic simulation models are being developed in several fields. This leads to the question of whether reasonably realistic and practical discrete event simulation (DES) based policy analysis tools can be developed ? A DES model was developed for the Salem district of Virginia using a simulation language, STROBOSCOPE. This simulation model can be used to simulate the bridge network behavior under different policies and observe the impact on the health of the network making it a useful tool for decision-making. The tool enables the formulation and testing of different bridge maintenance policies.
In this paper, we address the problem of protecting the underlying  attribute values when sharing data for clustering. The challenge is how  to meet privacy requirements and guarantee valid clustering results as well.
A Bayesian blackboard is just a conventional, knowledge-based blackboard system in which knowledge sources modify Bayesian networks on the blackboard. As an architecture for intelligence analysis and data fusion this has many advantages: The blackboard is a shared workspace or &quot;corporate memory&quot; for collaborating analysts; analyses can be developed over long periods of time with information that arrives in dribs and drabs; the computers contribution to analysis can range from data-driven statistical algorithms up to domain-specific, knowledge-based inference; and perhaps most important, the control of intelligence-gathering in the world and inference on the blackboard can be rational, that is, grounded in probability and utility theory. Our Bayesian blackboard architecture, called AIID, serves both as a prototype system for intelligence analysis and as a laboratory for testing mathematical models of the economics of intelligence analysis.
Small scale software developments need specific low cost and lowoverhead methods and  tools to deliver quality products within tight time and budget constraints. This is particularly  true of testing, because of its cost and impact on final product reliability. We propose  a lightweight approachtoembed tests into components, making them self testable. We also  propose a method to evaluate testing efficiency, based on mutation techniques, which ultimately  provides an estimation of a component&apos;s quality. This allows the software developer  to consciously trade reliability for resources. Our methodology has been implemented in the  Eiffel, Java, C++ and Perl languages. The Java implementation, built on top of iContract, is  outlined here.
Animals and robots perceiving and acting in a world require an ontology that accommodates entities, processes, states of affairs, etc., in their environment. If the perceived environment includes information-processing systems, the ontology should reflect that. Scientists studying such systems need an ontology that includes the first-order ontology characterising physical phenomena, the second-order ontology characterising perceivers of physical phenomena, and a (recursive) third order ontology characterising perceivers of perceivers, including introspectors. We argue that second- and third-order ontologies refer to contents of virtual machines and examine requirements for scientific investigation of combined virtual and physical machines, such as animals and robots. We show how the CogAff architecture schema, combining reactive, deliberative, and meta-management categories, provides a first draft schematic third-order ontology for describing a wide range of natural and artificial agents. Many previously proposed architectures use only a subset of CogAff, including subsumption architectures, contention-scheduling systems, architectures with `executive functions&apos; and a variety of types of `Omega&apos; architectures.
Preprocessing is an often used approach for solving hard instances of propositional satisfiability (SAT). Preprocessing can be used for reducing the number of variables and for drastically modifying the set of clauses, either by eliminating irrelevant clauses or by inferring new clauses. Over the years, a large number of formula manipulation techniques has been proposed, that in some situations have allowed solving instances not otherwise solvable with stateof -the-art SAT solvers. This paper proposes probing-based preprocessing, an integrated approach for preprocessing propositional formulas, that for the first time integrates in a single algorithm most of the existing formula manipulation techniques. Moreover, the new unified framework can be used to develop new techniques. Preliminary experimental results illustrate that probing-based preprocessing can be effectively used as a preprocessing tool in state-of-theart SAT solvers.
In this paper we propose a process and service oriented framework, which offers a structural and conceptual orientation in the field of electronic payment. It renders possible an integral view on electronic payment that goes beyond the frame of an individual system. To do this, we have generalized the systems-oriented approaches to a phase-oriented payment model. Using this model, requirements and supporting services for electronic payment can be sorted systematically and described from both the customers&apos; and the merchants&apos; viewpoint. Providing integrated payment processes and services is proving to be a difficult task. With this paper we would like to outline the necessity for a Payment Service Provider to act as a mediator for suppliers and users of electronic payment systems.
We consider the problem of finding the optimal pair of string patterns for discriminating between two sets of strings, i.e. finding the pair of patterns that is best with respect to some appropriate scoring function that gives higher scores to pattern pairs which occur more in the strings of one set, but less in the other. We present an O(N&amp;sup2;) time algorithm for finding the optimal pair of substring patterns, where N is the total length of the strings. The algorithm looks for all possible Boolean combination of the patterns, e.g. patterns of the form p &amp;and; &amp;not;q, which indicates that the pattern pair is considered to match a given string s, if p occurs in s, AND q does NOT occur in s. The same algorithm can be applied to a variant of the problem where we are given a single set of sequences along with a numeric attribute assigned to each sequence, and the problem is to find the optimal pattern pair whose occurrence in the sequences is correlated with this numeric attribute. An effcient implementation based on suffix arrays is presented, and the algorithm is applied to several nucleotide sequence datasets of moderate size, combined with microarray gene expression data, aiming to find regulatory elements that cooperate, complement, or compete with each other in enhancing and/or silencing certain genomic functions.
Motivation: Is protein secondary structure primarily determined by local interactions between residues closely spaced along the amino acid backbone or by non-local tertiary interactions ? To answer this question, we measure the entropy densities of primary and secondary structure sequences, and the local inter-sequence mutual information density.
A three-dimensional model of diffusion limited coral growth  is introduced. As opposed to previous models, in this model we take a  &quot;polyp oriented&quot; approach. Here, coral morphogenesis is the result of the  collective behaviour of the individual coral polyps. In the polyp oriented  model, branching occurs spontaneously, as opposed to previous models  in which an explicit rule was responsible for branching. We discuss the  mechanism of branching in our model. Also, the effects of polyp spacing  on the coral morphology are studied.
This paper presents our work on supporting flexible query evaluation over large distributed, heterogeneous, and autonomous sources. Flexibility means that the query evaluation process can be configured according to application contextspecific, resources constraints and also can interact with its execution environment.
There are many useful observable characteristics of the state of a tracked object. These characteristics could include normalized size, normalized speed, normalized direction, object color, position, and object shape among other characteristics. Although these characteristics are by no means completely independent of each other, it is desirable to determine a separate, compact description of each of each of these aspects. Using this compact factored description, different aspects of individual sequences can be estimated and described without overwhelming computational or storage costs. In this work, we describe Factored Latent Analysis (FLA) and its application to deriving factored models for segmenting sequences in each of K separate characteristics. This method exploits temporally local statistics within each of the latent aspects and their interdependencies to derive a model that allows segmentation of each of the observed characteristics. This method is data driven and unsupervised. Activity classification results for multiple challenging environments are shown.
Migrating applications from conventional to temporal database management technology has received  scant mention in the research literature. This paper formally defines three increasingly restrictive notions  of upward compatibility which capture properties of a temporal SQL with respect to conventional SQL  that, when satisfied, provide for a smooth migration of legacy applications to a temporal system. The  notions of upward compatibility dictate the semantics of conventional SQL statements and constrain the  semantics of extensions to these statements. The paper evaluates the seven extant temporal extensions to  SQL, all of which are shown to complicate migration through design decisions that violate one or more  of these notions. We then outline how SQL--92 can be systematically extended to become a temporal  query language that satisfies all three notions.
Self-organizing large amounts of textual data in accordance to some topics structure is an increasingly important application of clustering. Adaptive Resonance Theory (ART) neural networks possess several interesting properties that make them appealing in this area. Although ART has been used in several research works as a text clustering tool, the level of quality of the resulting document clusters has not been clearly established yet. In this paper, we present experimental results with binary ART that address this issue by determining how close clustering quality is to an upper bound on clustering quality.
We describe two modifications to the FreeBSD 4.6 NFS server to increase read throughput by improving the read-ahead heuristic to deal with reordered requests and stride access patterns. We show that for some stride access patterns, our new heuristics improve end-to-end NFS throughput by nearly a factor of two. We also show that benchmarking and experimenting with changes to an NFS server can be a subtle and challenging task, and that it is often difficult to distinguish the impact of a new algorithm or heuristic from the quirks of the underlying software and hardware with which they interact. We discuss these quirks and their potential effects.
For many years the Brightness Constancy Constraint Equation (BCCE) has been used for optical flow and related computer vision computations. However, almost all cameras have some kind of automatic exposure feature such as Automatic Gain Control (AGC), so that the overall exposure level of the image varies as the camera is aimed at brighter or darker portions of a scene. Moreover, because most cameras have some kind of unknown nonlinear response function, the change due to AGC cannot be captured by merely applying a multiplicative constant to the pixels of each image. We propose, therefore, a Lightspace Change Constraint Equation (LCCE) that accounts for exposure change (AGC) together with the nonlinear response function of the camera. The response function can be automatically &quot;learned&quot; by an intelligent image processing system presented with differently exposed capture of the same subject matter in overlapping regions of registered images. Most importantly, a Logarithmic Lightspace Change Constraint Equation (LLCCE) is shown to have a very simple mathematical formulation. The LCCE (and Log LCCE) is applied to the estimation of the projective coordinate transformation between pairs of images in a sequence, and is compared with examples where the BCCE fails.
Technology trends present new challenges for processor architectures and their instruction schedulers. Growing transistor density will increase the number of execution units on a single chip, and decreasing wire transmission speeds will cause long and variable on-chip latencies. These trends will severely limit the two dominant conventional architectures: dynamic issue superscalars, and static placement and issue VLIWs. We present a new execution model in which the hardware and static scheduler instead work cooperatively, called Static Placement Dynamic Issue (SPDI). This paper focuses on the static instruction scheduler for SPDI. We identify and explore three issues SPDI schedulers must consider---locality, contention, and depth of speculation. We evaluate a range of SPDI scheduling algorithms executing on an Explicit Data Graph Execution (EDGE) architecture. We find that a surprisingly simple one achieves an average of 5.6 instructions-per-cycle (IPC) for SPEC2000 64-wide issue machine, and is within 80% of the performance without on-chip latencies. These results suggest that the compiler is effective at balancing on-chip latency and parallelism, and that the division of responsibilities between the compiler and the architecture is well suited to future systems. 1
This paper describes the problems and an adaptive solution for process control in rubber industry. We show that the human and economical benefits of an adaptive solution for the approximation of process parameters are very attractive. The
Under traditional IP multicast, application-level FEC can only be implemented on an end-to-end basis between the sender and the clients. Emerging overlay and peer-to-peer (p2p) networks open the door for new paradigms of network FEC. The deployment of FEC within these emerging networks has received very little attention (if any). In this paper, we analyze and optimize the impact of Network-Embedded FEC (NEF) in overlay and p2p multimedia multicast networks. Under NEF, we place FEC codecs in selected intermediate nodes of a multicast tree. The NEF codecs detect and recover lost packets within FEC blocks at earlier stages before these blocks arrive at deeper intermediate nodes or at the final leaf nodes. This approach significantly reduces the probability of receiving undecodable FEC blocks. In essence, the proposed NEF codecs work as signal regenerators in a communication system and can reconstruct most of the lost data packets without requiring retransmission. We develop an optimization algorithm for the placement of NEF codecs within random multicast trees. Our theoretical analysis and simulation results show that a relatively small number of NEF codecs placed in (sub-)optimally selected intermediate nodes of a network can improve the throughput and overall reliability dramatically.
We show that network motifs found in natural regulatory networks may also be found in  an artificial regulatory network model created through a duplication / divergence process. It  is shown that these network motifs exist more frequently in a genome created through the  aforementioned process than in randomly generated genomes. These results are then compared  with a network motif analysis of the gene expression networks of Escherichia Coli and  Saccharomyces cerevisiae. In addition, it is shown that certain individual network motifs may  arise directly from the duplication / divergence mechanism.
Statistical density estimation techniques are used in many computer vision applications such as object tracking, background subtraction, motion estimation and segmentation. The particle filter (Condensation) algorithm provides a general framework for estimating the probability density functions (pdf) of general non-linear and non-Gaussian systems. However, since this algorithm is based on a Monte Carlo approach, where the density is represented by a set of random samples, the number of samples is problematic, especially for high dimensional problems. In this paper, we propose an alternative to the classical particle filter in which the underlying pdf is represented with a semi-parametric method based on a mode finding algorithm using mean-shift. A mode propagation technique is designed for this new representation for tracking applications. A quasi-random sampling method [14] in the measurement stage is used to improve performance, and sequential density approximation for the measurements distribution is performed for efficient computation. We apply our algorithm to a high dimensional colorbased tracking problem, and demonstrate its performance by showing competitive results with other trackers. 1
This paper proposes a research methodology  for attacking the problem of  providing fluent and natural discourse  about space and spatially situated tasks  between nave users and robots. We suggest  flexible and adaptive ontology mediation,  parameterized according to  empirically determined discourse and  contextual factors, as a suitable architecture  with clear applications for the treatment  of natural human-human dialog  also.
In this paper we present an overview of classical results about the variance reduction technique of control variates. We emphasize aspects of the theory that are of importance to the practitioner, as well as presenting relevant applications. 1 
BGP is the de-facto inter-domain routing protocol and it is essential to understand how well BGP performs in the Internet. As a step toward this understanding, this paper studies the routing performance of a sample set of prefixes owned by the U.S. Department of Defense (DoD). We examine how reliably the sample set is connected to the Internet and how it affects the rest of the Internet. We show that our sample set receives reliable connectivity, with the exception of a few prefixes. We also show that, on average, the sample set has minimal impact on global routing, but certain BGP features used by DoD routers result in periods of excessive routing overhead. During some stressful periods, our sample set, only 0.2% of all prefixes, contributed over 80% of a particular BGP update class. We explain how the BGP design allows certain local changes to propagate globally and amplifies the impact of our sample prefixes.
We study indexing techniques for main  memory, including hash indexes, binary  search trees, T-trees, B+-trees, interpolation  search, and binary search on arrays. In a 
An approach for tight coupling of process models and software development tools --- with the metaphor of component-based software development environments --- supporting &quot;eXecutable Process Models&quot; (XPM) is presented. In this paper, we focus on the direction from the components of a software development environment towards the process models in order to automatically acquire process model information solving various drawbacks compared to a manual acquisition of this information. Requirements on a process modeling language for using this information are discussed.
While many community-driven development (CDD) initiatives may be  successful, their impact is often limited by their small scale. Building on past and  ongoing work on CDD, this study addresses the fundamental question: how can CDD  initiatives motivate and empower the greatest number of communities to take control of  their own development? What are the key contextual factors, institutional arrangements,  capacity elements, and processes related to successful scaling-up of CDD, and,  conversely, what are the main constraints or limiting factors, in different contexts?  Drawing upon recent literature and the findings from five case studies, key lessons on  how best to stimulate, facilitate, and support the scaling-up of CDD in different  situations, along with some major challenges, are highlighted.  Lessons include the need for donors and supporters of CDD, including  governments, to think of the process beyond the project, and of transformation or  transition rather than exit. Donor push and community pull factors need to be balanced to  prevent supply-driven, demand-driven development. Overall, capacity is pivotal to  successful CDD and its successful scaling-up over time. Capacity is more than simply  resources, however; it also includes motivation and commitment, which, in turn, require  appropriate incentives at all levels. Capacity development takes time and resources, but  it is an essential upfront and ongoing investment, with the capacity and commitment of  facilitators and local leaders being particularly important. A learning by doing  cultureone that values adaptation, flexibility, and openness to changeneeds to be  fostered at all levels, with time horizons adjusted accordingly. The building of a library  of well-documented, context-specific experiences th...
Though there may be millions of professionals worldwide acting as a designer, architect, or engineer in the design, realisation, and implementation of information systems, there is not yet a well established and clearly identified body of knowledge that can be said to define the profession.
In recent times, the way people access information from the web has undergone a transformation. The demand for information to be accessible from anywhere, anytime, has resulted in the introduction of Personal Digital Assistants (PDAs) and cellular phones that are able to browse the web and can be used to find information using wireless connections. However, the small display form factor of these portable devices greatly diminishes the rate at which these sites can be browsed. This shows the requirement of efficient algorithms to extract the content of web pages and build a faithful reproduction of the original pages with the important content intact.
proof. Let us examine why.      ; ffM  1 :A 2ffA 1      2 :A 2  ffE.    M 2 :A 1  We can make the following inferences.    V 1 =  ffx:A 2 .M ff 1 By type preservation and inversion   At this point we cannot proceed: we need a derivation of  [V 2 /x]M ff 1 ffff V for some V  to complete the derivation of M 1    M 2 ffff V . Unfortunately, the induction hypothesis does not tell us anything about [V 2 /x]M ff 1 . Basically, we need to extend it so it makes a statement about the result of evaluation (    ffx:A 2 .M ff 1 ,inthis  case).  Sticking to the case of linear application for the moment, we call a term M &quot;good&quot; if it evaluates to a &quot;good&quot; value V .AvalueVis &quot;good&quot; if it is a function  ffx:A 2 .M ff 1 and if substituting a &quot;good&quot; value V 2 for x in M ff 1 results in a &quot;good&quot; term. Note that this is not a proper definition, since to see if V is &quot;good&quot; we may need to substitute any &quot;good&quot; value V 2 into it, possibly including V itself. We can make this definition inductive if we observe that the value
to 100 km/sec per Mpc. The most likely value for the deceleration parameter q o is , which corresponds to a flat universe. But all measurements are in fact quite dispersed around this value, and seem now to indicate an open universe.  Two other cosmological facts are related to the Big Bang theory. The second cosmological fact often quoted and extensively discussed by Rees is that on earth we are immersed in a cosmic microwave background radiation, like being in a furnace kept at a temperature of t 0 = 2.7 Kelvin. This accidental discovery seemed to confirm the socalled Big Bang model for theorists. The third cosmological fact depends on the measurement of elementary abundances in the universe, and it seems that the proportions of hydrogen, deuterium, helium, lithium, etc., have changed little since their formation at the time of the Big Bang.  Needless to say, when one discovers several facts which fit within the same theory, one is tempted not to go beyond and to relegate all o
Remaining elusive while navigating to a goal in a dynamic environment containing an observer requires taking advantage of opportunistic cover as it occurs. A reactive navigation approach is needed that recognizes the utility of environment features in offering protective cover. We present an approach that allows stealthy traverses in unknown environments containing dynamic objects. It is a frontier-based method that allows a robot to follow in the obscuring shadow of objects despite their dynamics, and take advantage of more opportunistic cover if it becomes available. An analysis of our approach in off-line modeling and experiments conducted in simulation and outdoor environments demonstrate its effectiveness in achieving high quality solutions for stealthy navigation.
We consider multi-robot systems that include sensor nodes and aerial or ground robots networked together. Such networks are suitable for tasks such as large-scale environmental monitoring or for command and control in emergency situations. We present a sensor network deployment method using autonomous aerial vehicles and describe in detail the algorithms used for deployment and for measuring network connectivity and provide experimental data collected from field trials. A particular focus is on determining gaps in connectivity of the deployed network and generating a plan for repair, to complete the connectivity. This project is the result of a collaboration between three robotics labs (CSIRO, USC, and Dartmouth.)  1 
Airports are an ideal application area for simulation. The processes are in a continuous state of change, are complex and stochastic, involve many moving objects, and require a good performance that can be measured in several different performance indicators. Within airports, but also between airports, the same kind of questions are answered over and over again. Often, however, new simulation models are built for each question, if possible copying some parts of previous models. Structured reuse of simulation components is rarely seen. This paper shows an approach for airport terminal modeling that departs from the assumption that reusable simulation building blocks can form the core of a powerful airport modeling tool, which is able to answer different questions at airports better and faster than traditional models. The building blocks have been implemented in the commercially available simulation language eM-Plant. Several studies carried out with this library were very successful.
dist dist flag, described in section 3.1, can also be used in place of -d.  The number of goods and bids must also be specified for each run. If every instance is to have the same number of goods and bids, the -goods and -bids flags are used to provide these numbers. It is also possible to choose the numbers of goods and bids separately for each instance from a uniform distribution over a specified range. This is done with the -random goods and -random bids flags. Both are followed by two integers specifying the minimum and maximum values of the range.  If the -default hard flag is used (see section 3.5), all of the above parameters take on default values and are not required. A specific distribution can still be chosen, but the number of bids and goods, if they are (redundantly) entered, must be 1000 and 256, respectively, since our hardness models are based on this problem size. The 2.1 release of CATS may allow variable problem sizes.    L1 and L5 are excluded because it is impossib
Countermeasures against node misbehavior and selfishness are mandatory requirements in mobile ad hoc networks. Selfishness that causes lack of node activity cannot be solved by classical security means that aim at verifying the correctness and integrity of an operation. In this paper we outline an original security mechanism (CORE) based on reputation that is used to enforce cooperation among the nodes of a MANET. We then investigate on its robustness using an original approach: we use game theory to model the interactions between the nodes of the ad hoc network and we focus on the strategy that a node can adopt during the network operation. As a first result, we obtained the guidelines that should be adopted when designing a cooperative security mechanism that enforces mobile nodes cooperation. Furthermore, we were able to show that when no countermeasures are taken against misbehaving nodes, network operation can be heavily jeopardized. We then showed that the CORE mechanism is compliant with guidelines provided by the game theoretic model and that, under certain conditions, it assures the cooperation of at least half of the nodes of a MANET.
The goal of information extraction from the Web is to provide an integrated view on heterogeneous information sources. A main problem with current wrapper/mediator approaches is that they rely on very different formalisms and tools for wrappers and mediators, thus leading to an &quot;impedance mismatch&quot; between the wrapper and mediator level. Additionally, most approaches currently are tailored to access information from a fixed set of sources.
The purpose of this paper is to give a very simple method for  nonlinearly estimating the fundamental matrix using the minimum number of seven parameters. Instead of minimally parameterizing it, we rather update what we call its orthonormal representation, which is based on its singular value decomposition. We show how this method can be used for efficient bundle adjustment of point  features seen in two views. Experiments on simulated and real data show that this implementation performs better than others in terms of computational cost, i.e.,  convergence is faster, although methods based on minimal parameters are more likely to fall into local minima than methods based on redundant parameters.
A model of coalition government formation is presented in which inefficient, nonminimal winning coalitions can form in Nash equilibrium. Predictions for five games are presented and tested experimentally. The experimental data support potential maximization as a refinement of Nash equilibrium. In particular, the data support the prediction that non-minimal winning coalitions occur when the distance between policy positions of the parties is small relative to the value of forming the government. These conditions hold in games 1, 3, 4 and 5, where subjects played their unique potential-maximizing strategies 91, 52, 82 and 84 percent of the time, respectively. In the remaining game (Game 2) experimental data support the prediction of a minimal winning coalition. Players A and B played their unique potential-maximizing strategies 84 and 86 percent of the time, respectively, and the predicted minimal-winning government formed 92 percent of the time (all strategy choices for player C conform with potential maximization in Game 2). In Games 1, 2, 4 and 5 over 98 percent of the observed Nash equilibrium outcomes were those predicted by potential maximization. Other solution concepts including iterated elimination of dominated strategies and strong/coalition proof Nash equilibrium are also tested.
In this paper, we propose a methodology developed in the  framework of the VISPO project for engineering a three-layer ontology,  based on the conceptualization, integration, synthesis and categorization  of XML data descriptions provided by a number of sources in a virtual  district, where different enterprises cooperate for business purposes. Ontologies  are proposed as an unifying framework for different viewpoints  by providing a shared understanding in a subject domain. Our methodology  generates an ontology organized into concepts and concept relationships  at different levels of detail, to provide multiple, unified views of  the datasources containing heterogeneous information about the domain  of interest.
ss affecting a segment because of its position in a string Coda V__V devoicing typical highly improbable deaspiration (C  h  --&gt;C) typical highly improbable velarisation (l,n--&gt;ff,ff) typical highly improbable   s-debuccalisation (s--&gt;h) typical highly improbable   liquid gliding (r,l--&gt;j) typical highly improbable    depalatalisation (ff--&gt;n) typical highly improbable    l-vocalisation (ff--&gt;w/o) typical highly improbable   r-vocalisation/ loss ([kaad] &quot;card&quot;) typical highly improbable   [NC] hom : homorganisation of nasals typical highly improbable spirantisation (b,d,g--&gt;ff,ffff) highly improbable typical voicing (t--&gt;d) highly improbable typical rhotacism (z--&gt;r) highly improbable typical c. only solution when using the familiar model of syllabic structure: criterion based on {__ff, __.C, V__V} = postvocalic pure adjacence V__V = flanked by vowels pure adjacence {__ff, __.C} = Coda pure position d. contradiction: the superset is defined in pure terms of adjacence. Hence, one of its subsets 
The Fast Factorised Back-Projection (FFBP) algorithm has received considerable attention recently for SAS image reconstruction. The FFBP algorithm provides a means of trading image quality and/or resolution for a reduction in computational cost over standard Back-Projection. In this paper we describe FFBP for SAS image reconstruction and compare it to the Wavenumber algorithm in terms of computational cost and image quality.
opportunity for equipping private households with inexpensive smart devices for controlling and  automating various tasks in our daily lives. Networking technology and standards have an important role  in driving this development. The omnipresence of the Internet via phone lines, TV cable, power lines, and  wireless channels facilitates ubiquitous networks of smart devices that will significantly change the way  we interact with home appliances. Home networking is considered to become one of the fastest growing  markets in the area of information technology. However, interoperability and flexibility of embedded  devices are key challenges for making &quot;Smart Home&quot; technology accessible for a broad audience. In  particular, the software programs that determine the behavior of the smart home must facilitate  customizability and extensibility. Unlike industrial applications that are typically engineered by highly  skilled programmers, control and automation programs for the smart home should be understandable to  laypeople. In this article, we discuss how recent technological progress in the areas of visual  programming languages, component software, and connection-based programming can be applied to  programming the smart home. Our research is carried out in tight collaboration with a corporate partner  in the area of embedded systems.
this paper is to survey the current semantic Web services languages and modeling frameworks by outlining their features and capabilities. We will then compare the approaches and identify the deficient features which need to be overcome to meet the requirements of the industry and the SWSL in developing a formal language/technology for supporting semantic Web services
... In this paper, we consider the case when the process is assumed to be fractional ARIMA and show that the new method still possesses the aforementioned qualities
There is experimental evidence that the performance of standard subspace algorithms from the literature (e.g. the  N4SID method)  may be surprisingly poor in certain experimental conditions. This happens typically when the past signals (past inputs  andoutputs)  and future input spaces are nearly parallel. In this paper we argue that the poor behavior may be attributed to a form of ill-conditioning of the underlying multiple regression problem, which may occur for nearly parallel regressors. An elementary error analysis of the subspace identification problem, shows that there are two main possible causes of ill-conditioning. The first has to do with near collinearity of the state and future input subspaces. The second has to do with the dynamical structure of the input signal and may roughly be attributed to &quot;lack of excitation&quot;. Stochastic realization theory constitutes a natural setting for analyzing subspace identification methods. In this setting, we undertake a comparative study of three widely used subspace methods (N4SID, Robust N4SID and  PO-MOESP)  The last two methods are proven to be essentially equivalent and the relative accuracy, regarding the estimation of the (A, C) parameters, is shown to be the same.
This paper presents and discusses the LOTOS specification of a real-time parallel kernel. The purpose of this specification exercise has been to evaluate LOTOS with respect to its capabilities to model real-time features with a realistic industrial product. LOTOS was used to produce the formal specification of TRANS-RTXC, which is a real-time parallel kernel developed by Intelligent Systems International. This paper shows that although timing constraints cannot be explicitly represented in LOTOS, the language is suitable for the specification of co-ordination of real-time tasks, which is the main functionality of the real-time kernel. This paper also discusses the validation process of the kernel specification and the role of tools in this validation process. We believe that our experience (use of structuring techniques, use of validation methods and tools, etc) is valuable for designers who want to apply formal models in their design or analysis tasks.
Among the various proposals answering the shortcomings of Document Type Definitions (DTDs), XML Schema is the most widely used. Although DTDs and XML Schema Defintions (XSDs) differ syntactically, they are still quite related on an abstract level. Indeed, freed from all syntactic sugar, XML Schemas can be seen as an extension of DTDs with a restricted form of specialization. In the present paper, we inspect a number of DTDs and XSDs harvested from the web and try to answer the following questions: (1) which of the extra features/expressiveness of XML Schema not allowed by DTDs are effectively used in practice; and, (2) how sophisticated are the structural properties (i.e. the nature of regular expressions) of the two formalisms. It turns out that at present real-world XSDs only sparingly use the new features introduced by XML Schema: on a structural level the vast majority of them can already be defined by DTDs. Further, we introduce a class of simple regular expressions and obtain that a surprisingly high fraction of the content models belong to this class. The latter result sheds light on the justification of simplifying assumptions that sometimes have to be made in XML research.
Introduction  Database systems based on SQL are well suited for homogeneous databases  -- either centralized or distributed. Most traditional database architectures, however, seem inadequate to handle differenttypes of heterogeneity.Interoperability at the system level can be achieved to some degree byinterposing an additional interface layer between a database system and the application, as in the ODBC solution [Mic94] and, more recently, in the analogous, Java-based JDBC proposal [HC96]. Other vendor-specific solutions provide network and protocol transparency by standardizing their SQL interface.  The problem of data,orsemantic, heterogeneity,however, still remains. Different systems that own different pieces of data maycomeinto conflict when they need to agree, at least in part, on the meaning of each other&apos;s data. This situation is common in loosely coupled database federations, where private data from a common domain of discourse is shared, and yet each local system insists on ma
This paper presents HapticFlow, a haptics-based direct mesh editing system founded upon the concept of PDE-based geometric  surface flow. The proposed flow-based approach for direct geometric manipulation offers a unified design paradigm that  can seamlessly integrate implicit, distance-field based shape modeling with dynamic, physics-based shape design. HapticFlow  provides an intuitive haptic interface and allows users to directly manipulate 3D polygonal objects with ease. To demonstrate  the effectiveness of our new approach, we developed a variety of haptics-based mesh editing operations such as embossing,  engraving, sketching as well as force-based shape manipulation operations
Recent results indicate that the same turbo principle which delivers near to optimal strategies for channel coding, can be used to obtain very efficient source coding schemes. We investigate this issue applying ten Brink&apos;s EXIT chart analysis and show how this technique can be used to select the most efficient match of component codes and puncturing matrices to compress discrete memoryless sources. Aiming at perfect reconstruction at the decoder, i.e. lossless source coding, we present an encoding algorithm, which gradually removes the redundancy while checking the decodability of the compressed bit stream. This concept of decremental redundancy is dual to the principle of incremental redundancy that characterizes hybrid ARQ (Type II) communication protocols. Both principles can be combined when the channel is noisy.
The C1 billion government drive to integrate information and communications technology (ICT) into UK schools and colleges has been &quot;rmly focused on the technological transformation of the teaching profession. In particular, the establishment of a National Grid for Learning (NGfL) remains dependent on the successful &amp;selling&apos; of ICT to teachers; many of whom have previously proved unwilling to use computers. In practice much of this task has been left to IT &quot;rms, eager to promote their products to a potentially lucrative educational marketplace. From this basis the present paper takes a detailed examination of educational computing advertising material currently being produced by IT &quot;rms in the UK. In particular it concentrates on how advertisements construct both the process of education and the teacher as a potential user of ICT. Four dominant themes emerge from this analysis: ICT as problematic for teachers; ICT as a problem solver for teachers; ICT as a futuristic form of education; and ICT as a traditional form of education. Despite the confficting, and often contra-factual, nature of these four discourses the paper argues that educational computing advertising is consistent in its disempowering portrayal of the teacher at the expense of both the computer and IT &quot;rm. This &amp;demotion&apos; of the teacher is likely to have negative e!ects on the way that teachers approach ICT as part of their professional routine, running contrary to the underlying aims of the National Grid for Learning initiative. ff 2000 Elsevier Science Ltd. All rights reserved.
Motivated by experience gained during the validation of a recent Approximate Mean Value Analysis (AMVA) model of modern shared memory architectures, this paper re-examines  the &quot;standard&quot; AMVA approximation for non-exponential FCFS queues. We find that this approximation is often inaccurate for FCFS queues with high service time variability. For such queues, we propose and evaluate: (1) AMVA estimates of the mean residual service time at an arrival instant that are much more accurate than the standard AMVA estimate, (2) a new AMVA technique that provides a much more accurate estimate of mean center residence time than the standard AMVA estimate, and (3) a new AMVA technique for computing the mean residence time at a &quot;downstream&quot; queue which has a more bursty arrival process than is assumed in the standard AMVA equations. Together, these new techniques increase the range of applications to which AMVA may be fruitfully applied, so that for example, the memory system architecture of shared memory systems with complex modern processors can be analyzed with these computationally efficient methods.
Bar and line graphs are a good medium when trying to understand overall trends and general relationships between data items. Sometimes it is, however, desirable to make more detailed comparisons between data items. In this case good tools are valuable, especially when examining a dense graph. This paper introduces two techniques that can be used in such tools. Spatial grouping and visual landmarks can be applied in a way that takes full advantage of the attributes of the human attention mechanism to facilitate visual comparisons.
We present in this paper a new complete method for distributed constraint optimization. This is a utility-propagation method, inspired by the sum-product algorithm. The original algorithm requires fixed message sizes, linear memory, and is time-linear in the size of the problem. However, it is correct only for tree-shaped constraint networks. In this paper, we show how to extend the algorithm to arbitrary topologies using cycle cutsets, while preserving the linear message size and memory requirements. We present some preliminary experimental results on randomly generated problems. The algorithm is formulated for optimization problems, but can be easily applied to satisfaction problems as well.
We investigate a number of issues related to the use of multiple trust authorities and multiple identities in the type of identifier based cryptography enabled by the Weil and Tate pairings. An example of such a system is the Boneh and Frank9I encryption scheme. We present various applications of multiple trust authorities. In particular we focus on how one can equate a trust authority with a way to add contextual information to an identity. 1 
There is a longstanding interest in how decisions about resource allocations are made within households and how those decisions affect the welfare of household members. Much empirical work has approached the problem from the perspective that if preferences differ, welfare outcomes will depend on the power of individuals within the household to exert their own preferences. Measures of power are therefore a central component of quantitative empirical approaches to understanding how differences in preferences translate into different welfare outcomes. Following most of the empirical studies in this genre, this paper focuses on dynamics within couples, although we recognize that dynamics among extended family members and across generations are of substantial interest. A number of different measures of power have been used in the literature. Because control over economic resources is seen as an important source of power, individual labor income, which one earns and so presumably controls to some degree, is one potential measure of power. However, whether and how much one works is a choice that is not likely to be independent of one&apos;s power in the household. Non- labor income has also been used as a measure of power, but even if non- labor income does not reflect contemporaneous choices, it likely does reflect past choices, particularly labor supply choices, and so is also a function of power. Levels of resources brought to the marriage by each spouse, over which they may individually retain control, are even less proximate to the current choices of household members, but nevertheless reflect one&apos;s taste in iii partners and therefore may not be exogenous to power. (In some instances, resources brought to the marriage may reflect decisionmaking by the couple&apos;s parents, dependi...
We present new insights and algorithms for converting reasoning problems in monadic First-Order Logic (includes only 1place predicates) into equivalent problems in propositional logic. Our algorithms improve over earlier approaches in two ways. First, they are applicable even without the unique-names and domain-closure assumptions, and for possibly infinite domains. Therefore, they apply for many problems that are outside the scope of previous techniques. Secondly, our algorithms produce propositional representations that are significantly more compact than earlier approaches, provided that some structure is available in the problem. We examined our approach on an example application and discovered that the number of propositional symbols that we produced is smaller by a factor of f ff 50 than traditional techniques, when those techniques can be applied. This translates to a factor of about 2  f  increase in the speed of reasoning for such structured problems.
This paper describes a simulation model of a large beverage distribution center. The brewery distribution center has a volume of 71,600 cubic meters and contains about 8,000 pallets. Every day 1,800 pallets are handled in or out of the system, and the object of this study was to verify the functionality of the automated storage and retrieval system and integrated conveyor system -- including elevators connecting five levels of the distribution center. The complex system is modeled with the powerful simulation software Arena. A brief discussion of the results is also given.
Poverty profiles are a useful way of summarizing information on the levels of poverty and the characteristics of the poor in a society. They also provide us with important clues to the underlying determinants of poverty. However, important as they are, poverty profiles are limited by the bivariate nature of their informational content. The bivariate associations typical in a poverty profile can sometimes be misleading; they beg the obvious question of the effect of a particular variable conditional on the other potential determinants. While there may be certain contexts where unconditional poverty profiles are relevant to a policy decision (see Ravallion 1996), often one would be interested in the &quot;conditional&quot; poverty effects of proposed policy interventions. It is not surprising therefore that empirical poverty assessments in recent years have seen a number of attempts at going beyond the poverty profile tabulations to engage in a multivariate analysis of living standards and poverty. This study for Egypt has a similar motivation. For Egypt, while there has been some work on a descriptive analysis of the characteristics of the poor, to our knowledge, there is no precursor to an empirical modeling of the determinants of poverty using nationally representative data. To a large extent, this has been due to the nonavailability of unit-record data from the Household Income, Expenditure and Consumption Survey (HIECS), the primary source of data on iii 1997 Egypt Integrated Household Survey (EIHS). Using the EIHS data, it is now possible to conduct a household-level multivariate analysis of living standards. The EIHS, being an integrated, multimodule survey, also offers the potential of a richer analysis of this issue than may have been possible from other data sources. In t...
INTRODUCTION  Dipolar coupling data are potentially of great use to NMR spectroscopists since they contain long range information (as opposed to NOE and scalar couplings). Since, however, the dipolar coupling of an isotropically tumbling molecule averages to zero, useful dipolar coupling data was, until recently, only available for the small number of paramagnetic proteins (1), and protein-- DNA complexes (2) that align spontaneously in strong magnetic fields. The recent introduction of liquid crystal media that induce tunable levels of physical alignment, such as phospholipid mixtures  (3), filimentous phage (4, 5), and purple membranes (6), should allow dipolar coupling data to be collected from essentially all nucleic acids and proteins.  The dipolar coupling between two nuclei is given by  DPQ (ff,ff)    a [(3 cos      1)    1.5R sin    ff cos 2ff], [1] where D  a subsumes the gyromagnetic ratios of the two nuclei, To whom correspondence should be addressed at Department of Chemistry, 
An appropriate choice of the computing devices employed in digital signal processing applications requires to characterize and to compare various technologies, so that the best component in terms of cost and performance can be used in a given system design. In this paper, a benchmark strategy is presented to measure the performances of various types of digital signal processing devices. Although different metrics can be used as performance indexes, Fast Fourier Transform (FFT) computation time and Real-Time Bandwidth (RTBW) have proved to be excellent and complete performance parameters. Moreover, a new index, measuring the architectural efficiency in computing FFT, is introduced and explained. Both parameters can be used to compare several digital signal processing technologies, thus guiding designers in optimal component selection.
TCP-AQM protocols can be interpreted as distributed primal-dual algorithms over the Internet  to maximize aggregate utility over source rates. In this paper we study whether TCP-AQM  together with shortest-path routing can maximize utility over both rates and routes. We show that  this is generally impossible because the addition of route maximization makes the problem NP-hard. We exhibit
Most of the real-time scheduling algorithms are based on &quot;open-loop&quot; strategies that do not take application demands into account. This precludes the scheduler to dynamically adjust task executions in order to optimize  performance. To overcome this limitation, we have focused  our work on scheduling techniques that are able to take  scheduling decisions based on continuous feedback information of the performance delivered by each task. Focusing on control applications, we present an early specification of a novel scheduling technique: Large Error  First (LEF). It uses feedback information from each controlled plant in order to assign priorities to each control task. For a given simulation set-up, comparing the performance of LEF versus open loop classical scheduling  techniques, encouraging simulation results have been  obtained.
This paper introduces communication systems (CS) as a unified model  for socially intelligent systems. This model derived from sociological systems  theory, combines the empirical analysis of communication in a social system with  logical processing of social information to provide a general framework for computational  components that exploit communication processes in multiagent systems.
One of the most significant findings... This paper reviews what is currently known about network traffic self-similarity and its significance. We then consider a matter of current research, namely, the manner in which network dynamics (specifically, the dynamics of transmission control protocol (TCP), the predominant transport protocol used in today&apos;s Internet) can affect the observed self-similarity. To this end, we first discuss some of the pitfalls associated with applying traditional performance evaluation techniques to highly-interacting, large-scale networks such as the Internet. We then present one promising approach based on chaotic maps to capture and model the dynamics of TCP-type feedback control in such networks. Not only can appropriately chosen chaotic map models capture a range of realistic source characteristics, but by coupling these to network state equations, one can study the effects of network dynamics on the observed scaling behavior. We consider several aspects of TCP feedback, and illustrate by examples that while TCP-type feedback can modify the self-similar scaling behavior of network traffic, it neither generates it nor eliminates it.
We investigate the use of XML as an open, cross-platform, and extendable file format for the description of hierarchical simulation models, including their graphical representations, initial model conditions, and model execution algorithms. We present HiMASS-x, an XML-centered suite of software applications that allows for cross-platform, distributed modeling and execution of hierarchical, componentized, and reusable simulation models.
It has become a matter of survival that many companies improve their supply chain efficiency. This presents an opportunity for simulation. However, there are many challenges that must be overcome for simulation to be a contributor to play an effective role. Four contributors discuss the opportunities that they see for simulation to play a meaningful role in the area of supply chain management.
High capacity of transmission lines (Ethernet in particular) is much higher than what imposed by MIDI today. So it is possible to use capturing interfaces with high-speed and high-resolution, thanks to the OSC protocol, for musical synthesis (either in realtime or non real-time). These new interfaces offer many advantages, not only in the area of musical composition with use of sensors but also in live and interactive performances. In this manner, the processes of calibration and signal processing are delocalized on a personal computer and augments possibilities of processing. In this demo, we present two hardware interfaces developed in La kitchen with corresponding processing to achieve a high-resolution, high-speed sensor processing for musical applications.
Egyptian labor market is moving from a period of high overall unemployment to one where unemployment is increasingly concentrated among specific groups whose access to the private-sector labor market is limited. Educated young women are more adversely affected than their male counterparts by the transition to a private-sector-led economy. There is no systematic link between youth unemployment among new entrants and poverty unless it is the head of the household who is unemployed. An economic policy environment that is favorable for labor-intensive, export-oriented industries would help absorb the new entrants into the labor market, and the prospect is particularly good for young female workers. Policymakers should consider a reduction in the femalespecific employer mandates (such as the existing provision for a generous maternity leave) that raise the cost of hiring women. iv v CONTENTS Acknowledgments.......................................................................................................... ix Executive Summary ....................................................................................................... xi 1. 
Activity monitoring deals with monitoring data (usually streaming data) for interesting events. It has several applications such as building an alarm or an alert system that triggers when outliers or change points are detected. We discuss
The implementation of a portfolio assessment strategy in the education and training environment is a time consuming process that should be performed within a specific framework, structure or model to accommodate diverse learners.
We present a new model for the distributed implementation of pi-like calculi. This model is a closemos h to a variety of calculi, and so perm02 strong correctness results that are easy to prove. In particular, we describe a distributed  abstractms hine called the fusion machnq . In it, only channels exist at runtim0 It uses aform of concurrent constraints called fusions---equations  on channelnamlff0ff05 h it stores as trees of forwarders between channels. We imH`B2ff t in the fusionms hine a solos calculus with explicit fusions. There are encodings into this calculusfrom the pi calculus and the explicit fusion calculus. We quantify the effciency of the latter bymz2ff of (co-)locations.  
This paper discusses the nature and significance of artificial illumination in computer games. It examines different game genres and finds their essence, in order to be able to locate a specific game typical to each genre. Games that existed prior to the computer are shown use light in a much more functional way, as opposed to digital games, which use illumination primarily for creating reality or aesthetic pleasure. Copyright 2004  KEYWORDS: Lighting design, Illumination, Shadows, Computer games, Game genres.
The high diversity in the capabilities of various mobile de-vices such as display capabilities and computation power makes the design of mobile information systems more challenging. A transcoding proxy is placed between a client and an information server to coordinate the mismatch be-tween what the server provides and what the client prefers. However, most research works in transcoding proxies in mo-bile computing environments are under the traditional client-server architecture and do not employ the data broadcast technique which is has been deemed a promising technique to design a power conservation, high scalable and high band-width utilization. In addition, the issue of QoS provision is also not addressed. In view of this, we design in this paper a QoS-aware transcoding proxy by utilizing the on-demand broadcasting technique. We first propose a QoS-aware transcoding proxy architecture, abbreviated as QTP, and model it as a queueing network. By analyzing the queueing network, several theoretical results are derived. We then propose a version decision policy and a service admission control scheme to provide QoS in QTP. The derived results are used to guide the execution of the proposed version decision policy and service admission control scheme to achieve the given QoS requirement. To measure the performance of QTP, several experiments are conducted. Experimental results show that the proposed scheme is more scalable than traditional client-server systems. In addition, the proposed scheme is able to effectively control the system load to attain the desired QoS.
This paper describes a study investigating the potential for two user modelling systems: a location-aware user modelling system providing easy access to applications, files and course materials commonly used by an individual student in different locations
This paper outlines the concept of a Global GIS, and defines various aspects of its  development, as well as various options and decisions that must be made. The emphasis is on  the advantages and disadvantages of maintaining a global topological structure, and whether  topology should be generated on the fly in response to a specific query. We first define  what we mean by space in this context, followed by a description of topological structures  and how we may use them in the context of graph traversal problems. We then describe some  appropriate data structures. After mentioning some of the real-world problems associated with polygon construction problems, we touch on how graphs may represent change over time. A global
Simulation models and business application software as they  are used for decision support in enterprise management are  both representations of an enterprises actual operations.  This paper describes a unified simulation and application  framework where it is possible to represent the entire performance process along a supply chain in a unified business  model, improve its performance with discrete event simulation technology, and then generate and implement the corresponding business application software from the same unified model, based on a so-called framework-based  application technology which allows implementation of  changes derived from simulation analysis with minimal effort and time. This enables a company to optimise not only  operational processes such as shopfloor or warehouse operations but also business processes such as planning, order  management and scheduling through simulation.   1 
Estimation of camera motion and structure of rigid objects  in the 3D world from multiple camera images by bundle adjustment  is often performed by iterative minimization methods due to their low  computational effort. These methods need a robust initialization in order  to converge to the global minimum. In this paper a new criterion for  keyframe selection is presented. While state of the art criteria just avoid  degenerated camera motion configurations, the proposed criterion selects  the keyframe pairing with the lowest expected estimation error of initial  camera motion and object structure. The presented results show, that the  convergence probability of bundle adjustment is significantly improved  with the new criterion compared to the state of the art approaches.
Using a newly constructed data set, we compare sources of funds and investment activities of venture capital (VC) funds in Germany, Israel, Japan and the UK. Sources of VC funds differ significantly across countries, e.g. banks are particularly important in Germany, corporations in Israel, insurance companies in Japan, and pension funds in the UK. VC investment patterns also differ across countries in terms of the stage, sector of financed companies and geographical focus of investments. We find that these differences in investment patterns are related to the variations in funding sources - for example, bank and pension fund backed VC firms invest in later stage activities than individual and corporate backed funds -- and we examine various theories concerning the relation between finance and activities. We also report that the relations differ across countries; for example, bank backed VC firms in Germany and Japan are as involved in early stage finance as other funds in these countries, whereas they tend to invest in relatively late stage finance in Israel and the UK. We consider the implication of this for the influence of financial systems on relations between finance and activities.
German orthography has the somewhat unique property of systematically marking nouns by  capitalizing their first letter. This gives the reader additional information with respect to the  syntactic structure of a sentence but also burdens the writer with the task of making this structure  explicit. In some older studies, the benefits of this information have been demonstrated for the  reading process, it still remains unclear though, how the writer accomplishes this task. Two  different processes are conceivable: The information is either delivered by the Orthographic  Output Lexicon or is syntactically generated whilst the sentence to be written is constructed. In a  series of experiments, evidence is provided for an interactive exchange between lexical and  syntactic processing dealing with the question of when capitalization should occur.
Auditory functional magnetic resonance imaging tasks are challenging since the MR scanner noise can interfere with the auditory stimulation. To avoid this interference a sparse temporal sampling method with a long repetition time (TR ff 17 s) was used to explore the functional anatomy of pitch memory. Eighteen right-handed subjects listened to a sequence of sine-wave tones (4.6 s total duration) and were asked to make a decision (depending on a visual prompt) whether the last or second to last tone was the same or different as the first tone. An alternating button press condition served as a control. Sets of 24 axial slices were acquired with a variable delay time (between 0 and 6 s) between the end of the auditory stimulation and the MR acquisition. Individual imaging time points were combined into three clusters (0 --2, 3-- 4, and 5-- 6 s after the end of the auditory stimulation) for the analysis. The analysis showed a dynamic activation pattern over time which involved the superior temporal gyrus, supramarginal gyrus, posterior dorsolateral frontal regions, superior parietal regions, and dorsolateral cerebellar regions bilaterally as well as the left inferior frontal gyrus. By regressing the performance score in the pitch memory task with task-related MR signal changes, the supramarginal gyrus (leftffright) and the dorsolateral cerebellum (lobules V and VI, leftffright) were significantly correlated with good task performance. The SMG and the dorsolateral cerebellum may play a critical role in short-term storage of pitch information and the continuous pitch discrimination necessary for performing this pitch memory task.
We consider the problem of converting a decimal number to a base b number. We present a conversion function that relates each digit in the base b system to the decimal value that is equal to the base b number in question. Thus, each base b digit of the related base b number can be obtained directly from the corresponding decimal number without the requirement of knowing any other base b digit.
A system is introduced that allows a string player to control a synthesis engine with the gestural skills he is used to. The implemented system is based on an electric viola and a synthesis engine that is directly controlled by the unanalysed audio signal of the instrument and indirectly by control parameters mapped to the synthesis engine. This method offers a highly string-specific playability, as it is sensitive to the kinds of musical articulation produced by traditional playing techniques. Nuances of sound variation applied by the player will be present in the output signal even if those nuances are  beyond traditionally measurable parameters like pitch, amplitude or brightness. The relatively minimal hardware requirements make the instrument accessible with little expenditure.
This paper describes a method for enumerating the ways in which combinations of vehicles can be observed at different survey points. The framework described is quite general and can be applied to a variety of problems where matches are to be found in data surveyed at a number of locations (or at a single location over a number of days). As an example, the framework is applied to the problem of false matches in licence plate survey data.
this paper we discuss the various XML schema [15,16], the port types defined, and some of our experiences building RFT
multimedia content service delivery. Its efficiency is maximized when all the service recipients have identical needs. In reality however, the end users may have a heterogeneous set of requirements for different service levels as well as different service components, depending on their system and network capabilities. We propose the notion of Service Adaptive Multicast (SAM) that balances the tradeoffs between providing individualized service to each client and maintaining an efficient overlay multicast tree structure. The novel aspects of our approach are (a) the ability to augment and transform existing paths into service paths with the desired attributes; and (b) integration of two tree maintenance processes: a receiver-initiated just-in-time adaptation of the multicast service tree driven by application/user perceived QoS, and a demand-driven tree maintenance process geared towards long-term tree quality. We demonstrate the performance of our approach using simulations of large client population.
butes, security levels, and the page size,  were varied for a Selection and Join query. We were particularly interested in  the relationship between performance degradation and changes in the quantity  of these properties. The performance of each scheme was measured in  terms of its response time.  The response times for the element level fragmentation scheme increased  as the numbers of tuples, attributes, security levels, and the page size were increased,  more significantly so than when the number of tuples and attributes  were increased. The response times for the attribute level fragmentation  scheme was the fastest, suggesting that the performance of the attribute level  scheme is superior to the tuple and element level fragmentation schemes. In  the context of assurance, this research has also shown that the distribution of  fragments based on security level is a more natural approach to implementing  security in MLS/DBMS systems, because a multilevel database is analogous  to a
We propose how Genetic Programming (GP) can be used  for developing, in real time, problem-specific heuristics for Branch and  Bound (B&amp;B) search. A GP run, embedded into the B&amp;B process, exploits  the characteristics of the particular problem being solved, evolving  a problem-specific heuristic expression. The evolved heuristic replaces  the default one for the rest of the B&amp;B search. The application of our  method to node selection for B&amp;B based Mixed Integer Programming  is illustrated by incorporating the GP node selection heuristic generator  into a B&amp;B MIP solver. The hybrid system compares well with the unmodified  solver utilizing DFS, BFS, or even the advanced Best Projection  heuristic when confronted with hard MIP problems from the MIPLIB3  benchmarking suite.
In today&apos;s Internet, only best-effort service is provided. With up-coming Quality of Service (QoS) requirements  raised by a wide range of communication-intensive, real-time multimedia applications, the best-effort service is no  longer sufficient. As a result, Differentiated Service Model (DiffServ) has been proposed as a cost-effective way to  provision QoS in the Internet.
This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&apos;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&apos;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
This paper examines genetic programming  as a machine learning technique  in the context of object detection.
This paper introduces a distributed localized algorithm where sensor nodes determine if they are located along the perimeter of a wireless sensor network. The algorithm works correctly in suffciently dense wireless sensor networks with a minimal requisite degree of connectivity. Using 1-hop and 2-hop neighbour information, nodes determine if they are surrounded by neighbouring nodes, and consequently, if they are located within the interior of the wireless sensor network. The algorithm requires minimal communication between nodes - a desirable property since energy reserves are generally limited and non-renewable.
Sonoelastography is the visualisation of elastic properties using ultrasound. It can enable tumours to be detected and localised based on their elasticity when they are less elastic than the surrounding soft tissue. In vibration sonoelastography the target tissues are vibrated while simultaneously recording ultrasound images. A technique for imaging relative elastic properties is proposed that uses a standard ultrasound machine. It combines B-scan and power Doppler signals to produce images of relative vibration amplitude. Preliminary results using simulations and liver phantoms are presented and the potential of the method to highlight areas of differing elasticity within an organ such as the breast is mentioned. The possibility of combining such a method with freehand 3D scanning, enabling B-scan and power Doppler signals to simultaneously populate a voxel array for subsequent visualisation is discussed.
It has been known for some time that Learning  Classifier Systems (Holland, 1986) have potential  for application as Data Mining tools. Parodi and  Bonelli (1990) applied the Boole LCS (Wilson,  1985) to a Lymphography data set and reported  82% classification rates. More recent work, such as  GA-Miner (Flockhart, 1995) has sought to extend  the application of LCS to larger commercial data  sets, introducing more complex attribute encoding  techniques, static niching, and hybrid genetic  operators in order to address the problems  presented by large search spaces. Despite these  results, the traditional LCS formulation has shown  itself to be unreliable in the formation of accurate  optimal generalisations, which are vital for the  reduction of results to a human readable form. XCS  (Wilson, 1995, 1998) has been shown to be capable  of generating a complete and optimally accurate  mapping of a test environment (Kovacs, 1996) and  therefore presents a new opportunity for the  application of Learning Classifier Systems to Data  Mining. As part of a continuing research effort this  paper presents some first results in the application  of XCS to a Data Mining task. It demonstrates that  XCS is able to produce a classification  performance and rule set which exceeds the  performance of most current Machine Learning  techniques when applied to the Monk&apos;s problems  (Thrun, 1991).
This paper shows that it is possible to dramatically reduce the memory consumption of classes loaded in an embedded Java virtual machine without reducing its functionalities. We describe how to pack the constant pool by deleting entries which are only used during the class loading process. We present some benchmarks which demonstrate the efficiency of this mechanism. We finally suggest some additional optimizations which can be applied if some restrictions to the functionalities of the virtual machine can be tolerated.
In this paper we introduce a conceptual model for information supply which abstracts from enabling technologies such as file types, transport protocols and rdf and daml+oil. Rather than focusing on technologies that may be used to actually implement information supply, we focus on the question: what is information supply and how does it relate to the data (resources) found on the Web today. By taking a high level of abstraction we can gain more insight in the information market, compare different views on it and even present the architecture of a prototype retrieval system (Vimes ) which uses transformations to deal with the heterogeneity of information supply.
The authors proposed a content management approach to develop a Webbased learning platform that were implemented and used to support both presential and distance education. The project, named EFTWeb, focus on the need to support both content and context. It provided the basis for the current research concerning the impact of new learning approaches. This paper presents current research extending EFTWeb to provide a broader environment that takes advantage of e-learning concepts by augmenting the framework to include beyond content and context, the experience dimension. The augmented framework relates education activities with the individual, the group, and the community, addressing how e-learning can be used to support learning experiences. ?  1. 
Few methods use molecular dynamics simulations based on atomically detailed force fields to study the proteinligand docking process because they are considered too time demanding despite their accuracy. In this paper we present a docking algorithm based on molecular dynamics simulations which has a highly flexible computational granularity. We compare the accuracy and the time required with well-known, commonly used docking methods like AutoDock, DOCK, FlexX, ICM, and GOLD. We show that our algorithm is accurate, fast and, because of its flexibility, applicable even to loosely coupled distributed systems like desktop grids for docking.
Human centered systems must focus attention in roles, users and tasks, aiming to making the full potential of computing ubiquitous. The paper proposes a generic design pattern for such systems, incorporating digital assistants and human representatives. These agents collaborate with people and deliberate socially for helping them to (a) participate in numerous physical and social contexts consistently and coherently, (b) build explicit social structures governed by social laws (i.e. agents values, permissions, preferences, contextual constraints), (c) deal with the dynamics of the activitiesenvironment and (d) manage the distributivity of the activities and environment.
This paper deals with multiwavelets and the different properties of approximation and smoothness associated with them. In particular, we focus on the important issue of the preservation of discrete-time polynomial signals by multifilterbanks. We introduce and detail the property of balancing for higher degree discrete-time polynomial signals and link it to a very natural factorization of the refinement mask of the lowpass synthesis multifilter. This factorization turns out to be the counterpart for multiwavelets of the well-known zeros at condition in the usual (scalar) wavelet framework. The property of balancing also proves to be central to the different issues of the preservation of smooth signals by multifilterbanks, the approximation power of finitely generated multiresolution analyses, and the smoothness of the multiscaling functions and multiwavelets. Using these new results, we describe the construction of a family of orthogonal multiwavelets with symmetries and compact support that is indexed by increasing order of balancing. In addition, we also detail, for any given balancing order, the orthogonal multiwavelets with minimum-length multifilters.
In this paper, we consider the compression of high-definition video sequences for bandwidth sensitive applications. We show that down-sampling the image sequence prior to encoding and then up-sampling the decoded frames increases compression efficiency. This is particularly true at lower bit-rates, as direct encoding of the high-definition sequence requires a large number of blocks to be signaled. We survey previous work that combines a resolution change and  compression mechanism. We then illustrate the success of our proposed approach through simulations. Both MPEG-2 and H.264 scenarios are considered. Given the benefits of the approach, we also interpret the results within the context of traditional spatial scalability.
Java class files can be transmitted more efficiently over a network if they are compressed. After an...
Many existing biometric systems collect ancillary information like  gender, age, height, and eye color from the users during enrollment. However,  only the primary biometric identifier (fingerprint, face, hand-geometry, etc.) is  used for recognition and the ancillary information is rarely utilized. We propose  the utilization of &quot;soft&quot; biometric traits like gender, height, weight, age, and ethnicity  to complement the identity information provided by the primary biometric  identifiers. Although soft biometric characteristics lack the distinctiveness and  permanence to identify an individual uniquely and reliably, they provide some  evidence about the user identity that could be beneficial. This paper presents a  framework for integrating the ancillary information with the output of a primary  biometric system. Experiments conducted on a database of 263 users show that  the recognition performance of a fingerprint system can be improved significantly  (ff 5%) by using additional user information like gender, ethnicity, and height.
The High Level Architecture (HLA) provides the specification of a common technical architecture for use across all classes of simulations in the US Department of Defense. It provides the structural basisfor simulation interoperability. The baseline definition of the HLAincludes the HLA Rules, the HLA Interface Specification (IFSpec), and the HLA Object Model Template (OMT). The HLA Rules are a set of 10 basic rules that define key principles used in the HLA as well as the responsibilities and relationships among the components of an HLA federation. The HLA IFSpec provides a specification of the functional interfaces between HLA federates and the HLA Runtime Infrastructure. The HLA OMT provides a common presentation format for HLA Simulation and Federation Object Models.
An attractive feature of many simulation packages is their availability on desktop computers and their potential for allowing the user to run a simulation model under different conditions in a highly interactive way. Such a way of studying a system is attractive because of its immediacy and the direct control it offers the user. However, partly as a consequence of this, good practice in the use of the methodology of the design of experiments is not always followed. As a result the efficiency and effectiveness of the overall simulation study may not be as good as it should be. In this paper we investigate how design of experiments methodology can be explicitly incorporated into interactive desktop studies. In particular we show how the optimal design of experiments methodology proposed by Cheng and Kleijnen (1998) for studying queues with highly heteroscedastic output can be used to provide a front-end advisory interface for controlling and conducting the study of an actual system. To illustrate our discussion, we show how the interface can be set up for the SIMUL8 simulation package and show its use in the actual analysis of a particular queueing model.
On-Line Analytical Processing (OLAP) technologies are being used widely for business-data analysis, and these technologies are also being used increasingly in medical applications, e.g., for patient-data analysis. The lack of effective means of handling data imprecision, which occurs when exact values are not known precisely or are entirely missing, represents a major obstacle in applying OLAP technology to the medical domain, as well as many other domains. OLAP systems are mainly based on a multidimensional model of data and include constructs such as dimension hierarchies and granularities. This paper develops techniques for the handling of imprecision that aim to maximally reusing these already existing constructs. With imprecise data now available in the database, queries are tested to determine whether or not they may be answered precisely given the available data; if not, alternative queries that are unaffected by the imprecision are suggested. When a user elects to proceed with a query that is affected by imprecision, techniques are proposed that take into account the imprecision in the grouping of the data, in the subsequent aggregate computation, and in the presentation of the imprecise result to the user. The approach is capable of exploiting existing multidimensional query processing techniques such as pre-aggregation, yielding an effective approach with low computational overhead and that may be implemented using current technology. The paper illustrates how to implement the approach using SQL databases.
Dimension reduction is critical for many database and data mining applications, such as effcient storage and retrieval of high-dimensional data. In the literature, a well-known dimension reduction scheme is Linear Discriminant Analysis (LDA). The common aspect of previously proposed LDA based algorithms is the use of Singular Value Decomposition (SVD). Due to the diffculty of designing an incremental solution for the eigenvalue problem on the product of scatter matrices in LDA, there is little work on designing incremental LDA algorithms. In this paper, we propose an LDA based incremental dimension reduction algorithm, called IDR/QR, which applies QR Decomposition rather than SVD. Unlike other LDA based algorithms, this algorithm does not require the whole data matrix in main memory. This is desirable for large data sets. More importantly, with the insertion of new data items, the IDR/QR algorithm can constrain the computational cost by applying effcient QR-updating techniques. Finally, we evaluate the effectiveness of the IDR/QR algorithm in terms of classification accuracy on the reduced dimensional space. Our experiments on several real-world data sets reveal that the accuracy achieved by the IDR/QR algorithm is very close to the best possible accuracy achieved by other LDA based algorithms. However, the IDR/QR algorithm has much less computational cost, especially when new data items are dynamically inserted.
Short vector SIMD instructions on recent microprocessors, such as SSE on Pentium III and 4, speed up code but are a major challenge to software developers. This report introduces a compiler that automatically generates C code enhanced with short vector instructions for digital signal processing (DSP) transforms, such as the fast Fourier transform (FFT).
In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming coprocessor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.
 Simultaneous data segmentation and model estimation refers to the problem of estimating a collection of models from sample data points, without knowing which points correspond to which model. This is a challenging problem in many disciplines, such as machine learning, computer vision, robotics and control, that is usually regarded as &quot;chicken-and-egg&quot;. This is because if the segmentation of the data was known, one could easily fit a single model to each group of points. Conversely, if the models were known, one could easily find the data points that best fit each model. Since in practice neither the models nor the segmentation of the data are known, most of the existing approaches start with an initial estimate for the either the segmentation of the data or the model parameters and then iterate between data segmentation and model estimation. However, the convergence of iterative algorithms to the global optimum is in general very sensitive to initialization of both the number of models and the model parameters. Finding a good initialization remains a challenging problem. This thesis
this paper was written around the  same time as Bell&apos;s landmark paper which addressed the problem of nonlocality [2]. This question of nonlocality had first been raised by Einstein, Podolsky and Rosen (EPR) who claimed that if quantum  mechanics were a complete model of reality, then nonlocal  interactions between particles had to exist. [3]. Since they felt that  nonlocality was impossible, quantum mechanics either had to be wrong or at least incomplete. An experiment was later performed that showed that nonlocal influences do exist once these particles interact and, that one can test the explicit quantum nature of systems via the use of EPR nonlocality [4]. And, as per Feynman, since this nonlocality cannot be duplicated by a classical system, this enables it to be used to test the quantum nature of systems [5]. The experiments performed and the results obtained were as follows
In overlay multicast, each receiver of the multicast group is free to choose its streaming rate subject to various constraints such as network capacity. We model the rate allocation in overlay multicast as a utility-based optimization problem. We associate a utility with each receiver, which is defined as a function of its streaming rate. Our goal is to maximize the aggregate utility of all receivers. We identify two constraints for this problem: network capacity constraint, and data constraint, which is unique in overlay multicast, mainly due to the dual role of end hosts as both receivers and senders. Based on this theoretical formalization, we propose distributed algorithms in synchronous and asynchronous network settings, both of which are proved to converge to the optimal point, where the aggregate utility of all receivers is maximized. We implement our algorithms using an end-host-based protocol. In contrast to traditional resource allocation schemes, which assume the network links to be capable of measuring flow rates, calculating and communicating control signals, our protocol purely relies on the coordination of end hosts to accomplish tasks originally assigned to network links. Our solution can be directly deployed without any changes to the existing network infrastructure. I. 
Gnutella is a classical Peer-to-Peer network designed for file-sharing. The absence of pure servers is one of its main properties, given that every Gnutella host is client and host in one. It uses the resources of the participants to distribute content, e.g. mp3 compressed audio files, and shares the processing capacity to provide the routing and searching capabilities for the network. In this work
The present paper compares the intonation of two question types in Estonian: tag questions (vi-questions) and morphosyntactically unmarked questions. Measurements of accent peaks in controlled data revealed a significant difference in the nuclear pitch accent between the two types. This finding is interpreted with reference to work on questions in other languages.
Virtual reality offers a new frontier for human interaction with simulation models. A virtual environment, such as that created with a CAVE, imposes either real-time or quasi-real-time performance on the simulation model. Beyond that general requirement, what others can be identified for simulation programs that drive a virtual reality or virtual environment interface? Based on experience with the Virginia Tech CAVE augmented by a literature search, we propose three key requirements for successful CAVEbased simulations: (1) Portability among CAVE-specific input/output devices, (2) effective and efficient interprocess communication, and (3) overcoming the limitations associated with input/output device interaction. Each requirement is described in some detail to both explain and justify its inclusion. Limitations and near- and intermediate -term research needs are identified.
Standard interfaces could help reduce the costs associated with simulation model construction and data exchange between simulation and other software applications -- and thus make simulation technology more affordable and accessible to a wide range of potential industrial users. Currently, small machine shops do not typically use simulation technology because of various difficulties and obstacles associated with model development and data translation. This paper provides an overview of work currently underway at the National Institute of Standards and Technology (NIST) to develop a software architecture, standard data interfaces, and a prototype generic machine shop simulator that can be readily, reconfigured for use by a large number of small machine shops. It also reviews prior work in this area and describes future work.
Web caches, content distribution networks, peer-to-peer file sharing networks, distributed file systems, and data grids all have in common that they involve a community of users who generate requests for shared data. In each case, overall system performance can be improved significantly if we can first identify and then exploit interesting structure within a community&apos;s access patterns. To this end, we propose a novel perspective on file sharing that considers the relationships that form among users based on the files in which they are interested. We propose a new structure that captures common user interests in data---the data-sharing graph--- and justify its utility with studies on three data-distribution systems: a high-energy physics collaboration, the Web, and the Kazaa peer-to-peer network. We find small-world patterns in the data-sharing graphs of all three communities. We analyze these graphs and propose some probable causes for these emergent small-world patterns. The significance of smallworld patterns is twofold: it provides a rigorous support to intuition and, perhaps most importantly, it suggests ways to design mechanisms that exploit these naturally emerging patterns.
In this paper we present an approach to rapid prototyping of advanced signal processing techniques for future wireless applications currently being adopted within Bell Labs Research. The aim of the &quot;Bell Labs Algorithm Development and Evaluation&quot; (BLADE) initiative is to devise a design framework specifically targeting the needs (and capabilities) of high-level algorithm designers (viz. communication engineers), which enables them to quickly &quot;translate&quot; a research idea into a working real-time system for practical experimentation purposes. The mixed DSP/FPGA implementation of a WCDMA testbed is used as an example to describe our initial experience with the proposed design methodology, which is based on a set of commercially available software tools and a platform consisting of off the shelf hardware modules.
This paper explores orbit mechanics in the Hill 3-body problem, concentrating on spacecraft  motion about Jupiter&apos;s moon Europa. Orbits about Europa are of particular interest due to the  proposed NASA Jupiter Icy Moons Orbiter Mission (JIMO). Using an averaging approach with  first order corrections, we develop an approximate theory of motion valid over a wide range of  initial conditions. Using this theory stable regions of orbital motion at high inclinations are  found. Regions where the averaging theory breaks down are characterized numerically and are  found to contain families of trajectories that are temporarily captured. We show that these  trajectories can be placed into high inclination stable orbits with a sequence of maneuvers.
Peer-to-peer (P2P) overlay networks have recently become one of the hottest topics in OS research. These networks bring with them the promise of harnessing idle storage and network resources from client machines that voluntarily join the system; self-configuration and automatic load balancing; censorship resistance; and extremely good scalability due to the use of symmetric algorithms. However, the use of unreliable client machines leads to two defects of these systems that precludes their use in a number of applications: storage is inherently unreliable, and lookup algorithms have long latencies. In this paper we propose a design of a robust peer-to-peer storage service, composed not of client nodes, but server nodes that are dedicated to running the peer-to-peer application. We argue that our system overcomes the defects of peer-to-peer systems while retaining their nice properties with the exception of utilizing spare resources of client machines. Our system is capable of surviving arbitrary failures of its nodes (Byzantine faults) and we expect it to perform and scale well, even in a wide-area network.
In this paper, we discuss the critical role of simulation input modeling in a successful simulation study. Two pitfalls in simulation input modeling are then presented and we explain how any analyst, regardless of their knowledge of statistics, can easily avoid these pitfalls through the use of the ExpertFit distribution-fitting software. We use a set of real-world data to demonstrate how the software automatically specifies and ranks probability distributions, and then tells the analyst whether the &quot;best&quot; candidate distribution is actually a good representation of the data. If no distribution provides a good fit, then ExpertFit can define an empirical distribution. In either case, the selected distribution is put into the proper format for direct input to the analyst &apos;s simulation software.
Efficient and consistent simulation data management is indispensable and a challenging problem to be solved in modeling manufacturing and business processes. An extensible markup language (XML) based simulation interface specification is being developed by the National Institute of Standards and Technology (NIST). The proposed NIST document contains a prototype generic simulation data specification, which is an endeavor to fill a void in exchanging reusable simulation data. A case study was performed at Boeing Commercial Airplanes (BCA), utilizing the NIST XML simulation interface specification. Entity classes in this case study simulation model contain asynchronous servers, multi-input-output buffers, bi-directional cranes, labors (manpower), processes, and machines on different shifts. This model can be executed from a batch control language document, that is derived from the proposed NIST XML-based simulation specification. This case study illustrates a feasible method for using the XMLbased NIST specifications in a discrete event simulation model of a manufacturing process.
We consider a generalization of edge coloring bipartite graphs in which every edge...
We demonstrate a system for capturing multi-thousand frame-per-second (fps) video using a dense array of cheap 30fps CMOS image sensors. A benefit of using a camera array to capture high-speed video is that we can scale to higher speeds by simply adding more cameras. Even at extremely high frame rates, our array architecture supports continuous streaming to disk from all of the cameras. This allows us to record unpredictable events, in which nothing occurs before the event of interest that could be used to trigger the beginning of recording.
Ensuring performance isolation and differentiation among workloads that share a storage infrastructure  is a basic requirement in consolidated data centers. Existing management tools rely on resource  provisioning to meet performance goals; they depend on detailed knowledge of the system characteristics  and the workloads. Provisioning is inherently slow to react to system and workload dynamics, and  in the general case, it is impossible to provision for the worst case.
We are concerned with the extraction of tables from exchange format representations of very diverse composite documents. We put forward a flexible representation scheme for complex tables, based on a clear distinction between the physical layout of a table and its logical structure. Relying on this scheme, we develop a new method for the detection and the extraction of tables by an analysis of the graphic lines. To deal
Using a similar analytical approach to a study in China, this paper analyzes the impact of  agricultural research on urban poverty reduction in India. State level data from 1970 to 1995  were used in the empirical analysis. It is found that in addition to its large impact on rural  poverty reduction, agricultural research investments have also played a major role in the  reduction of urban poverty. Agricultural research investments increase agricultural  production, and increased production in turn lowers food prices. The urban poor often  benefit proportionately more than the non-poor since they spend 50-80% of their income on  food. Among all the rural investments considered in this study, agricultural research has the  largest impact on urban poverty reduction per additional unit of investment. The results from  this study are similar to earlier findings for China.   Today, urban poverty still accounts for one quarter of total poverty in India, and this  share is expected to rise in the future. Policymakers cannot afford to be complacent about  this trend and continued investments are still needed to keep food prices low. Among all  government policy instruments, increased agricultural research is still the most effective way  to achieve this objective.    KEYWORDS: developing countries, India, agricultural research, urban, poverty, food price  . ii  TABLE OF CONTENTS     1. 
In this paper we give a synoptic view of the growth text processing technology  of information extraction (IE) whose function is to extract information about a  pre-specified set of entities, relations or events from natural language textsand to  record this information in structured representations called templates. Here we  describe the nature of the IE task, review the history of the area from its origins in AI  work in the 1960&apos;s and 70&apos;s till the present, discuss the techniques being used to carry  out the task, describe application areas where IE systems are or are about to be at work,  and conclude with a discussion of the challenges facing the area. What emerges is a  picture of an exciting new text processing technology with a host of new applications,  both on its own and in conjunction with other technologies, such as information  retrieval, machine translation and data mining.
That some propositions are testable, while others are not, was a fundamental idea in the philosophical program known as logical empiricism. That program is now widely thought to be defunct. Quine’s (1953) “Two Dogmas of Empiricism” and Hempel’s (1950) “Problems and Changes in the Empiricist Criterion of Meaning” are among its most notable epitaphs. Yet, as we know from Mark Twain’s comment on an obituary that he once had the pleasure of reading about himself, the report of a death can be an exaggeration. The research program that began in Vienna and Berlin continues, even though many of the specific formulations that came out of those circles are flawed and need to be replaced. Philosophers of science now generally agree that confirmation theory is a central subject. No one really doubts the importance of understanding what it takes for a statement to be confirmed or disconfirmed by an observation. There also is wide consensus that the design of experiments is an important issue, not just for philosophers, but for scientists as well. If a scientist wants to test a proposition, it is important to make sure that the experiment that is carried out actually bears on the proposition in question. Sometimes it is obvious whether this is the case, but at other times, subtle issues need to be sorted out to see whether this is so. The idea that some
Interactive storytelling (IS) is an incipient field that has not been completely  formalized yet. There is still a significant amount of research to be done.
Generating terrain models from contour input is still an important process. Most  methods have been unsatisfactory, as they either do not preserve the form of minor ridges and  valleys, or else they are poor at modelling slopes. A method is described here, based on curve  extraction and generalization techniques, that is guaranteed to preserve the topological relationships between curve segments. The skeleton, or Medial Axis Transform, can be extracted from the  Voronoi diagram of a well-sampled contour map and used to extract additional points that eliminate cases of &quot;flat triangles&quot; in a triangulation. Elevation estimates may be made at these points. Based  on this approach it is possible to make reasonable estimates of slopes for terrain models, and to  extract meaningful intermediate points for triangulated irregular networks (TINs).
We discuss some features of the new logic programming language  DALI for agents and multi-agent systems. In particular, we aim at illustrating  the treatment of proactivity, which is based on the mechanism of the internal  events. This mechanism is general and flexible, and it is different from all the  other approaches that can be found in the literature. In this paper, as a case-study  we discuss the design and implementation of an agent capable to perform simple  forms of planning. In particular, we demonstrate how it is possible in DALI to  perform STRIPS-like planning without implementing a meta-interpreter. In fact  a DALI agent, which is capable of complex proactive behavior, can build stepby  -step her plan by proactively checking for goals and possible actions.
The key problem to be solved in MBAC is to efficiently utilise the information provided by the sources and measured by the network. There is a direct relationship between the amount of information available and the resulting effectiveness of the admission algorithm. In this paper we analyse the tradeoff between complexity and effectiveness using a practial class of AC methods.
this paper is that in a composition where one of the velocities is that of light, the value c 0 (source fixed relative to receiver) is no longer the value to be used in the composition, for it stems directly from the concept of the &quot;unique wave&quot;, as opposed to the distributed wave concept. After making the required corrections we will have no reason to reject Galilean space-time from this standpoint
We construct two efficient Identity Based Encryption (IBE) systems that are selective identity secure without the random oracle model. Selective identity secure IBE is a slightly weaker security model than the standard security model for IBE. In this model the adversary must commit ahead of time to the identity that it intends to attack, whereas in the standard model the adversary is allowed to choose this identity adaptively. Our first secure IBE system extends to give a selective identity Hierarchical IBE secure without random oracles.
This paper presents some solutions of decision- making aspects integrated in an approach of process failure monitoring suitable for complex systems like flexible manufacturing systems. A hierarchical and modular structure corresponding to the monitoring and control requirements is described. Each control and monitoring module is able to accept any signal from the lower levels of the hierarchy and must be able to elaborate an acceptable solution for each failure. When a failure is  detected, the elaboration of a curative solution is furthered by the knowledge of the details of the  current situation. The prognosis function has to foresee the consequences of the failure on the future operation of the system in order to give relevant data to the decision function. Recovery evaluation is necessary to foresee what will be the consequences of the introduction of a corrective sequence on the current schedule.
It has long been hypothesized that lack of access to credit is the main reason  why, despite higher profitability of High Yielding Varieties (HYVs), farmers in  developing countries continue to allocate a portion of their land to traditional crop  varieties. The empirical testing of this hypothesis has generated a large body of  literature with differing conclusions. This paper re-examines the issue in the context  of a specially designed group based lending programs for small farmers in  Bangladesh, who neither have access to formal sources of credit nor do they qualify to  become members of other micro-credit organizations. Two measures of access to  credit, credit limit and amount borrowed at a given point in time, are used to analyze  the determinants of farm households land allocation decision. Under a variety of  model specifications, formulated within Heckmans two-step method, the results  show that credit limits from the lending programs and informal sources are significant  determinants of small farmers decision to cultivate HYV.     JEL Classification: D13; C25; O16.  Key Words: Micro-credit programs, Access to Credit, Credit Limit, Land Allocation Decision,  Selection Bias, Bangladesh.      ii     TABLE OF CONTENTS        1. 
European eels (Anguilla anguilla, L.) were fed on a commercial diet supplemented either with 15% by dry feed weight of menhaden oil (MO), an oil rich in highly unsaturated fatty acids of the n-3 series (n-3 HUFA), or with 15% by dry feed weight of coconut oil (CO), an oil composed primarily of saturated fatty acids (SFA). Following 90 days of feeding, the mean final masses of eels fed the two different oil supplements were similar, and higher than the mean final mass of a group fed the commercial diet alone. The diets created two distinct phenotypes of eels, distinguished by the fatty acid (FA) composition of their tissue lipids. Eels fed MO had significantly more total n-3 FA and n-3 HUFA in muscle and liver lipids than did eels fed CO, leading to higher n-3/n-6 and eicosapentaenoic acid/arachidonic acid ratios in the MO group. Measurements of O 2 uptake (MO 2 ) revealed that the MO group had a significantly lower routine metabolic rate (RMR) than the CO group. When exposed to progressive hypoxia, both groups regulated MO 2 at routine normoxic levels until critical water O 2 partial pressures that were statistically similar (9.621.08 kPa in MO versus 7.571.07 kPa in CO), beyond which they showed a reduction in MO 2 below RMR. The MO group exhibited a significantly lower MO 2 than the CO group throughout hypoxic exposure, but the percentage reductions in MO 2 below their relative RMR were equal in both groups. During recovery to normoxia, both groups exhibited an increase in MO 2 to rates significantly higher than their RMR. Throughout recovery, MO 2 was significantly lower in the MO group compared with the CO group, but the percentage increases in MO 2 relative to RMR were equal in both. During progressive hypoxia, neither group exhibited a marked ventilatory reflex re...
We discuss some belief revision theories and the implementation of some of them in SNeBR, the belief revision subsystem of the SNePS knowledge representation and reasoning system. The following guidelines are important to belief revision:  1. minimize information removal  2. retract less-plausible beliefs before more-plausible ones. Alterations to
Images of a visual object, such as human face, reside in a complicated manifold in the high dimensional image space, when the object is subject to variations in pose, illumination, and other factors. Viola and Jones [1, 2, 3] have successfully tackled difficult nonlinear classification problem for face detection using AdaBoost learning. Moreover, their simple-to-complex cascade of classifiers structure makes the learning and classification even more effective. While training with cascade has been used effectively in many works [4, 5, 6, 7, 2, 3, 8, 9, 10], an understanding of the role of the cascade strategy is still lacking.
This work considers the implementation of recursive identification algorithms based on hyperstability concepts with polyphase structures. It is shown that the SPR condition required for convergence of these schemes can always be met by using a sufficiently high polyphase expansion factor  M.ForagivenM , the degree of persistent excitation required for parameter convergence is obtained. When a priori knowledge about the unknown system is available, a compensating filter can be designed to avoid the need for a high M .
Computer games research has focused strongly on  improving the tactical intelligence of computer opponents.
eLearning has become an alternative approach to the traditional college and  university of education. Experiments have shown that producing of the teaching  contents for eLearning is a time-consuming and costly job. This paper introduces a  programming-free and easy-to-use multimedia authoring system, T-Cube, designed and  has already been used at the University of Trier for eLearning. As a rich-media  authoring system, T-cube constructs and presents multimedia contents to students with  multi-rate for either offline (CD/DVD/download) or online (in real time or on demand)  usage. The rich-media teaching contents, including video, audio and screenshot, are  recorded and encoded at the classroom and simultaneously published on Internet.
This paper describes an approach for using several levels of data fusion in the domain of autonomous off-road navigation. We are focusing on outdoor obstacle detection, and we present techniques that leverage on data fusion and machine learning for increasing the reliability of obstacle detection systems. We are combining color and infrared (IR) imagery with range information from a laser range finder. We show that in addition to fusing data at the pixel level, performing high level classifier fusion is beneficial in our domain. Our general approach is to use machine learning techniques for automatically deriving effective models of the classes of interest (obstacle and nonobstacle for example). We train classifiers on different subsets of the features we extract from our sensor suite and show how different classifier fusion schemes can be applied for obtaining a multiple classifier system that is more robust than any of the classifiers presented as input. We present experimental results we obtained on data collected with both the eXperimental Unmanned Vehicle (XUV) and a CMU developed robotic vehicle.
Organizations throughout the world are quickly moving to adopt process modeling and simulation as an integral part of their business decision-making and continuous improvement initiatives. With wider acceptance of simulation, these consumers are demanding tools that support a breadth of applications, scale to fit different needs through a project life cycle, and integrated with corporate modeling and database systems.
The validity and usefulness of system simulation has been well-established for decades in areas such as computer and communications systems, general manufacturing systems, and military systems along with many other areas. Recent years have seen the technique become entrenched in specific areas such as automotive manufacturing, semiconductor fabrication, and automated warehouse design.
The Semantic Web initiative has enabled a common infrastructure in knowledge representation for sharing knowledge of ontologies and instances through languages such as Resource Description Framework (RDF) and the Web Ontology Language (OWL). In this paper we present a framework for combining the shallow levels of semantic description commonly used in information extraction with the deeper semantic structures available in such ontology representation languages. In this approach named entities are considered as instances of concepts in a defined taxonomy, coreference relations as identity pools containing named entities and coreferring expressions, and events as predicateargument frames. The framework is being implemented within the Open Ontology Forge software for annotating media, currently text and images, in Web pages and locally stored document collections. We discuss the knowledge framework, some features of the system, and a road map for future development.
This module described the overview of the complete architecting method.
We consider the multi-armed bandit problem under the PAC (&quot;probably approximately correct&quot;)  model. It was shown by Even-Dar et al. (2002) that given n arms, a total of O        trials suffices in order to find an e-optimal arm with probability at least 1    d. We establish a  matching lower bound on the expected number of trials under any sampling policy. We furthermore  generalize the lower bound, and show an explicit dependence on the (unknown) statistics of the  arms. We also provide a similar bound within a Bayesian setting. The case where the statistics of  the arms are known but the identities of the arms are not, is also discussed. For this case, we provide  a lower bound of Q        on the expected number of trials, as well as a sampling  policy with a matching upper bound. If instead of the expected number of trials, we consider the  maximum (over all sample paths) number of trials, we establish a matching upper and lower bound  of the form Q        . Finally, we derive lower bounds on the expected regret, in the  spirit of Lai and Robbins.
This paper investigates the interactions of high and low performing distributed student teams using a set of categories to examine their written communication. The teams were involved in a software development project involving two universities located in different countries. This study tracks the progression and changes in the categories coded for each team&apos;s communication throughout the project&apos;s time line to determine characteristics of high and low performing teams. 
Determining the number of sources in a received wave-field is a well known and a well investigated problem. In this problem, the number of sources impinging on an array of sensors is to be estimated. The common approach for solving this problem is to use an information theoretic criterion like the Minimum Description Length (MDL), or the Akaike Information Criterion. Under the assumption that the transmitted signals are Gaussian, the MDL estimator takes both simple and intuitive form. Therefore, this estimator is commonly used even when the signals known to be non-Gaussian communication signals. However, its ability to resolve signals (resolution capacity) is limited by the number of sensors, minus one. In this paper, we study the MDL estimator that is based the correct, non-Gaussian signal distribution of digital signals. We show that this approach leads to both improved performance and improved resolution capacity, that is - the number of signals that can be detected by the resulting MDL processor is larger than the number of array sensors. In addition, a novel asymptotic performance analysis, which can be used to predict the performance of the MDL estimator analytically, is presented. Simulation results support the theoretical conclusions.
this paper we describe a Hopf-algebraic structure on a vector space which has as basis a family of trees, and survey some applications of this structure to combinatorics in Section 1, and to differential operators in Section 2. In Section 3 we indicate some possible future directions for this work. Much of the material discussed in this paper is work-in-progress. We describe many questions and give some answers
This paper describes a new method of combining ray-casting with segmentation. Volume rendering is performed at interactive rates on personal computers, and visualizations include both &quot;superficial&quot; ray-casting through a shell at each object&apos;s surface and &quot;deep&quot; ray-casting through the confines of each object. A feature of the approach is the option to smoothly and interactively dilate segmentation boundaries along all axes. This ability, when combined with selective &quot;turning off&quot; of extraneous image objects, can help clinicians detect and evaluate segmentation errors that may affect surgical planning. We describe both a...
Ultraslow diffusion is a physical model in which a plume of diffusing particles spreads at a logarithmic rate. Governing partial differential equations for ultraslow diffusion involve fractional time derivatives whose order is distributed over the interval from zero to one. This paper develops the stochastic foundations for ultraslow diffusion based on random walks with a random waiting time between jumps whose probability tail falls off at a logarithmic rate. Scaling limits of these random walks are subordinated random processes whose density functions solve the ultraslow diffusion equation. 1. 
Computer games are viewed by academics as un-grounded hack and patch experiments. Academic artificial intelligence is often viewed as unimplementable and narrow minded by the majority of non-AI programmer. By implementing
In cooperative game theory, a characterization of games with stable cores is known as  one of the most notorious open problems. We study this problem for a special case of the  minimum coloring games, introduced by Deng, Ibaraki &amp; Nagamochi, which arises from a  cost allocation problem when the players are involved in conflict. In this paper, we show that  the minimum coloring game on a perfect graph has a stable core if and only if every vertex of  the graph belongs to a maximum clique. We also consider the problem on the core largeness,  the extendability, and the exactness of minimum coloring games. As a consequence, we show  that it is coNP-complete to decide whether a given game has a large core, is extendable, or  is exact.
We have designed a readback-signal generator to provide noise-corrupted signals to a read channel simulator. It is implemented in a Xilinx Virtex-E field-programmable gate array (FPGA) device. The generator simulates in hardware the noise processes and distortions observed in hard drives. It uses embedded nonuniform random number generators to simulate the random characteristics of various disturbances in the read/write process. The signal generator can simulate readback pulses, intersymbol interference, transition noise, electronics noise, head and media nonlinearity, intertrack interference, and write timing error according to the characteristics specified by the user. A sample implementation operates at a 70-MHz clock speed. The design can easily be scaled for different error rates. The generator can be reconfigured in real time to give the user flexibility and increase the capacity of the FPGA device. The readback-signal generator can be integrated into an FPGA read channel simulator or serve as a test bench for data-recovery circuits.
Some long-held assumptions about the most demanding computations for arithmetic  coding are now obsolete due to new hardware. For instance, it is not advantageous  to replace multiplication---which now can be done with high precision in a  single CPU clock cycle---with comparisons and table-based approximations. A good  understanding of the cost of the arithmetic coding computations is needed to design  effcient implementations for the current and future processors. In this work we profile  these computations by comparing the running times of many implementations, trying  to change at most one part at a time, and avoiding small effects being masked by much  larger ones. For instance, we test arithmetic operations ranging from 16-bit integers  to 48-bit floating point; and renormalization outputs from single bit to 16 bits. To  evaluate the complexity of adaptive coding we compare static models and different  adaptation strategies. We observe that significant speed gains are possible if we do  not insist on updating the code immediately after each coded symbol. The results  show that the fastest techniques are those that effectively use the CPU&apos;s hardware:  full-precision arithmetic, byte output, table look-up decoding, and periodic updating.
When searching for multi-attribute services or products, understanding and representing user&apos;s preferences is a crucial task. However, many computer tools do not afford users to adequately focus on fundamental decision objectives, reveal hidden preferences, revise conflicting preferences, or explicitly reason about tradeoffs with competing decision goals. As a result, users often fail to find the best solution. From
A new model ecosystem of many interacting species is introduced in which the species are connected through a random matrix with a given connectivity. The model is studied both analytically and by numerical simulations. A probability distribution derived from the model is in good agreement with simulations and ffeld data. It is also shown that the connectivity, C, and the number of species, S, are linked through the scaling relation    = k(C) -1+ff , which is observed in real ecosystems. Our approach suggests a natural link between log-normal and power-law distributions of species abundances. c    2000 Elsevier Science B.V. All rights reserved.
Reactivity to I/O events is a crucial factor for the performance of modern multithreaded distributed systems. In our scheduler-centric approach, an application detects I/O events by requesting a service from a detection server, through a simple, uniform API. We show that a good choice for this detection server is the thread scheduler. This approach simplifies  application programming, significantly improves performance, and provides a much tighter control on reactivity.
Partial evaluation is a semantics-based program optimization  technique which has been investigated within different programming  paradigms and applied to a wide variety of languages. Recently, a partial  evaluation framework for functional logic programs has been proposed.
We study the stability and region of attraction properties of a family of receding horizon schemes for nonlinear systems. Using Dini&apos;s theorem on the uniform convergence of functions, we show that there is always a finite horizon for which the corresponding receding horizon scheme is stabilizing without the use of a terminal cost or terminal constraints. After showing that optimal infinite horizon trajectories possess a uniform convergence property, we show that exponential stability may also be obtained with a sufficient horizon when an upper bound on the infinite horizon cost is used as terminal cost. Combining these important cases together with a sandwiching argument, we are able to conclude that exponential stability is obtained for unconstrained receding horizon schemes with a general nonnegative terminal cost for sufficiently long horizons. Region of attraction estimates are also included in each of the results.
This paper presents the demonstration of a software system for integrative negotiation. The agents in this system conduct one-to-one negotiations, in which the values across multiple attributes are negotiated on simultaneously. It is demonstrated how the system supports both automated negotiation (i.e., conducted by a software agent) and human negotiation (where humans specify their bids). Furthermore, it is shown how, compared to fully closed negotiation, the efficiency of the reached agreements may be improved, either by using incomplete preference information revealed by the negotiation partner or by incorporating a heuristic, through which an agent uses the history of the opponent&apos;s bids in order to guess his preferences.
The paper presents a Cartesian impedance controller for exible joint robots based on the feedback of the complete state of the system, namely the motor position, the joint torque and their derivatives. The approach is applied to a quite general robot model, in which also a damping element is considered in parallel to the joint stiffness. Since passivity and asymptotic stability of the controller hold also for varying damping matrices, some possibilities of designing those gain matrices (depending on the actual inertia matrix) are addressed. The passivity of the controller relies on the usage of only motor side measurements for the position feedback. A method is introduced, which provides the exact desired link side stiffness based on this motor position information. Experimental results are validating the proposed controller.
Continued advancements in fabrication technology and reductions in feature size create challenges in maintaining both manufacturing yield rates and long-term reliability of devices. Methods based on defect detection and reduction may not offer a scalable solution due to cost of eliminating contaminants in the manufacturing process and increasing chip complexity. This paper proposes to use the inherent redundancy available in existing and future chip microarchitectures to improve yield and enable graceful performance degradation in fail-in-place systems. We introduce a new yield metric called performance averaged yield (    ) which accounts both for fully functional chips and those that exhibit some performance degradation. Our results indicate that at 250nm we are able to increase the    of a uniprocessor with only redundant rows in its caches from a base value of 85% to 98% using microarchitectural redundancy. Given constant chip area, shrinking feature sizes increases fault susceptibility and reduces the base    to 60% at 50nm, which microarchitectural redundancy then increases to 99.6%.
The dynamic and lossy nature of wireless communication poses major challenges to reliable, self-organizing multihop networks. These non-ideal characteristics are more problematic with the primitive, low-power radio transceivers found in sensor networks, and raise new issues that routing protocols must address. Link connectivity statistics should be captured dynamically through an efficient yet adaptive link estimator and routing decisions should exploit such connectivity statistics to achieve reliability. Link status and routing information must be maintained in a neighborhood table with constant space regardless of cell density. We study and evaluate link estimator, neighborhood table management, and reliable routing protocol techniques. We focus on a many-to-one, periodic data collection workload. We narrow the design space through evaluations on large-scale, high-level simulations to 50-node, in-depth empirical experiments. The most effective solution uses a simple time averaged EWMA estimator, frequency based table management, and cost-based routing.
The Simulation Modeling Language (SML^TM) is an open source, web-based, multi-language simulation development project guided by a consortium of industrial, academic and government simulation consultants, practitioners and developers. The vision of an open source simulation software initiative is to leverage the unique communication and distribution opportunities created by the internet to open the development of simulation software to a worldwide community of talented software developers, researchers and modelers. For the simulation community, the open source movement represents an opportunity to improve the quality of common core simulation functions, improve the potential for creating reusable modeling components from those core functions, and improve the ability to merge those components using XML, HLA and other simulation community standards. This paper describes the SML software, the goals of the SML organization and relates the origins, philosophy and procedures of the open source movement to the objectives and needs of the simulation community.
The traditional approach to measure the effciency of a (static) coverage task is the ratio of the intersection of the areas covered by sensors, to the total free space in the environment. Here we address the dynamic coverage problem, which requires all areas of free space in the environment to be covered by sensors in as short a time as possible. We introduce a frequency coverage metric that measures the frequency of every-point coverage, and propose a decentralized algorithm that utilizes locally available information about the environment to address this problem. Our algorithm produces exploratory, patrol-like behavior. Robots deploy communication beacons into the environment to mark previously visited areas. These nodes act as local signposts for robots which subsequently return to their vicinity. By deploying such (stationary) nodes into the environment robots can make local decisions about their motion strategy. We analyze the proposed algorithm and compare it with a baseline approach - a modified version of a static coverage algorithm described in [1].
We consider the AdaBoost procedure for boosting  weak learners. In AdaBoost, a key step is choosing  a new distribution on the training examples based  on the old distribution and the mistakes made by  the present weak hypothesis. We show how AdaBoost  &apos;s choice of the new distribution can be seen  as an approximate solution to the following problem:  Find a new distribution that is closest to the  old distribution subject to the constraint that the  new distribution is orthogonal to the vector of mistakes  of the current weak hypothesis. The distance  (or divergence) between distributions is measured  by the relative entropy. Alternatively, we could say  that AdaBoost approximately projects the distribution  vector onto a hyperplane dened by the mistake  vector. We show that this new view of AdaBoost  as an entropy projection is dual to the usual  view of AdaBoost as minimizing the normalization  factors of the updated distributions.
This paper describes DB(n) control algorithm extended on Antiwindup and state space version of algorithm. Author derived extended version of algorithm. This version of algorithm in conjunction with on-line identification has the adaptive self-tuning performance. The synthesis was derived for control loop with proportional third order behaviour of controlled plant. However, the algorithm can be applied for plants with higher order and plants with non-minimal phase or dead time, as is shown in simulation experiments. Simulation experiments were performed on analogue model of controlled plants as hybrid simulation in real time and are commented in the paper  Key words: DB control, sampling interval, control synthesis, simulation.
The way people use computers has changed in recent years, from  desktop single-machine settings to many computers and personal assistants in  widely different contexts. Personal Document Spaces (PDSs) now tend to span  several machines or locii. Moreover, the types and numbers of documents users  manipulate have also grown. The advent of pervasive computing will reinforce  this trend. In order to develop new approaches to help users manage their  PDSs, we must have an idea of what documents they contain are and how  these are organized across several locii. We performed an empirical study  where the PDSs of eleven users were analyzed in depth, allowing us to extract  a thorough characterization of those PDSs, both in terms of structure and contents.
Current concept learners are limited in their applicability as they generally rely on comparatively poor knowledge representation facilities (e.g. attribute value pairs, flattened horn clauses). The work carried out in support of my thesis has involved extending concept learning to a higher order setting by developing a novel representation based on closed Escher terms for highly structured data. The added expressiveness offered by the proposed representation results in an explosion of the search space, which is compounded by the increased complexity of its structure. This paper describes an investigation into the use of genetic programming techniques to allow the exploitation of higher-order features during the induction of structured concept descriptions.
From chronic shortages of milk, India has emerged today as the largest producer  of milk in the world crossing 80 million tonnes. This has been achieved largely through a  smallholder economy in which Operation Flood, one of the worlds largest dairy  development programme, played an important role. All this happened largely under  autarkic framework and regulated public policy dictated by import-substitution strategy.  Until 1991, the Indian dairy industry was highly regulated and protected through  quantitative restrictions (QRs) and stringent licensing provisions. Since early 1990s, India  embarked upon liberal policy framework, which got reinforced with the signing of  Uruguay Round Agreement on Agriculture (URAA) in 1994. This opening-up  increasingly exposed the Indian dairy sector to the global markets, which in-turn are  distorted by export subsidies, domestic support and prohibitive tariffs in developed  countries. This raises several issues:    negotiations, given scores of distortions that plague the world dairy markets?   promote its competitiveness in a fast globalizing world?  This study responds to these issues by empirically mapping the competitiveness of Indian  dairy sector over the period 1975-2000 and delineating policy options for international  negotiations and more importantly, domestic policy reforms, given Indias commitments  to the WTO.  ii   TABLE OF CONTENTS         I. BACKGROUND ........................................................................................................... 1    II. INDIAN DAIRY SECTOR........................................................................................... 3  Characteristics of Indian Dairy Sector .................................................................... 3  Policy Environment ............
This paper investigates whether inferences drawn about a population are sensitive to the manner by which those data are obtained. It compares information obtained using participatory appraisal techniques with a survey of households randomly drawn from a locally administered census that had been carefully revised. The community map tends to include household members who do not, in fact, reside in the enumerated locality. By contrast, the revised official census is slightly more likely to exclude household members who actually lived in the surveyed area. Controlling for the survey technique, we find that the revised official census produces higher estimates of average household size and wealth but lower estimates of total village size or wealth, than the community map. Pairwise comparison of the survey techniques, holding the households constant, shows that the community map leads, on average, to higher estimates of household size and lower estimates of wealth. iii CONTENTS Acknowledgments......................................................................................................... iv 1. 
This work presents a formal probabilistic approach for solving optimization problems in design automation. Prediction accuracy is very low especially at high levels of design flow. This can be attributed mainly to unawareness of low level layout information and variability in fabrication process. Hence a traditional deterministic design automation approach where each cost function is represented as a fixed value becomes obsolete. A new approach is gaining attention [15, 5, 2, 4, 12] in which the cost functions are represented as probability distributions and the optimization criteria is probabilistic, too. This design optimization philosophy is demonstrated through the classic buffer insertion problem [13]. Formally, we capture wirelengths as probability distributions (as compared to the traditional approach which considers wirelength as fixed values) and present several strategies for optimizing the probabilistic criteria. During the course of this work many problems are proved to be NP-Complete. Comparisons are made with the Van-Ginneken &quot;optimal under fixed wire-length&quot; algorithm. Results show that the Van-Ginneken approach generated delay distributions at the root of the fanout wiring tree which had large probability (0.91 in the worst case and 0.55 on average) of violating the delay constraint. Our algorithms could achieve 100% probability of satisfying the delay constraint with similar buffer penalty. Although this work considers wirelength prediction inaccuracies, our probabilistic strategy could be extended trivially to consider fabrication variability in wire parasitics.
Behovsbaserad IT fr bekvmt boende  Niklas Brunsberg, Per Thorslius  On todays market, users often come in second. Developing usable products and  services based on users needs to provide a high customer benefit has been a  somewhat hard to reach but desired goal for developers for a long time. Using an electronic pen and digital diaries along with interviews, this thesis examines  the users needs for products and help systems in relation to their home  environment. We evaluate digital diaries as a method of collecting and analysing user  data. Also a prototype is defined and partly developed using user centred system  design.
In this paper, we present a new recognition system for the fast detection and classification of objects in spatial 3D data. The system consists of two main components: A biologically motivated attention system and a fast classifier. Input is provided by a 3D laser scanner, mounted on an autonomous mobile robot, that acquires illumination independent range and reflectance data. These are rendered into images and fed into the attention system that detects regions of potential interest. The classifier is applied only to a region of interest, yielding a significantly faster classification that requires only 30% of the time of an exhaustive search. Furthermore, both the attention and the classification system benefit from the fusion of the bi-modal data, considering more object properties for the detection of regions of interest and a lower false detection rate in classification.
A learning machine---or a model---is usually trained by minimizing a given criterion (the expectation of the cost function) , measuring the discrepancy between the model output and the desired output. As is already well known, the choice of the cost function has a profound impact on the probabilistic interpretation of the output of the model, after training. In this work, we use the calculus of variations in order to tackle this problem. In particular, we derive necessary and sufficient conditions on the cost function ensuring that the output of the trained model approximates 1) the conditional expectation of the desired output given the explanatory variables; 2) the conditional median (and, more generally, the -quantile); 3) the conditional geometric mean; and 4) the conditional variance. The same method could be applied to the estimation of other summary statistics as well. We also argue that the least absolute deviations criterion could, in some cases, act as an alternative to the ordinary least squares criterion for nonlinear regression. In the same vein, the concept of &quot;regression quantile&quot; is briefly discussed.
Computer-based assessments usually generate a percentage mark. It is not self-evident how this relates to the final percentage mark or final grade for the work since this depends on (i) its relationship to the &quot;baseline&quot; mark expected for someone who only guesses, (ii) to the &quot;expectations&quot; for the piece of work in relation to the learning objectives and (iii) the grading scheme employed. For some question types it is possible to allow for guessing within the marking scheme for the question using negative marking but in general it is preferable to correct for guessing within a post-test grading scheme that allows for guessing. The relationship between the assessment learning objectives and essays where choice is available and topics can be avoided compared with computer-based assessments where no choice is available and topics cannot be avoided is considered. It is concluded that commonly maximum performance should not be set at a mark of 100% but that an allowance should be made for the maximum expected performance based on the learning objectives. The use of formulae in a spreadsheet to convert the marks into grades based on a statistical allowance for guessing or additionally allowing for the maximum expected mark is demonstrated. A spreadsheet pro forma containing all of the formulae for adjusting marks and determining grades can be obtained by selecting &quot;Grading&quot; from the menu at http://students.luton.ac.uk/biology/webol/.
exist only in paper form. Web based interactive access techniques for images of these documents can ensure wider dissemination and easy availability. In this paper, we have proposed an access mechanism based on word based indexing and personalized annotation. The word based indexing scheme exploits typical structural characteristics of Indian scripts. We have combined this word indexing technique with personalized annotation based hyperlinking and query scheme for providing an interactive access interface to a collection of Indian language documents.
ItPtT children (n    125) were classifiedint dyslexics, poor readers and ordinary readers. The dyslexics werefurtD4 classified int tt Boder and BakkersubtrPBD The children weretrePk wit tt form-resolving field (FRF), which measurescentre and peripheral visual recognit4P) Dyslexics show highercorrect identD--DzP) ofletDBz in tP periphery, supporty,tu notpo of a different distnt)-4z oflat--z masking. A numerical charactPkzTBEP) of individual FRFs----C2R----reliablydist--C2R----re bett-- dyslexics and ordinary readers. The wider distkT----P) ofrecognit-zsimilar  acrosstr various subtusP of dyslexia,suggest a general charactTBDP)- of visualperceptTTE and possibly adifferent visual-at-TzPtDmode.
A two-population Genetic Algorithm for constrained optimization  is exercised and analyzed. One population consists of feasible candidate solutions evolving toward optimality. Their infeasible but  promising offspring are transferred to a second, infeasible population. Four striking
h ground and open terms in a uniform way. To this aim, transition labels become pairs, whose components are called triggers (expressing the interaction of a context with its arguments) and effect (representing the behavior offered to the rest of the system, i.e. a possible context). Tiles can be represented as rectangles where the horizontal dimension is devoted to the assembling of states and the vertical dimension is dedicated to the evolution of components. Thus, triggers and effects form the left and right sides of tiles, respectively. The vertices of tiles are called interfaces, connecting the input and output observations to the initial (before the step) and final (after the step) configurations. Thanks to the abstract notions of configuration and observation, tiles allow us to develop a theoretical framework parametric in such structures (e.g. graphs or hypergraphs or trees or l-terms rather than terms), and able to capture analogies in the structures by means of suitable auxili
A new approach to the quantitative and qualitative standardless analysis of complex substances  based on experimental time-dependent vibronic luminescence spectra (in the UV and visible regions) is proposed.
Multimodal visual haptic user interfaces can be made more effective by accurately colocating the workspaces of their components. We have developed a coregistration technique for pose measurement devices based on nonlinear least squares parameter estimation. A reduced quaternion parameterization is used for representing the orientation component of coordinate transformations, which avoids the numerical instability of traditional approaches. The method is illustrated with two examples: the colocation  of a haptic device with a position tracker, and the coregistration of an optical and a magnetic tracking system.
We propose Monte Carlo algorithms to estimate the sample size and coverage of guaranteed-coverage tolerance intervals for nonnormal distributions. The current literature focuses on computation of the tolerance factor, but addresses less on the sample size, coverage, and confidence, which need to be set prior to the tolerance factor. The coverage estimation algorithm, which always converges, is based on our proof that the coverage is a quantile of an observable random variable. The sample-size estimation algorithm, which seems to converge in empirical results, is based on the general stochastic root-finding algorithm, retrospective approximation. Following previous sensitivity analysis for the tolerance factor, we analyze relationships among the sample size, coverage, and confidence.
Aggregating low-speed WAN links into a higher-speed logical link promises to improve data-transfer rates to collaborating communities of wireless mobile multihomed devices. Such bandwidth aggregation systems must adapt to link dynamics as the number of links and the channel conditions vary with time due to mobility, power dissipation, and channel interference. A monitoring architecture that accurately measures the link dynamics and promptly feeds this information to the system is vital to realize significant bandwidth aggregation performance gains. In this paper we present various architectural design alternatives for such a monitoring system, and evaluate them using both analysis and simulation. We show that a properly-designed monitoring system can accurately measure and quickly respond to changes in communication link performance while minimizing the control overhead.
Software architecture evaluation has been proposed as a means to achieve quality attributes such as maintainability and reliability in a system. The objective of the evaluation is to assess whether or not the architecture will lead to the desired quality attributes. Recently, there have been a number of evaluation methods proposed. There is, however, little consensus on the technical and non-technical issues that a method should comprehensively address and which of the existing methods is most suitable for a particular issue. This paper presents a set of commonly known but informally described features of an evaluation method and organizes them within a framework that should offer guidance on the choice of the most appropriate method for an evaluation exercise. In this paper, we use this framework to characterise eight SA evaluation methods.
In this paper, a multisensor data fusion system for object tracking is presented. It is able to track in real-time multiple targets in outdoor environments. The system can take advantage of the redundant information coming from different sensors monitoring the same scene. The measurements (positions of the targets) obtained from the available sources are fused together to obtain a more accurate estimate. Data fusion is performed considering sensor reliability at every time instant. A confidence measure has been employed to weight sensor data in the fusion process. Compared to single camera systems, the adopted approach has produced more accurate and continuous trajectories, reducing calibration and segmentation errors.
Reductions are important and time-consuming operations in many scientific codes. Effective parallelization of reductions is a critical transformation for loop parallelization, especially for sparse, dynamic applications. Unfortunately, conventional reduction parallelization algorithms are not scalable.
This paper proposes the method of language resource construction as well as rule generation as a basic step for bio-text mining. In the early researches, well defined dictionaries and rules by experts are used for named entity recognition [2, 3]. There have been researches that use various biological information resources such as SWISS-Prot, UMLS as a dictionary [5]. Extracting rules from corpus is one of the currently used methods. In these methods to make rules for named entity recognition, some useful additional information like contextual information or verbs adjacent to named entity is extracted from various corpora [1, 4]
to the expansion of arboreal structures in syntax during the same period. Feature Geometry for instance has built autosegmental structures that are self-restrictive in the sense that their arboreal properties exclude quite a number of processes and structures.  In sum, it is probably not wrong to say that the enrichment of representations was the specific answer to the problem of abstractness and overgeneration that united phonological research throughout the 80s.  The avent of constraint-based theories in the 90s has set back the role and the functional load of representations in a rather radical fashion. Different and mutually exclusive representations may be taken as the input into the constraint-chamber, and will be able to be modified in such a way that both produce the correct surface form. For instance, Optimality Theory has no claim to make on the question whether Codas exist, whether they are able to branch one, two, three, ten or twenty times. Any theory of syllabic represent
This paper focuses on ways and means of stimulating idea generation in collaborative situations involving designers, engineers, software developers, users and usability people. Particularly, we investigate tools of design, i.e. tools used in design to get ideas for a new interactive application and its use.
We introduce a formalism to represent and analyze protein-protein and  protein-DNA interaction networks. We illustrate the expressivity of this  language, by proposing a formal counterpart of Kohn&apos;s compilation on the  mammalian cell cycle control. This effectively turns an otherwise static  knowledge into a discrete transition system incorporating a qualitative description  of the dynamics. We then propose to use the Computation Tree  Logic CTL as a query language for querying the possible behaviours of the  system. We provide examples of biologically relevant queries expressed in  CTL about the mammalian cell cycle control and show the effectiveness  of symbolic model checking tools to evaluate CTL queries in this context.
Clock-gating techniques are very effective in the reduction of the switching activity in sequential logic circuits. In particular, recent work has shown that significant power reductions are possible with techniques based on finite state machine (FSM) decomposition. A serious limitation of previously proposed techniques is that they require the state transition graph (STG) of the FSM to be given or extracted from the circuit. Since the size of the STG can be exponential on the number of registers in the circuit, explicit techniques can only be applied to relatively small sequential circuits. In this paper, we present a new approach to perform FSM decomposition by direct manipulation of the circuit. This way, we do not require the STG, either explicit or implicit, thus further avoiding the limitations imposed by the use of BDDs. Therefore, this technique can be applied to circuits with very large STGs. We provide a set of experimental results that show that power consumption can be substantially reduced, in some cases by more than 70%.
this paper, a simple and robust prong features detection algorithm is proposed. A prong feature is an assisting feature that can be used in many applications. For instance, it can be used to identify a model that consists of several prong parts for model decomposition. It represents a useful feature for skeleton extraction as well as a comparable feature for object matching. In addition, it could also be a fast alignment feature for model alignment and morphing. Furthermore, it is an invariant feature for mesh simplification
This paper covers the development, design and simulation of an automatic real time control system applied in mechanical system of recovery of spur gears. using modern techniques of automatic welding. This automatic system permits to recover the initial geometric contour of the wheel by performing automatic welding runs across each tooth, keeping the mechanical properties intact.
Boolean equation system are a useful tool for verifying formulas from modal mu-calculus on transition  systems (see [18] for an excellent treatment). We are interested in an extension of boolean equation  systems with data. This allows to formulate and prove a substantially wider range of properties on much  larger and even infinite state systems. In previous works [11, 15] it has been outlined how to transform a  modal formula and a process, both containing data, to a so-called parameterised boolean equation system,  or equation system for short. In this article we focus on techniques to solve such equation systems.
Probabilistic k-testable models (usually known as k-gram models in the case of strings) can be  easily identified from samples and allow for smoothing techniques to deal with unseen events during  pattern classification. In this paper, we introduce the family of stochastic k-testable tree languages and  describe how these models can approximate any stochastic rational tree language. The model is applied  to the task of learning a probabilistic k-testable model from a sample of parsed sentences. In particular,  a parser for a natural language grammar that incorporates smoothing is shown.
this article are twofold. First, we wish to test the assumption that matching experimental conditions on initial phoneme controls voice key error. Second, we wish to provide an examination of voice key error for a common English onset phoneme, /s/, lending some emphasis to the relationship between internal circuitry of the voice key and associated error
THE USE OF RECURSIVE SIMULATION TO SUPPORT DECISIONMAKING  John B. Gilmer, Jr.
The design of workflows is a complicated task. In practice, the actual workflow processes will often differ from the processes as perceived by the management. Process mining supports the design and improvement of processes using transaction logs, which contain information about the actual process executions. This paper introduces basics about process mining, a common workflow log format and it gives an example: mining ad-hoc processes of Caramba – a process-aware collaboration system. It is discussed how Caramba-specific process information is converted to a common format using an application called TeamLog. Then EMiT – a process mining tool – mines the converted process information.
This research proposal intends to be the continuation of the previous successful colaboration...
DSGD Discussion Papers contain preliminary material and research results, and are circulated prior to a full peer review in order to stimulate discussion and critical comment. It is expected that most Discussion Papers will eventually be published in some other form, and that their content may also be revised. DSGD DISCUSSION PAPER NO. 5
Maintaining currency of search engine indices by  exhaustive crawling is rapidly becoming impossible  due to the increasing size and dynamic content  of the web. Focused crawlers aim to search only  the subset of the web related to a specific category,  and offer a potential solution to the currency  problem. The major problem in focused crawling  is performing appropriate credit assignment  to different documents along a crawl path, such  that short-term gains are not pursued at the expense  of less-obvious crawl paths that ultimately  yield larger sets of valuable pages. To address  this problem we present a focused crawling algorithm  that builds a model for the context within  which topically relevant pages occur on the web.
Real-time multimedia applications require processing of multiple data streams while maintaining responsiveness. Development of such applications can be greatly accelerated by the use of a middleware framework that abstracts operating system dependencies and provides optimized implementations of frequently used components. In this paper we present the Nizza multimedia framework which enables rapid creation, analysis, and optimization of real-time media applications. Our main goal is to provide a simplified modular design without sacrificing application performance. The framework is based on the dataflow paradigm. An integrated scheduler automates parallelism among the modules and distinguishes between sequential and combinational modules in order to leverage data parallelism. Nizza measures application performance statistics, allowing rapid development of multimedia applications and identification of performance bottlenecks. Our framework is crossplatform and has been used to develop applications on the Windows, Windows Mobile, and Linux operating systems. We present several example applications that were implemented using our framework that demonstrate the performance and usability of Nizza.
A key step in concurrent real-time system development is to build a  model from which the implementation is synthesized. It is thus important to understand  the relation between the properties of a model and its corresponding  implementation. In this paper, we first build two relations: 1) ff-weakening relations  on MITLR formulas, which are used to express real-time properties of the  system, and 2) ff-neighboring relations on timed state sequences, which are used  to describe the timing behavior of the system. Based on these relations, we formally  prove the real-time property preservation in approximations of concurrent  real-time systems. This result generalizes [11], which is restricted to sequential  real-time systems. Finally, we demonstrate how the result can be applied to the  real-time system development by a case study of a rail-road crossing system.
In this article, we investigate the use of the OO computational paradigm for the formulation of knowledge model patterns as OO analysis patterns. We seek to take advantage of research on design pattern specification, aimed at modelling patterns by means of structural and behavioural &quot;meta-level&quot; constraints, introducing appropriate modifications into the UML. We illustrate our argument with the formulation of an OO &quot;assessment pattern&quot; in analogy to the well known &quot;assessment task template&quot;.
As patents and other forms of intellectual property become more pervasive in the next generation of biotechnologies, designing polices and practices to ensure sufficient freedom to operate (i.e., the ability to practice or use an innovation) will be crucial for non-profit agencies in the developed and developing world, especially those intent on developing improved seed varieties and other technologies destined for commercial release. Are non-profits exempt from intellectual property claims? What constitutes infringement of a patent? How does a non-profit establish its freedom to operate? We address these issues in this paper and evaluate various options for accessing other people&apos;s technologies. Options include cross- licensing agreements, research-only or costfree licenses, market segmentation strategies, mergers or joint ventures, and patent pooling or clearinghouse mechanisms. Responding creatively to the new intellectual property environment will have far reaching consequences for the future of non-profit research. KEYWORDS: research, agricultural biotechnologies, patents, intellectual property . ii  ACKNOWLEDGMENTS  The authors thank Patricia Zambrano for her considerable help and the Swedish International Development Agency (SIDA) for supporting the research for this paper. iii  TABLE OF CONTENTS  1. 
This paper reviews donor experience with the design of development projects that are sensitive to gender-specific constraints. The review finds that the gap between intentions and implementation as regards gender-sensitivity is larger in agriculture than in health and nutrition. One of the reasons forwarded for this gap is the dearth of quantitative studies documenting the foregone benefits in terms of agricultural productivity of not promoting the economic advancement of women in agriculture. CONTENTS Acknowledgments ................................................... iv 1. 
The requirements, design principles, and statistical testing approaches of uniform random number generators for simulation are briefly surveyed. An object-oriented random number package where random number streams can be created at will, and with convenient tools for manipulating the streams, is presented. A version of this package is now implemented in the Arena and AutoMod simulation tools. We also test some random number generators available in popular software environments such as Microsoft&apos;s Excel and Visual Basic, SUN&apos;s Java, etc., by using them on two very simple simulation problems. They fail the tests by a wide margin.
This paper describes an architecture for the simulation of a transportation system that uses agent technology to deliver people and goods from their origins to their destinations. The intent of the architecture is to analyze incremental changes to the existing transportation paradigm to determine what type of system we could plan for in the future. Each entity within the system that has computational power is simulated as an agent. This architecture is useful for experimenting with many different algorithms and strategies for improving transportation systems.
Abrg; Resumen  1 
In a WDM optical network, each fiber link can carry a certain set of wavelengths ff=    }.
A paradigm system for the evolution of multicellular animals is constructed. In many evolutionary models, the non-linearity of the genome-phenome mapping is ignored. However, the results of evolutionary paradigm systems that did include a non-trivial, complex genome-phenome mapping have suggested a framework joining seemingly conflicting evolutionary &quot;points of view&quot; like neutral evolution, Punctuated evolution and &quot;gradualism&quot;. The embryonal
This article presents a system that can recover and track the 3D speech movements of a speaker&apos;s face for each image of a monocular sequence. A speaker-specific face model is used for tracking: model parameters are extracted from each image by an analysis-by-synthesis loop. To handle both the individual specificities of the speaker&apos;s articulation and the complexity of the facial deformations during speech, an accurate 3D model of the face geometry and an appearance model are built from real data. The geometric model is linearly controlled by only seven articulatory parameters. Appearance is seen either as a classical texture map or through local appearance of a relevant subset of 3D points. We compare several appearance models: they are either constant or depend linearly on the articulatory parameters. We evaluate these different appearance models with ground truth data.
Simple in-network data aggregation (or fusion) techniques for sensor networks have been the focus of several recent research efforts, but they are insufficient to support advanced fusion applications. We extend these techniques to future sensor networks and ask two related questions: (a) what is the appropriate set of data fusion techniques, and (b) how do we dynamically assign aggregation roles to the nodes of a sensor network ? We have developed an architectural framework, DFuse, for answering these two questions. It consists of a data fusion API and a distributed algorithm for energy-aware role assignment. The fusion API enables an application to be specified as a coarsegrained dataflow graph, and eases application development and deployment. The role assignment algorithm maps the graph onto the network, and optimally adapts the mapping at run-time using role migration. Experiments on an iPAQ farm show that the fusion API has low-overhead, and the role assignment algorithm with role migration significantly increases the network lifetime compared to any static assignment.
Building natural language spoken dialog systems  requires large amounts of human transcribed  and labeled speech utterances to reach  useful operational service performances. Furthermore,  the design of such complex systems  consists of several manual steps. The User  Experience (UE) expert analyzes and defines  by hand the system core functionalities: the  system semantic scope (call-types) and the dialog  manager strategy which will drive the  human-machine interaction. This approach is  extensive and error prone since it involves several  non-trivial design decisions that can only  be evaluated after the actual system deployment. Moreover,
Tactile displays are used to convey small-scale force and shape information to the fingertip. We describe a 6 x 6 tactile shape display design that is low in cost and easily constructed. It uses commercially available RC servomotors to actuate an array of mechanical pins. The pins deflect a maximum of 2 mm, with a resolution of 0.1 mm. The pin center spacing is 2 mm and the pin diameter is 1 mm. For the maximum deflection of 2 mm, the display can represent frequencies up to 7.5 Hz; smaller deflections lead to achievable frequencies up to 25 Hz because the servos are slew rate limited. This design is well suited to tactile display research, as it offers reasonable performance in a robust and inexpensive package.
The invention and evolution of the Dense Wavelength Division Multiplexing (DWDM) technology has brought a breakthrough to high-speed networks, and it has put a lot of pressure on research in the area of IP routers to catch up. Besides, with up-coming Quality of Service (QoS) requirements raised by a wide range of communication-intensive, real-time multimedia applications, the next-generation IP routers should be QoS-capable. Limited by the Moore&apos;s Law, one possible solution is to introduce parallelism as well as the Differentiated Service (DiffServ) scheme [5, 11] into the router architecture to provide QoS provision at a high speed and a low cost. In this paper, we propose a novel architecture called the High-Performance QoS-capable IP Router (HPQR). We address one key design issue in our architecture - the distribution of IP packets to multiple independent routing agents so that the workload at routing agents is balanced and the packet ordering is preserved. We introduce the Enhanced Hash-based Distributing Scheme (EHDS) as the solution. Simulations are carried out to study the effectiveness of EHDS. The results show that EHDS does meet our design goals very well.
In this paper, we study wireless network routing algorithms that use only short paths, for minimizing latency, and achieve good load balance, for balancing the energy use. We consider the special case when all the nodes are located in a narrow strip with width at most ff 3/2    0.86 times the communication radius. We present algorithms that achieve good performance in terms of both measures simultaneously. In addition, our algorithms only use local information and can deal with dynamic change and mobility effciently. Keywords: wireless network, load-balanced routing, short path routing  I. 
In this paper, we consider the transmission of video over multirate wireless direct-sequence code-division multiple access (DSCDMA) channels. The performance of transmitting scalable video over a multipath Rayleigh fading channel via a combination of multi-code multirate CDMA and variable sequence length multirate CDMA channel system is considered. At the other end, the signal is collected by an antenna array front and despreading is done using adaptive space-time auxiliary-vector (AV) filters. AV filter configurations suitable for multirate detection are designed and the rate-distortion optimization is carried out for each configuration. The experimental results show a comparison of the performance of such multirate DS-CDMA systems for wireless video transmission.
Conceptual data modeling for complex applications, such as multimedia and spatiotemporal applications, often results in large, complicated and difficult-to-comprehend diagrams. One reason for this is that these diagrams frequently involve repetition of autonomous, semantically meaningful parts that capture similar situations and characteristics. By recognizing such parts and treating them as units, it is possible to simplify the diagrams, as well as the conceptual modeling process. We propose to capture autonomous and semantically meaningful excerpts of diagrams that occur frequently as modeling patterns. Specifically, the paper concerns modeling patterns for conceptual design of spatiotemporal databases. Based on requirements drawn from real applications, it presents a set of modeling patterns that capture spatial, temporal, and spatiotemporal aspects. To facilitate the conceptual design process, these patterns are abbreviated by corresponding spatial, temporal, and spatiotemporal pattern abstractions, termed components. The result is more elegant and less-detailed diagrams that are easier to comprehend, but yet semantically rich. The Entity-Relationship model serves as the context for this study. An extensive example from a real cadastral application illustrates the benefits of using a component-based conceptual model.
over all possible choices. Figure 1 shows the resulting tree. In Tapestry [2], objects are published by placing pointers to them at each node along the path from the publisher to the root, and object location proceeds by checking for pointers along the path from the query source to the root. The stretch is determined by where the publish and search path first intersect, or in other words, the least common ancestor of the publisher and the query source. We now calculate the average stretch when the publisher and searcher are at distance one from each other. Half of such pairs of nearby nodes share the same parent, and so have location stretch of one, since the request need only travel a distance of one to reach the publisher. Half of the remaining pairs share the same grandparent, for these nodes, the stretch is three (see Figure 1 for an example). In general, for j log 2 n there are 2 pairs with location stretch 2 -1. The average stretch is then n ff j=1 2 (2 1),
Discrete event simulations often require a future event list  structure to manage events according to their timestamp.  The choice of an efficient data structure is vital to the  performance of discrete event simulations as 40% of the  time may be spent on its management. A Calendar Queue  (CQ) or Dynamic Calendar Queue (DCQ) are two data  structures that offers O(1) complexity regardless of the  future event list size. CQ is known to perform poorly over  skewed event distributions or when event distribution  changes. DCQ improves on the CQ structure by detecting  such scenarios in order to redistribute events. Both CQ and  DCQ determine their operating parameters (bucket widths)  by sampling events. However, sampling technique will fail  if the samples do not accurately reflect the inter-event gap  size. This paper presents a novel and alternative approach  for determining the optimum operating parameter of a  calendar queue based on performance statistics. Stress  testing of the new calendar queue, henceforth referred to as  the Statistically eNhanced with Optimum Operating  Parameter Calendar Queue (SNOOPy CQ), with widely  varying and severely skewed event arrival scenarios show  that SNOOPy CQ offers a consistent O(1) performance and  can execute up to 100 times faster than DCQ and CQ in  certain scenarios.     1 
This paper studies syntax and semantics of the Entity-Relationship (ER) and Relational  data model and their transformation. The ER model may be regarded as a  platform independent model and the Relational model as a prototypical platform specific  model. The paper studies the transformation between these models and proposes  to express that transformation again as a model.
Enabling and ruling coordination activities between  autonomous, possibly mobile, computing  entities in dynamic computing scenarios challenges  traditional approaches to distributed application  development and software engineering.
Enabled by the continued advances in storage technologies, the amounts of on-line data grow at a rapidly increasing pace. This development is witnessed in, e.g., so-called data webhouses that accumulate click streams from portals, and in other data warehouse-type applications. The presence of very large and continuously growing amounts of data introduces new challenges, one of them being the need for effective management of aged data. In very large and growing databases, some data eventually becomes inaccurate or outdated, and may be of reduced interest to the database applications. This paper offers a mechanism, termed persistent views, that aids in flexibly reducing the volume of data, e.g., by enabling the replacement of such &quot;low-interest,&quot; detailed data with aggregated data. The paper motivates persistent views and precisely defines and contrasts these with the related mechanisms of views, snapshots, and physical deletion. The paper also offers a provably correct foundation for implementing persistent views.
This paper studies a method to obtain sparseness and structure detection for a class of kernel machines related to Least Squares Support Vector Machines (LS-SVMs). The key method to derive such kernel machines is to adopt an hierarchical modeling strategy. Here, the first level consists of an LS-SVM substrate which is based upon an LS-SVM formulation with additive regularization trade-off. This regularization trade-off is tuned at higher levels such that sparse representations and/or structure detection are obtained. The conceptual levels are kept strictly separated by working with exact optimality conditions, while the hyper-parameters guide the interaction between the levels. From a computational point of view, all levels can be fused into a single convex optimization problem. Furthermore, the principle is applied in order to optimize the validation performance of the resulting kernel machine. Sparse representations as well as structure detection are obtained by using an  L    regularization scheme and a measure of maximal variation respectively at a higher level. A number of case studies indicate the usefulness of these approaches both with respect to interpretability of the final model as well as for generalization performance.
In this paper we build on previous theoretical work and describe the implementation and testing of a virus throttle - a program, based on a new approach, that is able to substantially reduce the spread of and hence damage caused by mobile code such as worms and viruses. Our approach is different from current, signature-based anti-virus paradigms in that it identifies potential viruses based on their network behaviour and, instead of preventing such programs from entering a system, seeks to prevent them from leaving. The results presented here show that such an approach is effective in stopping the spread of a real worm, W32/Nimda-D, in under a second, as well as several different configurations of a test worm.
We describe our on-going work in the creation of a web-based Intelligent  Tutoring System (ITS) for the mathematics section of the Scholastic Aptitude Test  (SAT). Wayang Oupost focuses on geometry problems, and uses web-based multimedia  to communicate concepts to the student. Decisions about problem and help selection  made on a remote web server, which stores data on student-system interactions and will  eventually reason about students&apos; cognitive abilities. Wayang has been designed with  alternative teaching strategies. Difficulties in developing ITS for high-stakes achievement  tests are analyzed.
Though musical performers routinely use eye movements to communicate with each other during musical performances, very few performers or composers have used eye tracking devices to direct musical compositions and performances. EyeMusic is a system that uses eye movements as an input to electronic music compositions. The eye movements can directly control the music, or the music can respond to the eyes moving around a visual scene. EyeMusic is implemented so that any composer using established composition software can incorporate prerecorded eye movement data into their musical compositions.
INTRODUCTION A complete coloring of agY3fi G   Eff is a partition P  =ffV    of the vertices V such that each induced subgedb    ff,  V i   P , is an independent set, and, for each pair of distinct sets V i ffV j   P , the induced subgub     V j   is not an independent set. Thelarg8W integW m for which  G has a completecoloring is called the achromatic number of thegebG and is denoted by ffffGff.  404 0196-6774/01 $35.00   2001 Elsevier Science All rigWW reserved The achromatic number was defined and studied by Harary et al. [7] and Harary and Hedetniemi [6].Computing the achromatic number for agG8 eral galb was proved NP-complete by Yannakakis and Gavril [11]. A simple proof of this fact appears in [5]. Bodlaender [1] proved, further, that the problem remains NP-complete even when we limit ourselves to connected gonne that are both intervalgterv and co-g3T3bV The NP-completeness of the achromatic number for trees was established only recently [9]. For gbGFT that are complements of trees thi
In this paper, we focus on the capacitated survivable network design problem when the survivability is expressed in terms of rerouting techniques. We propose an approach that permits to achieve optimal solution in reasonable time. Our approach uses both Benders decomposition and a cutting plane approach to reduce the high dimensionality of the problem. Some numerical results are also presented.
Most of the prior descriptions of the important relationships in Intelligent Tutoring System (ITS) projects have focused on the relationships involved in their use in classrooms, treating their presence in the classroom as a given. There has been some discussion of how intelligent tutors are developed [13] and of how an Intelligent Tutor, once developed, can be disseminated widely [5], but there has been considerably less discussion of the deployment of prototype ITSs. In this paper, we present a model of the relationships involved in deploying a prototype intelligent tutoring system in order to conduct formative evaluation. We show that field technical personnel play a pivotal role in this process, serving as vital conduits for information and negotiation between ITS researchers and school personnel such as teachers and principals. This model was developed using Contextual Inquiries [4] and interviews of project members.
Recent technology developments produce an increasingly large volume of information. Therefore visualization of these data requires sophisticated and efficient methods that take the amount of data into account. The information often gets lost or hidden in displays of traditional information visualization techniques. A significant improvement can be achieved using clustering and visual abstraction. The synergic approach introduced here combines visual and computer data mining. Its effect is demonstrated on a popular information visualization method - the parallel coordinates.
In this paper we introduce a new single pass clustering algorithm called GenIc designed with the objective of having low overall cost. We examine some of the properties of GenIc and compare it to windowed k-means. We also study its performance using experimental data sets obtained from network monitoring.
In this paper, we extend the definition of dyadic wavelets to include frequency warped wavelets. The new wavelets are generated and the transform computed in discrete-time by alternating the Laguerre transform with perfect reconstruction filterbanks. This scheme provides the unique implementation of orthogonal or biorthogonal warped wavelets by means of rational transfer functions. We show that the discrete-time warped wavelets lead to well-defined continuous-time wavelet bases, satisfying a warped form of the two-scale equation. The shape of the wavelets is not invariant by translation. Rather, the &quot;wavelet translates&quot; are obtained from one another by allpass filtering. We show that the phase of the delay element is asymptotically a fractal. A feature of the warped wavelet transform is that the cut-off frequencies of the wavelets may be arbitrarily assigned while preserving a dyadic structure. The new transform provides an arbitrary tiling of the time--frequency plane, which can be designed by selecting as little as a single parameter. This feature is particularly desirable in cochlear and perceptual models of speech and music, where accurate bandwidth selection is an issue. As our examples show, by defining pitch-synchronous wavelets based on warped wavelets, the analysis of transients and denoising of inharmonic pseudo-periodic signals is greatly enhanced.
... of hand-tailorable planners by compiling each domain description  into a separate domain-specific planner. We discuss  why and when this approach can be useful, and we present  experimental results showing that our approach produces significant  increases in the speed of planning.
Distributed programming pursues the execution of the subtasks a program can be divided into,  using a set of (possibly heterogeneous) computers. We describe preliminary work on the design  and implementation of a particular case of the more general Distributed Constraint Logic  Programming paradigm, which encompasses a wider range of solving methods and constraint  domains. This work is intended to exploit the disjunctive parallelism contained in Constraint  Logic Programming over finite domains, trying to optimize execution time. The search tree  of the labeling process is split into subtrees, intending to achieve parallelism by distributing  them across the network, among a number of workers. During exploration these subtrees are  processed, and at the same time new work is produced. A dynamic scheduling that fosters a  solidary model of work sharing is, in general, expected to offer more optimal results than static  strategies. The target of this scheduling is to keep all the workers equally busy, in order to  maximize the exploitation of parallelism as well as to minimize communication overhead.
Information retrieval is currently one of the most important usages of  the Internet. Scientific publishers are one of the sources of information, providing  access to digital libraries on a subscription basis. Contracts between publishers  and larger organizations specify how access is to be regulated. This paper  explores the legal implications and technical considerations of the use of software  agents to facilitate on- and off-campus access to the ScienceDirect digital  library.
A pattern language to deal with Business Resource Management is presented. It covers a great number of applications in business systems, including patterns for resource rental, trading and maintenance, and was designed based on practical experience in information systems development. Existing recurring patterns were applied to form patterns in our language, which were instantiated to this specific domain. The idea has been to make the language as complete as possible in order to be useful for the analysis of a wide variety of applications in this domain. Fifteen patterns are presented together with a diagram showing the precedence for checking the convenience of their utilization. Object models using UML notation are used both to present each pattern structure and a sample instantiation. The pattern language application to practical cases has shown that analysis is easily conducted, as it is a guideline for the work to be done.
In this paper we discuss the &quot;Parameterized Selforganizing Maps&quot; (PSOM) as a learning method for rapidly creating high-dimensional, continuous mappings. The PSOM can be viewed as the continous generalization of the discrete topology preserving map build by Kohonen&apos;s SOM algorithm. By making use of available topological information the PSOM shows excellent generalization capabilities from a small set of training data. Unlike most other existing approaches that are limited to the representation of input-output mappings, the PSOM provides as an important generalization a flexibly usable, continuous associate memory. This allows to represent several related mappings -- coexisting in a single and coherent framework. We pesent
Koorde is a new distributed hash table (DHT) based on Chord [15] and the de Bruijn graphs [2]. While inheriting the simplicity of Chord, Koorde meets various lower bounds, such as O(log n) hops per lookup request with only 2 neighbors per node (where n is the number of nodes in the DHT), and O(log n/ log log n) hops per lookup request with O(log n) neighbors per node.
This paper describes a top-down word image generation model for holistic handwritten word recognition. To generate a word image, it uses likelihoods based, respectively, on a linguistic model, a segmentation model, and a character generation model. In the recognition process with respect to a given input image, it first generates, for each word in a dictionary of possible words, a word image that approximates as closely as possible the input image. The model next calculates distance values between each generated word image and the input image and selects for recognition that generated word image having the smallest distance value. The proposed method has been evaluated in an experiment using handwritten word images, and results show it to be effective for use in handwritten word image recognition.
User interfaces to information systems can be considered systematic as they consist of two types of tasks performed on classes of a domain model: basic tasks performed on one class at a time (such as insert, delete, modify, sort, list, print) and complex tasks performed on parts or whole of one or several classes (e.g., tasks involving various attributes of different classes with constraints between and establishing relationships between). This paper presents how a wizard tool can produce user interfaces to such tasks according to a model-driven approach based on a domain model of the information system. This process consists of seven steps: database selection, data source selection, building the opening procedure, data source selection for control widgets, building the closing procedure, setting the size of the widgets, and laying them out. The wizard generates code for Visual Basic and eMbedded Visual Basic, thus enabling to obtain support for both stationary and mobile tasks simultaneously, while maintaining consistency.
antly, for most reactions only qualitative information is available---one may know the substrates and stoichiometry of a reaction but not much more. A mathematical representation that captures such qualitative information is that of a graph, for example, that of a substrate graph G S ff (V S , E S ). Its vertex set V S consists of all chemical compounds (substrates) that occur in the network. Two substrates S 1 ,S 2 are adjacent if there exists an edge e, i.e., e ff (S 1 ,S 2 ) ff E S , the edge set of this graph, if the two substrates occur (either as substrates or products) in the same chemical reaction. Such a network representation has the advantage of being intuitive and simple. Other graph-like representations of metabolic networks are possible, including bipartite graphs and hypergraphs [6]. However, hypergraphs are much less intuitive constructs than graphs, and the many analysis tools available for graphs have not yet been developed to the same extent for other graph representati
In general few components are reused as they are. Often, available components are incompatible with what is required. This necessitates component adaptations or the use of adapters between components. In this paper we develop algorithms for the synthesis of adapters, coercing incompatible components into meeting requirements. We concentrate on adapters for concurrent systems, where adapters are able to resolve synchronisation problems of concurrent components. A new interface model for components, which includes protocol information, allows us to generate these adapters semi-automatically.
Spreadsheets are among the most commonly used programming systems in the world. Existing spreadsheets are rife with errors, some of which have serious impacts. We are working on algorithms and strategies that automatically infer the structure of spreadsheets. This information can be used to develop systems that enable end users to develop safer spreadsheets.
Dynamic Bayesian networks are a promising approach to automatic  video content analysis which allows statistical inference and learning to be combined  with domain knowledge. When the particle filter (PF) is used for approximate  inference, video data can often be classified in real-time (supporting e.g.
In recent years, there has been increasing work in the area of content retrieval for sports. The idea is generally to extract important events or create summaries to allow personalisation of the media stream. While previous work in sports analysis has employed either the audio or video stream to achieve some goal, there is little work that explores how much can be achieved by combining the two streams. This paper combines both audio and image features to identify the key episode in tennis broadcasts. The image feature is based on image moments and is able to capture the essence of scene geometry without recourse to 3D modelling [1]. The audio feature uses PCA to identify the sound of the ball hitting the racket. The features are modelled as stochastic processes and the work combines the features using a likelihood approach. The results show that combining the features yields a much more robust system than using the features separately.
Statistical ranking and selection (R&amp;S) is a collection of experiment design and analysis techniques for selecting the &quot;population&quot; with the largest or smallest mean performance from among a finite set of alternatives. R&amp;S procedures have received considerable research attention in the stochastic simulation community, and they have been incorporated in commercial simulation software. One of the ways that R&amp;S procedures are evaluated and compared is via the expected number of samples (often replications) that must be generated to reach a decision. In this paper we argue that sampling cost alone does not adequately characterize the efficiency of ranking-and-selection procedures, and we introduce a new sequential procedure that provides the same statistical guarantees as existing procedures while reducing the expected total cost of application.
In this paper we examine the problems of synchronous collaboration of users in web-based learning environments. It is a strong challenge to develop efficient synchronous groupware systems which provide transparent collaboration of existing applications whereas participants may start at different points in time. Existing collaboration systems either provide transparency or the accommodation of latecomers. We developed a transparent support for accommodating latecomers in collaborative environments which may be integrated in any Java-based system on the web.
This report provides an evaluation of the community-level effects of the Programa Nacional de Educacion, Salud, y Alimentacion (PROGRESA) using household-level data from various rounds of PROGRESA&apos;s evaluation sample (the Encuesta de Evaluaci\n de los Hogares [ENCEL] surveys). These surveys, along with the Encuesta de  CaracterRsticas Socioecon\micas de los Hogares (ENCASEH) 1997 survey, are a  valuable source of information on household- and community-level characteristics before and after the implementation of the program.
Goal-seeking behavior in a connectionist model is demonstrated using the examples of foraging by a simulated ant and cooperative nest-building by a pair of simulated birds. The model, a control neural network, translates needs into responses. The purpose of this work is to produce lifelike behavior with a goal-seeking artificial neural network. The foraging ant example illustrates the intermediation of neurons to guide the ant to a goal in a semi-predictable environment. In the nest-building example, both birds, executing gender-specific networks, exhibit social nesting and feeding behavior directed toward multiple goals.
The population of the developing world is becoming more urban. Are poverty and undernutrition beginning to relocate to urban areas as well? We use survey data on poverty (from 8 countries) and on child undernutrition (from 14 countries) to address this question. Using data from the past 15-20 years, we find that in a majority of countries the absolute number of poor and undernourished individuals living in urban areas has increased, as has the share of poverty and undernourishment coming from urban areas. Given these trends and the current stock of knowledge as to the levels, determinants, and solutions to urban poverty and undernutrition, we argue for more research on these issues. CONTENTS  Acknowledgments ..................................................... v 1. 
This study provides some new insights into the scalability of reliable group communication mechanisms using overlays. These mechanisms use individual TCP connections for packet transfers between end-systems. End-systems store incoming packets and forward them to downstream nodes using different unicast TCP connections. In this paper we assume that buffers in end-systems are large enough for the transfers. It is shown that the throughput of the reliable overlay group communication scales in the sense that for all multicast tree sizes and topologies, the group throughput is strictly positive under natural conditions. This is in contrast with the IP supported multicast paradigm where reliable protocols have vanishing throughput when the group size tends to infinity. The scalability of packet delay and buffer occupancy is then investigated. In the absence of additional control, the occupancy of the buffer and the latency in the end-systems explodes with time. It is then shown that proactive rate throttle mechanism implemented at the source leads to finite packet latency and buffer occupancy in any end-system of the network provided certain moment conditions are satisfied by cross traffic in the routers.
Recent research has considered DNA as a medium for ultra-scale computation and for ultracompact  information storage. One potential key application is DNA-based, molecular cryptography  systems. We present some procedures for DNA-based cryptography based on one-time-pads  that are in principle unbreakable. Practical applications of cryptographic systems based on onetime  -pads are limited in conventional electronic media by the size of the one-time-pad; however  DNA provides a much more compact storage medium, and an extremely small amount of DNA  suffces even for huge one-time-pads. We detail procedures for two DNA one-time-pad encryption  schemes: (i) a substitution method using libraries of distinct pads, each of which defines  a specific, randomly generated, pair-wise mapping; and (ii) an XOR scheme utilizing molecular  computation and indexed, random key strings. These methods can be applied either for the encryption  of natural DNA or for artificial DNA encoding binary data. In the latter case, we also  present a novel use of chip-based DNA micro-array technology for 2D data input and output.
Although meaning has been a constant interest in psycholinguistics, the concepts and methods of formal semantics have been rarely used to make sense of empirical data. In this thesis we try to fill this gap presenting the results of two ERP studies on Dutch temporal semantics. In the first, we investigate the effects elicited by tense violations in temporal adverb constructions. The observed P600/SPS is interpreted as an index of semantic rather than morphosyntactic processing. Reichenbach&apos;s distinction between event and reference time is used to account for the LAN elicited by sentences in which the main verb is placed before the adverbial modifier. In the second study, we propose a refinement of an earlier experiment on English temporal connectives. We investigate the effects of manipulating world knowledge and the order in which events are mentioned in discourse. Factorial analyses and direct comparisons between conditions indicate that sentences with different inferential properties, triggered by the selective activation of world knowledge from declarative memory, have different ERP correlates. In brief, our work suggests that the use of formal semantics within the ERP paradigm can reinforce the criticism of existing hypotheses and accelerate the process of generation and testing of new ones.
The World Wide Web provides an increasingly powerful and popular publication mechanism. Web documents often contain a large number of images serving various different purposes. This paper focuses on images that are associated with a story or preview to a story. Such images often accompany the key content on a web page, thus their identification is important for applications such as web page summarization and mobile access. We present a novel algorithm for automatic identification of story/preview images which combines features extracted from both the image itself and the surrounding text. The effectiveness of this algorithm is demonstrated by experimental results on over 1500 images collected from 25 news web sites.
In this article we present the application of transformation-based learning (TBL) [1] to the task of assigning tags to postings in online chat conversations. We define a list of posting tags that have proven useful in chat-conversation analysis. We describe the templates used for posting act tagging in the context of template selection. We extend traditional approaches used in part-of-speech tagging and dialogue act tagging by incorporating regular expressions into our templates. We close with a presentation of results that compare favorably with the application of TBL in dialogue act tagging.
The rapid growth of the Web has increased the importance of decentralized metadata creation. Resource authors must create their own metadata to enable enhanced information seeking and retrieval, and they need effective interfaces to support their work. This paper reports a baseline study of author interactions with a metadata system and draws implications for the design of future interfaces.
We introduce a novel approach to modeling the dynamics  of human facial motion induced by the action of speech for the purpose  of synthesis. We represent the trajectories of a number of salient features  on the human face as the output of a dynamical system made up  of two subsystems, one driven by the deterministic speech input, and a  second driven by an unknown stochastic input. Inference of the model  (learning) is performed automatically and involves an extension of independent  component analysis to time-depentend data. Using a shapetexture  decompositional representation for the face, we generate facial  image sequences reconstructed from synthesized feature point positions.
We propose a new algorithm for the problem of state reduction in incompletely specified finite state machines. Unlike the most commonly used algorithms for this problem, our approach is not based on the enumeration of compatible sets, and, therefore, its performance is not dependent on its number. Instead, the algorithm uses techniques for finite state machine identification that are well known in the computer science literature, but have never been applied to this problem. We prove that the algorithm is exact and present results that show that, in a set of hard problems, it is much more efficient than both the explicit and implicit approaches based on the enumeration of compatible sets. We also present a complexity analysis for the special cases where worst case polynomial time bounds can be obtained and present experiments that validate empirically the bounds obtained.
Applications in financial engineering have relied heavily on Brownian Motion as a workhorse model for pricing derivative securities and implementing risk management programs. When more than one state variable is required, the standard approach is to use a multivariate Brownian Motion with constant correlations. This article briefly summarizes several important reasons why this approach is not adequate (and in some cases, can lead to disaster). Examples include fat tails, volatility clustering, large discrete jumps, parameter instability, and asymmetric correlations. Including such features makes analytic modelling less tractable, and potentially makes simulation a more attractive alternative.
this paper we deal with quasi-static problems in which finite-dimensional geometrically linear elastic structures may establish frictional contact with the surface of rigid obstacles. The nonlinearity of the three-dimensional Coulomb friction cone makes impossible the direct use of linear complementarity formulations to deal with three-dimensional frictional contact problems. In order to overcome this difficulty several formulations use pyramidal approximations of the friction cone. In this paper we consider the classical three-dimensional Coulomb friction cone without any pyramidal approximation. For a contact candidate node in the three-dimensional space, (ffu t , ffu n ) ff    and (r t , r n ) ff    denote the vector of incremental displacements and the vector of reactions, respectively. Here, the subscripts t and n denote the two tangential and the normal directions to the obstacle surface, respectively. Denoting the coefficient of friction by  &gt; 0, Coulomb&apos;s friction law  r n ff ffr t ff, r t  ffu t + r n ffffu t ff = 0, (1) can be written as the following linear complementarity condition over two second-order cones [2]  (ff n , ffu t )  (r n , r t ) = 0, ff n ff ffffu t ff, r n ff ffr t ff, (2) where ff n ff  R
In traditional computer systems, security is typically provided in a one-or-nothing manner; the system is either secure or insecure. Such an approach is insufficient for pervasive environments that contain heterogenous devices with varying computing resources. The small, portable handheld devices are often left unsecured due to their limited computing power. The approach is also inadequate for multimedia applications that require security as a controllable service attribute to maintain performance quality of service to levels that are acceptable to the users. Hence, we need a tunable and differentiable security framework. In this paper, we present a Quality of Protection(QoP) framework that resolves the inadequacies of the one-or-nothing approach by providing differential security levels for different device, user and application security requirements and preferences. We show that our QoP framework is necessary for multimedia applications to achieve the best possible security and performance levels in pervasive environments.
A considerable amount of attention has been lately paid to a number of data  hiding methods based on quantization, seeking to achieve in practice the results predicted  by Costa for a channel with side information at the encoder. In this paper we  analyze a multidimensional extension of the implementation of Costa&apos;s result known as  DC-QIM. The presented analysis is based on measuring the probabilities of decoding  error in the presence of two important kinds of additive channel distortions. DC-QIM is  able to achieve a probability of decoding error of less than 1e-02 for 0 dB of watermark to  noise ratio and only 20 samples per hidden bit. Empirical results supporting our analysis  are given both for synthetic data and real images.
This paper analyses what kind of coordination between cells is necessary when using cellular manufacture compared to a production situation that is functionally organized. Three levels of coordination are distinguished: internal, horizontal and vertical. Two types of relations between units are considered: sequential and lateral. It is shown that there is a change in coordination requirement if cellular manufacture is used in stead of functional manufacture. This is illustrated with some cases in which the production planning problems of firms that use cells in the production of parts are analyzed. 
this paper demonstrates that for the nearest, best known groups, the Local Group and the M81 group, all the major companions are redshifted with respect to the central galaxy. A total of 21 out of 21 permits a chance of only one in two million that the result could be accidental. Every test of additional groups at larger distances confirms the excess redshift of companions. (Arp 1983; Arp and Sulentic 1985; Valtonen and Byrd 1986; Arp 1987; Girardi et al. 1990)
In this paper we discuss the difficulties involved in the scheduling of applications on computational grids. We highlight two main sources of difficulties: firstly, the size of the grid rules out the possibility of using a cen-tralized scheduler
In this paper we will present a graph-transformation based method for the verification of heterogeneous first order logic (FOL) and Euler/Venn proofs. In previous work, it has been shown that a special collection of directed acyclic graphs (DAGs) can be used interchangeably with Euler/Venn diagrams in reasoning processes. Thus, proofs which include Euler/Venn diagrams can be thought of as proofs with DAGs where steps involving only Euler/Venn diagrams can be treated as particular DAG transformations. Here we will show how the characterization of these manipulations can be used to verify Euler/Venn proofs. Also, a method for verifying the use of heterogeneous Euler/Venn and FOL reasoning rules will be presented that is also based upon DAG transformations.
this paper, we focus on one problem in this general class: diagnosing computer configuration errors. In Section 2, we describe the Chronus diagnosis tool, which finds errors by searching through the timeline of previous system states. In Section 3, we relate some of our early successes and experiences with Chronus, and we describe some of its inherent limitations. After discussing related work, we conclude, and we describe future work on the more general problem of finding good configurations in a large search space
Data replication is used extensively in wide-area distributed systems to achieve low data-access latency. Minimizing the cost of the resources used for replication is a key problem in these systems. The paper proposes a method to calculate lower bounds for the replication cost required to achieve certain QoS goals. We obtain bounds for the general case as well as for certain classes of replica placement heuristics. We observe that the cost of heuristics depends heavily on the workload and QoS goal. Based on these results, we discuss the inherent properties of heuristics that affect their cost and applicability to different environments.
Complex control applications require capabilities for accommodating faults in the controlled plant. Fault accommodation involves the detection and isolation of faults, and then taking appropriate control actions to mitigate the fault effects and maintain control. This requires the integration of fault diagnostics with control in a feedback loop. This paper discusses a generic framework for building fault-adaptive control systems using a modelbased approach, with special emphasis on the modeling schemes that describe different aspects of the system at different levels of abstraction and granularity. The concepts are illustrated by a fault adaptive notional fuel system control example.
this paper, I present electroglottographic and acoustic data supporting the claim that the distinction between [s] and [z] is always categorical, i.e. that even the least voiced tokens of [z] are speakers treat presunto as a monomorphemic word, I mean that they do not treat it as a prefixed form -- but still it is likely that they are aware of the fact that the final -o is the masculine singular suffix. Thus, monomorphemic is used here as a synonym of non-prefixed
This paper introduces different types of admission control for IP Quality of Service. Three different approaches for admission control are introduced: measurement-based admission control, endpoint admission control, and policy-based admission control. The benefits of each approach are discussed relative to quality of service in an IP network. Some potential applications are also discussed.
This paper presents the Network Storage Manager (NSM)  developed in the Distributed Computing Laboratory at Jackson State  University. NSM is designed as a Java-based, high-performance, distributed  storage system, which can be utilized in the Grid environment.
IEEE International Conference on Computer Animation, Geneva, Switzerland --- May 1999  In this paper a new technique is introduced for automatically building recognisable moving 3D models of individual people. Realistic modelling of people is essential for advanced multimedia, augmented reality and immersive virtual reality. Current systems for whole-body model capture are based on active 3D sensing to measure the shape of the body surface. Such systems are prohibitively expensive and do not enable capture of high-quality photo-realistic colour. This results in geometrically accurate but unrealistic human models. The goal of this research is to achieve automatic low-cost modelling of people suitable for personalised avatars to populate virtual worlds.
this paper is based is that a theory be developed according to its fundamental underlying symmetry: for the electromagnetic field this is the symmetry of special relativity [1--5], a sub symmetry of general relativity. We accept the Poincar group as the group of special relativity, with ten generators and two invariants [6]. The electromagnetic field is considered to be a physical entity which is described by symmetry guided relations between group generators according to the following prescription [1--5]. Rotation generators are those of magnetic field components; boost generators are those of electric field components; translation generators are those of four potential components. It is shown in Sec. 2 that the Lie algebra of the Poincar group leads to relations between generator eigenvalues which, using the above prescription, are consistent with the Maxwell equations and recently inferred [1--5] cyclic relations between field components. Section 3 develops a helicity equation [7] from the underlying symmetry of the Poincar group as given in Sec. 2. This equation has been inferred independently by Dvoeglazov [8] and by Afanasev and Stepanofsky [9], following the introduction of relativistic field helicity by Ranada [10], and the earlier realization that helicity is a topological invariant [11]. The transition from the static symmetry characteristics of the Poincar group to an equation of motion (the helicity equation) is accomplished through the transition from momentum to coordinate representation P   is replaced by i    , where P    is the translation generator. This is synonymous with the well known quantum hypothesis, which is a successful calculating prescription in field theory and wave mechanics. This transition changes the fundamental group identity [12],  P ...
Facial expression interpretation, recognition and analysis is a key issue in visual communication and man to machine interaction. In this paper, we present a technique for extracting appearance parameters from a natural image or video sequence, which allow reproduction of natural looking expressive synthetic faces. This technique was used to perform face synthesis and tracking in video sequences as well as facial expression recognition and control.
We consider the distortion measure in vector quantization based  speaker identification system. The model of a speaker is a codebook generated  from the set of feature vectors from the speakers voice sample. The matching is  performed by evaluating the distortions between the unknown speech sample  and the models in the speaker database. In this paper, we introduce a weighted  distortion measure that takes into account the correlations between the known  models in the database. Larger weights are assigned to vectors that have high  discriminating power between the speakers and vice versa.
Vehicle text marks are unique features which are useful for identifying vehicles in video surveillance applications. We propose a method for finding such text marks. An existing text detection algorithm is modified such that detection is increased and made more robust to outdoor conditions. False alarm is reduced by introducing a binary image test which remove detections that are not likely to be caused by text. The method is tested on a captured video of a typical street scene.
In this paper, we introduce a novel technique for adaptive scalar quantization. Adaptivity is useful in applications, including image compression, where the statistics of the source are either not known a priori or will change over time. Our algorithm uses previously quantized samples to estimate the distribution of the source, and does not require that side information be sent in order to adapt to changing source statistics. Our quantization scheme is thus backward adaptive. We propose that an adaptive quantizer can be separated into two building blocks, namely, model estimation and quantizer design. The model estimation produces an estimate of the changing source probability density function, which is then used to redesign the quantizer using standard techniques. We introduce nonparametric estimation techniques that only assume smoothness of the input distribution. We discuss the various sources of error in our estimation and argue that, for a wide class of sources with a smooth probability density function (pdf), we provide a good approximation to a &quot;universal&quot; quantizer, with the approximation becoming better as the rate increases. We study the performance of our scheme and show how the loss due to adaptivity is minimal in typical scenarios. In particular, we provide examples and show how our technique can achieve signalto -noise ratios (SNR&apos;s) within 0.05 dB of the optimal Lloyd--Max quantizer (LMQ) for a memoryless source, while achieving over 1.5 dB gain over a fixed quantizer for a bimodal source.
this report has been developed with a linguistically based view of this process. In this paper we will attempt to briefly describe the theoretical bases for the principle features of the test. These theoretical considerations provide then, in turn, the basis for a judgment of test validity, particularly that which Cronbach (1960) in his classical analysis of validity refers to as &quot;construct validity&quot;
Over the past decade, donor-funded policies and programs designed to address  undernutrition in the Global South have shifted away from agriculture-based strategies  toward nutrient supplementation and food fortification programs. Given the potential  benefits resulting from agriculture-based nutrition interventions, this study uses Q  methodology to explore the views of a range of stakeholders from both developed and  developing countries on the value ofand constraints related togender-sensitive,  nutrition-oriented agricultural projects. The three distinct viewpoints that emerge from  this exercise all support the use of agricultural strategies to improve nutrition and  underline the importance of gender-sensitive approaches. The viewpoints differ,  however, on the relative importance of nutrition education, the strategic use of nutrient  supplementation and food fortification, and the degree to which agriculture-based  approaches have an impact on nutrition. The findings indicate that there is common  ground among a range of stakeholdersdonors, researchers, policymakers, and program  practitionerson the benefits of agriculture and gender-sensitive strategies to improve  nutrition. These areas of agreement can serve as a foundation for forging an effective  integrative strategy to improve nutrition that includes gender-sensitive agricultural  approaches.     Keywords: nutrition, agriculture, gender, Q methodology   iii Contents   Acknowledgments............................................................................................................... v    1. 
We show how distributed randomized network coding, a robust approach to multicasting in distributed network settings, can be extended to provide Byzantine modification detection without the use of cryptographic functions.
We consider codes consisting of arrays over an alphabet F , in which certain intersecting subsets of n   coordinates are required to form codewords of length n in prescribed codes over the alphabet F   . Two specific cases are studied. In the first case, referred to as a singly-intersecting coding scheme, the user data is mapped into n    (2m-1) arrays over an alphabet F , such that the n    m sub-array that consists of the left (respectively, right) m columns forms a codeword of a prescribed code of length n over    ; in particular, the center column is shared by the left and right sub-arrays. Bounds are obtained on the achievable redundancy region of singly-intersecting coding schemes, and constructions are presented which approach---and sometimes meet---these bounds. It is shown that singly-intersecting coding schemes can be applied in a certain model of broadcast channels to guarantee reliable communication. The second setting, referred to as a fully-intersecting coding scheme, maps the user data into n       m threedimensional arrays in which parallel n    m sub-arrays are all codewords of the same prescribed code over F   . Bounds and constructions are presented for these codes, with the analysis based on representing the n       m arrays as vectors over certain algebras on mm matrices.
In this paper, we present a new methodology to rapidly explore the large design space encountered in hardware /software systems. The proposed methodology is based on a fast and accurate estimation approach. It has been implemented as an extension to a hardware/software codesign flow to enable the exploration of a large number of multiprocessor architecture solutions from the very start of the design process. The effectiveness of this approach is illustrated by a significant application example.
Digital circuits are called combinational if they are memoryless: they have outputs that depend only on the current values of the inputs. Combinational circuits are generally thought of as acyclic (i.e., feed-forward) structures. And yet, cyclic circuits can be combinational. Cycles sometimes occur in designs synthesized from high-level descriptions. Feedback in such cases is carefully contrived, typically occurring when functional units are connected in a cyclic topology. Although the premise of cycles in combinational circuits has been accepted, and analysis techniques have been proposed, no one has attempted the synthesis of circuits with feedback at the logic level.
The use of the World Wide Web and Java-based mobile code provides new opportunities for distributed simulation. First, the infrastructure provided by the Internet eliminates the need for multiprocessor hardware, making it feasible to distribute simulation models over different hardware platforms through the Internet. Second, &quot;Internet-aware&quot; mobile Java code makes the applications fully portable and reusable. These enabling technologies are already exploited for distributed simulation modeling. This paper reports on the preliminary results of an on-going effort to construct a parallel discrete event simulation support system to distribute simulation experiments over the Internet with a view on simulation optimization. A research prototype for ranking and selection problems is described. The overall project goals are discussed.
Abduction can be seen as the formal inference corresponding  to human hypothesis making. It typically has the purpose of explaining  some given observation. In classical abduction, hypotheses could be  made on events that may have occurred in the past. In general, abductive  reasoning can be used to generate hypotheses about events possibly  occurring in the future (forecasting), or may suggest further investigations  that will confirm or disconfirm the hypotheses made in a previous  step (as in scientific reasoning). We propose an operational framework  based on Abductive Logic Programming, extending existing frameworks  in many respects, including accommodating dynamic observations and  hypothesis confirmation.
We consider the problem of coverage and exploration of an  unknown dynamic environment using a mobile robot. The environment  is assumed to be large enough such that constant motion by the robot is  needed to cover the environment. We present an effcient minimalist algorithm  which assumes that global information is not available (neither a  map, nor GPS). Our algorithm deploys a network of radio beacons which  assists the robot in coverage. The network is also used by the robot for  navigation. The deployed network can also be used for applications other  than coverage (such as multi-robot task allocation). Simulation experiments  are presented which show the collaboration between the deployed  network and mobile robot for the tasks of coverage/exploration, network  deployment and maintenance (repair), and mobile robot recovery (homing  behavior). We discuss a theoretical basis for our algorithm on graphs  and show the results of the simulated scenario experiments.
The problem of sequence and time-series segmentation has been discussed widely and it has been applied successfully in a variety of areas, including computational genomics, data analysis for scientific applications, and telecommunications. In many of these areas the sequences involved are multidimensional, and the goal of the segmentation is to discover sequence segments with small variability. One of the characteristics of existing techniques is that they force all dimensions to share the same segment boundaries, yet, it is often reasonable to assume that different dimensions are more correlated than others, and that concrete and meaningful states are associated only with a subset of dimensions.
Victor Teixeira de Almeida Ralf Hartmut Guting  Praktische Informatik IV  Fernuniversitat Hagen, D-58084 Hagen, Germany  {victor.almeida,  rhg}@fernuni-hagen.de  Abstract  studied in recent years. A wide and increasing range of database applications has to deal with spatial objects whose position changes continuously over time. The main interest of these applications is to effciently store and query the positions of these objects. To achieve this goal, index structures are required. Most of the proposals of index structures for moving objects deal with unconstrained 2-dimensional movement. The constrained movement is a special and a very important case of object movement. In this paper we propose a new index structure for moving objects in networks, the MON-Tree. We tested our proposal in an experimental evaluation with generated data sets. TheMON-Tree showed good scalability when increasing the number of objects and time units in the index structure, and the query window and time interval in querying.
We propose an approach that allows a robot to learn a task through  imitation, using motor representations, as suggested by recent findings in neuroscience.
ven quantitative predictions have been met with remarkable precision (e.g. Werren 1980, but see Hardy et al. 1998). In contrast, the theory has shed a rather pale light  onsex  ratio variation in vertebrates. This discrepancy is usually attributed  totax  onomic differences in the mode of  sex  determination (Williams 1979, Maynard Smith 1980, Bull &amp; Charnov 1988). Relative to the accuracy  ofsex  ratio control in haplodiploids, a chromosomal mechanism ofsex  determination, which is common in vertebrates, supposedly hinders  parentalsex  ratio manipulation. This cannot, however, be the whole story because there are several well-documented ex mples of  adaptivesex  allocation in vertebrates (Clark 1978, Conover &amp; Voorhees 1990, Daan et al. 1996, Komdeur et al. 1997, Kruuk et al. 1999).  Perhaps equally important is that most models of  sex  allocation lack the sophistication required to tackle the  complex  ities of vertebrate  sex  allocation and life histories. In addition to the mecha
This paper presents an overview of definition, identification and decision support for vague spatial objects. Vague objects are defined using concepts from fuzzy set theory. Identification is done using grey-level and multivariate textures from multivariate remote sensing images. The paper is illustrated with two case studies. The first considers an image with a composite of five textures regions, the second a real world application from the isle of Ameland.
An important goal in scheduling products through a manufacturing facility is to assure that the work is completed as close as possible to its due date. Work that is late creates downstream delays, while early completion can be detrimental if storage space is limited. This paper reports initial results in developing a scheduling procedure for an automated steel plate fabrication facility. The approach uses Tabu search combined with simulation to schedule product through a set of machines. Performance of the procedure is evaluated by comparison to the optimal solution for small problem instances, and to a good heuristic for larger problems. Results show that the Tabu search method works well for this problem. Combining Tabu search with simulation allows the incorporation of more realistic constraints on system operation.
This paper describes a methodology for solving Parameter Design (PD) problems in production and business systems of considerable complexity. The solution is aimed at determining optimum settings to system critical parameters so that each system response is at its optimum performance level with least amount of variability. When approaching such problem, analysts are often faced with four major challenges: representing the complex parameter design problem, utilizing an effective search method that is able to explore the problem&apos;s complex and large domain, making optimization decisions based on multiple and, often, conflicting objectives, and handling the stochastic variability of in system response as an integral part of the search method. to tackle such challenges, this paper proposes a solution methodology that integrates four state-of-the-art modules of proven methods: Simulation Modeling (SM), Genetic Algorithm (GA), Entropy Method (EM), and Robustness Module (RM).
With the wide acceptance of distributed computing a rapidly growing  number of application domains are emerging, leading to a growing  number of ad hoc solutions that are rigid and poorly interoperable. Our  response to this situation is a platform for building flexible and interoperable  execution environments called the Virtual Virtual Machine. This  article presents our approach, the architecture of the VVM and some of  its primary applications.
This report contributes to the field of anonymous communications over widely deployed communication networks. It describes
Linear type systems permit programmers to deallocate or explicitly recycle memory, but they are severly restricted by the fact that they admit no aliasing. This paper describes a pseudo-linear type system that allows a degree of aliasing and memory reuse as well as the ability to define complex recursive data structures. Our type system can encode conventional linear data structures such as linear lists and trees as well as more sophisticated data structures including cyclic and doubly-linked lists and trees. In the latter cases, our type system is expressive enough to represent pointer aliasing and yet safely permit destructive operations such as object deallocation. We demonstrate the flexibility of our type system by encoding two common compiler optimizations: destination-passing style and Deutsch-Schorr-Waite or &quot;link-reversal&quot; traversal algorithms.
A constant false alarm rate (CFAR) algorithm has been developed for use in multi-band mine detection. While it is often difficult to predict the spectral signatures of targets, the shape of the target may be known. This test exploits geometric target features and spectral differences between the target and the surrounding area. The algorithm is derived from a general statistical model of the data, which allows it to adapt to changing backgrounds and variable target signatures.
This paper describes a security architecture for the Mansion mobile agent system. Mansion is a logical framework designed to support large-scale heterogenous mobile agent applications. Mansion is implemented as a multilayered middleware system in which the lowest layer provides functionality that is common to most mobile agent systems and higher layers become increasingly application aware. The security architecture presented in this paper provides secure agent communication, secure mobile agent transport and startup, and secure auditing of all changes made to agents. The system uses self-certifying names for authenticating principals in the system and provides mechanisms to control information flow.
This paper describes the use of a radial basis function (RBF) neural network. It approximates the process parameters for the extrusion of a rubber profile used in tyre production.
The notion of fair e-cash schemes was suggested and implemented  in the last decade. It balances anonymity with the capability  of tracing users and transactions in cases of crime or misbehavior.
Learning styles, as well as the best ways of responding with corresponding instructional strategies, have been intensively studied in the classical educational (classroom) setting. There is much less research of application of learning styles in the new educational space, created by the Web. Moreover, authoring applications are scarce, and they do not provide explicit choices and creation of instructional strategies for specific learning styles. The main objective of the research described in this paper is to provide the authors with a tool which will allow them to incorporate different learning styles in their adaptive educational hypermedia applications. In this way, we are creating a semantically significant interface between classical learning styles and instructional strategies and the modern field of adaptive educational hypermedia.
Timely wireless communication is essential to allow real-time mobile applications, such as communication between mobile robots or inter-vehicle communication to be realized. The real-time event-based communication paradigm has been recognized as an appropriate highlevel communication scheme to connect autonomous components in large distributed control systems [1]. We investigate whether real-time event constraints can be guaranteed in a mobile ad hoc wireless network. In this
Although search engine technology has improved in recent  years, there are still many types of searches that return  unsatisfactory results. This situation can be greatly  improved if web pages use a semantic markup language to  describe their content. We have developed SHOE, a  language for this purpose, and in this paper describe a  scenario for how the language could be used by search  engines of the future. A major challenge to this system is  designing a query tool that can exploit the power of a  knowledge base while still being simple enough for the  casual user. We present the SHOE Search tool, which  allows the user to specify a context for his or her query,  and then uses the context to help the user build a query by  example.
This paper describes the exemplar based approach presented by UNED at Senseval-3. Instead of representing contexts as bags of terms and defining a similarity measure between contexts, we propose to represent terms as bags of contexts and define a similarity measure between terms. Thus, words, lemmas and senses are represented in the same space (the context space), and similarity measures can be defined between them. New contexts are transformed into this representation in order to calculate their similarity to the candidate senses. We show how standard similarity measures obtain better results in this framework. A new similarity measure in the context space is proposed for selecting the senses and performing disambiguation. Results of this approach at Senseval-3 are here reported.
The problem of allocating discrete computational resources motivates  interest in general multi-unit combinatorial exchanges. This paper considers the  problem of computing optimal (surplus-maximizing) allocations, assuming unrestricted  quasi-linear preferences. We present a solver whose pseudo-polynomial  time and memory requirements are linear in three of four natural measures of  problem size: number of agents, length of bids, and units of each resource. In  applications where the number of resource types is inherently a small constant,  e.g., computational resource allocation, such a solver offers advantages over more  elaborate approaches developed for high-dimensional problems.
This paper describes the design and implementation of Bibster, a Peer-to-Peer system for exchanging bibliographic data among Computer Science researchers. Bibster exploits ontologies in datastorage, query formulation, query-routing and answer presentation: When bibliographic entries are made available for use in Bibster, they are structured and classified according to two different ontologies. This ontological structure is then exploited to help user formulate their queries. Subsequently, the ontologies are used...
This paper introduces a Korean-Chinese-Japanese wordnet for nouns, verbs  and adjectives. This wordnet is constructed based on a hierarchy of shared semantic  categories originated from NTT Goidaikei (Hierarchical Lexical System). The Korean  wordnet has been constructed by mapping a semantic category to each Korean word  sense in a way that maps the same semantic hierarchy to the meanings of nouns, verbs,  and adjectives. The meaning of each verb searched in the corpus is compared with its  Japanese equivalent. The Chinese wordnet has been also constructed based on the same  semantic hierarchy in comparison with the Korean wordnet. In terms of the argument  structure, there is a semantic correspondence between Korean, Japanese and Chinese  verbs.
a  The idea of data transfer by physically severed connections has been applied in a simple realization of  the Lock-Keeper technology, the SingleGate Lock -  Keeper system. By means of it, the possibility of direct attacks to a protected network can be eliminated entirely and data can be exchanged between two networks through a completely secure and reliable way. As an advanced implementation of this technology, the DualGate Lock-Keeper is proposed by including another new &quot;gate&quot; unit. Along with this development,  not only the Lock-Keeper performance on data transfer,  especially the transmit speed, is improved significantly, but also some other new good characteristics appear simultaneously. All these changes make the DualGate  Lock-Keeper more efficient, flexible and applicable. Moreover, an architecture and its working principle of the Lock-Keeper Cluster which is built up by two DualGate Lock-Keeper are analyzed in detail in this paper.
this paper, we concentrate on the relation between prosody and modality semantics. In particular, we report on an experimental analysis of lexical modality and tonal disambiguation in Greek. We concentrate on two modal verbs, i.e. &quot;bo&apos;ri&quot; (fairly close to &quot;may&quot;) and &quot;&apos;prepi&quot; (fairly close to &quot;must&quot;). &quot;bo&apos;ri&quot; may convey &quot;ability&quot; and &quot;probability&quot; whereas &quot;&apos;prepi&quot; may convey &quot;obligation&quot; and &quot;logical entailment&quot;. Thus, both verbs are typical modal verbs in Greek with distinct meanings. In addition to these meanings, both verbs may however convey a variety of other meanings, which may also overlap, in accordance with the linguistic context and the pragmatic interpretation of the utterance
this paper, however, we consider a text compression method where the specific character set collating-sequence employed in encoding the text has a big impact on performance. We demonstrate that permuting the standard character collating-sequences yields a small win on Asian-language texts over gzip. We also show improved compression with our method for English texts, although not by enough to beat standard compression methods. However, we also design a class of artificial languages on which our method clearly beats gzip, often by an order of magnitude
The problem of phase estimation in a &quot;turbo receiver &quot; is considered for two different channel models. Several message passing algorithms for phase estimation are derived from the factor graph of the channel models: (1) straight sumproduct, applied to a quantized phase model; (2) LMS-type gradient methods; (3) a particle filter. All considered algorithms are suitable for use in a &quot;turbo receiver&quot; with joint iterative decoding and phase estimation.
It is known that the unit-norm constraint for equation -error based system identification is superior to the monic constraint since it produces consistent estimates for white measurement noise and also presents better approximation properties in reduced-order cases. Here, a new algorithm for unit-norm equation -error adaptive filtering is proposed. This scheme is inspired by the constrained optimization technique known as the method of multipliers. An analysis of stationary points and mean convergence properties is developed.
Existing skeletonization methods operate directly on the binary image ignoring the gray-level information.
This paper provides simulation practitioners and consumers with a grounding in how discrete-event simulation software works. Topics include discrete-event systems; entities, resources, control elements and operations; simulation runs; entity states; entity lists; and entity-list management. The implementation of these generic ideas in AutoMod and SLX is described. The paper concludes with several examples of &quot;why it matters&quot; for modelers to know how their simulation software works, including coverage of SIMAN, ProModel and GPSS/H as well as the other two tools.
Emulation is the process of exactly imitating a real system. Recent advances in simulation technology make it possible to emulate real world control systems by using a system&apos;s control logic to interact with a simulation model. Routing logic, PLC or PC control software, sequencing algorithms, and more can be integrated, tested, and debugged within a simulation environment. Simulation models communicate with control software and provide animation and statistical output for evaluating control logic and material handling systems.
Figure 4. Upper taxonomy for the ontology  For linguistics we are concerned with the grammatical qualities of instances of LINGUISTICUNIT, or those qualities which determine how instances of LINGUISTICUNIT behave in the grammar of a language. Instances of MORPHOSYNTACTICFEATURE include: TENSE, ASPECT,  MOOD, NUMBER, PERSON, PARTOFSPEECH, etc. A MORPHOSYNTACTICUNIT is said to stand in a HASGRAMINFO  relationship to particular instances of MORPHOSYNTACTICFEATURE.
The evolution of circuits with on-line built-in self-test is attempted in simulation for a full adder, a two bit multiplier and an edge triggered D-Latch. Results show that evolved designs perform full diagnosis using less or equal number of components than hand-designed equivalents.
We prove generalization error bounds for predicting entries in a partially  observed matrix by approximating the observed entries with a low-rank  matrix. To do so, we bound the number of sign configurations of lowrank  matrices using a result about realizable oriented matroids.
We extend Combinatory Categorial Grammar  (CCG) with a generalized notion of multidimensional  sign, inspired by the types of representations  found in constraint-based frameworks  like HPSG or LFG. The generalized  sign allows multiple levels to share information,  but only in a resource-bounded way through  a very restricted indexation mechanism. This  improves representational perspicuity without  increasing parsing complexity, in contrast to  full-blown unification used in HPSG and LFG. Well-
In this work, we study different mechanisms to incorporate constraints  into an evolutionary algorithm used for global optimization. The aim of the work  is twofold. First, we propose a competitive constraint-handling approach which  does not require a penalty function (nor penalty factors), and which is able to produce  very competitive results while performing less fitness function evaluations  than other algorithms representative of the state-of-the-art in the area. Second,  we measure the rate at which our approach reaches either the feasible region of  the search space or even the global optimum solution. Finally, we propose additional  test functions and perform an empirical study that aims to find some of  the features that make a constrained optimization problem difficult to solve by an  evolutionary algorithm.
This paper describes a node-centric technique for visualizing Resource  Description Framework (RDF) graphs. Nodes of interest are discovered  by searching over literals. Subgraphs for display are constructed by using  the area around selected nodes. Wider views are created by sorting and  displaying nodes based on the number of incoming and outgoing arcs. The
We examine the performance and accuracy of simulating M/G/1 queues when the service time is Pareto distributed with shape parameter, alpha, between one and three. Two applications of this problem are in insurance risk and telecommunications. When 2 &lt; alpha &lt;= 3, the theoretical distribution of the sample averages of the queue waiting times is a stable distribution. When alpha &lt;= 2, the mean waiting time does not exist. We provide a modified quantile simulation method, which is able to solve harder problems than existing methods; in addition, it requires less memory, and allows the user to emphasize accuracy or execution time. We also give numerical examples for other heavy-tailed distributions, such as the lognormal.
This paper examines one  of four methods that we have used to estimate the cost of flight in a neotropical nectarfeeding  bat Glossophaga soricina (Phyllostomidae), namely the use of kinematic and  morphological data and aerodynamic theory to estimate the mechanical power  requirements (power output) for hovering and horizontal forward flight. A hot-wire  anemometer was used to measure induced velocity (the velocity of air accelerated by the  wings) during hovering in order to estimate induced power. Our estimate of aerodynamic  power (the sum of induced, profile and parasite powers) required for a 0.0105kg  G. soricinato hover is 0.15W and our estimate of the inertial power (the power required to  oscillate the wings) is 0.19W. Thus, the total mechanical power for hovering is 0.34W or  32.4 Wkg    . The mechanical power required for horizontal forward flight, near the  minimum power flight speed (4.2ms    ) for a 0.0117kg bat is 0.14W (12.3Wkg    ), of  which 0.10W is aerodynamic power and 0.042W is inertial power. Comparison with our  results on metabolic power requirements estimated from nectar intake gives a mechanical  efficiency of 0.15 for hovering flight and of 0.11 for forward flight near the minimum  power speed
In multimedia and other streaming applications a significant portion of energy is spent on data transfers. Exploiting data reuse opportunities in the application, we can reduce this energy by making copies of frequently used data in a small local memory and replacing speed and power inefficient transfers from main off-chip memory by more efficient local data transfers. In this paper we present an automated approach for analyzing these opportunities in a program that allows modification of the program to use custom scratch pad memory configurations comprising a hierarchical set of buffers for local storage of frequently reused data. Using our approach we are able to reduce energy consumption of the memory subsystem when using a scratch pad memory by a factor of two on average compared to a cache of the same size.
Increasing factory throughput is a critical issue in the semiconductor industry, and a quick transition of material to the next location in the automation system plays a significant role in increasing throughput. A dynamic path-finding algorithm for a vehicle-based automated material handling system (AMHS) is discussed in this paper. The dynamic path-finding algorithm uses distance between nodes, node penalties, and the number of vehicles queued to calculate the total cost of a path. This paper introduces the use of historical data from the AMHS and discusses how to effectively utilize such data in critical situations to improve overall AMHS performance.
The Israeli economy business cycle properties are different from those... The goal of this paper is to provide a simple extension of the real business cycle model of a small open economy that can fit these facts. In particular, extending the standard model for two sectors of traded and non-traded goods and using the Israeli data for the parameter specification, we can well fit the above exceptional business cycle observations. Following Tesar (1993), our main deviation from the standard model is the CES utility function for traded and non-traded goods, which allows for a wide range of volatility in consumption of traded goods. The analysis starts with a small open economy with one traded good and endogenous labor supply as in Corria, Neves and Rebelo (1995) (CNR). We show that using this model with the Israeli economy moment estimators for the parameters, implies that consumption is extremely smooth when utility is Cobb-Douglas and is seventy five percent of output if the utility is as in Greenwood, Huffman and Hercowitz (1988) (GHH). Moreover, whenever...
This paper offers a mechanism, termed persistent views, that aids in flexibly reducing the volume of data, for example, by enabling the replacement of such `low-interest&apos;, detailed data with aggregated data. The paper motivates persistent views and precisely defines and contrasts these with the related mechanisms of views, snapshots and physical deletion. The paper also offers a provably correct foundation for implementing persistent views
Motivated by evidence that coordination and dependencies among engineering decisions in a software project are key to better understanding and better methods of software creation, we set out to create empirically testable theory to characterize and make predictions about coordination of engineering decisions. We demonstrate that our theory is capable of expressing some of the main ideas about coordination in software engineering, such as Conway&apos;s law and the effects of information hiding in modular design. We then used software project data to create measures and test two hypotheses derived from our theory. Our results provide preliminary support for our formulations.
Ray tracing is an image synthesis technique which simulates the interaction of light with surfaces. Most high-quality, photorealistic renderings are generated by global illumination techniques built on top of ray tracing. Real-time ray tracing has been a goal of the graphics community for many years. Unfortunately, ray tracing is a very expensive operation. VLSI technology has just reached the point where the computational capability of a single chip is suffcient for real-time ray tracing. Supercomputers and clusters of PCs have only recently been able to demonstrate interactive ray tracing and global illumination applications. In this
This chapter addresses the question how to verify distributed and communicating systems  in an effective way from an explicit process algebraic standpoint. This means  that all calculations are based on the axioms and principles of the process algebras.
This paper describes the architecture of a system being developed to defend information systems using coordinated autonomic responses. The system will also be used to test the hypothesis that an effective defense against fast, distributed information attacks requires rapid, coordinated, network-wide responses. The core components of the architecture are a run-time infrastructure (RTI), a communication language, a system model, and defensive components. The RTI incorporates a number of innovative design concepts and provides fast, reliable, exploitation-resistant communication and coordination services to the components defending the network, even when challenged by a distributed attack. The architecture can be tailored to provide scalable information assurance defenses for large, geographically distributed, heterogeneous networks with multiple domains, each of which uses different technologies and requires different policies. The architecture can form the basis of a field-deployable system. An initial version is being developed for evaluation in a testbed that will be used to test the autonomic coordination and response hypothesis.
Tutor Marked Assignments (TMAs) are the major mechanism by which Open University (OU) students receive feedback on their academic progress. This paper shows how the OU has used ICT to improve both the quality of feedback and speed of response to student work, reduce feelings of isolation and make the university seem less remote. The paper examines the system for online submission and return of TMAs, their on-screen marking, and the automated processing of scores and feedback. It shows how the system provides effective feedback to students and their tutors, using ICT to underpin the overall assessment process, and enables new teaching strategies to be used at an institutional level.
A coarse resolution primitive equation model of 1/4 resolution is implemented covering the whole Mediterranean Sea. Within this grid a 1/20 resolution model of the Liguro-Provenal basin and the northern part of the Tyrrhenian Sea is embedded. A third fine resolution model of 1/60 is nested in the latter one and simulates the dynamics of the Ligurian Sea. Comparisons between one-way and two-way nesting in simulating the Northern Current (NC) are made. The properties of the Eastern and Western Corsican Current and the Northern Current are investigated with this nesting system. Special attention is given to the variability of the NC. Meanders and interactions with Winter Intermediate Water lenses are shown. Topographic features also lead to a highly variable NC.
Introduction  The theory of codes is a fertile area at the intersection of formal language theory, error detection and correction, data compression and data security [6]. Theoretical research into codes is often interested with combinatorial properties of formal languages related to codes.  In particular, there has been substantial recent interest in classes of codes defined by certain &quot;finite subset&quot; conditions. In general, given a class C of codes and m  0, we may define a class C    m as follows:  L 2 C    m () (L    ` L; jL    j  m ) L    2 C):  Thus, for instance, a language L is an n-code if every language L    ` L of size at most n is a code [5]. Also studied are n-prefix-suffix codes [3], n-infix-outfix codes [8, 9, 7], n-intercodes [6, p. 555] and others. A general framework for defining such &quot;finite subset&quot; classes of languages is given, e.g., by Jurgensen and Konstantinidis [6, pp. 565--567].  Decidability problems for such classes of languages appear to be very difficult. I
Ito et al. [2] asked whether the regular languages are closed under (;)    . We show that they are not.  Let k  2 be arbitrary, and let \Sigma k = fff i ; fi i ; fl i ; j i g  i=1 . Then we define L k ` \Sigma    as  L k =      (ff i fi i )      (fl i j i )        fi i j i :  We claim that    (L k ) &quot;            i = fff          1 fl    2 \Delta \Delta \Delta fl    k : i j  1g: (1) and that (;)    (L k ) cannot be expressed as the intersection of k \Gamma 1 context-free languages.  We first establish (1). Let (i 1 ; i 2 ; \Delta \Delta \Delta ; i k ) 2 N    . Then note that            j 2 (\Delta \Delta \Delta (      (ff j fi j )      (fl j j j )    )[;]    fi 1 j 1 ) \Delta \Delta \Delta)[;]    fi k j k :  This establishes the right-to-left inclusion of (1). We now show the reverse inclusion. First, note that if ff 2 (;)    (L k ), then we can write ff = x 1 x 2 \Delta \Delta \Delta x k y 1 y 2 \Delta \Delta \Delta y k where x i 2 fff i ; fi i g    and y i 2 ffl i ; j i g    . To p
Let L 1 ; L 2 ` \Sigma    be languages. If L 1 ` L 2 , then we say that L 2 covers L 1 . Let C; D be classes of languages over \Sigma. We call a cover L 2 2 C of L 1 a C-minimal cover with respect to D if, for all L    2 C, L 1 ` L    ` L 2 implies L 2 \Gamma L    2 D.
Randomized incremental constructions are widely used in computational geometry, but they perform very badly on large data because of their inherently random memory access patterns. We define a biased randomized insertion order which removes enough randomness to significantly improve performance, but leaves enough randomness so that the algorithms remain theoretically optimal.
  This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve ...
says [51-year-old Cathy Tinsall] from [South  had [five children]. [The suicide note] included [lurid references] to [the economy] run under [the influence] of [Herr Pohl],  might stop [a British government] from running [its own economic policy].  ACL Student ResearchWorkshop, 10 July 2002 -- How Important is this Problem? In the PennWall Street Journal Treebank (Marcus et al., 1993): 19%   clauses 24%   clauses are preceded by complex noun phrases having the  Prep  ACL Student ResearchWorkshop, 10 July 2002 -- Clause Attachment Why not use a parser?  sentences in need of simplification don&apos;t come through a parser very well. Applications require speed Non-restrictive relative clauses are increasingly being treated the attachment decisions to anaphora resolution  ACL Student ResearchWorkshop, 10 July 2002 -- Example The board is dominated  heirs  late John T. Dorrance Jr.,  controlled about 58% of Campbell&apos;s stock.  (T/leta_s (S/np_vp (NP/det_n The_AT (N1/n board_NN1)) (V/be_ppart/- 
Bergmans and Cover identified the capacity region of the Gaussian degraded broadcast channel, where different receivers observe the transmitted signal with different signal to noise ratios. This paper presents a superposition turbo coding scheme that performs within 1 dB of the capacity region boundary of the degraded broadcast channel at BER of 10 -5 . Performance is consistent over the entire useful range of the power allocation parameter ff.
A compact color descriptor and an efficient indexing method for this descriptor are presented. The target application is similarity retrieval in large image databases using color. Colors in a given region are clustered into a small number of representative colors. The feature descriptor consists of the representative colors and their percentages in the region. A similarity measure similar to the quadratic color histogram distance measure is defined for this descriptor. The representative colors can be indexed in the three-dimensional (3-D) color space thus avoiding the high-dimensional indexing problems associated with the traditional color histogram. For similarity retrieval, each representative color in the query image or region is used independently to find regions containing that color. The matches from all of the query colors are then combined to obtain the final retrievals. An efficient indexing scheme for fast retrieval is presented. Experimental results show that this compact descriptor is effective and compares favorably with the traditional color histogram in terms of overall computational complexity.  Index Terms---Color indexing, dominant color feature, regionbased retrieval.  I. 
Streaming video as a form of media is becoming increasing popular on the Internet. Real-time media such as video requires delay constraints from the network to ensure good quality at the receiver. While watching a video stream on his portable device connected to the Internet through the last-hop wireless link, the mobile user of tomorrow will expect a good experience. But, the time-varying nature of the wireless link can cause video frames to be dropped/delayed, which can affect the quality of video at the receiver. In this paper, we propose a link layer scheme to improve the quality of MPEG video streaming over a wireless link. We use Bluetooth as the wireless technology on which to test our scheme. Our results show that the quality of streaming video can be substantially improved with our scheme, particularly in bad channel conditions.
Reinforcement learning deals with learning  optimal or near optimal policies while interacting  with the environment. Application  domains with many continuous variables are  diffcult to solve with existing reinforcement  learning methods due to the large search  space. In this paper, we use a relational representation  to define powerful abstractions  that allow us to incorporate domain knowledge  and re-use previously learned policies in  other similar problems. We also describe how  to learn useful actions from human traces using  a behavioural cloning approach combined  with an exploration phase. Since several conflicting  actions may be induced for the same  abstract state, reinforcement learning is used  to learn an optimal policy over this reduced  space. It is shown experimentally how a combination  of behavioural cloning and reinforcement  learning using a relational representation  is powerful enough to learn how to fly an  aircraft through different points in space and  different turbulence conditions.
At a fundamental level, the key challenge in data integration is to reconcile the semantics of disparate data sets, each expressed with a different database structure. I argue that computing statistics over a large number of structures offers a powerful methodology for producing semantic mappings, the expressions that specify such reconciliation. In essence, the statistics offer hints about the semantics of the symbols in the structures, thereby enabling the detection of semantically similar concepts. The same methodology can be applied to several other data management tasks that involve search in a space of complex structures and in enabling the next-generation on-the-fly data integration systems.
This document is intended to provide a statement of position of the PhD work currently being undertaken. The document consists of two elements. The first element is to provide a discussion of the work and its relationship with various areas of interest and work being undertaken by others. The second element of this document provides a summary of the work carried out so far and the future direction. It is anticipated that this document will contribute both towards the final thesis and upcoming supervisory boards. 1.3 Position Statement  1.3.1 Motivation and objectives  At present various electronic market places, auctions and negotiation systems exist, in the near future full electronic supply chains will be possible and indeed desirable to improve efficiency [1]. This situation, however, presents a problem. While humans are good at negotiations and situation analysis they are less able to handle large volumes of information and numbers of transactions. What is need is a computer-based system or strategy for handling these situations. The strategy does not need to be the perfect negotiator, although it must be competent, but it must be able to deal with more negotiations more rapidly than a human operator could. The first step in the process of creating this computer-based strategy is to develop a system for modelling the supply chains. Once the model has been developed it will be possible to describe specific supply chain situations or scenarios and begin the process of strategy development and analysis. A primary objective of the work is thus to create this modelling tool and to model various situations with it. Once this objective has been met (at least in part) a further objective is to develop strategies that are capable of handling the modelled situations. These si...
We present a formal model for multicast network protocols  working on arbitrary tree structures. We give sucient conditions under  which correctness of the protocol for all structures reduces to correctness  for the structures with at most one layer of internal nodes. If  additional conditions hold, we can reduce further to correctness for one  single structure. All these results can be applied to (an abstract version  of) the Pragmatic General Multicast protocol.
The key problems addressed by this work are: (1) how to run high-end applications on embedded systems, (2) how to provide high levels of flexibility in how, where, and when necessary processing and communication actions are performed, and (3) how to continuously meet end-user needs despite run-time variations in service locations, platform capabilities, and user requirements. Our approach addresses these problems by offering the following components. First, the InfoFabric platform supports flexibility in communication and processing. It is a lightweight publish /subscribe middleware end-users can use to subscribe to information channels of interest to them whenever they desire, and apply exactly the processing to such information they require, such that processing and communication actions are dynamically mapped to the underlying distributed embedded platforms. Second, to attain high performance and to meet embedded systems requirements like low power, new compiler and binary code generation techniques dynamically generate and install code. Run-time code generation customizes code in order to match current user needs to available platform resources. Third, to deal with run-time changes in resource availability, a kernel-level resource management mechanisms, called Q-fabric, is associated with middleware. Combined, these mechanisms efficiently carry the performance, usage, and requirements information needed for runtime adaptation of processing and communication actions.
Various Bayesian network classifier learning algorithms are implemented in Weka [10]. This note
Differential Power Analysis, first introduced by Kocher et al. in [14], is a powerful technique allowing to recover secret smart card information by monitoring power signals. In [14] a specific DPA attack against smart-cards running the DES algorithm was described. As few as 1000 encryptions were sufficient to recover the secret key. In this paper we generalize DPA attack to elliptic curve (EC) cryptosystems and describe a DPA on EC Diffie-Hellman key exchange and EC El-Gamal type encryption. Those attacks enable to recover the private key stored inside the smart-card. Moreover, we suggest countermeasures that thwart our attack.
Cet article traite de la ralisation d&apos;une tche de positionnement par asservissement visuel consistant amener une camra en face d&apos;un objet suppos plan. Notre approche est base sur l&apos;utilisation de plusieurs faisceaux laser fixs sur la camra de telle faon que leur configuration gomtrique permettent l&apos;obtention d&apos;un ensemble d&apos;informations visuelles particulirement intressant du point de vue de la loi de commande. Par consquent, le but ici de la lumire structure n&apos;est pas tant de faciliter le traitement de l&apos;image ou encore d&apos;apporter de l&apos;information dans le cas de figure d&apos;un objet faiblement textur mais surtout de conduire l&apos;laboration d&apos;une loi de commande particulirement optimise. En effet, nous montrerons que cette faon de faire permet d&apos;obtenir des proprits intressantes de dcouplage, de stabilit, mais aussi permet l&apos;obtention d&apos;une trajectoire optimale de la camra.
Software systems supporting industrial ecology, corporate material flow analysis and life cycle assessment, are normally used as environmental accounting systems. Using these software systems it is easy to model material and energy flow networks with hundreds or even thousands of processes. Environmental accounting systems are not only utilised to analyse and control existing material and energy flow systems, they are also applied to assess future scenarios in steady-state models. But these systems do not support dynamic modelling. Even though dynamic behaviour is not the main focus of environmental accounting systems, sometimes simulation models are required to estimate material and energy flows depending on specific decision criteria (stock-keeping policies, water circulation design options in manufacturing processes, etc.). This article describes a way to integrate continuous simulation approaches into an environmental accounting system such as the material flow network approach. The integration of continuous...
A new methodology for water resources systems reliability evaluation is presented. The proposed methodology considers both: mechanical reliability (probability of pipe failure) and reliability of hydraulic parameters in the nodes and links (pressure, velocity). On the basis of this methodology the model NetRel was developed. This model is useful for determining reliability of systems with different configurations and complexity. Also, methodology for optimal reliability allocation, based on genetic algorithms, is proposed. That methodology, coupled with the reliability evaluation method, is an efficient tool for solving problems of optimal allocation of water distribution network reliability.
In this paper, we introduce a new class of noise robust acoustic features derived from a new measure of autocorrelation, and explicitly exploiting the phase variation of the speech signal frame over time. This family of features, referred to as &quot;Phase AutoCorrelation&quot; (PAC) features, include PAC spectrum and PAC MFCC, among others. In regular autocorrelation based features, the correlation between two signal segments (signal vectors), separated by a particular time interval  , is calculated as a dot product of these two vectors. In our proposed PAC approach, the angle between the two vectors is used as a measure of correlation. Since dot product is usually more affected by noise than the angle, it is expected that PAC-features will be more robust to noise. This is indeed significantly confirmed by the experimental results presented in this paper. The experiments were conducted on the Numbers 95 database, on which &quot;stationary&quot; (car) and &quot;non-stationary&quot; (factory) Noisex 92 noises were added with varying SNR. In most of the cases, without any specific tuning, PAC-MFCC features perform better.
This paper presents an architecture for distributed computational economies based on peer-to-peer bartering. Our architecture is based on the position that computational economies ought to be bootstrapped based on a layer of simple and robust resource exchange. The architecture is comprised of three pieces: (i) resource discovery, (ii) secure resource peering, and (iii) bartering. Together, these pieces address the end-to-end problem of describing, discovering, and exchanging distributed resources in a secure and decentralized manner. Key in our approach is the ability to securely exchange resources across delegated paths of trust. This, combined with secure resource peering, allows peers to engage in resource exchange with directly connected peers, in addition to peers whom they do not have direct bartering relationships with. Given the bartering economy as a base, we envision an evolutionary path towards more complex scenarios by layering richer functionality at higher layers.
In this paper we investigate a new approach to the classification of mammo graphic images according to breast type based on the underlying texture contained within the breast tissue. Three methods for  quantifying the texture are considered and used as input in the evaluation of four different classifiers. In  this study we examine two classification tasks, a three-class classification problem between dense, glandular and fatty breast types and a two-class problem, differentiating between dense and fatty breast types. We use Receiver Operating Characteristic (ROC) analysis to evaluate the performance of the two-class problem. The data set used in this study is the Mammographic Image Analysis Society (MIAS) MINIMIAS database containing Medio-Lateral Oblique (MLO) views for each breast for 161 patients. For the three-class problem using a 3-layer feed-forward artificial neural network trained with conjugate gradient descent and 10-fold cross validation, we obtain a recognition rate on test of 70.4%. For the two-class problem test using a k-nearest neighbour classifier and 10-fold cross validation we  obtain the area under the ROC curve Az equal to 0.832. This study demonstrates a high sensitivity in the classification of breast types justifying the use of this prior knowledge for the detection of lesions in a proposed CAD system.
Our work is motivated by the desire to design packet switches with large aggregate capacity and fast line rates. In this paper, we consider building a packet switch from multiple lower speed packet switches operating independently and in parallel. In particular, we consider a (perhaps obvious) parallel packet switch (PPS) architecture in which arriving traffic is demultiplexed over identical lower speed packet switches, switched to the correct output port, then recombined (multiplexed) before departing from the system. Essentially, the packet switch performs packet-by-packet load balancing, or inverse multiplexing, over multiple independent packet switches. Each lower speed packet switch operates at a fraction of the line rate . For example, each packet switch can operate at rate . It is a goal of our work that all memory buffers in the PPS run slower than the line rate. Ideally, a PPS would share the benefits of an output-queued switch, i.e., the delay of individual packets could be precisely controlled, allowing the provision of guaranteed qualities of service. In this 
Well-calibrated probabilities are necessary in  many applications like probabilistic frameworks  or cost-sensitive tasks. Based on previous  success of asymmetric Laplace method in  calibrating text classi  ers&apos; scores, we propose  to use piecewise logistic regression, which is a  simple extension of standard logistic regression,  as an alternative method in the discriminative  family. We show that both methods  have the exibility to be piecewise linear  functions in log-odds, but they are based  on quite dierent assumptions. We evaluated  asymmetric Laplace method, piecewise  logistic regression and standard logistic  regression over standard text categorization  collections (Reuters-21578 and TRECAP)  with three classi  ers (SVM, Naive Bayes  and Logistic Regression Classi  er), and observed  that piecewise logistic regression performs  signi  cantly better than the other two  methods in the log-loss metric.
Active data mining constructs and evaluates possible models  explaining a dataset, and reasons about the cost and impact  of additional samples on refining and selecting among the  models. It is particularly appropriate for applications characterized  by expensive data collection, from either experiment  or simulation. This paper develops an active mining mechanism  based on a multi-level, qualitative analysis of correspondence.
Reflections exist in many natural images. For example, shiny surfaces attempt to reflect the surrounding scene, thus, resulting in a composite image that contains the mixture of reflected light and transmitted light. When viewed from a moving camera, different components appear to move differently relative to each other. Although multiple motion recovery problem has been previously studied by many researchers, few algorithms have been proposed to recover the component images themselves. In this paper, we present a novel approach to accurately separate the individual image components from a sequence of composite images. The separation of the images is realized in the Fourier domain, where each image component is represented by its frequency elements. A minimization framework exploring the magnitude and phase constraints between each frequency element is devised to find the correct motion vectors and decouple the composite frequency elements into individual components. Experimental results using both synthetic and real images demonstrate the efficiency and robustness of our algorithm.
This paper considers the joint iterative decoding of irregular low-density parity-check (LDPC) codes and channels with memory. It begins by introducing a new class of erasure channels with memory, known as generalizederasure channels. For these channels, a single parameter recursion for the density evolution of the joint iterative decoder is derived. This provides a necessary and sucient condition for decoder convergence, and allows the algebraic construction of sequences of LDPC degree distributions. Under certain conditions, these sequences can achieve the symmetric information rate (SIR) of the channel using only iterative decoding. Example code sequences are given for two channels, and it is conjectured that they each achieve the respective SIR. Keywords: joint iterative decoding, erasure channel, capacityachieving, LDPC codes  1. 
In this paper we present a review of current intensity-based approaches to modelling  the joint distribution of default times in a pool of obligors. First of all Cox  processes are introduced for a single obligor, then the probabilistic framework is  extended to the multivariate case. i    -to-default claims and percentile basket derivatives  are described as the financial claims that generate our pricing problem. The  definition of the linear correlation coeffcient is recalled and some of its limitations  as a measure of association are discussed. Three classes of model from the existing  literature are then analysed: models relying on the assumption of conditional  independence of default times, contagion models and models based on a copula representation  of dependent defaults. In particular we show that copula functions give  a complete representation of the dependence structure of continuous multivariate  distributions and separate it from the univariate margins. Among scalar measures  of association, nonparametric correlation coeffcients are shown to be more robust  global estimators of dependence than the Pearson coeffcient, especially outside of  the the class of elliptical distributions. Finally, we briefly discuss upper and lower  tail dependence as local dependence measures of bivariate extremes.
Recommender systems have been usually designed to support a single  user in a one-to-one relation between a human and a service provider. This paper  presents a collaborative radio community where the system delivers a personalization  service on the fly, on the basis of the group recommending, promoting a  shift from the one-to-one approach to a one-to-group scenario where the goal is  assisting people in forming communities.
. Let Y be a real algebraic subset of R  m  and let F : Y ! R  n  be a polynomial map. We show that there exist real polynomial functions g1 ; : : : ; g s on R  n  such that the Euler characteristic of fibres of F is the sum of signs of g i . The purpose of this paper is to give a new, self-contained and elementary proof of the following result.  Theorem. Let Y be a real algebraic subset of R  m  and let F : Y ! R  n  be a polynomial map. Then there exist real polynomials g 1 (y); : : : ; g s (y) on R  n  such that the Euler characteristic of fibres of F is the sum of signs of g i , that is (F  \Gamma1  (y)) = sgn g 1 (y) + \Delta \Delta \Delta + sgn g s (y): Our proof is based on a classical and elementary result expressing the number of real roots of a real polynomial of one variable as the signature of an associated quadratic form known already to Hermite [He1, He2] and Sylvester [Syl], see also [B], [BW], [BCR, p. 97]. In the proof we use a modern generalized version of this result...
The theory is that free and open source software is private property under the guise of common property. Such software is distributed mostly under the GNU General Public License. The intents in The GNU Manifesto suggest striking similarities between this license and communism. The resulting economic properties, however, are similar to those of Chinese-style socialism: both resulted from an increased separation of legal and economic ownership. The phenomenal growth of China in the last twenty five years and  of such software in the past few years could be attributed to such separation.
Dimitar P. Guelev ff  June 6, 2003  
Contents  1 Acknowledgments 5 2 Introduction 6 3 Theoretical Background 7 3.1 Digital models of a terrain . . . . . . . . . . . . . . . . . . . . 7 3.2 Visibility problems on terrains . . . . . . . . . . . . . . . . . 7 3.3 Concept of horizon . . . . . . . . . . . . . . . . . . . . . . . . 9 3.4 Existing algorithms for point visibility . . . . . . . . . . . . . 9 3.4.1 A brute-force approach . . . . . . . . . . . . . . . . . 9 3.4.2 An O(nff(n) log n) algorithm . . . . . . . . . . . . . . 10 4 Description of the Algorithm 13 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2 Topological sorting of the edges . . . . . . . . . . . . . . . . . 14 4.3 A divide-and-conquer approach . . . . . . . . . . . . . . . . . 14 4.3.1 The divide part . . . . . . . . . . . . . . . . . . . . . . 15 4.3.2 The conquer part . . . . . . . . . . . . . . . . . . . . . 15 5 Implementation and Integration in the RA 3 DIO project 20 5.1 Problems and solutions . . . . . . . . . . . . 
. This paper is concerned with various schemes for enhancing the performance of modal tableau procedures. It discusses techniques and strategies for dealing with the nondeterminism in tableau calculi, as well as simplification and backjumping. Benchmark results obtained with randomly generated modal formulae show the effect of combinations of different schemes. 1 Introduction  Usually the literature on theorem provers for modal logic confines itself to a description of the underlying calculus and methodology. Sometimes the description is accompanied with a consideration of the worst-case complexity of an algorithm based on the presented calculus or a small collection of benchmark results. Problems arising when implementing modal theorem provers and also considerations concerning optimisations towards increased effciency have received much less attention, which, of course, is typical in a field under development. Sometimes the description of the theorem prover mentions some simplification ...
Future time-division-duplex (TDD) systems operating over small wireless networks will utilize intelligent base station (BS)-coordinated dynamic channel-allocation algorithms in order to support high-bandwidth asymmetric traffic in adjacent cells. In this paper, we use extensive measurements of wireless Internet traffic from a large 802.11b network to create two random traffic models. One model, called &quot;binomial,&quot; is memoryless and the other, called &quot;dynamic,&quot; is based on an event-driven Markov state model with bidirectional flows and deterministic residence times. We then develop a two-BS two-zone wireless TDD interference model that describes the spatial features of interference between cochannel mobile stations (MSs) in adjacent BSs. This is a simplified precursor to more sophisticated models for multiple BSs and/or multisector BSs. We present a set of candidate TDD channel-allocation algorithms, which vary in their level of time-slot coordination and intelligent allocation between BSs. Lastly, we combine the three components (i.e., traffic models, interference models, and channel-allocation algorithms) to demonstrate the capacity for evaluating dynamic channel-allocation algorithms in realistic interference and Internet traffic scenarios. The results show that, for active MSs, the dynamic traffic model has a higher number of packet requests per time frame than the binomial traffic model, given the same mobile activity factor. Additionally, fixed channel-allocation algorithms generally perform much worse than pseudorandom and intelligent BS-coordinated algorithms, especially for asymmetric BSs. The pseudorandom algorithm performs well at low traffic, but suffers from severe interference blocking at high traffic. The intelligent BS-coordinated algorithm performs best, ...
Market forces are the most effective mechanism to fairly and efficiently allocate resources among competing service requests. However, for distributed resources, the implementation of a coherent market mechanism can be complex and costly. In this paper, we present the design and prototype implementation of a distributed resource allocation system that allows to apply flow-based market mechanisms to a network domain. The system design guarantees a constant execution complexity and an extremely simple layout of internal nodes. All relevant intelligence is located in the edge systems. We explain how the system can be used to realize various types of market mechanisms and show the potential for efficient implementation by means of lab experiments using the system prototype. This work is based on earlier conceptual proposals and theoretical analysis. It is focused on the system design and implementation aspects, as well as on questions of detail, which are usually ignored by existing theory work.
This paper presents a step-by-step decomposition of several locality-aware networks, that support distributed content-based location services. It explains their common principles and their variations with simple and clear intuition on analysis. Section 2 describes a novel technique for robustifying locality-aware overlays
Extraction of map objects such as roads, railroads, rivers and building boundaries from 1m resolution space images is one of the important research issue. Automation of this task is crucial for the success of the application but full and reliable automation is yet to be achieved. This paper describes the development of algorithms to extract two major map objects, roads and buildings. We adopt the &quot;semi-automatic&quot; approach for reliability and efficiency. For road extraction, we designed a new least squares template matching algorithm. For buildings, we combined line analysis and template matching for semi-automatic extraction. Our algorithms were tested with IKONOS images over a very dense urban scene. The algorithms developed showed promising results. The major contribution of this paper is the development of monoscopic algorithms little human intervention that produces a fair amount of information.
Large-scale computing often consists of many speculative tasks to test hypotheses, search for insights, and review potentially finished products. For example, speculative tasks are issued by bioinformaticists comparing DNA sequences and computer graphics artists adjusting scene properties. This paper promotes a new computing model for shared clusters and grids in which researchers and end-users exploring search spaces disclose sets of speculative tasks, request results as needed, and cancel unfinished tasks if early results suggest no need to continue. Doing so matches natural usage patterns, making users more effective, and also enables a new class of schedulers.
Majority of Bantu languages encode subjects by head-marking and objects by positional licensing. This reflects a point in the historical process whereby positional licensing of objects becomes obligatory due to the loss of inflecctional morphology. What we observe in synchronic grammar is considerable variation both across and within languages in the use of head-marking morphology for objects. This paper examines this variation under the general concept of DIFFERENTIAL OBJECT MARKING (DOM). I show that an Optimality-Theoretic LFG account of DOM in Bantu enables us to provide a unified account of differential marking of objects across typologically diverse languages---realized by case, agreement, or by lexical choice---which is conditioned by the same semantic/pragmatic factors (animacy and definiteness/specificity). The present analysis also illustrates that cross-linguistic variation and language-internal variation (= `optionality&apos;) operate within a single typological space made available by the system of universal, violable constraints.
Software modelling languages should possess a notation, semantics and treatment adapted to design patterns, but its correct specification is a challenge for many investigators. UML treats them as parameterised collaborations, however, this approach is not exempt of problems. The goal of this work is the elaboration of a simple and intuitive model for the structural specification of design patterns and its integration in UML. The patterns specified with this model expect to be true reusable templates that can be applied in different contexts. For that, we use a visual, complete, simple and easy to learn notation. We believe that this model could be used successfully for the construction of a future tool that facilitates the definition, application, visualization and validation of patterns.
We introduce the spi calculus, an extension of the pi calculus designed for the description and analysis of cryptographic protocols. We show how to use the spi calculus, particularly for studying authentication protocols. The pi calculus (without extension) suffices for some abstract protocols; the spi calculus enables us to consider cryptographic issues in more detail. We represent protocols as processes in the spi calculus and state their security properties in terms of coarsegrained notions of protocol equivalence.
Head Tracking and pose estimation are usually considered as two sequential and separate problems: pose is estimated on the head patch provided by a tracking module. However, precision in head pose estimation is dependent on tracking accuracy which itself could benefit from the head orientation knowledge. Therefore, this work considers head tracking and pose estimation as two coupled problems in a probabilistic setting. Head pose models are learned and incorporated into a mixed-state particle filter framework for joint head tracking and pose estimation. Experimental results on real sequences show the effectiveness of the method in estimating more stable and accurate pose values.
Xr provides a vector-based rendering API with output support for the X Window System and local image buffers. PostScript and PDF file output is planned. Xr is designed to produce identical output on all output media while taking advantage of display hardware acceleration through the X Render Extension.
This paper presents a method useful for the detection and interpolation of blotches in old films. The main advantage of this method is that it does not require motion estimation, which is difficult to compute in noisy and degraded image sequences. Our method imposes a set of criteria which must be fulfilled for the possible blotches. These criteria are temporal-uncorrelation, and high contrast with respect to the background. We propose to make use of morphological operators in order to establish the areas in need of restoration.
In an evolving specification, considerable development time and effort is spent handling  recurrent inconsistencies. Tools and techniques for detecting and resolving inconsistencies  only address part of the problem: they do not ensure that a resolution generated at a particular  stage will apply at all subsequent stages of the specification process. Previously, we have  advocated tolerance and management of inconsistency, rather than strict enforcement of  consistency. The advantages of this approach include the ability to delay resolution,  facilitation of concurrent development, and greater flexibility in development strategies.  However, this approach does not prevent inconsistencies themselves from evolving, and it  does not ensure that resolved inconsistencies remain resolved throughout subsequent  developments. We address these problems by explicitly recording relationships between  partial specifications (ViewPoints), representing both resolved and unresolved inconsistencies.  ...
Stigmergic systems solve global problems by using indirect communication mediated by an environment. Because they are localized and dynamic, stigmergic systems are self-organizing, robust and adaptive. These properties are useful for creating survivable systems, but stigmergic systems also raise new security concerns. Indirect communication makes systems more vulnerable in an open and hostile environment, and feedback mechanisms common to stigmergic algorithms can be exploited by attackers. In this paper we use AntNet, an adaptive routing algorithm inspired by biological ant foraging, to explore some of the security issues for stigmergic systems. We identify possible attacks and analyze their potency. We propose and evaluate mechanisms for defending against these attacks.
this paper, for any two vectors x, y      we write x    y (x&lt;y,resp.) to mean x i    y i (x i &lt;y i , resp.) for every i =1,...,n. If a    b then the box [a, b]((a, b], resp.) is the set of all x      satisfying a      b (a&lt;xff b, resp.)
One of the most popular systems for performing high resolution analog to digital conversion is the    modulator. Though common in applications, theoretical analysis of the    modulator is difficult due to the presence of a discontinuous quantizer in the modulator. This paper presents asymptotic results regarding the statistical behavior of the    modulator when inside the loop dithering is utilized. In some recent papers examining the stochastic behavior of the  61;  it was shown (via simulations) that the input to the quantizer can be accurately modeled as a stationary Gaussian process. Our analysis shows that both the input to the quantizer and the quantization noise are asymptotically stationary Gaussian processes, under mild assumptions on the input and the dither process. The results of this paper are derived by letting the quantizer stepsize approach zero, and the analytical approach is related to the stochastic analysis of adaptive filtering algorithms. Our analysis is valid for a large collection of stochastic input signals, including ARMA processes. Furthermore, previous stochastic analysis assumed that the quantizer never overloaded, while the present analysis does not make this assumption. It is also shown that analysis of the Differential Pulse Code Modulator is in fact analogous to the analysis of the    modulator. Simulation results presented for the first-order    modulator and two second-order    modulators demonstrate the practicality of the analysis.
We address two open theoretical questions in Policy Gradient Reinforcement  Learning. The first concerns the efficacy of using function approximation  to represent the state action value function, Q. Theory is presented  showing that linear function approximation representations of Q  can degrade the rate of convergence of performance gradient estimates  by a factor of O(ML) relative to when no function approximation of Q  is used, where M is the number of possible actions and L is the number  of basis functions in the function approximation representation. The second  concerns the use of a bias term in estimating the state action value  function. Theory is presented showing that a non-zero bias term can  improve the rate of convergence of performance gradient estimates by  O(1    (1/M)), where M is the number of possible actions. Experimental  evidence is presented showing that these theoretical results lead to  significant improvement in the convergence properties of Policy Gradient  Reinforcement Learning algorithms.
This paper presents a robust motion-detector video sensor. It is intended to operate in surveillance applications for long periods of time with time-varying noise level. It makes use of the fact that whenever there is no motion a similarity measure between frames tends to have similar values.
R  d  can be divided into a union of parallel (d\Gammak)-flats of the form x 1 = g 1 ; x 2 = g 2 ; : : : x k =  g k , where the g i are constant. Let C be a family of parallel (d \Gamma k)-dimensional convex sets, meaning that each is contained in one of the above parallel (d \Gamma k)-flats. We give a parameterization of the set of k-flats in R  d  , such that the set of k-flats which intersect, in a point, any set c 2 C, is convex. Parameterizing the lines in R  3  through horizontal convex sets as convex sets has applications to medical imaging, and interesting connections with recent work on light field rendering in computer graphics. The general case is useful for fitting k-flats to points in R  d  .  The following easy reduction is well known. Let C be a finite set of parallel line segments in  R  d  . We want to find a (d \Gamma 1)-transversal for C, that is, a hyperplane intersecting every segment in C. Such a hyperplane has to pass below the upper endpoint of each segment and ...
this paper we explore a theoretical approach to predicting the performance of combined systems and use its predictions to benchmark the real results of a number of combined systems. First we outline the theoretical framework used. Then we describe two sets of exploratory experiments. In the first experiment the predictions of five typical categorizers are combined for two different datasets. In the second experiment we took the dataset for which the combined systems had least success and developed two less orthodox categorizers, each of which breaks from the typical categorization approach in some way. Further combined systems are produced which use these new systems with some success. Our aim in these experiments was to start to clarify why combining text categorizers has not been very effective. Two simple models of combining present themselves: linear combining, where the scores of a number of classifiers are combined, and simple voting in which the decisions of the classifiers are taken as votes for or against the class. Frameworks exist to analyse both [11][12]. Linear combining requires that the classifiers combined produce scores which are an accurate estimate of class probability (see [13] for discussion of a categorizer that does this). If the categorizers are optimised to produce a clear yes or no decision for a particular class they may be biased to produce scores which are either very high (close to 1) or very low (close to 0). Combining scores of this type would not be expected to give satisfactory results, and should be avoided. Voting on the other hand allows results to be combined from any kind of categorizer. We wished to use a selection of existing categorizers, which might or might not give &quot;true&quot; estimates of class membership. Consequently voting was...
In this paper, the monotone convergence in the sense of sup-norm is studied. First, it is pointed out that, when a typical D-type iterative learning control (ILC) algorithm is applied to LTI systems, some huge overshoot in the sense of sup-norm may be observed even though the exponential convergence is guaranteed in the sense of  -norm. Then, an ILC law  using an intervalized learning scheme is proposed to  resolve such an undesirable phenomenon, and it is  shown that the learning gain affects the exponential  convergence in the sense of sup-norm.    1.  
Finding a good embedding of a unit disk graph given by its connectivity information is a problem of practical importance in a variety of fields. In wireless ad hoc and sensor networks, such an embedding can be used to obtain virtual coordinates. In this paper, we prove a non-approximability result for the problem of embedding a given unit disk graph. Particularly, we show that if non-neighboring nodes are not allowed to be closer to each other than distance 1, then two neighbors can be as far apart as      ff, where ff goes to 0 as n goes to infinity, unless P = NP . We further show that finding a realization of a d-quasi unit disk graph with   1/ ff 2 is NP -hard.
Introduction  A visually orienting mobile robot must cope with a number of changes to its environment. Most importantly, it must be able to identify its location even when objects in the environment have been moved or when the illumination conditions have changed. We report experiments with a template-based self-localization method that operates in real time on a Pentium 133 MHz PC (Balkenius and Kopp 1996a, b; 1997). The algorithm has been implemented in an autonomous mobile robot as an important part of its navigational system (Balkenius and Kopp 1996a, b; 1997).  The main task for the algorithm is to recognize landmarks robustly around the robot. These landmarks are represented as elastic templates and are automatically selected by the robot during learning. Our methods differ from other vision-based localization techniques in a number of respects. First, it does not require artificial landmark-symbols (Adorni, et al. 1996). Second, it can derive the exact angle toward a landmark wh
Finding a small dominating set is one of the most fundamental problems of traditional graph theory. In this paper, we present a new fully distributed approximation algorithm based on LP relaxation techniques. For an arbitrary parameter k and maximum degree ff, our algorithm computes a dominating set of expected size O kff    log ff|DSOPT        rounds where each node has to send O k      messages of size O(log ff). This is the first algorithm which achieves a non-trivial approximation ratio in a constant number of rounds.
The extension of the XSL (eXtensible Style sheet Language) by variables and  passing of data values between template rules has generated a powerful XML query  language: XSLT (eXtensible Style sheet Language Transformations). An informal  introduction to XSTL is given, on the bases of which a formal model of a fragment  of XSLT is defined. This formal model is in the spirit of tree transducers, and its  semantics is defined by rewrite relations. It is shown that the expressive power  of the fragment is already beyond that of most other XML query languages. Finally,  important properties such as termination and closure under composition are  considered.
This paper studies the performance of Massively Parallel Fuzzy Systems (MPFS) on the two spiral benchmark. Spiral data is contaminated with five different noise distributions. The recognition rates of the system are reported with varying levels of different types of noise. The behaviour of the system is investigated with additive, multiplicative, cumulative and non-cumulative noise. The results show that the MPFS system remains stable to different noise variations and the generalisation error remains consistently low. As the total noise in the system increases, the system witnesses a linear decrease in entropy and the generalisation error is easier to predict. The error rate is found to have two separate patterns of variation for cumulative and non-cumulative noise.  Keywords: MPFS, noise distribution, possibility, recognition rate, spiral benchmark  2 1 INTRODUCTION  Massively Parallel Fuzzy Systems (MPFS) were first proposed by Singh to solve pattern recognition problems in real-time...
Semi-supervised clustering employs a small  amount of labeled data to aid unsupervised  learning. Previous work in the area has utilized  supervised data in one of two approaches:  1) constraint-based methods that guide the clustering  algorithm towards a better grouping of the  data, and 2) distance-function learning methods  that adapt the underlying similarity metric used  by the clustering algorithm. This paper provides  new methods for the two approaches as well as  presents a new semi-supervised clustering algorithm  that integrates both of these techniques in  a uniform, principled framework. Experimental  results demonstrate that the unified approach  produces better clusters than both individual approaches  as well as previously proposed semisupervised  clustering algorithms.
This paper presents a new application for image-based visual servoing: computer graphics animation. Indeed, the control of a virtual camera in virtual environment is not a trivial problem and usually required skilled operators. Visual servoing, a now well known technique in robotics and computer vision, consists in positioning a camera according to the informations perceived in the images. Using this method within computer graphics context leads to a very intuitive approach of animation. Furthermore, in that case a full knowledge about the scene is available. It allows to easily introduce constraints within the control law in order to react automatically to modifications of the environment. In this paper, we apply this approach in two different contexts: highly reactive applications (virtual reality, video games) and the control of humanoid avatars.
Current search technologies work in &quot;one size fits all&quot; fashion. Therefore, the answer to a query is independent of specific user information need. In this paper, we describe a novel ranking technique for personalized search services that combines content-based and community-based evidences. The community-based information is used in order to provide context for queries and is influenced by the current interaction of the user with the service. Our algorithm is evaluated using data derived from an actual service available on the Web, an online bookstore. We show that the quality of content-based ranking strategies can be improved by the use of community information as another evidential source of relevance. In our experiments, the improvements reach up to 48% in terms of average precision.
The ASM formalism is tailored to develop a general, realistic  environment --- a computer network possibly monitored by an active  eavesdropper --- on which any crypto-protocols can be faithfully analysed. The well known
Effective image retrieval by content from database requires that visual image properties are used instead of textual labels to recover pictorial data. Retrieval by image similarity given a template image is particularly challenging. The diffculty is to derive a similarity measure that combines shape, grey level patterns and texture in a way that closely conforms to human perception. In this paper a system is presented which supports retrieval by image similarity based on elastic template matching. The template can be both a 1D template modeling the contour of an object, and a 2D template modeling a part of an image with a significant grey level pattern. The retrieval process is obtained as a continuous interaction by which the original query of the user can be refined or changed on the basis of the results provided by the system. ff 1999 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.
Preserving privacy of individuals when data are shared for clustering is a complex problem. The  challenge is how to protect the underlying attribute values subjected to clustering without jeopardizing  the similarity between data objects under analysis. To address this problem, data owners must  not only meet privacy requirements but also guarantee valid clustering results. To achieve this dual  goal, we propose a novel spatial data transformation method called Rotation-Based Transformation  (RBT). The major features of our data transformation are: a) it is independent of any clustering  algorithm, b) it has a sound mathematical foundation; c) it is ecient and accurate; and d) it does  not rely on intractability hypotheses from algebra and does not require CPU-intensive operations.
In this paper we present a system for intelligent robot navigation. Its main feature is the ability to integrate goal directed planning with information acquisition from the external world, and with reaction to failure. This ability is obtained by means of a two-layered architecture: the higher (strategical) level controls the activation of the modules of the lower (navigation) level, which can perform, in different ways, basic tasks such as path planning, activation of navigation and processing of information acquired from the external environment. By programming the strategical level with the suitable strategies, the system can control navigation with sophisticated modalities.  1 Introduction and motivations  Intelligent navigation systems can be loosely classified in terms of the planning based and reactive paradigms. Systems based on planning [Wil88] heavily rely on a prior representation of the world to determine the actions to execute. Experience with such systems shows that navi...
This paper suggests a new image compression scheme, using the discrete wavelet transformation (DWT), which is based on attempting to preserve the texturally important image characteristics. The main point of the proposed methodology lies on that, the image is divided into regions of textural significance employing textural descriptors as criteria and fuzzy clustering methodologies. These textural descriptors include cooccurrence matrices based measures and coherence analysis derived features. While rival image compression methodologies utilizing the DWT apply it to the whole original image, the herein presented novel approach involves a more sophisticated scheme in the application of the DWT. More specifically, the DWT is applied separately to each region in which the original image is partitioned and, depending on how it has been texturally clustered, its relative number of the wavelet coefficients to keep is then, determined. Therefore, different compression ratios are applied to the above specified image regions. The reconstruction process of the original image involves the linear combination of its corresponding reconstructed regions. An experimental study is conducted to qualitatively assessing the proposed compression approach. Moreover, this experimental study aims at comparing different textural measures in terms of their results concerning the quality of the reconstructed image.
Selection of significant genes via expression patterns is an important problem in microarray experiments. Owing to small sample size and the large number of variables (genes), the selection process can be unstable. This paper proposes a hierarchical Bayesian model for gene (variable) selection. We employ latent variables to specialize the model to a regression setting and uses a Bayesian mixture prior to perform the variable selection. We control the size of the model by assigning a prior distribution over the dimension (number of significant genes) of the model. The posterior distributions of the parameters are not in explicit form and we need to use a combination of truncated sampling and Markov Chain Monte Carlo (MCMC) based computation techniques to simulate the parameters from the posteriors. The Bayesian model is flexible enough to identify significant genes as well as to perform future predictions. The method is applied to cancer classification via cDNA microarrays where the genes BRCA1 and BRCA2 are associated with a hereditary disposition to breast cancer, and the method is used to identify a set of significant genes. The method is also applied successfully to the leukemia data.
Recent trends in the software industry have shown that the number of software development  projects and their increasing complexity will continue to increase the demand for skilled  programmers. However, debugging technology has not kept pace and valuable programmer  resources are being wasted in debugging cycles. Correcting this trend requires innovative  approaches to assisting programmers during the debugging cycle to make them more productive  both time-wise and with respect to how thoroughly the application is tested and debugged. We  have investigated techniques which allow programmers to visually steer an application. Through  these new mechanisms, programmer&apos;s can identify and locate problems without the need for  revisiting the code (as is done in typical debugging cycles). Through steering the programmer can  change the application environment to more thoroughly understand and test the algorithm.
Biometric authentication of gait, anthropometric data, human activities and movement disorders are presented in this paper using the Continuous Human Movement Recognition (CHMR) framework introduced in Part I. A novel biometric authentication of anthropometric data is presented based on the realization that no one is average sized in as many as 10 dimensions. These body part dimensions are quantified using the CHMR body model. Gait signatures are then evaluated using motion vectors, temporally segmented by gait dynemes, and projected into a gait space for an eigengait based biometric authentication. Left-right asymmetry of gait is also evaluated using robust CHMR left-right labeling of gait strides. Accuracy of the gait signature is further enhanced by incorporating the knee-hip angle-angle relationship popular in biomechanics gait research, together with other gait parameters. These gait and anthropometric biometrics are fused to further improve accuracy. The next biometric identifies human activities which requires a robust segmentation of the many skills encompassed. For this reason, the CHMR activity model is used to identify various activities from making a coffee to using a computer. Finally, human movement disorders were evaluated by studying patients with dopa-responsive Parkinsonism and age matched normals who were video taped during several gait cycles to determine a robust metric for classifying movement disorders. The results suggest that the R. D. Green is with the Human Interface Technology Lab, University of Canterbury, Christchurch, New  Zealand. He was with the School of Electrical and Information Engineering, The University of Sydney,  NSW 2006, Australia, (e-mail: richard.green@canterbury.ac.nz).
This study is the first known implementation of jitter control in an active network. Jitter control is performed by active packets in a distributed packet by packet basis within an active network rather than on a per flow basis as in today&apos;s passive networks. This provides many new benefits and challenges. The concept and results of an experimental validation of this method are presented.  Keywords---Active Networks, Jitter Control, Multimedia Quality of Service I. INTRODUCTION  This paper presents the concept and experimental validation of an active network based jitter control mechanism. The results show an improvement in the Quality of Service (QoS) for realtime multimedia distribution over the next generation Internet.  Previous work on jitter control has assumed a non-active network and has usually involved queuing and scheduling algorithms applied uniformly to a packet flow, for example [4]. In a non-active network, data control algorithms are node-centric and packets of data are...
We present a simple, behavior-based, distributed control algorithm to inspect a regular structure with a swarm of autonomous, miniature robots, using only on-board, local sensors. To estimate intrinsic advantages and limitations of the proposed control solution, we capture its characteristics at a higher abstraction level using non-spatial probabilistic microscopic and macroscopic models. Both models achieve consistent prediction on the chosen swarm metric and deliver a series of interesting qualitative and quantitative insights on further, counterintuitive, improvement of the distributed control algorithm. Modeling results were validated by experiments with one to twenty robots using a realistic simulator in the framework of a case study concerned with the inspection of a jet turbine.
The KeY system allows integrated informal and formal development of objectoriented  Java software. In this paper we report on a major industrial case study involving safety-critical software for computation of a particular kind of railway time table used by train drivers. Our case study includes formal specification of requirements on the analysis and the implementation level. Particular emphasis in our research is put on the challenge of how authoring and maintenance of formal specifications can be made easier. We demonstrate that the technique of specification patterns implemented in KeY for the language OCL yields significant improvements. 1 
This paper presents a computational model which explains the formation of cognitive maps similar to those found in mammals and may be used for navigation tasks in open environments (as opposed to mazes). The model is inspired by the discovery of place as well as head direction cells in the rat&apos;s hippocampus. The modelled cognitive maps consist of mainly topological relations between `place cells&apos; and are generated by a modified version of the Kohonen self-organizing map. A model which simulates physical forces as in natural systems is applied to extend the relatively sparse topological map with metric information. Both the cognitive map and the force model are closely related to functions that could be performed in the brain. The performance of the model is tested in navigation experiments with a simulated robot in an open environment. The robot is equipped with distance sensors and a compass, but has no information about its current coordinate position.
We have developed a new method for global optimization,  designed to maximize stack energy. Our method  includes upper bounds for the stack energy and maximization  of the stack energy for each common midpoint.
High-speed cellular data systems demand fast downlink scheduling algorithms and Multiple-Input Multiple-Output (MIMO) techniques. The associated multiuser diversity and antenna diversity play a central role in achieving high system throughput and fair resource allocation among multiple users. For such systems we evaluate the cross-layer interactions between channel-dependent scheduling schemes and MIMO techniques, such as Space-Time Block Coding (STBC) or Bell Labs Layered Space-Time (BLAST), and propose a new scheduling algorithm named the Alpha-Rule. The evaluation shows that the STBC/MIMO provides reliable channel but at certain cost of spectral efficiency. Comparatively BLAST/MIMO provides larger capacity and enables higher scheduling throughput. Thus BLAST/MIMO may be a more suitable technique for high-rate packet data transmission at the physical layer. At the medium access control (MAC)-layer, the Alpha-Rule is shown to be more flexible or efficient to exploit the diversity gains than the exiting max-C/I or Proportionally Fair (PF) scheduling schemes. It enables online tradeoff between aggregate throughput, per-user throughput, and per-user resource allocation.
Abstract: While the technology of warfare is  constantly changing, particularly in the areas of  weapons, equipment, mobility, and communications,  the fundamental structure of military organizations has  remained stable since Roman times. In this paper we  present an ontology of the basic categories and  relationships needed to represent modern military  organizations (i.e. government-sponsored nonparamilitary  organizations). In particular, we focus on  the subordination relationships and command and  control flows that bind military organizations together.  
Short and medium length codes with redundant parity-check equations are known to exhibit a superior performance with iterative decoding. We give a tentative explanation of this effect using extrinsic-information transfer (EXIT) charts and quantify the resulting performance improvement. Our simulations show that the performance improvement predicted by EXIT charts is slightly larger than the actual improvement and we explain why this is the case.
Grid computing-- the assemblage of heterogeneous distributed clusters of computers viewed as a single virtual machine-- promises to serve as the next major paradigm in distributed computing. Since Grids are assemblages of (usually) autonomous systems (autonomous clusters, supercomputers, or even single workstations) scheduling can become a complex affair which must take into consideration not just the requirements (and scheduling decisions) made at the point of the job&apos;s origin, but also the scheduling requirements (and decisions) made at remote points on the fabric, and in particular scheduling decisions made by a remote autonomous system onto which the local job has been scheduled. The current existing scheduling models range from static, where each of the programs is assigned once to a processor before execution of the program commences, to dynamic, where a program may be reassigned to different processors, or a hybrid approach, which combines characteristics of both techniques [1,4,5].
From a commercial perspective, the Web has promised much more than it has delivered and the dream of worldwide Internet business enterprises has yet to become a reality. Electronic commerce currently resembles a vast, sprawling bazaar in which visitors must wander through countless market stalls. For most potential customers, this is frustrating and time-consuming. In traditional commerce, middlemen, or brokers, make it easier for customers to find, compare and buy because they aggregate goods and services from a variety of sources and display them in a way which is helpful to customers. In the electronic marketplace of the near future, there are likely to be large numbers and varieties of brokers. We believe that the widespread and rapid deployment of brokering services depends upon the availability of building blocks which are sufficiently generic that they can be tailored to produce a wide range of specific brokers. We call this collection of building blocks a generic broker. If such a generic broker framework is not available then specialised brokers will be implemented in an ad-hoc manner with a consequent wastage of effort, while their construction will be beyond the resources and skills of many companies who would otherwise benefit from them. In this paper we present the design of the Metabroker system, a generic framework for the construction of specialist electronic brokers. Our design is based upon the integration of distributed object, metadata and object database technologies.
Constructing timetables of work for personnel in healthcare institutions  is a highly constrained and diffcult problem to solve. In this  chapter, we will present an overview of our development of the algorithms  that underpin a commercial nurse rostering decision support system that  is in use in over 40 hospitals in Belgium. As such, we are particularly  concerned with the real world regulations and requirements of Belgian  institutions. We have concentrated upon short term rostering which  involves assigning tasks to nurses in a hospital ward. Of course, the  over-riding requirement of our algorithms is to assure a permanent level  of care for the patients. However, our approaches are also required to  consider administrative requirements, the nurses&apos; contracts and their  personal preferences.
Since a content-based image retrieval (CBIR) system services people, its image characterization and similarity measure must closely follow perceptual characteristics. In this study, we enumerate a few psychological and physiological invariants and show how they can be considered by a CBIR system. We propose distance functions to measure perceptual similarity for color, shape, and spatial distribution. In addition, we believe that an image search engine should model after our visual system, which adjusts to the environment and adapts to the visual goals. We show that we can decompose our visual front-end into filters of different functions and resolutions. A pipeline of filters can be dynamically constructed to meet the requirement of a search task and to adapt to individuals&apos; search objectives. 
This paper explores the use of software transformations as a formal foundation for software evolution. More precisely, we express software transformations in terms of assertions (preconditions, postconditions and invariants) on top of the formalism of graph rewriting. This allows us to tackle scalability issues in a straightforward way. Useful applications include: detecting syntactic merge conflicts, removing redundancy in a transformation sequence, factoring out common subsequences, etc.
Toolkits such as PlaceLab [1] have been successful in making location information freely available for use in experimental ubiquitous computing applications. As users&apos; expectations of ubiquitous computing applications grow, we envisage a need for tools that can deliver a much richer set of contextual information. The high-level situation of the current environment is a key contextual element, and this position paper focuses on a method to provide this information for an ad-hoc group of people and devices. The contributions of this paper are i) a demonstration of how information retrieval (IR) techniques can be applied to situation determination in context-aware systems, ii) a proposal of a novel approach to situation determination that combines these adapted IR techniques with a process of cooperative interaction, and iii) a report of preliminary results. The approach offers a high level of utility and accuracy, with a greater level of automation than other contemporary approaches.
Risk estimation in the context of statistical disclosure control is usually model  based. Many models and related methods have been proposed in recent years,  and some programs are distributed to implement them. However, the literature  and the program manuals rarely discuss statistical questions such as the sensitivity  or robustness of the estimates relative to the assumptions of the models,  goodness of fit tests for the validity of the model, and variance estimates of the  risk measures proposed.
We present in this paper a Bayesian CAD system for robotic applications. We address the problem of the propagation of geometric uncertainties and how esian CAD system for robotic applications. We address the problem of the propagation of geometric uncertainties and how to take this propagation into account when solving inverse problems. We describe the methodology we use to represent and handle uncertainties using probability distributions on the system&apos;s parameters and sensor measurements. It may be seen as a generalization of constraint-based approaches where we express a constraint as a probability distribution instead of a simple equality or inequality. Appropriate numerical algorithms used to apply this methodology are also described. Using an example, we show how to apply our approach by providing simulation results using our CAD system.
The ability to physically cluster a database table  on multiple dimensions is a powerful technique  that offers significant performance benefits in  many OLAP, warehousing, and decision-support  systems. An industrial implementation of this  technique for the DB2 Universal Database^TM  (DB2 UDB) product, called multidimensional  clustering (MDC), which co-exists with other  classical forms of data storage and indexing  methods, was described in VLDB 2003. This  paper describes the first published model for  automating the selection of clustering keys in  single-dimensional and multidimensional  relational databases that use a cell/block storage  structure for MDC. For any significant  dimensionality (3 or more), the possible solution  space is combinatorially complex. The  automated MDC design model is based on whatif  query cost modeling, data sampling, and a  search algorithm for evaluating a large  constellation of possible combinations. The  model is effective at trading the benefits of  potential combinations of clustering keys against  data sparsity and performance. It also effectively  selects the granularity at which dimensions  should be used for clustering (such as week of  year versus month of year). We show results  from experiments indicating that the model  provides design recommendations of comparable  quality to those made by human experts. The  model has been implemented in the IBM DB2  UDB for Linux, UNIX and Windows  Version 8.2 release.
When reservation-based service differentiation is offered in an IP network, it is crucial for a network provider to appropriately charge for service invocations. For internal calculation as well as external price representation, a notion of costs and prices for communication services is needed. In this paper, general requirements for cost and price calculation are analysed for packetswitched multi-service networks. We conclude that internal price calculation should be linear, based on resource usage and uniform across multiple service classes. Accordingly, we specify refinements for service class definitions and present a price calculation method for the IETF&apos;s Integrated Services architecture. The application of such a price representation to existing pricing and charging approaches is shown. Finally, certain economic aspects of the Guaranteed service class are analysed and the results are expressed using linear price representation.
A joint project between the Interdept. Research Center of the University of Padova (CIRGEO), a research group of Istituto Tecnico per la Ricerca Scientifica e Tecnologica (IRST) of Trento and the Visual Information Technology group (VIT) of the NRC Canada, in Ottawa, has been undertaken, with the main aim to create a set of 3D models of an historical building by means of photogrammetry and laser scanning-based surveying techniques. Beside the investigation of their geometric accuracy, a photorealistic representation suited for interactive navigation and manipulation in VR environment was a further objective of thie project. To this aim, the main room in the Aquila tower in Buonconsiglio castle (Trento, Italy) was choosen, as it featured a relative simple geometry along with artistically and historically very precious frescoed walls. In this paper a description of both surveying and modeling procedures adopted and results of a comparison test between employed techniques are presented.
A general framework for collision detection is presented. Then, we look at each stage and compare different approaches by extensive benchmarks. The results suggest a way to optimize the performance of the overall framework.
This paper examines the effects of  scanning with various types of materials, which are used in industrial piping system, in terms of measurement accuracy
Network gaming is fast becoming a significant source of traffc on the Internet. Most research related to game traffc behavior on the Internet has relied on data acquired either by collecting traces at the game server, by polling the game server through queries, or by collecting traces in a LAN environment. In this paper, we study traces collected over a period of one week on the access networks of four different markets serviced by a large ISP that provides broadband fixed wireless (BFW) service. We consider Counter-Strike, a fast-action game based on the amount of traffc generated, its popularity among the subscribers in all the markets, and its sensitivity to Quality of Service (QoS) requirements. We discuss player behavior and game traffc behavior in various markets to identify any effects of the migration to broadband services for the last-mile access. We also investigate some of the factors that would influence the provisioning of QoS to such applications, namely latency and bandwidth usage.
Efficient storage of types within a compiler is necessary to avoid large blowups in space during compilation. Recursive
We present a theory of may testing for asynchronous calculi with locality and no name matching. Locality is a non-interference property that is common in systems based on object-paradigm. Concurrent languages such as Join and Pict disallow name matching, which is akin to pointer comparison in imperative languages, to provide for an abstract semantics that would allow useful program transformations. May testing is widely acknowledged to be an eective notion for reasoning about safety properties. We provide a trace-based characterization of may testing for versions of asynchronous -calculus with locality and no name matching, which greatly simpli es establishing equivalences between processes. We also exploit the characterization to provide a complete axiomatization for the nitary fragment of the calculi.
this paper we implement an interlink between a lighting control system prototype and SEMPER&apos;s lighting simulation module Lumina
A method for reconstruction of 3D polygonal models from multiple views is presented. The method uses sampling techniques to construct a texture-mapped semi-regular polygonal mesh of the object in question. Given a set of views and segmentation of the object in each view, constructive solid geometry is used to build a visual hull from silhouette prisms. The resulting polygonal mesh is simplified and subdivided to produce a semi-regular mesh. Regions of model fit inaccuracy  are found by projecting the reference images onto the mesh from different views. The resulting error images for each view  are used to compute a probability density function, and several points are sampled from it. Along the epipolar lines corresponding  to these sampled points, photometric consistency is evaluated. The mesh surface is then pulled towards the regions  of higher photometric consistency using free-form deformations. This sampling-based approach produces a photometrically consistent solution in much less time than possible with previous multi-view algorithms given arbitrary camera placement.
This paper presents an evolutionary algorithm for  searching for the optimal implementations of signal  transforms and compares this approach against  other search techniques. A single signal processing  algorithm can be represented by a very large  number of different but mathematically equivalent  formulas. When these formulas are implemented  in actual code, unfortunately their running  times differ significantly. Signal processing algorithm  optimization aims at finding the fastest formula.
this paper we present a natural language processing method for extracting causal relations between genetic phenomena and diseases. After presenting the results of a preliminary evaluation, we suggest the use of a graphical display application for viewing the semantic predications produced by the system
Common practice for managing the credit risk of lending portfolios is to calculate maximum loss within the &quot;value at risk&quot; framework. Most financial institutions use largescale Monte Carlo simulations to do this. However, such simulations may impose heavy calculation loads. This paper proposes a simplified method that approximates maximum loss with minimal simulation burden. Our method divides a portfolio into sub-portfolios at each credit rating level and calculates the maximum loss of each sub-portfolio. We assume that the sub-portfolio&apos;s structure provokes little fluctuation in the ratio between the maximum loss and the standard deviation. We therefore begin with a sub-portfolio in which each exposure is of the same amount (a homogeneous sub-portfolio). Simple calculations provide the standard deviation for both the heterogeneous sub-portfolio whose risk is to be measured and the homogeneous subportfolio. The maximum loss for the homogeneous sub-portfolio can be obtained by using analytical techniques rather than simulations. The maximum loss for a heterogeneous subportfolio is then approximated by multiplying the ratio of the maximum loss and standard deviation of the homogeneous sub-portfolio by the standard deviation of the heterogeneous one. Simulation examples indicate that this approximation is effective in all portfolios except those including extremely large exposures. This paper also describes a technique for using the total maximum loss of all sub-portfolios to find the maximum loss for the entire portfolio. Key words: Credit Risk, Lending Portfolio, Monte Carlo Simulation, Credit Concentration/Diversification, Correlation between Default Events JEL classification: G21  * Research Division 1, Institute for Monetary and Economic Studies, Bank of Japan (E-mai...
Device Interface. The layered architecture has allowed research organizations and commercial vendors to port MPICH to a great variety of multiprocessor and multicomputer platforms and distributed environments.  CHAPTER 1. INTRODUCTION 14 1.4 SCALA  P  3  T+ calculates at compile time a set of performance parameters which reflect the quality of the chosen parallelization strategy based on the following data obtained by a single profile run of SCALA [31], a post-execution performance analysis tool developed at the Institute of Software Technology and Parallel Systems in Vienna:  ffl Statement execution counts (how many times has a statement been executed during runtime of the program?)  ffl Loop iteration counts (what is the average number of iterations of a specific loop throughout the execution of the program?)  ffl Branching probabilities for conditional statements (how many times was a specific condition evaluated to TRUE throughout the execution of the program?)  In following we des...
Introduction  A geologic repository for the high-level radioactive waste (HLW) where forty thousand canisters are to be disposed of is planned in Japan.  The HLW disposal business in Japan shifted from the research and development stage to the business stage by the establishment of Nuclear Waste Management Organization of Japan in 2000. The site selection of the repository is the next stage of the disposal business. It is assumed that the optimization of the repository design under the geological environment of the repository location, and the safety performance assessment of the repository considering repository layout will be performed after the repository site selection.  The conventional safety performance assessment of the Japanese HLW repository is summarized in H12 report [1]. As the purpose of the conventional Japanese performance assessment is to demonstrate the feasibility of the HLW repository in Japan, performance assessment is based on the conservative assumptions.  For ex
Distributed Knowledge Management is an approach to Knowledge  Management based on the principle that the multiplicity (and heterogeneity) of  perspectives within complex organizations should not be viewed as an obstacle  to knowledge exploitation, but rather as an opportunity that can foster innovation  and creativity. Despite a wide agreement on this principle, most current KM systems  are based on the idea that all perspectival aspects of knowledge should be  eliminated in favor of an objective and general representation of knowledge. In  this paper we propose a peer-to-peer architecture (called KEx), which embodies  the principle above in a quite straightforward way: (i) each peer (called a K-peer)  provides all the services needed to create and organize &quot;local&quot; knowledge from  an individual&apos;s or a group&apos;s perspective, and (ii) social structures and protocols  of meaning negotiation are defined to achieve semantic coordination among autonomous  peers (e.g., when searching documents from other K-peers).
The Advanced Relation Model for Program Visualization (ARM 4 PV) is proposed as a framework model for analysis and visualization of computer programs. The ARM 4 PV is composed of two sub-systems. The systems sub-system provides for the identification, extraction, representation and visualization of relational information about the program. The visualization sub-system provides for various different types of visualizations that can be incorporated into the systems sub-system. A detailed overview of the ARM 4 PV is given. The format and representation of relational information is presented. Several visualizations that show the application of the ARM 4 PV are also presented.
This PhD thesis addresses the following problem: exploiting of trust information in order to enhance the accuracy and the user acceptance of current Recommender Systems (RS). RSs suggest to users items they will probably like. Up to now, current RSs mainly generate recommendations based on users&apos; opinions on items. Nowadays, with the growth of online communities, emarketplaces, weblogs and peer-to-peer networks, a new kind of information is available: rating expressed by an user on another user (trust). We analyze current RS weaknesses and show how use of trust can overcome them. We proposed a solution about exploiting of trust into RSs and underline what experiments we will run in order to test our solution.
This paper describes a new public-key cryptosystem based  on the hardness of computing higher residues modulo a composite RSA  integer. We introduce two versions of our scheme, one deterministic and  the other probabilistic. The deterministic version is practically oriented:  encryption amounts to a single exponentiation w.r.t. a modulus with at  least 768 bits and a 160-bit exponent. Decryption can be suitably optimized  so as to become less demanding than a couple RSA decryptions.
The use of a massively parallel architecture to provide real-time performance of an  application often requires the development of a version of that application tailored  specifically to the given architecture [HIL93]. We found this to be the case during the  implementation of a state-of-the-art visualization environment for a proprietary SIMD  architecture. This paper discusses methods found to be useful for improving  performance on the SIMD architecture and describes different strategies for the  allocation of processors and processor memory. The processor allocation strategy  found to be most useful is shown to correspond with the number of processors  available. We also found that the requirement of displaying each rendered frame  quickly, such that the display is updated in real-time, greatly affected our processor  allocation strategy and resulted in a non-intuitive mapping function.
Reputation systems can be tricked by the spread of false reputation ratings, be it false accusations or false praise. Simple solutions such as exclusively relying on one&apos;s own direct observations have drawbacks, as they do not make use of all the information available. We propose a fully distributed reputation system that can cope with false disseminated information. In our approach, everyone maintains a reputation rating and a trust rating about everyone else that they care about. From time to time first-hand reputation information is exchanged with others; using a modified Bayesian approach we designed and present in this paper, only second-hand reputation information that is not incompatible with the current reputation rating is accepted. Thus, reputation ratings are slightly modified by accepted information. Trust ratings are updated based on the compatibility of second-hand reputation information with prior reputation ratings. Data is entirely distributed: someone&apos;s reputation and trust is the collection of ratings maintained by others. We enable redemption and prevent the sudden exploitation of good reputation built over time by introducing re-evaluation and reputation fading.
We give a simple example of a variety V of modal algebras that is canonical but cannot be axiomatised by canonical equations or first-order sentences. We then show that the variety RRA of representable relation algebras, although canonical, has no canonical axiomatisation. Indeed, we show that every axiomatisation of these varieties involves infinitely many noncanonical sentences. Using probabilistic methods...
This paper presents a framework to achieve real-time augmented reality applications. We propose a framework based on the visual servoing approach well known in robotics. We consider pose or viewpoint computation as a similar problem to visual servoing. It allows one to take advantage of all the research that has been carried out in this domain in the past. The proposed method features simplicity, accuracy, efficiency, and scalability wrt. to the camera model as well as wrt. the features extracted from the image. We illustrate the efficiency of our approach on augmented reality applications with various real image sequences.
In this report we present a workbench for natural language grammars which was designed as a computational tool to help verifying the theoretical consistency and the empirical adequacy of syntactic and semantic analysis of natural language expressions.  
The generalized Vickrey auction (GVA) is a strategy-proof  combinatorial auction, in which truthful bidding is the optimal  strategy for an agent. In this paper we address a fundamental  problem with the GVA, which is that it requires  agents to compute and reveal their values for all combinations  of items. This can be very difficult for bounded-rational  agents with limited or costly computation. We propose an experimental  design for an iterative combinatorial auction. We  have a theoretical proof that the the auction implements the  outcome of the Vickrey auction in special cases, and initial  experimental results support our conjecture that the auction  implements the outcome of the Vickrey auction in all cases.
This paper presents a new static type system for multi-threaded programs; well-typed programs in our system are guaranteed to be free of data races and deadlocks. Our type system allows programmers to partition the locks into a fixed number of equivalence classes and specify a partial order among the equivalence classes. The type checker then statically verifies that whenever a thread holds more than one lock, the thread acquires the locks in the descending order. Our system also allows...
This paper describes a negotiation protocol proposed for inter-agent cooperation in a multi-agent system we developed for optimisation and dynamic integrated scheduling of steel production. The negotiation protocol is a two-level bidding mechanism based on the contract net protocol. The purpose of this protocol is to allow the agents to cooperate and coordinate their actions in order to find globally near-optimal robust schedules, which are able to optimise the original production goals whilst minimising the disruption caused by the occurrence of unexpected real-time events. Experimental results show the performance of this negotiation protocol to coordinate the agents in generating good quality robust schedules. This performance is evaluated in terms of stability and utility measures used to evaluate the robustness of the steel production processes in the presence of real-time events.
The educational community in general considers computer-assisted learning to be very beneficial. As a result, numerous new educational software applications are being developed. An educational application can become very effective if it is adaptive and individualised to the student. However, one important aspect of students that has been overlooked so far, and should be included in such individualisation models, is students&apos; behaviour and emotional state that affects their learning. This paper describes how system observations of students&apos; behavioural characteristics, during their interaction with an educational application, may provide important evidence about students&apos; emotions while they learn. The information collected from these observations mainly concerns students&apos; behaviour while using the application, combined with students&apos; reactions and responds to questions depending on the correctness of their answers. The system&apos;s inferences about students&apos; emotions are used to adapt interaction to each individual student&apos;s needs taking into account their character and mood.
We present an algorithm for computing stable collision-free motions for humanoid robots given fullbody  posture goals. The motion planner is part of a simulation environment under development for providing  high-level software control for humanoid robots. Given a robot&apos;s internal model of the environment and a  statically-stable desired posture, we use a randomized path planner to search the configuration space of the robot  for a collision-free path. Balance constraints are imposed on incremental search motions in order to maintain  the overall dynamic stability of the computed trajectories. The algorithm is presented along with preliminary  results using an experimental implementation on a dynamic model of the H5 humanoid robot.
Many applications of wireless sensor networks are useful only when connected to an external network. Previous research on transport layer protocols for sensor networks has focused on designing protocols specifically targeted for sensor networks. The deployment of TCP/IP in sensor networks would, however, enable direct connection between the sensor network and external TCP/IP networks. In this paper we focus on the performance of TCP in the context of wireless sensor networks. TCP is known to exhibit poor performance in wireless environments, both in terms of throughput and energy efficiency. To overcome these problems we introduce a mechanism called Distributed TCP Caching (DTC). The DTC mechanism uses segment caching and local retransmissions to avoid expensive end-to-end retransmissions. We show by simulation that DTC significantly improves TCP performance so that TCP can be useful even in wireless sensor networks.
A significant body of literature is available on distributed transaction commit protocols. Surprisingly, however, the relative merits of these protocols have not been studied with respect to their quantitative impact on transaction processing performance. In this paper, using a detailed simulation model of a distributed database system, we profile the transaction throughput performance of a representative set of commit protocols. A new commit protocol, OPT, that allows transactions to &quot;optimistically&quot; borrow uncommitted data in a controlled manner is also proposed and evaluated. The new protocol is easy to implement and incorporate in current systems, and can coexist with most other optimizations proposed earlier. For example, OPT can be combined with current industry standard protocols such as Presumed Commit and Presumed Abort. The experimental results show that distributed commit  processing can have considerably more influence than distributed  data processing on the throughput per...
Particle filtering (PF) is now established as one of the most popular methods for visual tracking. Within this framework, two assumptions are generally made. The first is that the data are temporally independent given the sequence of object states, and the second one is the use of the transition prior as proposal distribution. In this paper, we argue that the first assumption does not strictly hold and that the second can be improved. We propose to handle both modeling issues using motion. Explicit motion measurements are used to drive the sampling process towards the new interesting regions of the image, while implicit motion measurements are introduced in the likelihood evaluation to model the data correlation term. The proposed model allows to handle abrupt motion changes and to filter out visual distractors when tracking objects with generic models based on shape representations. Experimental results compared against the CONDENSATION algorithm have demonstrated superior tracking performance.
This paper is a short tour of the synchronization methods for real-time with special attention  to the RTCore (RTLinux) real-time operating system. A glossary is provided for some tutorial  material.
We present a distributed variant of Q-learning that allows to learn the optimal cost-to-go function in stochastic cooperative multi-agent domains without communication between the agents.
This paper describes the documentation that comes with our hands-on course on real-time systems,  as we present it to fourth-year mechatronics engineering students. This hands-on course is one part of a  cluster of three, the others being more theoretical courses on real-time software, and on processors and  interfacing for real-time systems.  The hands-on course is using real-time Linux (RTLinux and RTAI), and its examples focus on mechatronic  systems for mechanical engineers, i.e., mostly motion control applications. The course was first  given in Dutch, but its documentation and code examples are now reworked and extended for this year&apos;s  course into an English version, which will be available under the GNU Free Documentation License (FDL)  license [3]. The aim is to serve as a starting point for cooperation in the field of educational material  for real-time courses for beginning engineering students, because there is a big need to complement the  excellent real-time operating system projects with &quot;higher-level&quot; documentation and guidance.  1 
Deductive verification of object-oriented programs suffers from the lack of modularity.
The complexity of the reachability problem for live and safe free-choice Petri nets has been open for several years. Several partial results seemed to indicate that the problem is polynomial. We show that this is unlikely: the problem is NP-complete. 1 Introduction  Free-choice Petri nets were first defined and studied in the early seventies [1, 7]. Today, they are accepted as the largest class of Petri nets for which relevant analysis problems can be solved in polynomial time.  1  A series of papers, starting with [5] and culminating with [10], has shown that the problem of deciding if a free-choice Petri net is live and bounded can be solved in O(n \Delta m) time, where n and m are the number of places and transitions of the net, respectively. In turn, many analysis problems of live and bounded free-choice Petri nets have also been shown to have polynomial time complexity [4]. Due to this series of results, the reachability problem of live and bounded free-choice Petri nets, i.e., th...
Multihypothesis motion-compensating predictors combine several motion-compensated signals to predict the current frame signal. More than one motion-compensated signal, or hypothesis, is selected for transmission. Long-term memory motion-compensated prediction is a further concept for effcient video compression and is an example for forward-adaptive hypothesis switching. One motion-compensated signal is selected from multiple reference frames for transmission.
The proposed standard for the IEEE 802.3 Ethernet Passive Optical Network includes a random delayed transmission scheme for registration of new nodes. Although the scheme performs well on low loads, our simulation demonstrates the degraded and undesirable performance of the scheme at higher loads. We propose a simple modification to the current scheme that increases its range of operation and is compatible with the IEEE draft standard. We demonstrate the improvement in performance gained without any significant increase in registration delay.
Recently, an adaptive Bayesian receiver for blind detection in flat-fading channels was developed by the present authors, based on the sequential Monte Carlo methodology. That work is built on a parametric modeling of the fading process in the form of a state-space model and assumes the knowledge of the second-order statistics of the fading channel. In this paper, we develop a nonparametric approach to the problem of blind detection in fading channels, without assuming any knowledge of the channel statistics. The basic idea is to decompose the fading process using a wavelet basis and to use the sequential Monte Carlo technique to track both the wavelet coefficients and the transmitted symbols. A novel resampling-based wavelet shrinkage technique is proposed to dynamically choose the number of wavelet coefficients to best fit the fading process. Under such a framework, blind detectors for both flat-fading channels and frequency-selective fading channels are developed. Simulation results are provided to demonstrate the excellent performance of the proposed blind adaptive receivers.
In this paper, we investigate the benefit of network coding over routing for  multiple independent unicast transmissions. We compare the maximum achievable  throughput with network coding and that with routing only. We show that the  result depends crucially on the network model. In directed networks, or in undirected  networks with integral routing requirement, network coding may outperform  routing. In undirected networks with fractional routing, we show that the potential  for network coding to increase achievable throughput is equivalent to the potential  of network coding to increase bandwidth effciency, both of which we conjecture to  be non-existent.
We propose a software development methodology which is founded on concepts used to model early requirements. Our proposal adopts the i* modeling framework [21], which offers the notions of actor, goal and (actor) dependency, and uses these as a foundation to model early and late requirements, architectural and detailed design. The paper outlines the methodology, named Tropos, through an example, and sketches a formal language which underlies the methodology and is intended to support formal analysis. The methodology seems to complement well proposals for agent-oriented programming platforms.
this article, we explore further these new and unanticipated uses that are made of email, and suggest potential design ideas to support them better. We present the findings from four months of fieldwork conducted at three companies, and ensuing analysis during which we confirmed and expanded some earlier intuitions about the use of email as a PIM tool. We conclude that email is definitely overloaded, but also that this process depends on factors such as a user&apos;s role and the nature of their workplace. A field study  Prior to the work reported here, extensive fieldwork on personal information management and email was conducted in a series of studies. These totaled over 60 formal and informal interviews with PC users in a range of professions from the creative arts, through business and administration to scientific (including site visits, face-to-face interviews and phone surveys in the USA and UK). Based on this research, one of the authors, during the design of a prototype information management tool, made the following observations about email for PC users:   Email is used throughout the day by many people    It is the major means of non-face-to-face communication    It is now the main means of document exchange   It is co-opted by its users for many information management functions, such as to-dos (by marking-up or re-sending oneself messages) and contact management (by sorting by name and filtering)    Email is overloaded, providing inadequate support for certain tasks it is routinely used to accomplish. Curious about these findings, we decided to gather more detailed information about current information management behaviors of email users. Twenty-eight interviews were conducted at three different email-experienced organizations to gain three different organizationa...
Current watercolor painting systems produce convincing results, but trade this quality for a simulation that is much too slow for interactive use. We describe the implementation of a paper model that allows for real-time creation of watercolor images by simulating the mechanics of pigment and water throughout a three-layer paper model. In order to keep the simulation real-time while painting on large surfaces, we adopt a distributed paper canvas by dividing it in a grid of subpapers, each of which is delegated to a remote process.
A rendezvous problem for a team of autonomous vehicles, which communicate over quantized channels, is analyzed. The paper illustrates how communication topologies based on uniform and logarithmic quantizations influence the performance. Since a logarithmic quantizer in general imposes fewer bits to be communicated compared to a uniform quantizer, the results indicate estimates of lower limits on the amount of information that needs to be exchanged in order for the vehicles to meet. Simulation examples illustrate the results. 1 
Statistical methods, such as independent component analysis, have  been successful in learning local low-level features from natural image  data. Here we extend these methods for learning high-level representations  of whole images or scenes. We show empirically that independent  component analysis is able to capture some intuitive natural image categories  when applied on histograms of outputs of ordinary Gabor-like  filters. This can be taken as an indication that maximizing the independence  or sparseness of features may be a meaningful strategy even  on higher levels of image processing, for such advanced functionality as  object recognition or image retrieval from databases.
In many applications like radio astronomy and electron spectroscopy three sources contribute to the measured signal: the desired image, the noise, and an unknown slowly varying background. The inferential problem is to separate the signal from the noise and the background. We accomplish this task employing Bayesian probability theory.
This paper investigates the use of programming language constructs to realize adaptive behavior in support of collaboration among users of wearable and handheld computers. A prototype language, Adaptive Java, contains primitives that permit programs to modify their own operation in a principled manner. In a case study, Adaptive Java was used to construct MetaSocket components, whose composition and behavior can be adapted to changing conditions during execution. MetaSockets were then integrated into Pavilion, a web-based collaboration framework, and experiments were conducted on a mobile computing testbed containing wearable, handheld, and laptop computer systems. Performance results demonstrate the utility of MetaSockets to improving the quality of interactive audio streams and reliable data transfers among collaborating users.
There is a number of applications requiring a community of many senders to transmit some real-time information to a single receiver. Using unicast connections...
With the proliferation of computers that don&apos;t look like computers, such as mobile phones,  TV&apos;s, PDA&apos;s, end-users wish to take advantage of them to access their applications and  information, wherever they are and whatever the device they use.  This article discusses a common mean to access a same service from various kinds of  devices, and presents how the smart card, a medium compatible with most terminal kinds,  can be used as application bootstrap.  1. Introduction  Nowadays, people are more and more nomadic. The scope of their information system is not reduced to a single PC anymore. Most of today&apos;s applications are designed in the context of local area networks. However, LANs do not ideally address the users&apos; needs because they reduce the application accessibility to the scope of one activity (work, home, etc.). Thus, today&apos;s applications suffer from exiguous scope and do not benefit totally from the potential of large-scale networks.  The number of different kinds of devices us...
Intensity modulated radiation therapy (IMRT) can be very effective in the treatment of primary tumours. The challenge in therapy planning is to develop treatment plans that allow effective treatment of the tumour while at the same time limiting the radiation dose to surrounding tissue and organs at risk.
We investigate the performance implications of providing transaction atomicity for firm-deadline real-time applications operating on distributed data. Using a detailed simulation model, the real-time performance of a representative set of classical transaction commit protocols is evaluated. The experimental results show that data distribution has a significant influence on real-time performance and that the choice of commit protocol clearly affects the magnitude of this influence. We also propose and evaluate a new commit protocol, PROMPT (Permits Reading Of Modified Prepared-data for Timeliness), that is specifically designed for the real-time domain. PROMPT allows transactions to &quot;optimistically&quot; borrow, in a controlled manner, the updated data of transactions currently in their commit phase. This controlled borrowing reduces the data inaccessibility and the priority inversion that is inherent in distributed real-time commit processing. A simulation-based evaluation shows PROMPT to ...
An adaptive transversal equalizer based on the least-mean-square (LMS) algorithm, operating in an environment with a temporally correlated interference, can exhibit better steady-state mean-square-error (MSE) performance than the corresponding Wiener filter. This phenomenon is a result of the nonlinear nature of the LMS algorithm and is obscured by traditional analysis approaches that utilize the independence assumption (current filter weight vector assumed to be statistically independent of the current data vector). To analyze this equalizer problem, we use a transfer function approach to develop approximate analytical expressions of the LMS MSE for sinusoidal and autoregressive interference processes. We demonstrate that the degree to which LMS may outperform the corresponding Wiener filter is dependent on system parameters such as signal-to-noise ratio (SNR), signal-to-interference ratio (SIR), equalizer length, and the step-size parameter.
The polygon retrieval problem on points is the problem of preprocessing a set of n  points on the plane, so that given a polygon query, the subset of points lying inside it  can be reported effciently.
Turkey is one of the rare countries with a rich cultural heritage. We have been left with a historic heritage covering the centuries from pre-historic to modern times, which have left the marks of their civilizations on the lands of Anatolia. Therefore these riches should be preserved and left to the next generations with the care such treasures deserve. We have to be aware of the responsibility of leaving these treasures to the next generations as well as displaying them to the tourists. The saving methods of these treasures are supported by modern technology. Using these methods enables us to increase the efficiency and impact of the saving process. Using the advanced technologies in the disciplines other than architecture and by cooperation among the different fields, the future of cultural heritage can be guaranteed in order to save the historical objects and areas. It is necessary to organize the plans and works covering the necessary technical documents of the objects and lands to preserve these objects and lands. With the help of these documents, it will be possible to reach the desired information for the works carried on these objects and lands. The information system generated by the use of digital photogrammetry and Geographical Information System together will enable any person interested in saving the cultural heritage to reach the necessary data. As mentioned above, integration of photogrammetry and GIS leads to the efficient use of data, analysis and presentation opportunities, which are very important for saving the cultural heritage. The aim of this study is to use this technology for the photogrammetric documentation of the cultural heritage and to enable the use of the documentation by using GIS.
In the framework of the Joint Archaeological Mission of the Universities of Bologna and Lecce at the ancient town of Soknopaiou Nesos (Fayyum, Egypt) a wide-range of geomatic methodologies were experimented (GPS, total station surveys, low-height aerial and close-range photogrammetry, Visual Reality, high resolution satellite imagery, etc.). The surveys were conducted with the aim of recording metrical and non-metrical information inside a common reference system for the development of a GIS for the archaeological area.
The tradeoff between performance and scalability is a fundamental issue in distributed sensor networks. In this paper, we propose a novel scheme to effciently organize and utilize network resources for target localization. Motivated by the essential role of geographic proximity in sensing, sensors are organized into geographically local collaborative groups. In a target tracking context, we present a dynamic group management method to initiate and maintain multiple tracks in a distributed manner. Collaborative groups are formed, each responsible for tracking a single target. The sensor nodes within a group coordinate their behavior using geographically-limited message passing. Mechanisms such as these for managing local collaborations are essential building blocks for scalable sensor network applications.
We perform an extensive theoretical and empirical analysis of the use of auxiliary variables and implied constraints in modelling a class of non-binary constraint satisfaction problems called problems of distance. This class of problems include 1-d, 2-d and circular Golomb rulers. We identify a large number of different models, both binary and non-binary, and compare theoretically the level of consistency achieved by generalized arc consistency on them. Our experiments show that the introduction of auxiliary variables and implied constraints can significantly reduce the size of the search space. For instance, our final models reduce the time to find an optimal 10-mark Golomb ruler 50-fold.
In this paper, the rate-based congestion control algorithm that has been standardized in the ATM Forum is evaluated. Its behavior is analyzed by utilizing a firstorder fluid approximation to provide control parameter tuning. We obtain the maximum queue length at the switch and conditions for avoiding under-utilization. The results are then applied to TCP over ABR service class. More specifically, we evaluate the TCP-level performance through a simulation technique when TCP is applied to ABR service class with the rate-based congestion control. We first compare rate-based control of ABR service and EPD applied to UBR service, and show that rate based congestion control achieves better fairness and higher throughput in most circumstances. However, we demonstrate that rate-based control requires careful tuning of control parameters to obtain its effectiveness.  I. INTRODUCTION  In the ATM Forum, standardization effort has been devoted for the rate-based congestion control, which is aimed ...
Although there are a large number of software development methodologies for standalone software, little effort is being paid into investigating specialised methodologies that target the development of Distributed Applications (DAs) in the era of Internet and Web-based applications. Rather than focusing on business models, developers usually spend considerable effort in implementing connectivity between software components that comprise these applications. Since a large number of competing technologies exist, these solutions face serious technology-migration and design reuse problems. This paper advocates approaching the design activity from a business rather than technological perspective by defining a service-oriented software architecture that satisfies the functional requirements in a particular domain. The paper also suggests identifying existing or new design patterns to capture common business process functionalities and fulfil the non-functional requirements. For evaluation purposes, we are applying our approach to Capital Market Systems (CMS) through the development of a prototype system using Web Service technology.
Constraint optimization is at the core of many problems in Artificial Intelligence. In this paper, we frame model-based diagnosis as a constraint optimization problem over lattices. We then show how it can be captured in a framework for &quot;soft&quot; constraints known as semiring-CSPs. The well-defined mathematical properties of a semiring-CSP permits us to devise efficient solution methods based on decomposing diagnostic problems into trees and applying dynamic programming. We relate the approach to SAB and TREE*, two diagnosis algorithms for tree-structured systems, which correspond to special cases of semiring-based constraint optimization.
A newly deployed multi-hop radio network is unstructured and lacks a reliable and effcient communication scheme. In this paper, we take a step towards analyzing the problems existing during the initialization phase of ad hoc and sensor networks. Particularly, we model the network as a multihop quasi unit disk graph and allow nodes to wake up asynchronously at any time. Further, nodes do not feature a reliable collision detection mechanism, and they have only limited knowledge about the network topology. We show that even for this restricted model, a good clustering can be computed effciently. Our algorithm effciently computes an asymptotically optimal clustering. Based on this algorithm, we describe a protocol for quickly establishing synchronized sleep and listen schedule between nodes within a cluster. Additionally, we provide simulation results in a variety of settings.
. This paper investigates the relationship between systems to enact software processes and systems to  coordinate distributed, heterogeneous and concurrent objects. In particular, we describe in detail how one of these  coordination systems systems---the &quot;Coordination Language Facility&quot; (CLF), developed at the Rank Xerox  Research Centre---can be used to model and execute a sample software development process: bug reporting. The  main advantages of using CLF are: i) language facility that allows to dynamically change both the core process  model and the application-specific process templates; ii) modular architecture that allows to easily reconfigure,  migrate and replicate each process component in a distributed, heterogeneous environment.  1 
Machine learning algorithms search a space of possible hypotheses and estimate the error of each hypotheses using a sample. Most often, the goal of classification tasks is to find a hypothesis with a low true (or generalization) misclassification probability (or error rate); however, only the sample (or empirical) error rate can actually be measured and minimized. The true error rate of the returned hypothesis is unknown but can, for instance, be estimated using cross validation, and very general worst-case bounds can be given. This doctoral dissertation addresses a compound of questions on error assessment and the intimately related selection of a &quot;good&quot; hypothesis language, or learning algorithm, for a given problem. In the first
Current auctions often expose bidding agents to two difficult, yet common,  problems. First, bidding agents often have the opportunity to participate in  successive auctions selling the same good, with no dominant bidding strategy  in any single auction. Second, bidding agents often need to acquire a bundle of  goods by bidding in multiple auctions, again with no dominant bidding strategy  in any single auction. This paper introduces an options-based infrastructure that  respects the autonomy of individual sellers but still enables bidders to utilize a  dominant, truthful strategy across multiple auctions.
The Internet and the World-Wide Web (WWW) are revolutionizing knowledge  exchange by linking heterogeneous information repositories into a kind of gigantic  world-wide digital library. Yet up until now, knowledge management on the WWW  has mainly been provided by navigation tools like Mosaic and Netscape, and by engines  like Alta Vista, Lycos and Yahoo which support navigation by automating the search for  user-relevant WWW sites. The simplicity of this paradigm has been the key to the initial  success of the Web infrastructure but now falls short of more complex applications  needed by an ever-growing community of users. Prominent among these needs is  flexible information gathering from multiple knowledge sources to ad-hocratically  serve the requests of specific user groups. For instance, Network Publication Systems  (NPS) for large organizations need flexible integration of enquiry information like  Who&apos;s Who services and tables of contents of journals with E-print archival material,  as well as flexible adaptation of local query services. Agent technology can provide  the right answer to these demands. In this paper, we describe agent-based information  gathering on the WWW in the context of a NPS for the European Physicist Society. In  our approach, we exploit constraints to implement information gathering with maximal  flexibility.
The main contribution of this paper is to discuss in depth the issues related to the design of computer interfaces for users with language limitations. Language limitations are found to various degrees in different users because of their age or health. In this paper, one of the largest applications of human-computer interaction, the Internet, is explored. This paper will discuss syntactic and semantic language limitations in brief and their implication on human computer communication. A number of solutions are offered that will lead to intelligent interfaces that facilitate not only visual needs of the user, but also their language needs.  Keywords: Language Limitations, Interface Design, Internet, Hypertext  3 1. MOTIVATION  The main motivation for this paper comes from previous work done by several researchers in the areas of language, psychology and communication. In the broad area of Human Computer Interaction (HCI), one of the main aim is to facilitate the communication link betwe...
Peer-to-peer (P2P) computing is currently attracting enormous attention.
Discovering co-expressed genes and coherent expression  patterns in gene expression data is an  important data analysis task in bioinformatics research  and biomedical applications. Although  various clustering methods have been proposed,  two tough challenges still remain on how to integrate  the users&apos; domain knowledge and how to  handle the high connectivity in the data. Recently,  we have systematically studied the problem and  proposed an effective approach [3]. In this paper,  we describe a demonstration of GPX (for Gene  Pattern eXplorer), an integrated environment for  interactive exploration of coherent expression patterns  and co-expressed genes in gene expression  data. GPX integrates several novel techniques, including  the coherent pattern index graph, a gene  annotation panel, and a graphical interface, to  adopt users&apos; domain knowledge and support explorative  operations in the clustering procedure.
This paper first describes the methodology of journal descriptor (JD) indexing, based on human indexing at the journal level using only 127 descriptors, and applying statistical methods that associate this journal indexing with text words in a training set of MEDLINE    citations. These associations form the basis for automatic indexing of documents outside the training set. The paper then presents the new technique of semantic type (ST) indexing, based on JD indexing associated with each of 134 ST&apos;s, and applying the standard cosine coefficient measure to compare the similarity between the JD indexing of a document and the JD indexing of each ST. The ST indexing of the document is the list of ST&apos;s ranked in decreasing order of similarity between the JD indexing of the document and the JD indexing of the ST&apos;s. Discussion of the potential usefulness and application of the very general indexing provided by JD&apos;s and ST&apos;s comprises the remainder of the paper. JD&apos;s have been used for more than thirty years to search MEDLINE by discipline, and discipline-based indexing is in evidence on the Web. It is suggested, with several examples, that ST&apos;s may convey a unique slant of a document&apos;s content not normally represented in standard indexing vocabularies. Use of ST indexing to rank retrieved output is mentioned as a possible application. Notwithstanding the importance of methodology and performance issues, the intent of this paper is to explore questions of the potential utility and applicability of JD and ST indexing
An Incremental Elicitation Approach to Limited-Precision Auctions Auction-based mechanisms are increasingly being used for automating resource allocation among large numbers of agents. To make these sort of mechanisms viable one needs to consider the issues of communication and computation expenditure required by these protocols as well as their stability. In this thesis we study limited-precision, iterative mechanisms with dominant strategy equilibria designed for allocation of a single good. Our goal is to limit the communication between the players and the mechanism, reduce the amount of information revealed by the players, as well as minimize the players&apos; computational costs. We accomplish this by placing a number of operational constraints that permit the above objectives. We prove several necessary conditions that severely restrict the space of mechanisms satisfying our criteria. We develop a number of mechanisms and show that with a large and variable number of players, in the case of limited-precision, iterative mechanisms are superior to single-shot mechanisms.
We incorporate prior knowledge to construct nonlinear algorithms  for invariant feature extraction and discrimination. Employing a  unified framework in terms of a nonlinear variant of the Rayleigh  coefficient, we propose non-linear generalizations of Fisher&apos;s discriminant  and oriented PCA using Support Vector kernel functions.
The widespread adoption of XML holds out the promise that document structure can be exploited to specify precise database queries. However, the user may have only a limited knowledge of the XML structure, and hence may be unable to produce a correct XQuery, especially in the context of a heterogeneous information collection.
Logics of desires, preferences and goals have recently been  proposed in planning and agent theory. In this paper we introduce a  dynamic logic with utilitarian desires and we discuss the relation between  desires and utilities. Given that an agent&apos;s desires refer to his utility  function, which we assume to be constant, we resolve the paradox that  the stability of the agent&apos;s utility function does not imply the stability of  his desires. We illustrate the use of the logic to formalize certain aspects  of negotiation. In particular, we show how one agent can inuence the  behavior of another agent by inuencing his desires.
by  Qun Ma  In post-genomic computational biology and bioinformatics, long simulations of the dynamics of molecular systems, particularly biological molecules such as proteins and DNA, require advances in time stepping computational methods. The most severe problem of these algorithms is instability. The objective of this dissertation is to present original work in constructing multiscale multiple time stepping (MTS) algorithms for molecular dynamics (MD) that allow large time steps. First, through nonlinear stability analysis and numerical experiments, we reveal that MTS integrators such as Impulse suffer nonlinear overheating when fft = T/3 or possibly fft = T/4 when constant--energy MD simulations are attempted, where fft is the longest step size and T is the shortest period of the modes in the system. Second, we present Targeted MOLLY (TM), a new multiscale integrator for MD simulations. TM combines an efficient implementation of B-spline MOLLY exploiting analytical Hessians of energies and a self--consistent dissipative leapfrog integrator. Results show that TM allows very large time steps for slow forces (and thus multiscale) for the numerically challenging flexible TIP3P water systems (Jorgensen, et al. J. Chem. Phys., vol 79, pp 926--935, 1983) while still computing the dynamical and structural properties accurately. Finally, we show yet another new MOLLY integrator, the Backward Euler (BE) MOLLY in which hydrogen bond forces can easily be included in the averaging and thus stability might be further improved.
In this paper we examine the benefits and limitations of mechanism design as it applies to multi-agent meeting scheduling. We look at the problem of scheduling multiple meetings between various groups of agents that arise over time. Each of the agents has private information regarding their time preferences for meetings. Our aim is to extract this information and assign the meetings to times in a way that maximises social welfare. We discuss problems with previous attempts to design incentive compatible (IC) and individually rational (IR) mechanisms for the task. We focus on the problem of determining when agents are available. In particular, we show that when agents with general valuation functions are asked to supply their availability for meeting times, there is no IC and IR mechanism. Given this impossibility result, we show how the likelihood of violating IR can be reduced through agents expressing their value for the presence of others at meetings. We also show how requesting agent preferences for entire schedules helps to eliminate IC problems.
Triangular systems are the subgraphs of the regular triangular grid which are formed by a simple circuit of the grid and the region bounded by this circuit. They are used to model cellular networks where nodes are base stations. In this paper, we propose an addressing scheme for triangular systems by employing their isometric embeddings into the Cartesian product of three trees. This embedding provides a simple representation of any triangular system with only three small integers per vertex, and allows to employ the compact labeling schemes for trees for distance queries and routing. We show that each such system with n vertices admits a labeling that assigns O(log    n) bit labels to vertices of the system such that the distance between any two vertices u and v can be determined in constant time by merely inspecting the labels of u and v, without using any other information about the system. Furthermore, there is a labeling, assigning labels of size O(log n) bits to vertices, which allows, given the label of a source vertex and the label of a destination, to compute in constant time the port number of the edge from the source that heads in the direction of the destination. These results are used in solving some problems in cellular networks. Our addressing and distance labeling schemes allow efficient implementation of distance and movement based tracking protocols in cellular networks, by providing information, generally not available to the user, and means for accurate cell distance determination. Our routing and distance labeling schemes provide elegant and efficient routing and connection rerouting protocols for cellular networks.
According to the diversity principle, diverse evidence is strong evidence. There has been considerable evidence that people respect this principle in inductive reasoning. However, exceptions may be particularly informative. Medin, Coley, Storms, and Hayes (2003) introduced a relevance theory of inductive reasoning, and used this theory to predict exceptions, including the non-diversity by property reinforcement effect. A new experiment that investigated this phenomenon is reported here. Subjects made inductive strength judgments and similarity judgments for stimuli from Medin et al. The inductive strength judgments showed the same pattern as in Medin et al., however the similarity judgments suggested that the pattern should be interpreted as a diversity effect rather than non-diversity. It is concluded that the evidence regarding the predicted non-diversity by property reinforcement effect does not give distinctive support for relevance theory, although this theory does address other r...
This paper presents our recent work on developing parallel algorithms and software for solving the global minimization problem for molecular conformation, especially protein folding. Global minimization problems are difficult to solve when the objective functions have many local minimizers, such as the energy functions for protein folding. In our approach, to avoid directly minimizing a &quot;difficult&quot; function, a special integral transformation is introduced to transform the function into a class of gradually deformed, but &quot;smoother&quot; or &quot;easier&quot; functions. An optimization procedure is then applied to the new functions successively, to trace their solutions back to the original function. The method can be applied to a large class of nonlinear partially separable functions including energy functions for molecular conformation and protein folding. Mathematical theory for the method, as a special continuation approach to global optimization, is established. Algorithms with different solution tracing strategies are developed. Different levels of parallelism are exploited for the implementation of the algorithms on massively parallel architectures.
This paper presents the challenges and implementation choices selected for this  system concerning data representation, human computer interaction, and information rendering
Most voting schemes rely on a number of authorities. If too many of these authorities are dishonest then voter privacy may be violated. To give stronger guarantees of voter privacy Kiayias and Yung \cite{KY} introduced the concept of elections with perfect ballot secrecy. In this type of election scheme it is guaranteed that the only thing revealed about voters&apos; choices is the result of the election, no matter how many parties are corrupt. Our first contribution is to suggest a simple voting scheme with perfect ballot secrecy that is more efficient than \cite{KY}. Considering the question of achieving maximal privacy in other protocols, we look at anonymous broadcast. We suggest the notion of perfect message secrecy; meaning that nothing is revealed about who sent which message, no matter how many parties are corrupt. Our second contribution is an anonymous broadcast channel with perfect message secrecy built on top of a broadcast channel.
this paper we we will use the term object (sub- image) for this purpose. Second, to denote a function extracting properties of an image. Here we will use feature extraction function. Third, to describe a common property of an image or subimage. We will use feature in this sense. In query processing we use search image for the example image for which the user wants to obtain similar images from the database. An image which is compared to the search image is called a candidate image in our terminology.
Aspect-oriented programming (AOP) is a new programming paradigm whose goal is to more cleanly modularize crosscutting concerns such as logging, synchronization, and event notification which would otherwise be scattered throughout the system and tangled with functional code. However, while AOP languages provide promising ways to separate crosscutting concerns, they can also break conventional encapsulation mechanisms, making it diffcult to reason about code without the aid of external tools.
Negotiation is considered in general very context sensitive. Since our research laboratory has successfully developed decision support systems in Australian Family Law, we have used our domain expertise to construct a variety of Family Law negotiation support systems. Family_Winner uses point allocation and heuristics to advise upon structuring the mediation process and provides solutions based on trade-off and compensation strategies. Heuristic utility functions were developed from cases supplied to us by the Australian Institute of Family Studies. Family_Winner operates best when it is possible to allocate points to issues, and creative decision-making is not required. Whilst conducting an evaluation of the Family_Winner system, we observed that Family_Winner, in focusing upon providing advice with regard to bargaining, had neglected considering issues of justice. In a domain such as Family Law, issues of justice are of paramount concern. This indicates that use of negotiation support systems should be limited to domains in which principles of equity do not conflict with user satisfaction. When Family_Winner was used in a variety of other negotiation domains (international disputes, enterprise bargaining and company mergers) the advice offered strongly resembled the eventual negotiated outcome.
Defining dependency models is sometimes an easier, more intuitive way for ontology representation than defining reactive rules directly, as it provides a higher level of abstraction. We will shortly introduce the ADI (Active Dependency Integration) model capabilities, emphasizing new developments:  1. Support of automatic dependencies instantiation from an abstract definition that expresses a general dependency in the ontology, namely a &quot;template&quot;.
Software systems are typically composed of numerous components, each of which is responsible for a different function, e.g., one component may be responsible for remote communication, while another may provide a graphical user interface. Different implementations of a component may be possible, with each implementation tailored for a specific set of applications or environments. Being able to reconfigure software systems to make use of these different implementations with the minimum of effect on existing users and applications is desirable. Configurable software systems are also important for a number of other reasons: additional components or modifications to those currently available, may be required. For example, new versions of software components may be necessary due to the discovery of design flaws in a component; a RPC which provides unreliable message delivery may be suitable for an application in a local area network, but if the application is to be used in a wide area network, a different RPC implementation, which guarantees message delivery, may be necessary. Therefore, software is often required to be configurable, enabling modifications to occur with minimal effect on existing users. To allow this configurability, components should only be available through interfaces that are clearly separated from their implementations, allowing users to be isolated from any implementation changes. Objectoriented programming techniques offer a good basis upon which this separation can be provided. This paper describes a model for constructing configurable software based upon this separation, and illustrates this with a software development system we have implemented which supports these ideas in C++.
This paper describes the strategy followed for dimensioning the coefficient width of the adaptive filters (FFE, echo and NEXT Cancellers) in a Gigabit Ethernet PHY Silicon implementation. These filters account for an important part of the circuit area and power consumption. It is therefore critical to ensure that they are properly dimensioned.
The vision of the Semantic Web is to reduce manual discovery and usage of Web resources (documents and services) and to allow software agents to automatically identify these Web resources, integrate them and execute them for achieving the intended goals of the user. Such a composed Web service may be represented as a workflow, called service flow. Current Web service standards are not sufficient for automatic composition. This paper presents different types of compositional knowledge required for Web service discovery and composition. As a proof of concept, we have implemented our framework in a cardiovascular domain which requires advanced service discovery and composition across heterogeneous platforms of multiple organizations.
We consider the problem of estimating the state of a discrete--time  dynamic system comprising a linear system equation and a nonlinear measurement  equation based on measurements corrupted by non--Gaussian noise. The problem is  solved by recursively calculating the complete posterior density of the state given  the measurements. For representing the resulting non--Gaussian posterior, a new  exponential type density, the so called pseudo Gaussian density, is introduced. By  converting the original nonlinear system to an equivalent linear representation in  a higher--dimensional space, the parameters of the pseudo Gaussian posterior are  obtained by means of a linear estimator operating in the higher--dimensional space.
In this work we combine Genetic Programming (GP) and intelligent  agents to build a realistic foreign exchange currency market simulator. GP is  used to express and evolve trading strategies. In the paper we analyse the  decisions made in the design of the simulator with respect to authenticity of the  representation and the efficiency of the system. A number of experimental  results are also reported.
Estimation of distribution algorithms (EDAs) are population-based  heuristic search methods that use probabilistic models and which have  been successfully applied to continuous optimization problems. When applied  to constrained optimization problems, most EDAs (as well as genetic  algorithms) handle constraints by penalizing invalid solutions. This paper  presents PolyEDA, a new EDA approach that is able to directly consider  linear inequality constraints by using Gibbs sampling. Gibbs sampling  allows us to sample new individuals inside the boundaries of the polyhedral  search space described using a set of linear inequality constraints by  iteratively constructing a density approximation that lies entirely inside  the polyhedron. Due to its ability to consider linear inequality constraints,  PolyEDA can be used for highly constrained optimization problems, where  even the generation of valid solutions is a non-trivial task. Results for  different variants of a constrained Rosenbrock problem show a higher performance  of PolyEDA in comparison to a standard EDA using rejection  sampling.
The Internet Engineering Task Force (IETF) is currently working on the development of Differentiated Services (DiffServ). DiffServ seems to be a promising technology for next-generation IP networks supporting Quality-of-Services (QoS). Emerging applications such as IP telephony and time-critical business applications can benefit significantly from the DiffServ approach since the current Internet often can not provide the required QoS. This paper describes an implementation of Differentiated Services for Linux routers and end systems. The implementation is based on the Linux traffic control package and is, therefore, very flexible. It can be used in different network environments as first-hop, boundary or interior router for Differentiated Services. In addition to the implementation architecture, the paper describes performance results demonstrating the usefulness of the DiffServ concept in general and the implementation in particular. KEYWORDS:  QUALITY-OF-SERVICE (QOS), INTERNET PROTO...
Two factors can confound the interpretation of an  enterprise model. First, the dynamics of the control  technology interact in complex ways with those of the  plant, and engineers need to be able to distinguish these  effects. Second, &quot;mean field&quot; approximations of the  behavior of the system may be useful for qualitative  examination of the dynamics, but can differ in surprising  ways from the behavior that emerges from the  interactions of discrete agents. This paper examines  these effects in the context of a specific research project  applying agent-based control to a military air  operations scenario.  1. Introduction  Modern military operations can overwhelm a commander. The information available from satellite and other sensors floods conventional analysis methods. Enemy forces using advanced technology can hide or change location faster than conventional planning cycles can respond, and coordinating central orders across thousands of friendly resources can slow response even f...
A high precision and easy-to-use CCD camera calibration technique for industrial vision metrology is discussed. A well-known method is self-calibration by convergent camera configuration of a two- or three-dimensional target field. Only with this technique the central part of a sensor area is precisely calibrated, but off the centre the precision rapidly deteriorates. The presented technique is a simultaneous adjustment of both pan and close exposures, which compensates the lack of distortion data in the fringe area of the sensor and offers both uniform and high-precision calibration. Some patterns of camera configuration are compared in an experiment in terms of the precision and its uniformity over the sensor. And the combination of convergent pan exposures and vertical close exposures is proved the best.
In this paper, we consider the hierarchical control problem for a class  of uncertain hybrid dynamical systems. The continuous dynamics of this class of  uncertain hybrid systems are described by linear difference state equations, whose  right side functions are unknown but lie within some convex hulls of known functions. Our control
We outline a parametric model of a system of unmanned aerial vehicles (UAVs) on a mission. The UAVs have to accomplish their mission composed of several tasks as eciently as possible, while satisfying a heterogeneous set of physical and communication constraints. UAVs can be viewed as an example of a highly dynamic multi-agent system (MAS). These UAVs may be required to autonomously make decisions, communicate, coordinate, adapt to rapidly changing environments and eciently perform their tasks in real-time and under the limitations of local, incomplete and/or noisy knowledge of their surroundings. In particular, an individual UAV in our work can be viewed as an agent: it is autonomous, goal-driven, can aect and be aected by its environment, has its own behavior strategy, can communicate with its peers, and may nd it bene cial to cooperate and coordinate not only to avoid collisions, but also in order to accomplish its set of tasks more eectively. We focus herein on two aspects of agent-based modeling of UAVs: modeling autonomous decision-making of the individual vehicles viewed as autonomous agents, and dierent models of UAV coordination.
This article reports on an extensive database of American Sign Language (ASL) motions, handshapes, words and sentences. Research on automatic recognition of ASL requires a suitable database for the training and the testing of algorithms. The databases that are currently available do not allow for algorithmic development that requires a step-by-step approach to ASL recognition -- from the recognition of individual handshapes, to the recognition of motion primitives, and, finally, to the recognition of full sentences. We have sought to remove these deficiencies in a new database -- the Purdue RVL-SLLL ASL database.
We describe a technique for extracting data from  lists and tables and grouping it by rows and  columns. This is done completely automatically,  using only some very general assumptions about  the structure of the list. We have developed a suite  of unsupervised learning algorithms that induce the  structure of lists by exploiting the regularities both  in the format of the pages and the data contained in  them. Among the tools used are AutoClass for automatic  classication of data and grammar induction  of regular languages. The approach was tested  on 14 Web sources providing diverse data types,  and we found that for 10 of these sources we were  able to correctly nd lists and partition the data into  columns and rows.
In a single document repository or a library system, the authentication  and authorization issues can be handled relatively simply with traditional  technologies. However the situation is different in distributed and heterogeneous  publication databases. The European DataGrid database access and  security design is, among other things, able to take care of the authentication  of each user, and the roles of different users across organizational borders.
Question answering (NLQA) systems which retrieve a textual fragment from a document collection that represents the answer to a question are an active field of research. But evaluations currently involve a large amount of manual effort.
In 1903, Arthur Cushny, Professor of Materia Medica and Therapeutics at the University of Michigan, published an article in the Journal of the...
We provide maximum likelihood estimators of term structures of  conditional probabilities of bankruptcy over relatively long time horizons,  incorporating the dynamics of firm-specific and macroeconomic  covariates. We find evidence in the U.S. industrial machinery and  instruments sector, based on over 28,000 firm-quarters of data spanning  1971 to 2001, of significant dependence of the level and shape of  the term structure of conditional future bankruptcy probabilities on  a firm&apos;s distance to default (a volatility-adjusted measure of leverage)  and on U.S. personal income growth, among other covariates. Variation  in a firm&apos;s distance to default has a greater relative effect on the  term structure of future failure hazard rates than does a comparatively  sized change in U.S. personal income growth, especially at dates more  than a year into the future.
This paper considers the problem of efficiently generating  a sequence of secrets with the special property that the knowledge of  one or several secrets does not help an adversary to find the other ones. This is
The meta level notion has been applied to dierent contexts. In this paper we present  its application to an interoperable system. The incorporated meta level mechanism permits  to improve the management of the system by: 1. allowing to exploit the existing  mapping information and, 2. enhancing the query processing task through the use of  query patterns. Mapping information is the abstraction that provides the link between  the terms that appear in the semantic view (class level) oered to the user of the interoperable  system and the instances of these terms actually supported by stored data  (instance level) in distributed and heterogeneous data sources. Our mechanism allows one  to describe mapping expressions and reasoning with them. Query patterns are associated  to already obtained generic plans. Our mechanism recognizes a user query (description  in the class level) as an instance of a query pattern (description in the metaclass level).  Then, a proper instantiation of the correspondent generic plan solves the plan generation  step of the query processing task.  Dealing with a meta level, one aspect that requires a particular attention is the kind of  language selected to specify metaclasses. This paper presents a logic-based language that  allows the specication of structural and semantic constraints of objects that belong to  the class level. Our proposal is founded on the Description Logics (DL) technology. We  have dened a DL with two combined languages: one to specify the schema part in the  metaclass level, and another for the view part. The description of semantic constraints on  class level objects is achieved by the integration of suitable predicates of Concrete domains  into the DL language. The trade-o between expressivity and computational comple...
Statistical Rate Monotonic Scheduling (SRMS) is a generalization of the classical RMS results of Liu and Layland [LL73] for periodic tasks with highly variable execution times and statistical QoS requirements. The main tenet of SRMS is that the variability in task resource requirements could be smoothed through aggregation to yield guaranteed QoS. This aggregation is done over time for a given task and across multiple tasks for a given period of time. Similar to RMS, SRMS has two components: a feasibility test and a scheduling algorithm. SRMS feasibility test ensures that it is possible for a given periodic task set to share a given resource without violating any of the statistical QoS constraints imposed on each task in the set. The SRMS scheduling algorithm consists of two parts: a job admission controller and a scheduler. The SRMS scheduler is a simple, preemptive, fixed-priority scheduler. The SRMS job admission controller manages the QoS delivered to the various tasks through admi...
Data mining is a complex process that aims to derive an accurate predictive  model starting from a collection of data. Traditional approaches assume  that data are given in advance and their quality, size and structure are independent  parameters. In this paper we argue that an extended vision of data mining should  include the step of data acquisition as part of the overall process. Moreover the  static view should be replaced by an evolving perspective that conceives the data  mining as an iterative process where data acquisition and data analysis repeatedly  follow each other.
Traditional programming languages assume that real-world  systems have &quot;intuitive&quot;, mind-independent, preexisting concept hierarchies.
This paper describes a distributed, linear-time algorithm for localizing sensor network nodes in the presence of range measurement noise and demonstrates the algorithm on a physical network. We introduce the probabilistic notion of robust quadrilaterals as a way to avoid flip ambiguities that otherwise corrupt localization computations. We formulate the localization problem as a two-dimensional graph realization problem: given a planar graph with approximately known edge lengths, recover the Euclidean position of each vertex up to a global rotation and translation. This formulation is applicable to the localization of sensor networks in which each node can estimate the distance to each of its neighbors, but no absolute position reference such as GPS or fixed anchor nodes is available. We implemented the algorithm on a physical sensor network and empirically assessed its accuracy and performance. Also, in simulation, we demonstrate that the algorithm scales to large networks and handles real-world deployment geometries. Finally, we show how the algorithm supports localization of mobile nodes.
Arvand is the name of robots specially designed and constructed  by sharif CE team for playing soccer according to RoboCup  rules and regulations for the middle size robots. Two different types of  robots are made, players and the goal keeper. A player robot consists  of three main parts: mechanics (motion mechanism and kicker), hardware  (image acquisition, processing unit and control unit) and software  (image processing, wireless communication, motion control and decision  making). The motion mechanism is based on two drive unit, two steer  units and a castor wheel. We designed a special control board which  uses two microcontrollers to carry out the software system decisions and  transfers them to the robot mechanical parts. The software system written  in C++ performs real time image processing and object recognition.
this article, we ffrst outline the basic concepts of fuzzy logic and Fuzzy Control. Thereafter, wesketcha  visual language whichintegrates fuzzy set diagrams in the visual representation of rules. The basic concepts are inherited from the complete visual programming language Pictorial Janus ffPJff. However, we signiff- cantly simplify PJ&apos;s visual concepts in order to adapt it for our purpose
Since detecting race conditions in a multithreaded or multiprocess  program is an NP-hard problem, there is no efficient algorithm that can  help detect race conditions in a program. As such, there are no easy-touse  pedagogical tools. Most operating systems and concurrent  programming textbooks only provide a formal definition and some trivial  examples. This is insufficient for students to learn how to detect race  conditions. This paper attempts to fill this gap by presenting a set of wellorganized  examples, each of which contains one or more race conditions,  for instructors to use in the classroom. This set of materials has been  classroom tested for two years and the student&apos;s reaction has been very  positive.
By using an information survivability control system, the survivability of critical networked information systems can be enhanced using a variety of fault-tolerance mechanisms. Essential to the effective implementation of such mechanisms is communication from the error detection component to the various application nodes in the network. In this paper, we introduce a technique called Selective Notification for the communication of commands and alerts in very large distributed systems. The technique combines intentional addressing, content addressing and sender qualification in a single decoupled event-delivery mechanism. We show that effective targeted command and alert dissemination is achievable, and that Selective Notification allows systems to apply a wide range of event connectivity policies. We present details of an implementation of Selective Notification and the results of performance assessment experiments. Based on our preliminary performance data, we conclude that Selective Notification can be used to support survivability architectures in Internet-sized systems.
Escape analysis of object-oriented languages allows us to stack  allocate dynamically created objects and to reduce the overhead of synchronisation  in Java-like languages. We formalise the escape property E ,  computed by an escape analysis, as an abstract interpretation of concrete  states. We de  ne the optimal abstract operations induced by E  for a framework of analysis known as watchpoint semantics. The implementation  of E inside that framework is a formally correct abstract  semantics (analyser) for escape analysis. We claim that E is the basis for  more re  ned and precise domains for escape analysis.
Our position statement    is that current software evolution techniques suffer from a lack of documentation on software developers&apos; intentions and that mechanisms to support evolution can vastly be improved by making these intentions explicit in the software. We provide an intuitive deffnition of ffintentions&quot;, explain which kinds of intentions can be distinguished, discuss how suchintentions can be made explicit in the software by attaching annotations to software artefacts and argue how such information can provide support during the software development process ffand during software evolution in particular ff. Because of the preliminary status of this work, we mainly try to discover the important research questions to be answered and research topics to be investigated.
The aim of Instrumented Gait Analysis is to support clinicians&apos; decision making process concerning diagnosis and treatment strategies. Movement Analysis systems are used to measure data such as joint kinematics or kinetics during gait in a quantitative way. However, data evaluation is performed subjectively by experienced physicians for a specific clinical problem (diagnose individual disabilities in the locomotor system, plan and validate therapies). There is a growing need for methods for objective, standardized data analysis even in case of individual variations in the very complex gait pathologies. This article covers the development of a modular, computer -based methodology to quantify the degree of pathological gait in comparison to normal behaviour, as well as to automatically search for interpretable gait abnormalities and to visualize the results. The successful application of the novel methods to a group of CP (Infantile cerebral palsy) patients is demonstrated.
A feature-guided image interpolation scheme is presented. It is an effective and improved, shape-based interpolation method used for interpolating image slices in medical applications. The proposed method integrates feature line-segments to guide the shape-based method for better shape interpolation. An automatic method for finding these line segments is given. The proposed feature -guided shape-based method can manage translation, rotation and scaling situations when the slices have similar shapes. It can also interpolate intermediate shapes when the successive slices do not have similar shapes. This method is experimentally evaluated using artificial and real two-dimensional and three-dimensional data. The proposed method generated satisfactory interpolated results in these experiments. We demonstrate the practicality, effectiveness and reproducibility of the proposed method for interpolating medical images.
We present a complete system architecture for fully automated markerless augmented reality (AR). The system constructs a sparse metric model of the real-world environment, provides interactive means for specifying the pose of a virtual object, and performs model-based camera tracking with visually pleasing augmentation results. Our approach does not require camera pre-calibration, prior knowledge of scene geometry, manual initialization of the tracker or placement of special markers. Robust tracking in the presence of occlusions and scene changes is achieved by using highly distinctive natural features to establish image correspondences. 1. 
The advent of medical imaging and new operating techniques has revolutionized the working methods of medical professionals. This change requires physicians and surgeons to undergo additional training. This is why the development of appropriate tools, like medical simulators, is of paramount importance. Within this context, we focus our work on physical modeling of soft tissue deformations and collision detection in virtual environments. First, we present various physical models and the numerical resolution methods associated with deformable objects. We then propose a new model developed for soft tissue simulation, by successively presenting the aspects related to the formulation of the model, the resolution of the model, and the treatment of the physical interactions. This model, based on Pascal&apos;s principle, allows us in a relatively simple way to represent biological tissue, thus making it possible for interactive simulation. Next, we present various existing algorithms for collision detection, as well as the difficulty in adapting these algorithms in medical simulators where complex deformable objects form the base of the simulated environment. We then propose the algorithms developed in our work to deal with this problem within the framework of the medical simulators. These algorithms have better numerical robustness and are optimized, allowing us to treat deformable bodies effectively. We apply our results within the framework of an echographic simulator of the human thigh and a simulator for the arthroscopic reconstruction of the ACL (anterior cruciate ligament of the knee).
We present solutions to statically load-balance scatter operations in parallel codes run on Grids. Our loadbalancing strategy is based on the modification of the data distributions used in scatter operations. We need to modify the user source code, but we want to keep the code as close as possible to the original. Hence, we study the replacement of scatter operations with a parameterized scatter, allowing a custom distribution of data. The paper presents: 1) a general algorithm which finds an optimal distribution of data across processors; 2) a quicker guaranteed heuristic relying on hypotheses on communications and computations; 3) a policy on the ordering of the processors. Experimental results with an MPI scientific code of seismic tomography illustrate the benefits obtained from our load-balancing.
Many dynamic planning and management problems are typically characterised by a level of uncertainty regarding the value of data input such as supply and demand patterns. Assigning inaccurate values to them could invalidate the results of the study. Consequently, deterministic models are inadequate for the representation of these problems where the most crucial parameters are either unknown or are based on an uncertain future. In these cases, the scenario analysis technique could be an alternative approach. Scenario analysis can model many real problems in which decisions are based on an uncertain future, whose uncertainty is described by means of a set of possible future outcomes, called &quot;scenarios&quot;. In this paper we present a scenario analysis approach to dynamic multi-period systems by integrating scenario optimisation and subsequent deterministic reoptimisation. In the scenario optimisation phase we represent data uncertainty by a robust chance optimisation model obtaining a so-called barycentric value with respect to selected decision variables. The successive reoptimisation model based on this barycentric solution allows planning a part of the risk of a wrong decision, reducing the negative consequences deriving from it.
We argue that constraint programs with one or more  matrices of decision variables provide numerous  benefits, as they share many patterns for which general  methods can be devised, such as for symmetry  breaking. On a wide range of real-life application  domains, we demonstrate the generality and utility  of such matrix modelling.  1 
In this paper, we explore an architecture, called K-Trek, that enables mobile users to travel across knowledge distributed over a large geographical area (ranging from large public buildings to a national park). Our aim is providing, distributing, and enriching the environment with location-sensitive information for use by agents on board of mobile and static devices. Local interactions among KTrek devices and the distribution of information in the larger environment adopt some typical peer-to-peer patterns and techniques. We introduce the architecture, discuss some of its potential knowledge management applications, and present a few experimental results obtained with simulation.
The Web is becoming a universal information dissemination  medium, due to a number of factors  including its support for content dynamicity. A  growing number of Web information providers  post near real-time updates in domains such as  auctions, stock markets, bulletin boards, news,  weather, roadway conditions, sports scores, etc.
This paper describes the Remote Agent flight experiment for spacecraft commanding and control. In the Remote Agent approach, the operational rules and constraints are encoded in the flight software. The software may be considered to be an autonomous &quot;remote agent&quot; of the spacecraft operators in the sense that the operators rely on the agent to achieve particular goals. The experiment will
r &lt;amy.hall@eng.sun.com&gt;.  3  Editor&apos;s Notes  About the series--- The Perspectives series is a collection of essays written by individuals  from Sun Microsystems Laboratories. These essays express ideas and opinions held by the  authors on subjects of general rather than technical interest. Sun Microsystems Laboratories publishes  these essays as a courtesy to the authors to share their views with interested friends and colleagues.  The opinions and views expressed herein are solely those of the authors, and do not in  any way represent those of Sun Microsystems Laboratories, nor Sun Microsystems, Inc.  ~~~~~~~~  About the author--- Dr. Ivan E. Sutherland recently won the prestigious Price Waterhouse  Information Technology Leadership Award for Lifetime Achievement, as well as an honored  place in the Smithsonian&apos;s Permanent Collection of Information Technology (IT) Innovation.  He is widely known for his pioneering contributions 
This paper describes the vision system that was developed for the RoboCup F180 team FU-Fighters.
Current developments in processing data  streams are based on the best-effort principle  and therefore not adequate for many application  areas. When sensor data is gathered  by interface hardware and is used for triggering  data-dependent actions, the data has to  be queried and processed not only in an efficient  but also in a deterministic way. Our  streaming system prototype embodies novel  data processing techniques. It is based on an  operator component model and runs on top of  a real-time capable environment. This enables  us to provide real Quality-of-Service for data  stream queries.
Supervised learning in neural networks based on the popular backpropagation method can be often trapped in a local minimum of the error function. The class of backpropagation-type training algorithms includes local minimization methods that have no mechanism that allows them to escape the influence of a local minimum. The existence of local minima is due to the fact that the error function is the superposition of nonlinear activation functions that mayhave minima at different points, which sometimes results in a nonconvex error function. This work investigates the use of global search methods for batch-mode training of feedforward multilayer perceptrons. Global search methods are expected to lead to &quot;optimal&quot; or &quot;near-optimal&quot; weight configurations byallowing the neural network to escape local minima during training and, in that sense, they improve the efficiency of the learning process. The paper reviews the fundamentals of simulated annealing, genetic and evolutionary algorithms as well as some recently proposed deflection procedures. Simulations and comparisons are presented.
We give an explicit and easy-to-verify characterization for subsets  in finite total orders (infinitely many of them in general) to be uniformly  definable by a first-order formula.
A faceted ontology consists of a set of facets, where each facet  consists of a predefined set of terms structured by a subsumption relation.
In multimedia processing, it is well-known that the sumof -absolute-differences (SAD) operation is the most time-consuming operation when implemented in software running on programmable processor cores. This is mainly due to the sequential characteristic of such an implementation. In this paper, we investigate several hardware implementations of the SAD operation and map the most promising one in FPGA. Our investigation shows that an adder tree based approach yields the best results in terms of speed and area requirements and has been implemented as such by writing high-level VHDL code. The design was functionally verified by utilizing the MAX+plus II 10.1 Baseline software package from Altera Corp. and then synthesized by utilizing the LeonardoSpectrum software package from Exemplar Logic Inc. Preliminary results show that the design can be clocked at 380 Mhz. This result translates into a faster than real-time full search in motion estimation for the main profile/main level of the MPEG-2 standard.
The problem of Prediction in Ungauged Basins (PUB) is intimately linked with the concept of regionalisation; namely the transfer of information from one catchment that is gauged to another that is not. But such regionalisation exercises can be dangerous and should be attempted only with great care. The present paper addresses what the authors believe to be one essential aspect of regionalisation: namely, the importance of considering only `top-down&apos; models that are parametrically efficient (parsimonious) and fully `identifiable&apos; from the available catchment data. We argue further that many mechanistic model parameters are more naturally defined in the context of continuous-time, differential equation models (normally derived by the application of natural `laws&apos;, such as mass and energy conservation). As a result, there are advantages if such models are identified and estimated directly in this continuous-time, differential equation form, rather than being formulated and estimated as discrete-time models. The arguments presented in the paper are illustrated by an example in which the top-down, Data-Based Mechanistic (DBM) approach to modelling is applied to a set of precipitation-flow data. This involves the application of an advanced method of continuous-time transfer function identification and estimation; and the interpretation of this estimated model in physically meaningful terms, as required by the DBM modelling approach.
This paper presents a low communication-overhead parallel algorithm for pattern matching in biological sequences. Given such a sequence of length n and a pattern of length m, we conclude an algorithm with five computation/communication phases, each requiring  O(n) computation time and only O(p) message units. The low communication overhead of the algorithm is essential to achieving reasonable speedups on clusters, where the interprocessor communication latency is usually higher. Previous parallel implementations use straightforward domain decomposition based on existing sequential algorithms and rely on parallel machines with low-latency interconnection network and fast hardware support for processor synchronization.
We present a framework for verifiable concurrent programming in Java based on a design pattern for concurrency controllers. Using this pattern, a programmer can write concurrency controller classes defining a synchronization policy by specifying a set of guarded commands and without using any of the error-prone synchronization primitives of Java. We present a modular verification approach that exploits the modularity of the proposed pattern, i.e., decoupling of the controller behavior from the threads that use the controller. To verify the controller behavior (behavior verification) we use symbolic and infinite state model checking techniques, which enable verification of controllers with parameterized constants, unbounded variables and arbitrary number of user threads. To verify that the threads use a controller in the specified manner (interface verification) we use explicit state model checking techniques, which allow verification of arbitrary thread implementations without any restrictions. We show that the correctness of the user threads can be verified using the concurrency controller interfaces as stubs, which improves the efficiency of the interface verification significantly. We also show that the concurrency controllers can be automatically optimized using the specific notification pattern. We demonstrate the effectiveness of our approach on a Concurrent Editor implementation which consists of 2800 lines of Java code with remote procedure calls and complex synchronization constraints.
For parallel programs to become portable, they must be executable with uniform efficiency  on a varietyofhardware platforms, which is not the case at present. In 1990,  Valiant proposed Bulk-Synchronous Parallelism (BSP) as a model on which portable  parallel programs can be built [Val90a]. We argue that shared-memory BSP is efficiently  implementable on a wide variety of parallel hardware, and that BSP forms a useful basis  for providing an even higher level programming interface based on Sequential Consistency  (SC). A list of OS memory and thread management features needed to support  BSP and SC parallel programs are given, under the assumption that the parallel computer  is space-shared among multiple parallel task, rather than time-shared. Known  techniques to realize efficiently the most important of these features are sketched.
. This paper describes the development of a natural interface to a virtual  environment. The interface is through a natural pointing gesture and replaces  pointing devices which are normally used to interact with virtual environments.  The pointing gesture is estimated in 3D using kinematic knowledge of the arm  during pointing and monocular computer vision. The latter is used to extract the  2D position of the user&apos;s hand and map it into 3D. Off-line tests of the system  show promising results with an average errors of 76mm when pointing at a screen  2m away. The implementation of a real time system is currently in progress and  is expected to run with 25Hz.  1 
A transistor network consisting of linear positive resistors, q exponential diodes, and p Ebers-Moll model bipolar transistors has at most (d+1)  d  2  d(d\Gamma1)=2 dc operating points, where d = q+2p. Bounds are also obtained for the number of operating points in circuits using other models for bipolar transistors.  1. Introduction  Circuits with nonlinear elements may have multiple discrete dc operating points. In contrast, circuits consisting of positive linear resistors possess either one dc operating point, or, in special cases, a continuous family of dc operating points. We consider the problem of estimating upper bounds for the number of operating points of circuits consisting of linear positive resistors, exponential diodes, and Ebers-Moll modeled [2] bipolar transistors. (Inductors and capacitors do not play a role in establishing a circuit&apos;s dc operating point and can be removed from the circuits by being short-circuited and open-circuited, respectively.) Lee and Willson [8]...
We show how a theory of specification refinement and program development can be constructed as a conservative extension of our existing logic for Z. The resulting system can be set up as a development method for Z, or as a generalisation of a refinement calculus (with a novel semantics). In addition to the technical development we illustrate how the theory can be used in practice. 1. 
While computer vision systems can clearly assist in surveillance tasks, taking the human out of the loop entirely proves to be diffcult or undesirable in many applications. Human operators are needed to detect events missed by automatic methods, to identify false alarms, and to interpret and react appropriately to complex situations. Akey challenge in partially-automated systems is how best to combine machine algorithms for event detection, analysis and tracking with telepresence systems controlled by one or more human operators. Given the disparity in performance between the human visual system and typical robotic cameras, we argue that direct coupling of human and machine is inappropriate. We propose instead to couple human and machine components indirectly, through a database called the Panoramic Image Server. We show how this loose coupling allows machine and operator surveillance priorities to be resolved while providing a fast and natural telepresence environment for one or more h...
We have developed task scheduler logic (TSL) to automate reasoning about scheduling and concurrency in systems software. TSL can detect race conditions and other errors as well as supporting lock inference: the derivation of an appropriate lock implementation for each critical section in a system. Lock inference solves a number of problems in creating flexible, reliable, and efficient systems software. TSL is based on a notion of asymmetrical preemption relations and it exploits the hierarchical inheritance of scheduling properties that is common in systems software.
The erosion of biocomplexity in the Tisza River Basin developed slowly and incrementally over the past 130 years since implementation of the original Vsrhelyi river engineering plan. The Hungarian public view, blinded by flood and toxic spill catastrophes, missed the slow and subtle changes to natural, social and human capital precipitated by the reshaping of the TRB landscape and its agriculture for flood defence and grain production. While conversion of the TRB from a fruit/nut/ fishery polyculture to a wheat monoculture produced a great deal of financial capital for an aristocratic minority, the gradual drain of alternatives forms of capital left the region less and less resilient in the face of ecological (floods), economic (globalization) and political (war) shocks. Domination by central authorities over the past 50 years reduced local civic capacity to levels of passivity that make most communities incapable of innovating to find sustainability solutions, and this trend is reinforced by ongoing paternalistic attitudes in the Hungarian national government. Poverty, passivity, apathy and the severe consequences of failure in the event of flooding have severely reduced Adaptive Capacity, the potential to innovate and adapt to uncertainty. Both Nature and Society have evolved considerably since 1870, so simple reverse engineering futilely aims to resurrect a system that no longer exists. Since the knowledge to un-straighten and reflood a river basin is in its infancy, we must learn as we go along, humble in the knowledge that management interventions often only increase uncertainty and can push the system further into a degraded state. This paper describes an initiative to use conceptual and formal modelling within an Adaptive Management framework to facilitate a re...
The problem addressed in this work is to develop a comprehensive mathematical programming model for the efficient scheduling of oil-refinery operations. Our approach is first to decompose the overall problem spatially into three domains: the crude-oil unloading and blending, the production unit operations and the product blending and delivery. In particular, the first problem involves the crude-oil unloading from vessels, its transfer to storage tanks and the charging schedule for each crude-oil mixture to the distillation units. The second problem consists of the production unit scheduling which includes both fractionation and reaction processes and the third problem describes the finished product blending and shipping end of the refinery. Each of those sub-problems is modeled and solved in a most efficient way using continuous time representation to take advantage of the relatively smaller number of variables and constraints compared to discrete time formulation. The proposed methodology is applied to realistic case studies and significant computational savings can be achieved compared with existing approaches.
Debugging concurrent programs is difficult because concurrent programs contain both sequential errors and additional concurrent errors. It is essential to have a symbolic debugger that truly understands concurrency to improve concurrent debugging capabilities and reduce debugging time. KDB was designed to be such a concurrent debugger, however it was far from complete. This thesis presents extensions to KDB&apos;s functionality, usability, and portability. Restricted conditional breakpoints, attachment of KDB to a running application, behavioural groups, programmatic interface, and improved user interface have been added to KDB to extend its functionality. KDB has been modified to understand  ¯C++ programs better so that inserted code is hidden from users improving KDB&apos;s usability. Finally, KDB has been ported to the i486 architecture on the Linux OS, increasing portability. KDB is written in ¯C++ and is a concurrent application, so it was possible to test the extensions while debugging the...
Existing security mechanisms for serving documents on the World Wide Web typically require use of either an underlying security transport mechanisms ffe.g., SSLff or alternate servers, browsers and data streams ffe.g., SHTTPff. In this paper we introduce a simpler method using plugins which provides moderate security for serving private documents within the standard HTTP mechanism and socket layer. This new method operates by providing a security plugin within a standard web-browser environment. It provides a somewhat lower level of functionality and security than the alternatives mentioned above but requires much less overhead, especially on the server end, and appears to be very appropriate for serving low-security, nonpublic documents, ffles and images over the world wide web. The method can be easily adapted to provide other advantages, such as automatic ffwater-marking&quot; of decoded material with the name of the decoder, and the deployment of content-speciffc compression algorithms. 1 Intr...
Lately, Geographic Information Systems are increasingly exploited in archaeological studies or, generally, in projects aimed to the  preservation of cultural heritage. Among the large number of reasons that make GIS a suitable tool to manage this kind of programs,   at once its possible to identify two main topics for this achievement.   The first one concerns the high capacities of GIS to enable advanced storage, representation and management of spatial data,  connecting them to collections of different nature data (archaeological, architectural, historical, etc.); these archives can be suitably  set and managed in several semantic levels with a spatial reference, overlaying topogaphic maps. The second order of reasons is   connected with present development of geographic mapping: the improvements in acquisition, processing, management and digital   representation metodologies request proper tools, in order to make new mapping products usable and widespread.   The recent growth of satellite images analyses and aerial photograms interpretation have in fact substantial role in cultural heritage   documentation, supporting objectives of modern Archaeology in spotting areas of probable location of archaeological sites or finds.   The designed GIS of Hierapolis, whose carrying out is going on, can based itself on a huge amount of metric data relating  monuments and diggings, collected during the last years thanks to topographic and photogrammetric surveys. Besides close range  surveys aimed to document architectural structures and ruins, always originating vectorial representation, a complete digital site map  at the urban scale 1:1000 has been accomplished.   The purpose to offer the chance of global data managing of city environment and architectural structures (scientist...
We present an approach to distributing messages among highly mobile hosts in ad hoc networks. We focus on using direct radio communication between moving vehicles on the road that requires no additional infrastructure. Thus, the vehicles need to organize access to the radio channel in a decentralized manner. We derive the medium access control from the standard IEEE 802.11. Also, the vehicles use omnidirectional antennas implying that a sender can transmit to multiple hosts simultaneously. As an example, we study a road accident that is reported to nearby vehicles. Simulations show us the quality of the proposed protocol by measuring how many vehicles inside a zoneof -relevance are informed under various conditions. 1
This paper    describes the use of aspect-oriented programming to improve performance of fault-tolerant (FT) servers built with middleware support. Its contribution is to shift method call logging from middleware to application level in primary-backup replication. The novelty consists in no burden being placed on application writers, except for a simple component description aiding automatic generation of aspect code. The approach is illustrated by describing how synchronization aspects are weaved in an application, and modifications of an FTCORBA platform to avoid middleware level logging. Evaluation is performed using a telecom application enriched with aspects, running on top of the aspectsupporting platform. We compare overheads with earlier results from runs on the base-line platform. Experiments show a drop of around 40% of original overheads. This is due to methods starting execution before previous ones end, in contrast to ordering enforced at middleware level where methods are executed sequentially, not adapting to application knowledge.
A recently developed fractal transform-based digital image information hiding technique has been further enhanced to resist the well-known StirMark attack. It is semi-blind in that it does not need original images in retrieving embedded information. It has been shown theoretically that the technique is robust against certain geometrical attacks, specifically, translation, scaling and small angle (less than 5 degree) rotation. More detailed test results by StirMark attack are reported in this paper. These results have demonstrated that the technique can resist most of the default StirMark attack functions, including the StirMark randomization and bending, which is a combination of several geometrical attacks followed by other manipulations, and has defeated most of the existing information hiding techniques.
The ICM-DISCO (Docking and Interface Side-Chain Optimization) protein--proteindocking method is a direct stochastic global energy optimization from multiple starting positions of the ligand. The first step is performed by docking of a rigid all-atom ligand molecule to a set of soft receptor potentials precalculated on a 0.5 grid from realistic solvent-corrected force-field energies. This step finds the correct solution as the lowest energy conformation in almost 100% of the cases in which interfaces do not change on binding. The second step is needed to deal with the induced changes and includes the global optimization of the interface side-chains of up to 400 best solutions. The CAPRI predictions were performed fully automatically with this method. Available experimental information was included as a filtering step to favor expected docking surfaces. In three of the seven proposed targets, the ICM-DISCO method found a good solution  (&gt;50% of correct contacts) within the five submitted models. The procedure is global and fully automated. We demonstrate that the algorithm handles the induced changes of surface side-chains but is less successful if the backbone undergoes largescale rearrangements. Proteins 2003;52:113--117.
Most current peer-to-peer lookup schemes keep a small amount of routing state per node, typically logarithmic in the number of overlay nodes. This design assumes that routing information at each member node must be kept small, so that the bookkeeping required to respond to system membership changes is also small, given that aggressive membership dynamics are expected. As a consequence, lookups have high latency as each lookup requires contacting several nodes in sequence. In this paper, we question these...
This paper describes our ongoing research  project on text simplification for congenitally  deaf people. Text simplification we are aiming  at is the task of offering a deaf reader a syntactic  and lexical paraphrase of a given text for  assisting her/him to understand what it means.
Amitabha Bagchi, Amitabh Chaudhary, and Christian Scheideler  Dept. of Computer Science  Johns Hopkins University  3400 N. Charles Street  Baltimore, MD 21218, USA  {bagchi,amic,scheideler}@cs.jhu.edu  Petr Kolman  Inst. for Theoretical Computer Science  Charles University  Malostransk e n am. 25  118 00 Prague, Czech Republic  kolman@kam.mff.cuni.cz  ABSTRACT  In this paper we consider the k edge-disjoint paths problem (k-EDP), a generalization of the well-known edge-disjoint paths problem. Given a graph G = (V, E) and a set of terminal pairs (or requests) T , the problem is to find a maximum subset of the pairs in T for which it is possible to select paths such that each pair is connected by k edgedisjoint paths and the paths for different pairs are mutually disjoint. To the best of our knowledge, no nontrivial result is known for this problem for k &gt; 1. To measure the performance of our algorithms we will use the recently introduced flow number F of a graph. This parameter is known to fulfill  F = O(ffff -1 log n), where ff is the maximum degree and ff is the edge expansion of G. We show that a simple, greedy online algorithm achieves a competitive ratio of   F ), which naturally extends the best known bound of O(F ) for k = 1 to higher k. To achieve this competitive ratio, we introduce a new method of converting a system of k disjoint paths into a system of k length-bounded disjoint paths. We also show that any deterministic online algorithm has a competitive ratio of ffff k    F ).
Graph matching is an important component in many object recognition algorithms. Although most graph matching algorithms seek a one-to-one correspondence between nodes, it is often the case that a more meaningful correspondence exists between a cluster of nodes in one graph and a cluster of nodes in the other. We present a matching algorithm that establishes many-to-many correspondences between nodes of noisy, vertex-labeled weighted graphs. The algorithm is based on recent developments in efficient lowdistortion metric embedding of graphs into normed vector spaces. By embedding weighted graphs into normed vector spaces, we reduce the problem of many-to-many graph matching to that of computing a distribution-based distance measure between graph embeddings. We use a specific measure, the Earth Mover&apos;s Distance, to compute distances between sets of weighted vectors. Empirical evaluation of the algorithm on an extensive set of recognition trials demonstrates both the robustness and efficiency of the overall approach. 1.
Non-binary constraint satisfactionproblfa (CSPs) can be sol ed in two different ways. We can eithertranslzfi theproblH into an equivali t binary one and sol e it using welE7VofffiR6V7ol binary CSP techniques or use extended versions of binary techniques directl on the non-binaryproblin Recentl , it has been shown that the hidden variabl encoding is a promising method oftransloffz6 non-binary CSPs into binary ones. In this paper we make atheoretical andempirical investigation of arc consistency and searchalxRfioffzH for the hidden variabl encoding. WeanalkE the potential benefits ofapplxzz arc consistency on the hidden encoding compared togeneralffzfi arc consistency on the non-binary representation. Weal6 show that searchalkHRoffzz for nonbinary constraints can be emulk6x by corresponding binaryalryofixHE that operate on the hidden variabl encoding and onl instantiate original variabloff Empirical resulc on variousimplsoz tations of suchal gorithms reveal that the hidden variabl is competitive and in many cases better than the non-binary representation for certain cltain of non-binary constraints. 1 
The possibility of aliasing between objects constitutes one of  the primary challenges in understanding and reasoning about correctness  of object-oriented programs. Ownership types provide a principled way of  specifying statically enforcable restrictions on object aliasing. Ownership  types have been used to aid program understanding and evolution, verify  absence of data races and deadlocks in multithreaded programs, and  verify absence of memory errors in programs with explicit deallocation. This paper
This paper gives the main definitions relating to dependability, a generic concept including as special case such attributes as reliability, availability, safety, confidentiality, integrity, maintainability, etc. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability (faults, errors, failures), and the attributes of dependability. The discussion on the attributes encompasses the relationship of dependability with security, survivability and trustworthiness.
This paper investigates the application of model predictive control to the yaw  angle of an autonomous underwater vehicle. A simple line of sight guidance scheme is  utilised to generate the reference heading, which is to be followed. Simulation results are  presented to demonstrate the suitability of the proposed algorithm. Copyright 2002  IFAC  Keywords: Model predictive control, autonomous underwater vehicle, guidance, genetic  algorithm, navigation.
We present a regularized method for solving an inverse problem in Diffusion Tensor Magnetic Resonance Imaging (DT-- MRI) data. In the case of brain images, DT--MR obtains a tensor field that indicates the local orientation of nerve bundles. Now days, the spatial resolution of this technique is limited by the partial volume effect; i.e. the measured tensors at voxels that contain fiber crossings or bifurcations results from the addition of several tensors, each one oriented with its corresponding nerve bundle. In this paper, we proposed a method for recovering the original intra--voxel information. We assume that the observed tensors are a linear combination of a given tensor base, therefore, the aim of our approach is the computation of the unknown coefficients of this linear combination. By regularizing the problem, we introduce the prior information about the piecewise smoothness of nerve bundles orientation. Such regularization process performs an anisotropic filtering of the coefficients. As a result, we recover a multi--tensor field. Moreover, we propose to estimate the nerve bundles trajectory by performing stochastic walks of particles in the computed multi--tensor field. In order to demonstrate the performance of the method, we ran several experiments using both synthetic and real data.
In this paper new, phone-duration-based features for confidence measures (CMs) using a classifier are proposed. In misrecognized utterances, the segmentation and thus the phoneme durations often deviate severely from what can be observed in the training data. Also the found segmentation for one recognized phoneme often covers several &apos;real&apos; phonemes, that have different spectral properties. So such phoneme durations often indicate that a misrecognition took place and we derived some new features based on these durations. In addition to these new features we used some related to the acoustic score of the N-best hypotheses. Using the full set of 46 features we achieve a correct classification rate of 90% at a false rejection rate of 5.1% on an isolated word, command&amp;control task using a rather simple neural network (NN) classifier. Simultaneously, we try to detect out of vocabulary (OOV) words with the same approach and succeed in 91% of the cases. We then combine this CM with unsupervised MAP and MLLR speaker adaptation. The adaptation is guided by the CM and the acoustic models are only modified if the utterance was recognized with high confidence.
The speci  cation language JML (Java Modelling Language)  includes so-called assignable clauses, also known as modifies clauses,  for specifying which  elds may change their value as side-eect of a  method. This paper uses abstract interpretation over a trace semantics  for a simple object-oriented language to de  ne a correct static analysis  for checking the correctness of assignable clauses.
This thesis seeks to address the incorporation of a low-level cognitive ability into reactive, behavior-based artificial intelligence architectures. Specifically, it addresses the need to generate short-term, observation-based expectations about the world and react appropriately to the violation of those expectations. In it I discuss the motivation for incorporating expectations into a reactive behavior-based architecture, outline the qualitative properties of expectations and the conditions under which they may be violated, propose a model for generating expectations and responding to their violation, detail one implementation of such a model, and finally propose this work as a starting point from which future work on higher-order cognition and behavior might begin.
In contrast to classical assumptions in Video on Demand (VoD) research, the main requirements for VoD in the Internet are adaptiveness, support of heterogeneity, and last not least high scalability. Hierarchically layered video encoding is particularly well suited to deal with adaptiveness and heterogeneity support for video streaming. A distributed caching architecture is key to a scalable VoD solution in the Internet. Thus, the combination of caching and layered video streaming is promising for an Internet VoD system, yet, requires thoughts about some new issues and challenges. In this paper, we investigate one particular of these issues: how to deal with retransmissions of missing segments for a cached layered video in order to meet users&apos; demands to watch high quality video with relatively little quality variations. We devise a suite of fairly simple retransmission scheduling algorithms and compare these against existing ones by simulative experiments.
This work is part of a proactive information retrieval project that aims at estimating relevance from implicit user feedback. The noisy feedback signal needs to be complemented with all available information, and textual content is one of the natural sources. Here we take the first steps by investigating whether this source is at all useful in the challenging setting of estimating the relevance of a new document based on only few samples with known relevance. It turns out that even sophisticated unsupervised methods like multinomial PCA (or Latent Dirichlet Allocation) cannot help much. By contrast, feature extraction supervised by relevant auxiliary data may help. I. 
. By adding just one edge at a time, we can for a given permutation  of edges determine exactly which edge causes a graph to become  uncolorable. In addition, we track the development of frozen pairs, that  is pairs which must be the same color under all three colorings of the  graph. We call this measurement process the frozen development process.  This set of pairs shows an explosive jump just at the threshold. By  identifying pairs of vertices that are frozen, we create a collapsed graph,  which has the same set of 3-colorings. Corresponding to the jump in  frozen pairs, there is an exceedingly rapid collapse of this graph near the  phase transition. It is this sudden freezing that causes the threshold. The  evidence supports the contention that there is a sudden (discontinuous  in the limit) change in the frozenness with respect to 3-coloring.  However, we also argue that if the measure were to be taken with respect  to minimization of violated edges, then this sudden drop would mo...
Synchronized Multimedia Integration Language (SMIL) is a W3C recommendation for encoding multimedia presentations. It provides presentational control over not just the spatial layout of the document, but also the relationship between elements over time. At the present time there does not appear to be a high quality Open Source implementation of the SMIL 2.0 specification available. This paper describes one attempt at an implementation. Some ideas about where future software development could take this implementation to fulfill the requirements of other projects are also mentioned.
There is a need for formal methods to verify correctness of software and hardware systems. Automated verification techniques basically explore the state space of a system in order to establish whether or not it behaves correctly. The main drawback of such methods is the state explosion problem. The size of the state space can grow exponentially in the number of components of the system, especially in asynchronous concurrent systems. In early stages of system development, errors are likely to appear. As a matter of fact, in practice, automated verification has been shown to be more successful in finding errors in systems than in proving correctness. Usually, one applies reachability algorithms like depth-first, and breadth-first search for this purpose. Breadth-first search is, in general, not memory-efficient, but offers shortest counterexamples. On the other hand, depth-first search is more memory-efficient, but delivers suboptimal counterexamples. We propose and analyze the use of...
Soccer is a rich domain for the study of multiagent learning issues. Not only must the  players learn low-level skills, but they must also learn to work together and to adapt to the  behaviors of different opponents. We are using a robotic soccer system to study these different  types of multiagent learning: low-level skills, collaborative, and adversarial. Here we describe  in detail our experimental framework. We present a learned, robust, low-level behavior that  is necessitated by the multiagent nature of the domain, namely shooting a moving ball. We  then discuss the issues that arise as we extend the learning scenario to require collaborative  and adversarial learning.